[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"online book captures information blog, educational information MOOCs Improving Statistical Inferences Improving Statistical Questions, scientific work one place. goal make information easier find, accessible users, present date information based progressive insights, recent scientific developments, new statistical software.used right re-use adapt open access articles, without adding quotation marks citing . work shared Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. Thanks collaborators Casper Albers, Farid Anvari, Aaron Caldwell, Harlan Cambell, Nicholas Coles, Lisa DeBruine, Marie Delacre, Zoltan Dienes, Noah van Dongen, Alexander Etz, Ellen Evers, Jaroslav Gottfriend, Seth Green, Christopher Harms, Arianne Herrera-Bennett, Joe Hilgard, Peder Isager, Maximilian Maier, Neil McLatchie, Brian Nosek, Pepijn Obels, Amy Orben, Anne Scheel, Janneke Staaks, Leo Tiokhin, Mehmet Tunç, Duygu Uygun Tunç, contributed work done past part formed basis book.find mistakes, suggestions improvement, let know D.Lakens@tue.nl. hope material use .Dr. Daniël Lakens","code":""},{"path":"pvalue.html","id":"pvalue","chapter":"1 Using p-values to test a hypothesis","heading":"1 Using p-values to test a hypothesis","text":"One question interests scientists whether differences exist measurements collected different conditions. answer question ordinal claim. example, researcher might hypothesize time students spend learning new information dedicated retrieval information tests (condition ), compared spending time studying (condition B). collecting data, observing mean grade higher students student spend part time tests, researcher can make ordinal claim student performence better condition condition B. Ordinal claims quantify size effect. Ordinal claims can used state difference conditions. non-directional (two-sided) test question whether difference either direction, directional (one-sided) test question whether effect specific direction, cases alternative difference.make ordinal claims, researchers typically rely methodological procedure known hypothesis test. One part hypothesis test consists computing p-value examining whether statistically significant difference. 'Significant' means something worthy attention. hypothesis test used distinguish signal (worth paying attention ) random noise empirical data. worth distinguishing statistical significance, used claim whether observed effect signal noise, practical significance, depends whether size effect large enough worthwhile consequences real life. use methodological procedure decide whether ordinal claim can made functions safeguard confirmation bias. Depending desires, scientists might tempted interpret data support hypothesis, even . hypothesis test, used correctly, controls amount time researchers fool make ordinal claims.","code":""},{"path":"pvalue.html","id":"philosophical-approaches-to-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.1 Philosophical approaches to p-values","text":"look p-values computed, important examine supposed help us make ordinal claims testing hypotheses. definition p-value probability observing sample data, extreme data, assuming null hypothesis true. definition tell us much interpret p-value.interpretation p-value depends statistical philosophy one subscribes . Ronald Fisher published 'Statistical Methods Research Workers' 1925 popularized p-values. Fisherian framework p-value interpreted descriptive continuous measure compatibility observed data null-hypothesis (Greenland et al., 2016). compatibility observed data null model falls 1 (perfectly compatible) 0 (extremely incompatible), every individual can interpret p-value “statistical thoughtfulness\". According Fisher (1956), p-values \"generally lead probability statement real world, rational well-defined measure reluctance accept hypotheses test\". Fisher tried formalize philosophy approach called 'fiducial inference', recevied widespread adoption approaches, decision theory, likelihoods, Bayesian inference. Fisherian p-value thus describes incompatibility data single hypothesis, known significance testsing. main reason significance test limited, researchers specify null-hypothesis, specify alternative hypothesis.Neyman Pearson built insights p-values William Gosset (inventor Student's t-test) Ronald Fisher, developed approached called statistical hypothesis testing. main difference significance testing approach developed Fisher statistical hypothesis test null-hypothesis alternative hypothesis specified. Neyman-Pearson framework goal statistical tests guide behavior researchers respect two hypotheses. Based results statistical test, without ever knowing whether hypothesis true , researchers choose tentatively act null hypothesis alternative hypothesis true. psychology, researchers often use imperfect hybrid Fisherian Neyman-Pearson frameworks, Neyman-Pearson approach , according Dienes Dienes (2008) “logic underlying statistics see professional journals psychology”.Neyman-Pearson hypothesis test performed observed p-value used check smaller chosen alpha level, matter much smaller . example, alpha level 0.01 used, p = 0.006 p = 0.000001 lead researchers decide act state world best described alternative hypothesis. differs Fisherian approach p-values, lower p-value, greater psychological reluctance researcher accept null-hypothesis testing. Neyman-Pearson hypothesis test see goal inference quantifying continuous measure compatibility evidence. Instead, Neyman (1957) writes:content concept inductive behavior recognition purpose every piece serious research provide grounds selection one several contemplated courses action.Intuitively, one might feel like decisions act based results single statistical test, point often raised criticism Neyman-Pearson approach statistical inferences. However, criticisms rarely use definition ‘act’ Neyman used. true , example, decision implement new government policy based single study result. However, Neyman considered making scientific claim ‘act’ well, wrote (1957, p. 10) concluding phase study involves:act decision take particular action, perhaps assume particular attitude towards various sets hypothesesCox (1958) writes:might argued making inference 'deciding' make statement certain type populations therefore, provided word decision interpreted narrowly, study statistical decisions embraces inference. point one main general problems statistical inference consists deciding types statement can usefully made exactly mean.Thus, Neyman-Pearson approach, p-values form basis decisions claims make. science, claims underly novel experiments form auxiliary hypotheses, assumptions underlying hypotheses assumed accurate order test work planned. example, important participants can see color planned experiment, assume true Ishihara test successfully identifies participants colorblind.","code":""},{"path":"pvalue.html","id":"creating-a-null-model","chapter":"1 Using p-values to test a hypothesis","heading":"1.2 Creating a null model","text":"\nFigure 1.1: Scientists tendency worship p-values value 0.05.\nAssume ask two groups 10 people much liked extended directors cut Lord Rings (LOTR) trilogy. means total sample size (N) 20, sample size group (n) 10. first group consists friends, second groups consists friends wife. friends rate trilogy score 1 10. can calculate average rating friends, 8.7, average rating wife’s friends, 7.7. can compare scores groups looking raw data, plotting data.\nTable 1.1: Ratings Lord Rings extended trilogy two groups friends.\ncan see groups overlap mean ratings differ 1 whole point. question faced following: difference two groups just random variation, can claim friends like extended directors cut Lord Rings (LOTR) trilogy wife’s friends?null-hypothesis significance test try answer question calculating probability observed difference (case, mean difference 1) extreme difference, assumption real difference much friends wife’s friends like extended directors cut LOTR, just looking random noise. probability called p-value. probability low enough, decide claim difference. probability low enough, refrain making claim difference.null-hypothesis assumes ask infinite number friends infinite number wife’s friends much like LOTR, difference huge groups exactly 0. However, sample drawn draw population, random variation likely lead difference somewhat larger smaller 0. can create null model quantifies expected variation observed data, just due random noise, tell us constitutes reasonable expectation much differences groups can vary difference population.practical create null model terms standardized distribution, makes easier calculate probability specific values occur, regardless scale used collect measurements. One version null model differences t-distribution, can used describe differences expected drawing samples population. null model built assumptions. case t-distribution, assumption scores normally distributed. reality, assumptions upon statistical methods built never met perfectly, statisticians examine impact violations assumptions methodological procedures. Statistical tests still useful practice impact violations statistical inferences small enough.can quantify distribution t-values expected difference population probability density function. plot probability density function t-distribution 18 degrees freedom (df), corresponds example collect data 20 friends (df = N - 2 two independent groups). continuous distribution, probabilities defined infinite number points, probability observing single point (e.g., t = 2.5) always zero. Probabilities measured intervals. reason, p-value computed, defined 'probability observing data', 'probability observed data, extreme data'. creates interval (tail distribution) probability can calculated.","code":""},{"path":"pvalue.html","id":"calculating-a-p-value","chapter":"1 Using p-values to test a hypothesis","heading":"1.3 Calculating a p-value","text":"t-value can computed mean sample, mean population, standard deviation sample, sample size. computing probability observing t-value extreme extreme one observed, get p-value. comparison movie ratings two groups friends , performing two-sided Student's t-test yields t-value 2.5175 p-value 0.02151.can graph t-distribution (df = 18) highlight two tail areas start t-values 2.5175 -2.5175.\nFigure 1.2: t-distribution 18 degrees freedom.\n","code":"\nt.test(df_long$rating ~ df_long$`Friend Group`, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  df_long$rating by df_long$`Friend Group`\n## t = 2.5175, df = 18, p-value = 0.02151\n## alternative hypothesis: true difference in means between group Friends Daniel and group Friends Kyra is not equal to 0\n## 95 percent confidence interval:\n##  0.1654875 1.8345125\n## sample estimates:\n## mean in group Friends Daniel   mean in group Friends Kyra \n##                          8.7                          7.7"},{"path":"pvalue.html","id":"which-p-values-can-you-expect","chapter":"1 Using p-values to test a hypothesis","heading":"1.4 Which p-values can you expect?","text":"educational video 'Dance p-values', Geoff Cumming explains p-values vary experiment experiment. However, reason 'trust p' mentions video. Instead, important clearly understand p-value distributions prevent misconceptions. p-values part frequentist statistics, need examine can expect long run. never experiment hundreds times, limited number studies lifetime, best way learn expect long run computer simulations.Take moment try answer following two questions. p-values can expect observe true effect, repeat study one-hundred thousand times? p-values can expect true effect, repeat study one-hundred thousand times? know answer, worry - learn now. know answer, worth reflecting know answer essential aspect p-values. like , simply never taught . see, essential solid understanding interpret p-values.p-values can expect completely determined statistical power study, probability observe significant effect, true effect. statistical power ranges 0 1. can illustrate simulating one-sample t-tests. idea simulate IQ scores group people. know standard deviation IQ scores 15. now, set mean IQ score simulated group 106, compare average IQ score people (known 100 – ’s IQ tests normalized). testing people simulated sample IQ differs average (know correct answer ‘yes’, made simulation).simulation, generate n = 71 normally distributed IQ scores mean M (106 default) standard deviation 15. perform one-sample t-test store p-value. plot distribution \nFigure 1.3: Distribution p-values power = 50%.\nx-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power.simulation result illustrates probability density function p-values. probability density function provides probability random variable specific value (Figure 1.2 t-distribution). p-value random variable, can use probability density function plot p-value distribution (Hung et al., 1997; Ulrich & Miller, 2018), Figure 1.4. can vary sample size, effect size, alpha level online Shiny app. Increasing sample size effect size increase steepness p-value distribution, means probability observe small p-values increases. p-value distribution function statistical power test.\nFigure 1.4: Probability density function p-values two-sided t-test.\ntrue effect, p-values uniformly distributed. means every p-value equally likely observed null hypothesis true. words, true effect, p-value 0.08 just likely p-value 0.98. remember thinking counterintuitive first learned (well completing PhD), makes sense think goal guarantee H0 true, alpha % p-values fall alpha level. set alpha 0.01, 1% observed p-values fall 0.01, set alpha 0.12, 12% observed p-values fall 0.12. can happen p-values uniformly distributed null-hypothesis true.\nFigure 1.5: Distribution p-values power = 50%.\n","code":"\np <- numeric(100000) # store all simulated *p*-values\n\nfor (i in 1:100000) { # for each simulated experiment\n  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data\n  y <- rnorm(n = 71, mean = 105, sd = 15) # Simulate data\n  p[i] <- t.test(x, y)$p.value # store the *p*-value\n}\n\n(sum(p < 0.05) / 100000) # compute power\nhist(p, breaks = 20) # plot a histogram"},{"path":"pvalue.html","id":"lindleys-paradoxlindley","chapter":"1 Using p-values to test a hypothesis","heading":"1.5 Lindley's paradox(#lindley)","text":"statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. known Lindley's paradox (Lindley, 1957), sometimes Jeffreys-Lindley paradox (Spanos, 2013). distribution p-values function statistical power (Cumming, 2008), higher power, right-skewed distribution becomes (.e., likely becomes small p-values observed). true effect p-values uniformly distributed, 1% observed p-values fall 0.04 0.05. statistical power extremely high, p-values fall 0.05, p-values fall 0.01. Figure 1.6 see high power small p-values (e.g., 0.001) likely observed effect effect (e.g., dotted black curve representing 99% power falls grey horizontal line representing uniform distribution null true p-value 0.01).Yet perhaps surprisingly, observing p-value 0.04 likely null hypothesis (H0) true alternative hypothesis (H1) true high power, illustrated fact Figure 1.6 density p-value distribution higher null true, test 99% power, 0.04. Lindley's paradox shows p-value example 0.04 can statistically significant, time evidence null hypothesis. Neyman-Pearson approach made claim maximum error rate 5%, likelihood Bayesian approach, conclude data supports null. Lindley's paradox illustrates different statistical philosophies reach different conclusions, p-value can directly interpreted measure evidence, without taking power test account. Although necessary, researchers might desire prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis. can achieved lowering alpha level function sample size (Good, 1992; Leamer, 1978; Maier & Lakens, 2022), explained chapter error control.\nFigure 1.6: P-value distribution 0 (grey horizontal line, 50% power (black solid curve), 99% power (black dotted curve, p-values just 0.05 likely H0 true H1 true).\n","code":""},{"path":"pvalue.html","id":"correctly-reporting-and-interpreting-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.6 Correctly reporting and interpreting p-values","text":"Although strict Neyman-Pearson perspective sufficient report p < \\(\\alpha\\) p > \\(\\alpha\\), researchers report exact p-values. facilitates re-use results secondary analyses (Appelbaum et al., 2018), allows researchers compare p-value alpha level preferred use (Lehmann & Romano, 2005). claims made using methodological procedure known maximum error rates, p-value never allows state anything certainty. Even set alpha level 0.000001 single claim can error, Fisher (1935) reminds us, '“one chance million” undoubtedly occur, less appropriate frequency, however surprised may occur us”. uncertainty sometimes reflected academic writing, researchers can seen using words 'prove', 'show', 'known'. slightly longer accurate statement hypothesis test read:claim /effect, acknowledging scientists make claims using methodological procedure, misled, long run, alpha % beta % time, deem acceptable. foreseeable future, new data information emerges proves us wrong, assume claim correct.Remember Neyman-Pearson framework researchers make claims, necessarily believe truth claims. example, OPERA collaboration reported 2011 observed neutrinos traveled faster speed light. claim made 0.2---million Type 1 error rate, assuming error purely due random noise. However, none researchers actually believed claim true, theoretically impossible neutrinos move faster speed light. Indeed, later confirmed equipment failures cause anomalous data: fiber optic cable attached improperly, clock oscillator ticking fast. Nevertheless, claim made explicit invitation scientific community provide new data information prove claim wrong.researchers “accept” “reject” hypothesis Neyman-Pearson approach statistical inferences, communicate belief conclusion substantive hypothesis. Instead, utter Popperian basic statement based prespecified decision rule observed data reflect certain state world. Basic statements describe observation made (e.g., \"observed black swan\") event occurred (e.g., \"students performed better exam trained spaced practice, \").claim data observed, theory used make predictions. Data never 'proves' theory true false. basic statement can corroborate prediction derived theory, . many predictions deduced theory corroborated, can become increasingly convinced theory close truth. 'truth-likeness' theories called verisimilitude. shorter statement hypothesis test presented therefore read 'p = .xx, corroborates prediction, alpha level y%', 'p = .xx, corroborate prediction, statistical power y% effect size interest'. Often, alpha level statistical power mentioned experimental design section article, repeating might remind readers error rates associated claims.Even made correct claims, underlying theory can false. Popper (2002, p. 94) reminds us “empirical basis objective science thus nothing ‘absolute’ basis ”. argues science built solid bedrock, piles driven swamp notes “simply stop satisfied piles firm enough carry structure, least time .” Hacking (1965) writes: “Rejection refutation. Plenty rejections must tentative.” reject null model, tentatively, aware fact might done error, without necessarily believing null model false, without believing theory used make predictions true. Neyman (1960, p. 13) inferential behavior : “act behave future (perhaps new experiments performed) particular manner, conforming outcome experiment”. knowledge science provisional.statisticians recommend interpreting p-values measures evidence. example, Bland (2015) teaches p-values can interpreted 'rough ready' guide strength evidence, p > 0.1 indicates 'little evidence', 0.01 < p < 0.05 indicates 'evidence', p < 0.001 'strong evidence'. incorrect (Lakens, 2022), clear previous sections Lindley's paradox, uniform p-value distributions. want quantify evidence, see chapters [likelihoods][#likelihoods] [Bayesian statistics][#bayes].","code":""},{"path":"pvalue.html","id":"preventing-common-misconceptions-about-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.7 Preventing common misconceptions about p-values","text":"p-value probability observed data, extreme data, assumption null hypothesis true. understand means, might especially useful know doesn’t mean. First, need know ‘assumption null hypothesis true’ looks like, data expect null hypothesis true. Although null hypothesis can value, assignment assume null hypothesis specified mean difference 0. example, might interested calculating difference control condition experimental condition dependent variable.useful distinguish null hypothesis (prediction mean difference population exactly 0) null model (model data expect collect data null hypothesis true). null hypothesis point 0, null model distribution. visualized textbooks power analysis software using pictures can see , t-values horizontal axis, critical t-value somewhere 1.96 – 2.00 (depending sample size). done statistical test comparing two groups based t-distribution, p-value statistically significant t-value larger critical t-value.personally find things become lot clearer plot null model mean differences instead t-values. , can see null model mean differences can expect compare two groups 50 observations true difference two groups 0, standard deviation group . standard deviation 1, can also interpret mean differences Cohen’s d effect size. also distribution can expect Cohen's d 0, collecting 50 observations.first thing notice expect mean null model 0. Looking x-axis, see plotted distribution centered 0. even mean difference population 0 imply every sample draw population give mean difference exactly zero. variation around population value, function standard deviation sample size.y-axis graph represents density, provides indication relative likelihood measuring particular value continuous distribution. can see likely mean difference true population value zero, larger differences zero become increasingly less likely. graph two areas colored red. areas represent 2.5% extreme values left tail distribution, 2.5% extreme values right tail distribution. Together, make 5% extreme mean differences expect observe, given number observations, true mean difference exactly 0. mean difference red area observed, corresponding statistical test statistically significant 5% alpha level. words, 5% observed mean differences far enough away 0 considered surprising. null hypothesis true, observing ‘surprising’ mean difference red areas Type 1 error.Let’s assume null model Figure true, observe mean difference 0.5 two groups. observed difference falls red area right tail distribution. means observed mean difference relatively surprising, assumption true mean difference 0. true mean difference 0, probability density functions shows expect mean difference 0.5 often. calculate p-value observation, lower 5%. probability observing mean difference least far away 0 0.5 (either left mean, right, two-tailed test) less 5%.One reason prefer plot null model raw scores instead t-values can see null model changes sample size increases. collect 5000 instead 50 observations, see null model still centered 0 – null model now expect values fall close around 0.distribution much narrower distribution mean differences based standard error difference means. value calculated based standard deviation sample size, follows:\\(\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+\\frac{\\sigma_{2}^{2}}{n_{2}}}\\)formula shows standard deviations group (σ) squared divided sample size group, added together, square root taken. larger sample size bigger number divide , thus smaller standard error difference means. n = 50 example :\\(\\sqrt{\\frac{1^{2}}{50}+\\frac{1^{2}}{50}}\\)standard error differences means thus 0.2 n = 50 group, n = 5000 0.02. Assuming normal distribution 95% observations fall 1.96 SE. 50 samples per group, mean differences fall -1.96 * 0.2 = -0.392, +1.96 * 0.2 = 0.392, can see red areas start approximately -0.392 0.392 n = 50. 5000 samples per group, mean differences fall -1.96 * 0.02, +1.96 * 0.02; words -0.0392 0.0392 n = 5000. Due larger sample size n = 5000 observations per group, expect observe mean differences sample closer 0 compared null model 50 observations.collected n = 5000, observe mean difference 0.5, clear difference even surprising collected 50 observations. now almost ready address common misconceptions p-values, can , need introduce model data null true. sampling data model true mean difference 0, alternative model look like? software (G*power, see Figure 1.7) visualize null model (red curve) alternative model (blue curve) output:\nFigure 1.7: Screenshot Gpower software\nstudy, rarely already know true mean difference (already knew, study?). let’s assume -knowing entity. Following Paul Meehl, call -knowing entity ‘Omniscient Jones’. collect sample 50 observations, Omniscient Jones already knows true mean difference population 0.5. , expect variation around 0.5 alternative model. figure shows expected data pattern null hypothesis true (now indicated grey line) shows alternative model, assuming true mean difference 0.5 exists population (indicated black line).Omniscient Jones said true difference much larger. Let’s assume another study, now collect 50 observations, Omniscient Jones tells us true mean difference 1.5. null model change, alternative model now moves right.can play around alternative null models online app: http://shiny.ieis.tue.nl/d_p_power/. app allows specify sample size group independent t-test (2 infinity), mean difference (0 2), alpha level. plot, red areas visualize Type 1 errors. blue area visualizes Type 2 error rate (discuss ). app also tells critical value: vertical line (n = 50 line falls mean difference 0.4) verbal label says: “Effects larger 0.4 statistically significant”. Note true effects smaller -0.4, even though second label , app shows situation two-sided independent t-test.can see left vertical line indicates critical mean difference blue area part alternative model. Type 2 error rate (1 - power study). study 80% power, 80% mean differences observe fall right critical value indicated line. alternative model true, observe effect smaller critical value, observed p-value larger 0.05, even true effect. can check app larger sample size, right entire alternative distribution falls, thus higher power. can also see larger sample size, narrower distribution, less distribution fall critical value (long true population mean larger critical value). Finally, larger alpha level, left critical mean difference moves, smaller area alternative distribution falls critical value.app also plots 3 graphs illustrate power curves function different alpha levels, sample sizes, true mean differences. Play around app changing values. Get feel variable impacts null- alternative models, mean difference statistically significant, Type 1 Type 2 error rates.far, several aspects null models become clear. First , population value traditional null hypothesis value 0, sample draw, observed difference falls distribution centered 0, thus often slightly larger smaller 0. Second, width distribution depends sample size standard deviation. larger sample size study, narrower distribution around 0. Finally, mean difference observed falls tails null model, can considered surprising. away null-value, surprising result . null model true, surprising values happen probability specified alpha level (called Type 1 errors). Remember Type 1 error occurs researcher concludes difference population, true mean difference population zero.now finally ready address common misconceptions p-values. Let’s go list common misconceptions reported scientific literature. examples might sounds like semantics. easy first glance think statement communicates right idea, even written version formally correct. However, statement formally correct, wrong. exactly people often misunderstand p-values, worth formally correct interpreted.","code":""},{"path":"pvalue.html","id":"misconception1","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.1 Misunderstanding 1: A non-significant p-value means that the null hypothesis is true","text":"common version misconception reading sentence ‘p > 0.05 can conclude effect’. Another version sentence ‘difference, (p > 0.05)’.look misconception detail, want remind one fact easy remember, enable recognize many misconceptions p-values: p-values statement probability data, statement probability hypothesis probability theory. Whenever see p-values interpreted probability theory hypothesis, know something right. Examples statements hypothesis ‘null hypothesis true’, ‘alternative hypothesis true’, statements say probability null alternative model true 100%. subtler version statement ‘observed difference due chance’. observed difference ‘due chance’ (instead due presence real difference) null hypothesis true, , statement implies 100% probable null hypothesis true.conclude ‘effect’ ‘difference’ similarly claiming 100% probable null hypothesis true. since p-values statements probability data, refrain making statements probability theory solely based p-value. ’s ok. p-values designed help identify surprising results noisy data generation process (aka real world). designed quantify probability hypothesis true.Let’s take concrete example illustrate non-significant result mean null hypothesis true. figure , Omniscient Jones tells us true mean difference 0.5. can see , alternative distribution visualized probability mean differences expect null hypothesis true centered 0.5. observed mean difference 0.35. value extreme enough statistically different 0. can see , value fall within red area null model (hence, p-value smaller alpha level).Nevertheless, see observing mean difference 0.35 quite likely given true mean difference 0.5, observing mean difference 0.35 much likely alternative model, null model. can see comparing height density curve difference 0.35 null model, approximately 0.5, height density curve alternative model, approximately 1.5. See chapter likelihoods details.p-value tells us mean difference 0.35 extremely surprising, assume null hypothesis true. can many reasons . real world, Omniscient Jones tell us true mean difference, possible true effect, illustrated figure .say instead? solution subtle, important. Let’s revisit two examples incorrect statements made earlier. First, ‘p > 0.05 can conclude effect’ incorrect, might well effect (remember p-values statements data, probability effect effect). Fisher’s interpretation p-value can conclude rare event happened, null hypothesis false (writes literally: “Either exceptionally rare chance occurred, theory random distribution true”). might sound like statement probability theory, really just stating two possible scenarios low p-values occur (made Type 1 error, alternative hypothesis true). remain possible, quantify probability either possible reality (e.g., saying 95% probable null hypothesis false). Neyman-Pearson perspective p > .05 means can act null hypothesis can rejected, without maintaining desired error rate 5%.interested concluding effect absent, null hypothesis testing tool use. null hypothesis test answers question ‘can reject null hypothesis desired error rate’. can , p > 0.05, conclusion can drawn based p-value (remember concept 無 ‘mu’: answer neither yes ). Luckily, statistical approaches developed ask questions absence effect equivalence testing, Bayes factors, Bayesian estimation (see Harms & Lakens, 2018, overview). assignment week 6 learn equivalence tests detail.second incorrect statement ‘difference’’. statement somewhat easier correct. can instead write ‘statistically significant difference’. Granted, bit tautological, basically saying p-value larger alpha level two different ways, least statement formally correct. difference ‘difference’ ‘statistically significant difference’ might sound like semantics, first case formally saying ‘difference 0’ second saying ‘difference large enough yield p < .05’. Although never seen anyone , informative message might ‘given sample size 50 per group, alpha level 0.05, observed differences extreme 0.4 statistically significant, observed mean difference 0.35, reject null hypothesis’. feels like unsatisfactory conclusion, remember null hypothesis test designed draw interesting conclusions absence effects – need learn equivalence tests get satisfactory answers null effects.","code":""},{"path":"pvalue.html","id":"misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.2 Misunderstanding 2: A significant p-value means that the null hypothesis is false.","text":"opposite misconception one discussed previously. Examples incorrect statements based misconception ‘p < .05, therefore effect’, ‘difference two groups, p < .05’. , statements imply 100% probable null model false, alternative model true.simple example extreme statements incorrect, imagine generate series numbers R using following command:command generates 50 random observations distribution mean 0 standard deviation 1 (long run – mean standard deviation vary sample generated). Imagine run command , observe mean 0.5. figure visualizes scenario. can perform one-sample t-test 0, test tells us, p < .05, data observed surprisingly different 0, assuming random number generator R functions generates data true mean 0.significant p-value allow us conclude null hypothesis (“random number generator works”) false. true mean 50 samples generated surprisingly extreme. low p-value simply tells us observation surprising. observe surprising observations low probability null hypothesis true – still happen. Therefore, significant result mean alternative hypothesis true – result can also Type 1 error, example , Omniscient Jones knows case.Let’s revisit incorrect statement ‘p < .05, therefore effect’. correct interpretation significant p-value requires us acknowledge possibility significant result might Type 1 error. Remember Fisher conclude “Either exceptionally rare chance occurred, theory random distribution true”. correct interpretation terms Neyman Pearson statistics : “can act null hypothesis false, wrong 5% time long run”. Note specific use word ‘act’, imply anything whether specific hypothesis true false, merely states act null-hypothesis false time observe p < alpha, make error alpha percent time.formally correct statements bit long. scientific articles, often read shorter statement : ‘can reject null hypothesis’, ‘can accept alternative hypothesis’. statements might made assumption readers add ‘5% probability wrong, long run’. might useful add ‘5% long run error rate’ least first time make statement article remind readers.example strong subjective prior probability random number generator R works. Alternative statistical procedures incorporate prior beliefs Bayesian statistics (week 2) false positive report probabilities (week 3). frequentist statistics, idea need replicate study several times. observe Type 1 error every now , unlikely observe Type 1 error three times row. Alternatively, can lower alpha level single study reduce probability Type 1 error rate.","code":"\nrnorm(n = 50, mean = 0, sd = 1)##  [1] -0.10542606  1.01505935 -0.64066722 -1.07093346 -1.13728239  0.95253861\n##  [7]  0.72452512  1.17600867  0.17391640  0.57650315 -0.90940472 -0.54306443\n## [13] -0.29076605 -2.70983196 -0.04853286  1.07959391 -1.28578314  1.01988326\n## [19] -0.69331320 -0.37169394 -0.30048825  0.01559035  0.77903180  0.86122009\n## [25] -0.42054593 -0.81477953  0.69985855  1.39973526  0.50799504 -1.68467655\n## [31]  1.04332806 -0.74935607 -0.13098121  0.96468287  2.11325246  1.40058737\n## [37] -0.18515209  1.32335820 -0.80310917  0.70879827 -0.82634514  0.92633710\n## [43] -0.30604728 -0.10622924 -0.13763026  0.43023054 -0.29327920 -1.93906352\n## [49]  0.41581406  1.16765945"},{"path":"pvalue.html","id":"misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.3 Misunderstanding 3: A significant p-value means that a practically important effect has been discovered","text":"common concern interpreting p-values ‘significant’ normal language implies ‘important’, thus ‘significant’ effect interpreted ‘important’ effect. However, question whether effect important completely orthogonal question whether different zero, even large effect . effects practical impact. smaller effect, less likely effects noticed individuals, effects might still large impact societal level. Therefore, general take home message statistical significance answer question whether effect matters practice, ‘practically important’. answer question whether effect matters, need present cost-benefit analysis.issue practical significance often comes studies large sample size. seen , increasing sample size, width density distribution around null-value becomes narrow, values considered surprising fall closer closer zero.plot null model large sample size (e.g., n = 10000 per group) see even small mean differences (differences extreme mean difference 0.04) considered ‘surprising’. still means really difference population, observe differences larger 0.04 less 5% time, long run, 95% observed differences smaller mean difference 0.04. becomes difficult argue practical significance effects. Imagine specific intervention successful changing people’s spending behavior, implementing intervention people save 12 cents per year. difficult argue effect make individual happier. However, money combined, yield 2 million, used treat diseases developing countries, real impact. cost intervention might considered high goal make individuals happier, might consider worthwhile goal raise 2 million charity.effects psychology additive (can combine transfer increase happiness 0.04 scale points), often difficult argue importance small effects subjective feelings. cost-benefit analysis might show small effects matter lot, whether case can inferred p-value.Note nothing problem interpretation p-value per se: p < 0.05 still correctly indicates , null hypothesis true, observed data considered surprising. However, just data surprising, mean need care . mainly verbal label ‘significant’ causes confusion – perhaps less confusing think ‘significant’ effect ‘surprising’ effect, necessarily ‘important’ effect.","code":""},{"path":"pvalue.html","id":"misconception4","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.4 Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.","text":"misinterpretation one possible explanation incorrect statement p-value ‘probability data observed chance.’ Assume collect 20 observations, Omniscient Jones tells us null hypothesis true (example generated random numbers R). means sampling distribution figure .reality, means 100% time observe significant result, false positive (Type error). Thus, 100% significant results Type 1 errors.important distinguish probabilities collecting data analyzing result, probabilities collecting data analyzing results. Type 1 error rate controls, studies perform future null hypothesis true, 5% observed mean differences fall red tail areas. seen data falls tail areas p < alpha, know null hypothesis true, observed significant effects always Type 1 error. read carefully, notice misunderstanding cause differences question asked. \"observed p < .05, probability null hypothesis true?\" different question \"null hypothesis true, probability observing (extreme) data”. latter question answered p-value. first question can answered without making subjective judgment probability null hypothesis true prior collecting data (see lectures Bayesian statistics week 2).","code":""},{"path":"pvalue.html","id":"misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.5 Misunderstanding 5: One minus the p-value is the probability that the effect will replicate when repeated.","text":"impossible calculate probability effect replicate, based p-value. main reason know true mean difference. Omniscient Jones, knew true mean difference (e.g., difference two groups 0.5 scale points) know statistical power test. statistical power probability find significant result, alternative model true (.e. true effect). example, reading text left bar app, see N = 50 per group, alpha level 0.05, true mean difference 0.5, probability finding significant result (statistical power) 69.69%. observe significant effect scenario (e.g., p = 0.03) true 97% probability exact replication study (sample size) yield significant effect. probability study yields significant effect determined statistical power - p-value previous study.\ncan generally take away last misunderstanding fact probability replication depends presence versus absence true effect. words, stated , true effect exists level statistical power informs us frequently yield significant result (e.g., 80% power means observe significant result 80% time). hand, effect null (non-existent) significant results observed 5% time long run (.e. Type 1 error rate given alpha 0.05). Therefore, either statistical power alpha level equals probability replication, depending isn’t true effect.","code":""},{"path":"pvalue.html","id":"test-yourself","chapter":"1 Using p-values to test a hypothesis","heading":"1.8 Test Yourself","text":"Copy code R run code.x-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power (minor variations simulation).Q1: Since statistical power probability observing statistically significant result, true effect, can also see power figure . ?can calculate number p-values larger 0.5, divide number simulations.can calculate number p-values first bar (contains ‘significant’ p-values 0.00 0.05) divide p-values bar total number simulations.can calculate difference p-values 0.5 minus p-values 0.5, divide number total number simulations.can calculate difference p-values 0.5 minus p-values 0.05, divide number number simulations.Q2: Change sample size n <- 26 n <- 51. Run simulation selecting lines pressing CTRL+Enter. power simulation now increased sample size 26 people 51 people?55%60%80%95%Q3: look distribution p-values, notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution much flatter 50% powerThe p-value distribution much normally distributed 50% powerFeel free increase decrease sample size see happens run simulation. done exploring, make sure n <- 51 .Q4: happen true difference simulated samples average IQ score? situation, probability observe effect, might say ‘0 power’. Formally, power defined true effect. However, can casually refer 0 power. Change mean sample 100 (set m <- 106 m <- 100) now difference mean sample, population value testing one-sample t-test. Run script . notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution basically completely flat (ignoring minorvariation due random noise simulation)p-value distribution normally distributedThe question builds simulation true difference groups.Q5: Look leftmost bar plot, look frequency p-values bar formal name bar?power (true positives)true negativesThe Type 1 error (false positives)Type 2 error (false negatives)Let’s take look just p-values 0.05. Bear next steps – worth . Find variable determines many bars , statement bars <- 20. Change bars <- 100. now get 1 bar p-values 0 0.01, one bar p-values 0.01 0.02, 100 bars total. red dotted line now indicate frequency p-values null hypothesis true, every bar contains 1% total number p-values. want look p-values 0.05, cut plot 0.05. Change xlim = c(0, 1) xlim = c(0, 0.05). Instead seeing p-values 0 1, see p-values 0 0.05. Re-run simulation (still m <- 100). \nsee uniform distribution, now every bar contains 1% p-values, p-value distribution flat almost impossible see (zoom y-axis later assignment). red line now clearly gives frequency bar, assuming null hypothesis true.Change mean simulation line 9 m <- 107 (remember n still 51). Re-run simulation. ’s clear high power. p-values left-bar, contains p-values 0.00 0.01.Q6: plot last simulation tells 90.5% power. power use alpha 5%. can also use alpha 1%. statistical power simulated studies use alpha 1%, looking graph? Pick answer closest answer simulations.~90%~75%~50%~5%able look p-values around 0.03 0.04, zoom y-axis well. part code plot draw, change ylim = c(0, nSims) ylim = c(0, 10000). Re-run script.Change mean sample 108 m <- 108), leave sample size 51. Run simulation. Look distribution changed compared graph .Look fifth bar left. bar now contains p-values 0.04 0.05. notice something peculiar. Remember red dotted line indicates frequency bar, assuming null hypothesis true. See bar p-values 0.04 0.05 lower red line. simulated studies 96% power. power high, p-values 0.04 0.05 rare – occur less 1% time (p-values smaller 0.01). null hypothesis true, p-values 0.04 0.05 occur exactly 1% time (p-values uniformly distributed). Now ask : high power, observe p-value 0.04 0.05, likely null-hypothesis true, alternative hypothesis true? Given likely observe p-values 0.04 0.05 null hypothesis true, alternative hypothesis true, interpret p-value significant alpha 0.05 likely null hypothesis true, alternative hypothesis true.simulations, know true effect , real world, don’t know. high power, use alpha level 0.05, find p-value p = .045, data surprising, assuming null hypothesis true, even surprising, assuming alternative hypothesis true. shows significant p-value always evidence alternative hypothesis.Q7: know high (e.g., 98%) power smallest effect size care , observe p-value 0.045, correct conclusion?effect significant, provides strong support alternative hypothesis.effect significant, without doubt Type 1 error.high power, use alpha level smaller 0.05, therefore, effect can considered significant.effect significant, data likely null hypothesis alternative hypothesis.Q8: Let’s assume random number generator R works, use rnorm(n = 50, mean = 0, sd = 1) generate 50 observations, mean observations 0.5, one-sample t-test yields p-value 0.03, smaller alpha level (set 0.05). probability observed significant difference (p < alpha) just chance?3%5%95%100%Q9: statement true?probability replication study yield significant result 1-p.probability replication study yield significant result 1-p multiplied probability null hypothesis true.probability replication study yield significant result equal statistical power replication study (true effect), alpha level (true effect).probability replication study yield significant result equal statistical power replication study + alpha level.Q10: non-significant p-value (.e., p = 0.65) mean null hypothesis true?- result Type 2 error, false negative.Yes, true negative.Yes, p-value larger alpha level null hypothesis true., need least two non-significant p-values conclude null hypothesis true.Q11: correct way present non-significant p-value (e.g., p = 0.34 assuming alpha level 0.05 used independent t-test)?null hypothesis confirmed, p > 0.05There difference two conditions, p > 0.05The observed difference statistically different 0.null hypothesis true.Q12: observing significant p-value (p < .05) mean null hypothesis false?, p < .05 means alternative true, null hypothesis wrong., p-values never statement probability hypothesis theory.Yes, exceptionally rare event occurred.Yes, difference statistically significant.Q13: statistically significant effect always practically important\neffect?, extremely large samples, extremely small effects can statistically significant, small effects never practically important., alpha level theory set 0.20, case significant effect practically important., important effect depends cost-benefit analysis, surprising data null hypothesis.true.Q14: correct definition p-value?p-value probability null hypothesis true, given data extreme extreme data observed.p-value probability alternative hypothesis true, given data extreme extreme data observed.p-value probability observing data extreme extreme data observed, assuming alternative hypothesis true.p-value probability observing data extreme extreme data observed, assuming null hypothesis true.","code":"\nnsims <- 100000 # number of simulations\n\nm <- 106 # mean sample\nn <- 26 # set sample size\nsd <- 15 # SD of the simulated data\n\np <- numeric(nsims) # set up empty vector\nbars <- 20\n\nfor (i in 1:nsims) { # for each simulated experiment\n  x <- rnorm(n = n, mean = m, sd = sd)\n  z <- t.test(x, mu = 100) # perform the t-test\n  p[i] <- z$p.value # get the p-value\n}\npower <- round((sum(p < 0.05) / nsims), 2) # power\n\n# Plot figure\nhist(p,\n  breaks = bars, xlab = \"P-values\", ylab = \"number of p-values\\n\", \n  axes = FALSE,  main = paste(\"P-value Distribution with\", \n                              round(power * 100, digits = 1), \"% Power\"),\n  col = \"grey\", xlim = c(0, 1), ylim = c(0, nsims))\naxis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))\naxis(side = 2, at = seq(0, nsims, nsims / 4), \n     labels = seq(0, nsims, nsims / 4), las = 2)\nabline(h = nsims / bars, col = \"red\", lty = 3)"},{"path":"errorcontrol.html","id":"errorcontrol","chapter":"2 Error control","heading":"2 Error control","text":"previous chapter p-values learned Neyman-Pearson approach hypothesis testing goal make scientific claims controlling often make fool long run. Benjamini (2016) notes, p-value \"offers first line defense fooled randomness, separating signal noise\". indications banning use p-values increases ability researchers present erroneous claims. Based qualitative analyses scientific articles published null-hypothesis significance ban journal Basic Applied Social Psychology Fricker et al. (2019) conclude: “researchers employ descriptive statistics found likely overinterpret /overstate results compared researcher uses hypothesis testing p < 0.05 threshold”. Researchers often control error rates make claims, sometimes intentionally use flexibility data analysis 'p-hack' cherry-pick one many performed analyses shows results wanted see. error-statistical approach statistical inferences, problematic behavior, Mayo (2018) writes:problem cherry picking, hunting significance, host biasing selection effects – main source handwringing behind statistics crisis science – wreak havoc method’s error probabilities. becomes easy arrive findings severely tested.## outcome can expect perform study?perform study plan make claim based statistical test plan perform, long run probability making correct claim erroneous claim determined three factors, namely Type 1 error rate, Type 2 error rate, probability null-hypothesis true. four possible outcomes statistical test, depending whether result statistically significant , whether null hypothesis true, .False Positive (FP): Concluding true effect, true effect (H0 true). also referred Type 1 error, indicated α.False Negative (FN): Concluding true effect, true effect (H1 true). also referred Type 2 error, indicated β.True Negative (TN): Concluding true effect, true effect (H0 true). complement False Positives, thus indicated by1-α.True Positive (TP): Concluding true effect, true effect (H1 true). complement False Negatives, thus indicated 1-β.probability observing true positive true effect , long run, equal statistical power study. probability observing false positive null hypothesis true , long run, equal alpha level set, Type 1 error rate.\nFigure 2.1: Difference Type 1 Type 2 errors. Figure made Paul Ellis\n, next study perform, four possible outcomes likely? First, assume set alpha level 5%. Furthermore, assume designed study 80% power (example, assume Omniscient Jones knows indeed exactly 80% power). last thing specify probability null hypothesis true. Let’s assume next study idea null hypothesis true , equally likely null hypothesis true, alternative hypothesis true (probability 50%). can now calculate likely outcome study .perform calculation, take moment think know answer. might designed studies 5% alpha level 80% power, believed equally likely H0 H1 true. Surely, useful reasonable expectations result expect, perform study? Yet experience, many researchers perform without thinking probabilities . often hope observe true positive, even situation described , likely outcome true negative. now calculate probabilities.assume perform 200 studies 5% alpha level, 80% power, 50% probability H0 true. many false positives, true positives, false negatives, true negatives expect long run?table see 2.5% studies false positive (5% Type 1 error rate,\nmultiplied 50% probability H0 true). 40% studies true positive (80% power multiplied 50% probability H1 true). probability false negative 10% (20% Type 2 error rate multiplied 50% probability H1 true). likely outcome true negative, 47.5% (95% probability observing non-significant result, multiplied 50% probability H0 true).might enthusiastic outlook, like perform studies higher probability observing true positive. ? can reduce alpha level, increase power, increase probability H1 true. probability observing true positive depends power, multiplied probability H1 true, design studies values high. Statistical power can increased changes design study (e.g., increasing sample size). probability H1 true depends hypothesis testing. probability H1 true high outset, risk testing hypothesis already established enough certainty. solution, might happen often career, come test hypothesis trivial, explaining peers makes lot sense. words, come idea , explaining , think extremely plausible. creative research ideas likely rare academic career, ever . research needs ground-breaking. also extremely valuable perform replication extension studies relatively likely H1 true, scientific community still benefits knowing findings generalize different circumstances.","code":""},{"path":"errorcontrol.html","id":"positive-predictive-value","chapter":"2 Error control","heading":"2.1 Positive predictive value","text":"John Ioannides wrote well known article titled \"Published Research Findings False\" (J. P. . Ioannidis, 2005). time, learned set alpha 5%, Type 1 error rate higher 5% (long run). two statements related? aren’t 95% published research findings true? key understanding difference two different probabilities calculated. Type 1 error rate probability saying effect, effect. Ioannides calculates positive predictive value (PPV), conditional probability study turns show statistically significant result, actually true effect. probability useful understand, people often selectively focus significant results, due publication bias, research areas significant results published.real-life example useful understand concept positive predictive value concerned number vaccinated vaccinated people admitted hospital COVID symptoms. places equal numbers patients vaccinated unvaccinated. understand concept positive predictive value, might believe reveals equally likely end hospital, whether vaccinated . incorrect. Figure 2.2 nicely vizualizes, probability person vaccinated high, probability vaccinated person ends hospital much lower probability unvaccinated person ends hospital. However, select individuals end hospital, computing probability conditional hospital.\nFigure 2.2: vaccinated people hospital.\nuseful understand probability , observed significant result experiment, result actually true positive. words, long run, many true positives can expect, among positive results (true positives false positives)? known Positive Predictive Value (PPV). can also calculate many false positives can expect, among positive results (, true positives false positives). known False Positive Report Probability (Wacholder et al., 2004), sometimes also referred False Positive Risk (Colquhoun, 2019).\\[PPV = \\frac{\\text{True}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives} +\n                                                \\text{False}\\ \\text{Positives})}\\]\\[FPRP = \\frac{\\text{False}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives}\n                                                  + \\text{False}\\ \\text{Positives})}\\]PPV FPRP combine classic Frequentist concepts statistical power alpha levels prior probabilities H0 H1 true. depend proportion studies effect (H1 true), effect (H0 true), addition statistical power, alpha level. , can observe false positive null hypothesis true, can observe true positive alternative hypothesis true. Whenever perform study, either operating reality true effect, operating reality effect – don’t know reality .perform studies, aware outcomes studies (significant non-significant findings). read literature, publication bias, often access significant results. thinking PPV (FPRP) becomes important. set alpha level 5%, long run 5% studies H0 true (FP + TN) significant. literature significant results, access true negatives, \npossible proportion false positives literature much larger 5%.continue example , see 85 positive results (80 + 5) 200 studies. false positive report probability 5/85 = 0.0588. time, alpha 5% guarantees (long run) 5% 100 studies null hypothesis true Type 1 errors: 5%*100 = 0.05. also true. 200 studies, 0.05*200 = 10 possibly false positives (H0 true experiments). 200 studies performed (H0 true 50% studies), proportion false positives experiments 2.5%. Thus, experiments , proportion false positives , long run, never higher Type error rate set researcher (e.g., 5% H0 true experiments), can lower (H0 true less 100% experiments).\nFigure 2.3: Screenshot output results PPV Shiny app Michael Zehetleitner Felix Schönbrodt \n(Note: FDR FPRP different abbreviations thing)People often say something like: “Well, know 1 20 results published literature Type 1 errors”. able understand true practice, learning positive predictive value. also explains common p-value misconception \"observed significant finding, probability made Type 1 error (false positive) 5%.\" correct. Even use 5% alpha level, quite reasonable assume much 5% significant findings published literature false positives. 100% studies perform, null hypothesis true, studies published, 1 20 studies, long run, false positives (rest correctly reveal statistically significant difference). scientific literature, positive predictive value can quite high, specific circumstances, might even high published research findings false. happen researchers examine mostly studies null-hypothesis true, low power, Type 1 error rate inflated due p-hacking types bias.","code":""},{"path":"errorcontrol.html","id":"type-1-error-inflation","chapter":"2 Error control","heading":"2.2 Type 1 error inflation","text":"\nFigure 2.4: Quote 1830 book Babbage Reflections Decline Science England Causes available \nperform multiple comparisons, risk Type 1 error rate inflates. multiple comparisons planned, cases possible control Type 1 error rate lowering alpha level individual analysis. widely known approach control multiple comparisons Bonferroni correction alpha level divided number tests performed (Dunn, 1961). However, researchers also often use informal data analysis strategies inflate Type 1 error rate. Babbage (1830) already complained problematic practices 1830, two centuries later, still common. Barber (1976) provides depth discussion range approaches, eyeballing data decide hypotheses test (sometimes called 'double dipping'), selectively reporting analyses confirm predictions, ignoring non-significant results, collecting many variables performing multitudes tests, performing sub-group analyses planned analysis yields nonsignificant results, nonsignificant prediction derive new hypothesis supported data, test hypothesis data hypothesis derived (sometimes called HARKing (Kerr, 1998)). Many researchers admit used practices inflate error rates (Chin et al., 2021; Fiedler & Schwarz, 2015; John et al., 2012; Makel et al., 2021; van de Schoot et al., 2021). used practices first scientific article published, fully aware problematic (Jostmann et al., 2016).paradigms, researchers lot flexibility compute main dependent variable. Elson colleagues examined 130 publications use Competitive Reaction Time Task, participants select duration intensity blasts delivered competitor (Elson et al., 2014). task used measure 'aggressive behavior' ethical manner. compute score, researchers can use duration noise blast, intensity, combination therefore, averaged number trials, several possible transformations data. 130 publications examined reported 157 different quantification strategies total, showing calculations dependent variable unique, used single article. One might wonder authors sometimes use different computations across articles. One possible explanation used flexibility data analysis find statistically significant results.\nFigure 2.5: Plot publications using CRTT (blue) unique quantifications meaure (red). Figure FlexibleMeasures.com Malte Elson\n","code":""},{"path":"errorcontrol.html","id":"optional-stopping","chapter":"2 Error control","heading":"2.3 Optional stopping","text":"\nFigure 2.6: Screenshot scientific paper explicitly admitting using optional stopping\nOne practice inflates Type 1 error rate known optional stopping. optional stopping, researcher repeatedly analyzes data, continues data collection test result statistically significant, stops significant effect observed. quote published article figure example researchers transparently report used optional stopping, commonly people disclose use optional stopping methods sections. last years, many researchers learned optional stopping problematic. lead general idea collect data, look whether results significant, stop data collection result significant, , continue data collection. correct conclusion, example becoming inflexible. correct approach collect data batches, called sequential analysis, extensively developed statisticians, used many studies. example, safety efficacy Pfizer–BioNTech COVID-19 vaccine used experimental design planned analyze data 5 times, controlled overall Type 1 error rate lowering alpha level interim analysis.\nFigure 2.7: Screenshot planned interim analyses examining safety Efficacy BNT162b2 mRNA Covid-19 Vaccine.\nmain lesson certain research practices can increase flexibility efficiency studies perform, done right, practices can inflate Type 1 error rate done wrong. Let’s therefore try get better understanding inflating Type 1 error rate optional stopping, correctly using sequential analysis.Copy code R run . script simulate ongoing data collection. 10 participants condition, p-value calculated performing independent t-test, t-test repeated every additional participant collected. , p-values plotted function increasing sample size.example, Figure , see p-value plotted y-axis (0 1) sample size plotted x-axis (0 200). simulation, true effect size d = 0, meaning true effect. can thus observe true negatives false positives. sample size increases, p-value slowly moves (remember chapter p-values true effect, p-values uniformly distributed). Figure 2.8, p-value drops grey line (indicating alpha level 0.05) collecting 83 participants condition, drift back upwards larger p-values. figure, becomes clear often look data, larger total sample size, higher probability one analyses yield p < \\(\\alpha\\). resources infinite, Type 1 error rate 1, researcher can always find significant result optional stopping.\nFigure 2.8: Simulated p-values additional observation null true.\ntrue effect, see p-values also vary, eventually drop alpha level. Due variation, just know exactly . perform -priori power analysis, can compute probability looking specific sample size yield significant p-value. Figure 2.9 see simulation, now true small effect d = 0.3. 200 observations per condition, sensitivity power analysis reveals 85% power. analyze data interim analysis (e.g., 150 observations) often already find statistically significant effect (74% power). illustrates benefit sequential analyses, control error rates, can stop early interim analysis. Sequential analyses especially useful large expensive studies uncertainty true effect size.\nFigure 2.9: Simulated p-values additional observation d = 0.3.\nformally examine inflation Type 1 error rate optional stopping simulation study. Copy code R run code. Note 50000 simulations (needed get error rates reasonably accurate) take time run.simulation perform multiple independent t-tests simulated data, looking multiple times maximum sample size reached. first four lines, can set important parameters simulation. First, maximum sample size condition (e.g., 100). , number looks (e.g., 5). best, can look data every participant (e.g., 100 participants, can look 100 times – actually 98 times, need 2 participants condition t-test!). can set number simulations (, clearer pattern , longer simulation takes), alpha level (e.g., 0.05). Since can make Type 1 error true effect, effect size set 0 simulations.perform single test, Type 1 error rate probability finding p-value lower alpha level, effect. optional stopping scenario look data twice, Type 1 error rate probability finding p-value lower alpha level first look, probability finding p-value lower alpha level first look, finding p-value lower alpha level second look. conditional probability, makes error control little bit complex multiple looks completely independent.much optional stopping inflate Type 1 error rate? p-values can expect optional stopping?Start running simulation without changing values, simulating 100 participants condition, looking 5 times data, alpha 0.05. Note 50.000 simulations take ! see something similar Figure 2.10 (based 500.000 simulations make pattern clear).\nFigure 2.10: Simulation 500000 studies performing 5 interim analyses alpha level 5%\nsee 100 bars, one % (one p-values 0.00 0.01, one p-values 0.01 0.02, etc.). horizontal line indicates p-values fall, uniformly distributed (true effect, explained chapter p-values).distribution p-values peculiar. see compared uniform distributions, bunch results just alpha threshold 0.05 missing, seem pulled just 0.05, much higher frequency outcomes compared data analyzed multiple times comes . Notice relatively high p-values (e.g., p = 0.04) common lower p-values (e.g., 0.01). see chapter bias detection statistical techniques p-curve analysis can pick pattern.using alpha level 5% 5 looks data, overall Type 1 error rate inflated 14%. lower alpha level interim analysis, overall Type 1 error rate can controlled. shape p-value distribution still look peculiar, total number significant test results controlled desired alpha level. well-known Bonferroni correction (.e., using alpha level \\(\\alpha\\) / number looks), Pocock correction slighlty efficient. information perform interim analyses controlling error rates, see dedicated chapter sequential analysis.","code":"\nn <- 200 # total number of datapoints (per condition) after initial 10\nd <- 0.0 # effect size d\n\np <- numeric(n) # store p-values\nx <- numeric(n) # store x-values\ny <- numeric(n) # store y-values\n\nn <- n + 10 # add 10 to number of datapoints\n\nfor (i in 10:n) { # for each simulated participants after the first 10\n  x[i] <- rnorm(n = 1, mean = 0, sd = 1)\n  y[i] <- rnorm(n = 1, mean = d, sd = 1)\n  p[i] <- t.test(x[1:i], y[1:i], var.equal = TRUE)$p.value\n}\n\np <- p[10:n] # Remove first 10 empty p-values\n\n# Create the plot\nplot(0, col = \"red\", lty = 1, lwd = 3, ylim = c(0, 1), xlim = c(10, n), \n     type = \"l\", xlab = \"sample size\", ylab = \"p-value\")\nlines(p, lwd = 2)\nabline(h = 0.05, col = \"darkgrey\", lty = 2, lwd = 2) # draw line at p = 0.05\n\nmin(p) # Return lowest p-value from all looks\ncat(\"The lowest p-value was observed at sample size\", which.min(p) + 10) \ncat(\"The p-value dropped below 0.05 for the first time at sample size:\", \n    ifelse(is.na(which(p < 0.05)[1] + 10), \"NEVER\", which(p < 0.05)[1] + 10)) \nN <- 100 # total datapoints (per condition)\nlooks <- 5 # set number of looks at the data\nnsims <- 50000 # number of simulated studies\nalphalevel <- 0.05 # set alphalevel\n\nlook_at_n <- ceiling(seq(N / looks, N, (N - (N / looks)) / (looks-1))) # looks\nlook_at_n<-look_at_n[look_at_n > 2] #Remove looks at N of 1 or 2\nlooks<-length(look_at_n) #if looks are removed, update number of looks\n\nmatp <- matrix(NA, nrow = nsims, ncol = looks) # Matrix for p-values l tests\np <- numeric(nsims) # Variable to save pvalues\n\n# Loop data generation for each study, then loop to perform a test for each N\nfor (i in 1:nsims) {\n  x <- rnorm(n = N, mean = 0, sd = 1)\n  y <- rnorm(n = N, mean = 0, sd = 1)\n  for (j in 1:looks) {\n    matp[i, j] <- t.test(x[1:look_at_n[j]], y[1:look_at_n[j]], \n                         var.equal = TRUE)$p.value # perform the t-test, store\n  }\n  cat(\"Loop\", i, \"of\", nsims, \"\\n\")\n}\n\n# Save Type 1 error rate smallest p at all looks\nfor (i in 1:nsims) {\n  p[i] <- ifelse(length(matp[i,which(matp[i,] < alphalevel)]) == 0, \n                 matp[i,looks], matp[i,which(matp[i,] < alphalevel)])\n}\n\nhist(p, breaks = 100, col = \"grey\") # create plot\nabline(h = nsims / 100, col = \"red\", lty = 3)\n\ncat(\"Type 1 error rates for look 1 to\", looks, \":\", \n    colSums(matp < alphalevel) / nsims)\ncat(\"Type 1 error rate when only the lowest p-value for all looks is reported:\", \n    sum(p < alphalevel) / nsims)"},{"path":"errorcontrol.html","id":"justifyerrorrate","chapter":"2 Error control","heading":"2.4 Justifying Error Rates","text":"reject H0 , may reject true; accept H0 , may accepting false, say, really alternative Bt true. two sources error can rarely eliminated completely; cases important avoid first, others second. reminded old problem considered LaplaceE number votes court judges needed convict prisoner. serious convict innocent man acquit guilty? depend upon consequences error ; punishment death fine ; danger community released criminals ; current ethical views punishment? point view mathematical theory can show risk errors may controlled minimised. use statistical tools given case, determining just balance struck, must left investigator.Even though theory Type 1 Type 2 error rate justified researcher, Neyman Pearson (1933) write , practice researchers tend imitate others. default use alpha level 0.05 can already found work} Gosset t-distribution (Cowles & Davis, 1982; Kennedy-Shaffer, 2019), believed difference two standard deviations (z-score 2) sufficiently rare. default use 80% power (20% Type 2 error rate) similarly based personal preferences Cohen (1988), writes:proposed convention , investigator basis setting desired power value, value .80 used. means beta set .20. value offered several reasons (Cohen, 1965, pp. 98-99). chief among takes consideration implicit convention alpha .05. beta .20 chosen idea general relative seriousness two kinds errors order .20/.05, .e., Type errors order four times serious Type II errors. .80 desired power convention offered hope ignored whenever investigator can find basis substantive concerns specific research investigation choose value ad hoc.see conventions built conventions: norm aim 80% power built norm set alpha level 5%. Although nothing special alpha level 5%, interesting reflect become widely established. Uygun Tunç et al. (2021) argue one possible reason , far conventions go, alpha level 5% might low enough peers take claims made error rate seriously, time high enough peers motivated perform independent replication study increase decrease confidence claim. Although lower error rates establish claims convincingly, also require resources. One might speculate\nresearch areas every claim important enough careful justification costs benefits, 5% pragmatic function facilitating conjectures refutations fields otherwise lack coordinated approach knowledge generation, faced limited resources.Nevertheless, researchers proposed move away default use 5% alpha level. example, Johnson (2013) proposes default significance level 0.005 0.001. Others cautioned blanket recommendation additional resources required reduce Type 1 error rate might worth costs (Lakens et al., 2018). lower alpha lever requires larger sample size achieve sstatistical power. sample size can increased, lower alpha level reduces statistical power, increases Type 2 error rate. Whether desireable evaluated case case basis.two main reasons abandon universal use 5% alpha level. first reason carefully choose alpha level decision-making becomes efficient (Mudge et al., 2012). researchers use hypothesis tests make dichotomous decisions methodological falsificationist approach statistical inferences, certain maximum sample size willing able collect, typically possible make decisions efficiently choosing error rates combined cost Type 1 Type 2 errors minimized. aim either minimize balance Type 1 Type 2 error rates given sample size effect size, alpha level set based convention, weighting relative cost types errors (Maier & Lakens, 2022).example, imagine researcher plans collect 64 participants per condition detect d = 0.5 effect, weighs cost Type 1 errors 4 times much Type 2 errors. exactly scenario Cohen (1988) described, 64 participants per condition relative weigfht Type 1 Type 2 errors yields 5% Type 1 error rate 20% Type 2 error rate. Now imagine researchers realizes resources collect 80 observations instead just 64. interest effect size d = 0.5, relative weight Type 1 Type 2 errors 4 satisfied set alpha level 0.037 Type 2 error rate 0.147. Alternatively, researcher might decided collect 64 observations, balance error rates, set alpha level weighted combined error rate minimized, achieved alpha level set 0.033, vizualized Figure 2.11 (information, see Maier & Lakens (2022)).\nFigure 2.11: Weighted combined error rate, minimized alpha = 0.037.\nJustifying error rates can leads situations alpha level increased 0.05, leads optimal decision making. Winer (1962) writes: “frequent use .05 .01 levels significance \n197 matter convention little scientific logical basis. power tests likely low levels significance, Type 1 Type 2 errors approximately equal importance, .30 .20 levels significance may appropriate .05 .01 levels.” reasoning design 70% power smallest effect size interest balance Type 1 Type 2 error rates sensible manner. course, increase increase alpha level deemed acceptable authors can justify costs increase Type 1 error rate sufficiently compensated benefit decreased Type 2 error rate. encompass cases (1) study practical implications require decision making, (2) cost-benefit analysis provided gives clear rationale relatively high costs Type 2 error, (3) probability H1 false relatively low, (4) feasible blackuce overall error rates collecting data.One also carefully reflect choice alpha level experiment achieves high statistical power effect sizes considered meaningful. study 99% power effect sizes interest, thus 1% Type 2 error rate, uses default 5% alpha level, also suffers lack balance, use lower alpha level lead balanced decision, increase severity test.second reason relevant large data sets, related Lindley's paradox. statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis, recommended lower alpha level function sample size. need discussed Leamer (1978), writes \"rule thumb quite popular now, , setting significance level arbitrarily .05, shown deficient sense every reasonable viewpoint significance level decreasing function sample size.\" idea approach reduce alpha level Bayes factor likelihood computed significant results never evidence null hypothesis (online Shiny app perform calculations, see ).","code":""},{"path":"errorcontrol.html","id":"why-you-dont-need-to-adjust-your-alpha-level-for-all-tests-youll-do-in-your-lifetime.","chapter":"2 Error control","heading":"2.5 Why you don't need to adjust your alpha level for all tests you'll do in your lifetime.","text":"researchers criticize corrections multiple comparisons one might well correct tests lifetime [Perneger (1998). choose use Neyman-Pearson approasch statistics reason correct tests perform lifetime work done life tests single theory, use last words decide accept reject theory, long one individual tests performed yielded p < α. Researchers rarely work like .Instead, Neyman-Pearson approach hypothesis testing, goal use data make decisions act. Neyman (Neyman, 1957) calls approach inductive behavior. outcome experiment leads one take different possible actions, can either practical (e.g., implement new procedure, abandon research line) scientific (e.g., claim effect). error-statistical approach (Mayo, 2018) inflated Type 1 error rates mean become likely able claim support hypothesis, even hypothesis wrong. reduces severity test. prevent , need control error rate level claim.useful distinction literature multiple testing union-intersection testing approach, intersection-union testing approach (Dmitrienko & D’Agostino Sr, 2013). union-intersection approach, claim made -least-one test significant. cases, correction multiple comparisons required control error rate. intersection-union approach, claim made performed tests statistically significant, correction multiple comparisons required (indeed, assumptions researchers even increase alpha level intersection-union approach).might seem researchers can get corrections multiple comparisons formulating hypothesis every possible test perform. Indeed, can. ten ten correlation matrix, researcher might state testing 45 unique predictions, uncorrected alpha level. However, readers might reasonably question whether 45 tests predicted theory, 45 tests show significant result, meager track record predictions make us doubt theory derived . different ways control error rates, easiest Bonferroni correction ever--slightly less conservative Holm-Bonferroni sequential procedure. number statistical tests becomes substantial, sometimes preferable control false discovery rates, instead error rates (Benjamini & Hochberg, 1995).","code":""},{"path":"errorcontrol.html","id":"power-analysis","chapter":"2 Error control","heading":"2.6 Power Analysis","text":"far largely focused Type 1 error control. clear Figure 2.9, true effect p-values eventually become smaller alpha level sample size becomes large enough. designing experiment goal choose sample size provides desired Type 2 error rate effect size interest. can achieved performing -priori power analysis. important highlight goal -priori power analysis achieve sufficient power true effect size. true effect size always unknown designing study. goal -priori power analysis achieve sufficient power, given specific assumption effect size researcher wants detect. Just like Type error rate maximum probability making Type error conditional assumption null hypothesis true, -priori power analysis computed assumption specific effect size. unknown assumption correct. researcher can make sure assumptions well justified. Statistical inferences based test Type II error controlled conditional assumption specific effect size. allow inference , assuming true effect size least large used -priori power analysis, maximum Type II error rate study larger desired value.Figure 2.12 see expected distribution observed standardized effect sizes (Cohen's d) independent t-test 50 observations condition. bell-shaped curve left represents expectations null true, red areas tail represent Type 1 errors. bell-shaped curve right represents expectations alternative hypothesis true, d= 0.5. vertical line d = 0.4 represents critical effect size. sample size alpha level 0.05, observed effect sizes smaller d = 0.4 statistically significant. true effect, outcomes Type 2 errors, illustrated blue shaded area. remainder curve reflects true positives, true effect, observed effect sizes statistically significant. power test percentages distribution right larger critical value.\nFigure 2.12: Distribution d = 0 d = 0.5 independent t-test n = 50.\nissue Type 2 error control discussed detail nthe chapter sample size justification. Even thought topic Type 2 error control briefly discussed , least important Type 1 error control. informative study high probability observing effect effect. Indeed, default recommendation aim 80% power leaves surprisingly large (20%) probability Type 2 error. researcher cares making decision error, researcher care whether decision error false positive false negative, argument made Type 1 Type 2 errors weighed equally. Therefore, desiging study balanced error rates (e.g., 5% Type 1 error 95% power) make sense.","code":""},{"path":"errorcontrol.html","id":"test-yourself-1","chapter":"2 Error control","heading":"2.7 Test Yourself","text":"","code":""},{"path":"errorcontrol.html","id":"questions-about-the-positive-predictive-value","chapter":"2 Error control","heading":"2.7.1 Questions about the positive predictive value","text":"Q1: example start chapter, see control Type 1 error rate 5% using alpha 0.05. Still, 50% probability H0 true, proportion false positives experiments performed turns much lower, namely 2.5%, 0.025. ?proportion false positives experiments performed variable distribution around true error rate – sometimes ’s higher, sometimes ’s lower, due random variation.proportion false positives experiments performed 5% H0 true 200 studies.proportion false positives experiments performed 5% 50% power – power increases 50%, proportion false positives experiments performed becomes smaller.proportion false positives experiments performed 5% 100% power, becomes smaller power lower 100%.Q2: FWhat make biggest difference improving probability find true positive? Check answer shifting sliders online PPV app.Increase % -priori true hypothesesDecrease % -priori true hypothesesIncrease alpha levelDecrease alpha levelIncrease powerDecrease powerIncreasing power requires bigger sample sizes, studying larger effects. Increasing % -priori true hypotheses can done making better predictions – example building reliable findings, relying strong theories. useful recommendations want increase probability performing studies find statistically significant result.Q3: Set “% priori true hypotheses” slider 50%. Leave ‘α level’ slider 5%. Leave ‘% p-hacked studies’ slider 0. title Ioannidis’ paper ‘published research findings false’. One reason might studies often low power. value power PPV 50%. words, level power significant result just likely true, false?80%50%20%5%seems low power alone best explanation published findings might false, unlikely power low enough scientific literature. Ioannidis (2005) discusses scenarios becomes likely published research findings false. assume ‘p-hacked studies’, studies show significant result due bias, enter literature. good reasons believe happens, discussed chapter. ‘presets Ioannidis’ dropdown menu, can select situations. Explore , pay close attention ones PPV smaller 50%.Q4: general, published findings false? Interpret ‘low’ ‘high’ answer options relation values first example chapter 50% probability H1 true, 5% alpha, 80% power,\n0% bias.probability examining true hypothesis low, combined either low power substantial bias (e.g., p-hacking).probability examining true hypothesis high, combined either low power substantial bias (e.g., p-hacking).alpha level high, combined either low power substantial bias (e.g., p-hacking).power low p-hacking high (regardless % true hypotheses one examines).Q5: Set “% priori true hypotheses” slider 0%. Set “% p-hacked studies” slider 0%. Set “α level” slider 5%. Play around power slider. statement true?\nWithout p-hacking, alpha level 5%, 0% hypotheses true, proportion false positives experiments performed 100%.PPV depends power studies.regardless power, PPV equals proportion false positives experiments performed.regardless power, proportion false positives experiments performed 5%, PPV 0% (significant results false positives).","code":""},{"path":"errorcontrol.html","id":"questions-about-optional-stopping","chapter":"2 Error control","heading":"2.7.2 Questions about optional stopping","text":"Q1: Run script plots p-value sample size increases 20 times, count often lowest p-value ends 0.05 (calculate long run probability happening extensive simulations later).Q2: true effect, can observe true positive false negative. Change effect size d <- 0.0 d <- 0.3. relatively small true effect, 200 participants condition, 85% power (85% probability finding significant effect). Run script . one possible example trajectory p-values sample size increases. Run script 20 times. Take good look variation p-value trajectory. Remember N = 200, 85% times p-value ended 0.05. script returns sample size p-value lowest (often, always, maximum sample size, true effect) sample size p-value drops 0.05 first time. statement true?p-value drops 0.05, stays 0.05.p-value randomly moves 0 1, every now end 0.05.p-value often drops 0.05 well 200 participants \ncondition. around 50% simulations, already happens N = 100.p-value typically move 0.05 stay time,\ngiven large enough sample, always move back p > 0.05.Q3: Change effect size d <- 0.8, can regarded large effect. Run script 20 times. Take good look variation p-value trajectory. statement true?p-value randomly moves 0 1, every now end 0.05.p-values drop stay 0.05 much earlier true effect size 0.3.p-values meaningful effect sizes large (e.g., d = 0.8), meaningless effect sizes small (e.g., d = 0.3).examine large effect, whenever p-value drops 0.05, always stay 0.05 sample size increases.Q4: Looking Figure 2.10, statement true?Optional stopping impact Type 1 error rate.Optional stopping inflates Type 1 error rate. can see first five bars (p-values 0.00 0.05), substantially higher horizontal line.Optional stopping inflates Type 1 error rate. can see bars just 0.05, dip substantially uniform distribution present true effect.Q5: script simulate optional stopping provides written output. One summary gives Type 1 error rate individual look. One summary gives Type 1 error rate optional stopping used. running script default values, statement true?look, Type 1 error rate higher alpha level (0.05).\nusing optional stopping (reporting lowest p-value), Type 1 error rate higher 0.05.look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping (reporting lowest p-value), alpha level also approximately equals alpha level (0.05).look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping, Type 1 error rate also higher alpha level (0.05).Q6: Change number looks simulation 2 (change 'looks <- 5' 'looks <- 2'), leave settings . Run simulation . Type 1 error rate using optional stopping 1 interim analysis, rounded 2 digits? (Note due small number simulations, exact alpha level get might differ little bit \nanswer options ).0.050.080.120.18Q7: Wagenmakers (2007) notes: “user NHST always obtain significant result optional stopping (.e., analyzing data accumulate stopping experiment whenever p-value reaches desired significance level)”. correct. ’s true p-value always drop alpha level point time. , need rather large number observations. can calculate maximum Type 1 error rate due optional stopping maximum sample size. example, maximum Type 1 error rate optional stopping used collecting 200 participants condition, looking 200 times (198 times, given can’t perform t-test sample size 1 2 people)? Set number participants 200, number looks 200, number simulations 10000 (simulation take even longer!), alpha 0.05.maximum Type 1 error rate collecting 200 participants \ncondition independent t-test, using optional stopping, rounded 2\ndigits? (Note simulation take , still, due \nrelatively small number simulations, exact alpha level get might\ndiffer little bit answer options – choose answer option\nclosest result).0.050.110.200.41Q8: Wikipedia, look entry Pocock boundary: https://en.wikipedia.org/wiki/Pocock_boundary . ethical reasons look data, data collected. clear medicine, similar arguments can made research areas (see Lakens, 2014). Researchers often want look data multiple times. perfectly fine, long design study number looks advance, control Type 1 error rate.Pocock boundary provides easy way control type 1 error rate sequential analyses. Sequential analysis formal way optional stopping. Researchers use slightly lower alpha level look, make sure overall alpha level (looks) larger 5%.Set number participants 100, number looks 5, \nnumber simulations 50000 (back original script). Wikipedia article Pocock boundary, find corrected alpha level 5 looks data. Change alpha level simulation value. Run simulation. following statements true?Type 1 error rate look approximately 0.03, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.03, overall alpha level approximately 0.15.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.08.Q9: Look graph p-value distribution using Pocock boundary, compare graph got using Pocock boundary. can flip back forth plots generated RStudio using blue arrows plots tab. statement true?Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also \nlikely slightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) less likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also less\nlikely slightly higher p-values (p = 0.04).","code":""},{"path":"likelihoods.html","id":"likelihoods","chapter":"3 Likelihoods","heading":"3 Likelihoods","text":"addition frequentist Bayesian approaches statistical inferences, likelihoods provide third approach statistical inferences (Pawitan, 2001). Unlike Bayesian approaches, likelihoodists incorporate prior inferences. likelihoodists Taper Lele (2011) write:believe Bayes' rule Bayesian mathematics flawed, axiomatic foundational definition probability Bayesianism doomed answer questions irrelevant science. care believe, barely care believe, interested can show.Unlike Neyman-Pearson frequentist approach, likelihoodists interested quantifying relative evidence, unlike Fisherian frequentist approach, likelihoodists specify null alternative model, quantify relative likelihood data models.Likelihood approaches statistical inferences form bridge frequentist approaches Bayesian approaches, independent third approach statistical inferences. time, likelihood functions important part Neyman-Pearson statistics Neyman-Pearson lemma, shows likelihood ratio test powerful test H0 H1, useful determining critical value used reject hypothesis. Bayesian approaches, likelihood combined prior compute posterior probability distribution.can use likelihood functions make inferences unknown quantities. Let’s imagine flip coin 10 times, turns heads 8 times. true probability (indicate Greek letter theta, p) coin landing heads?binomial probability observing x successes n studies :\\[\nPr\\left( k;n, p \\right) = \\frac{n!}{k!\\left( n - k \\right)!}p^{k}{(1 - p)}^{n - k}\n\\]p probability success, k observed number successes, n number trials. first term indicates number possible combinations results (e.g., start eight successes, end eight successes, possible combinations), multiplied probability observing one success trials, multiplied probability observing success remaining trials.Let’s assume expect fair coin. binomial probability observing 8 heads 10 coin flips, p = 0.5? answer :\\[\nPr\\left(0.5;8,10 \\right) = \\frac{10!}{8!\\left( 10 - 8 \\right)!}*0.5^{8}*{(1 - 0.5)}^{10 - 8}\n\\]\nR probability computed :using function:Let’s assume don’t information coin. (might believe coins fair; priors discussed talk Bayesian statistics next chapter). equation Pr(k;n,p) gives probability observing k successes n trials coin’s probability success p. Based data observed, can ask question: value p make observed data likely? answer question, can plug values k n find value p maximizes function. Ronald Fisher called maximum likelihood estimation (considered one important developments 20th century statistics, Fisher published first paper 1912 third year undergraduate 22 (Aldrich, 1997)). Since p can value 0 1, can plot values known likelihood function, can see maximum easily.\nFigure 3.1: Binomial likelihood function 8 successes 10 trials.\nlikelihood Pr(k;n,p) plotted possible values p (0 1) surprising given data observed, likely value true parameter 8 10, p = 0.8, likelihood 0.30 (highest point y-axis). example, p = 0.8 called maximum likelihood estimator. important know likelihood meaning isolation. sense, differs probability. can compare likelihoods function across different values p. can read value θ, see given observed data, low values p (e.g., 0.2) likely.Probabilities likelihoods related, different. Note equation Pr involves information data (k, n) information parameter (p). compute probability, view p fixed (instance, fair coin, plug p = 0.5) estimate probability different outcomes (k, n). resulting function probability mass function. compute likelihood, instead view observed data fixed (e.g., observing 5 heads 10 coin tosses), view Pr function p, estimating value maximizes likelihood particular sample.Likelihoods example statistical inference: observed data, use data draw inference different population parameters. formally, likelihood function (joint) density function evaluated observed data. Likelihood functions can calculated many different models (binomial distributions, normal distributions, see Millar (2011)).likelihood curve rises falls , except extremes, 0 heads heads observed. plot likelihood curves 0 heads 10 coin flips, likelihood curve looks like:\nFigure 3.2: Binomial likelihood function 0 successes 10 trials.\nLikelihoods can easily combined. Imagine two people flipping coin independently. One person observes eight heads 10 flips, observes 4 heads 10 flips. might believe give likelihood curve one person flipping coin 20 times, observing 12 heads, indeed, . plot , likelihood curves standardized dividing curve maximum likelihood curve. curves now maximum 1, can easily compare different likelihood curves.\nFigure 3.3: Combining likelihoods.\ncurve left 4 10 heads, one right 8 10 heads. black dotted curve middle 12 20 heads. grey curve, exactly underneath 12 20 heads curve, calculated multiplying likelihood curves: L(p_combined) = L(p = 0.8) * L(p = 0.4).Figure 3.4 see likelihood curves 10, 100, 1000 coin flips, yield 5, 50, 500 heads, respectively. likelihood curves standardized make easily comparable. sample size increases, curves become narrow (dashed line n = 10, dotted line n = 100, solid line n = 1000). means sample size increases, data become increasingly less likely population parameters removed observed number heads. words, collected increasingly strong evidence p = 0.5, compared possible population parameters.\nFigure 3.4: Likelihood function 5/10, 50/100 500/1000 heads coin flips.\ncan use likelihood function compare possible values θ. example, might believe coin flipped fair, even though flipped eight ten heads. fair coin p = 0.5, observed p = 0.8. likelihood function allows us compute relative likelihood different possible parameters. much likely observed data hypothesis unfair coin average turn heads 80% time, compared alternative theory fair coin turn heads 50% time?can calculate likelihood ratio:\\[\n\\frac{L(p = 0.8)}{L(p = 0.5)}\n\\]0.302/0.044 = 6.87. plot, circles show points \nlikelihood curve L(p = 0.5) L(p = 0.8).\nFigure 3.5: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.8.\ncan subjectively interpret likelihood ratio, tells us unfair coin observed data 6.87 times likely hypothesis coin turn heads 80% time, hypothesis fair coin. convincing ? Let’s round likelihood ratio 7, imagine two bags marbles. One bag contains 7 blue marbles. second contains 7 marbles, one different color rainbow, violet, indigo, blue, green, yellow, orange, red. Someone randomly picks one two bags, draws marble, shows . marble blue: certain marble came bag blue marbles, compared bag rainbow coloured marbles? strong likelihood ratio tells us believe data generated unfair coin turns heads 80% time, relative fair coin, given observed 8 heads 10 tosses.Note likelihood ratios give us relative evidence one specified hypothesis, another specified hypothesis. likelihood ratio can calculated two hypothesized values. example, Figure 3.6 , likelihood ratio calculated compares hypothesis fair coin (p = 0.5) alternative hypothesis coin comes heads 80% time (p = 0.8), observed 4 heads 10 coin flips. see observed data 0.2050/0.0055=37.25 times likely (ignoring rounding differences – try calculate numbers hand using formula provided earlier) hypothesis fair coin hypothesis coin turns heads 80% time.\nFigure 3.6: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.4.\nlikelihood ratio 1 means data equally likely hypotheses. Values away 1 indicate data likely one hypothesis . ratio can expressed favor one hypothesis (example L(p = 0.5)/L(p = 0.8) vice versa (L(p = 0.8)/L(p = 0.5). means likelihood ratio 37.25 H0 relative H1 equivalent likelihood ratio 1/37.25 = 0.02685 H1 relative H0. Likelihood ratios range 0 infinity, closer zero infinity, stronger relative evidence one hypothesis . see chapter Bayesian statistics likelihood ratios sense similar (special case ) Bayes Factor.Likelihoods relative evidence. Just data likely one possible value p another value p doesn’t mean data come either two distributions. values might generate even higher likelihood values. example, consider situation flip coin 100 times, observe 50 heads. compare p = 0.3 versus p = 0.8, find likelihood ratio 803462, implying 803461 times evidence data p = 0.3 p = 0.8. might sound pretty conclusive evidence p = 0.3. relative evidence p = 0.3 compared p = 0.8. look likelihood function, clearly see , surprisingly, p = 0.5 value maximizes likelihood function. Just one hypothesis likely another hypothesis, mean third hypothesis even likely.\nFigure 3.7: Computing likelihood ratio p = 0.3 relative p = 0.8 observing p = 0.5 100 coin flips.\n","code":"\nfactorial(10)/(factorial(8)*(factorial(10-8))) * 0.5^8 * (1 - 0.5)^(10-8)\ndbinom(x = 2, size = 10, prob = 0.5)"},{"path":"likelihoods.html","id":"likelihood-of-mixed-results-in-sets-of-studies","chapter":"3 Likelihoods","heading":"3.1 Likelihood of mixed results in sets of studies","text":"Science cumulative process, evaluate lines research, single studies. One big problem scientific literature nonsignificant results often never published (Fanelli, 2010; Franco et al., 2014). time, statistical power hypothesis tests never 100% (often much lower), mathematical reality unlikely (“good true”) set multiple studies yields exclusively significant results. (Francis, 2014; Schimmack, 2012). can use binomial likelihoods examine likely observe mixed results, understand mixed results nevertheless strong evidence presence effect. following largely based Lakens & Etz (2017).probability observing significant nonsignificant result study depends Type 1 error rate (\\(\\alpha\\)), statistical power test (1-\\(\\beta\\)), probability null hypothesis true (Wacholder et al., 2004). four possible outcomes study: true positive, false positive, true negative, false negative. H0 true, probability observing false positive depends \\(\\alpha\\) level Type 1 error rate (e.g., 5%). H1 true, probability observing true positive depends statistical power performed test (often recommended minimum 80%), turn depends \\(\\alpha\\) level, true effect size, sample size. \\(\\alpha\\) level 5%, H0 true, false positive occur 5% probability (long error rates controlled, e.g., preregistered studies) true negative occur 95% probability. test 80% power, H1 true, true positive probability 80%, false negative probability 20%.perform multiple studies, can calculate binomial probability observe specific number significant nonsignificant findings (J. P. Ioannidis & Trikalinos, 2007). can calculate probability finding exactly two significant results three studies assuming null hypothesis true. H0 true, probability significant results equals \\(\\alpha\\) level, thus \\(\\alpha\\) level carefully controlled (e.g., preregistered studies) p = 0.05. k = 2, n = 3, p = .05, binomial probability function tells us probability finding exactly two significant results three studies 0.007 (0.05 × 0.05 × 0.95 = 0.002375, three orders two three results can observed, 0.002375 × 3 = 0.007).calculate likelihood assuming H1 true, need make assumption power study. Let’s provisionally assume studies powered 80% thus p = .80. probability observing exactly two significant results three studies, assuming power 0.8, 0.384 (0.8 × 0.8 × 0.2 = 0.128, three orders two three results can significant, 0.128 × 3 = 0.384). words, set perform 3 studies, hypothesis correct, test hypothesis 80% power, 38.4% probability observing 2 3 significant results, 9.6% probability observe 1 3 significant results (extremely unlucky individual, 0.8% probability finding significant results three studies, even though true effect). Unless power extremely high, mixed results expected sets studies.likelihoods p = .05 p = .80 highlighted Figure 3.8 circles dotted vertical lines.can use likelihood data assuming H0 H1 true calculate likelihood ratio, 0.384/0.007 = 53.89, tells us observed outcome exactly two significant results three studies 53.89 times likely H1 true studies 80% power, H0 true studies carefully controlled 5% Type 1 error rate. Likelihood ratios 8 32 proposed benchmarks moderately strong strong evidence, respectively (Royall, 1997), implies finding two significant results three studies considered strong evidence H1, assuming 80% power. Shiny app perform calculations available .\nFigure 3.8: Computing likelihood ratio 2 three significant results, assuming alpha 5% 80% power.\nsets studies, likelihood ratio favor H1 versus H0 observing mix significant nonsignificant findings can become surprisingly large. Even though evidence appears mixed, actually strong evidence favor true effect. example, researcher performs six studies 80% power 5% level finds three significant outcomes three nonsignificant outcomes, cumulative likelihood ratio convincingly large 38--1 favor H1 consider set studies strong evidence true effect. Intuitively, researchers\nmight feel convinced set studies three six results statistically significant. math, see set studies can strong evidence favor true effect. better understanding probabilities might important step mitigating negative effects publication bias.Hopefully, researchers become inclined submit nonsignificant findings publication better understanding evidential value lines research mixed results. Publishing performed studies lines research reduce publication bias, increase informational value data scientific literature. Expecting studies lines research statistically significant reasonable, important researchers develop realistic expectations draw meaningful inferences lines research. don’t good feeling real patterns studies look like, continuously exposed scientific literature reflect reality. Almost multiple study papers scientific literature present statistically significant results, even though unlikely given power studies, probability study correct predictions (Scheel, Schijen, et al., 2021). Educating researchers binomial probabilities likelihood ratios straightforward way develop realistic expectations research lines contain evidential value favor H1 actually look like.","code":""},{"path":"likelihoods.html","id":"likettest","chapter":"3 Likelihoods","heading":"3.2 Likelihoods for t-tests","text":"far computed likelihoods binomial probabilities, likelihoods can computed statistical model (Glover & Dixon, 2004; Pawitan, 2001). example, can compute relative likelihood observing t-value null alternative hypothesis (Figure 3.9). course, observed data likely assume observed effect equals true effect, examining likelihood reveals many alternative hypotheses relatively likely null hypothesis. also holds observing nonsignificant results, can likely alternative hypothesis interest, null hypothesis. reason incorrect say effect p > \\(\\alpha\\) (see p-value misconception 1).\nFigure 3.9: Likelihood ratio observed t-value H0 H1.\n","code":""},{"path":"likelihoods.html","id":"test-yourself-2","chapter":"3 Likelihoods","heading":"3.3 Test Yourself","text":"Q1: statement correct perform 3 studies?H1 true, alpha = 0.05, power = 0.80, almost likely observe one non-significant results (48.8%) observe significant results (51.2%).alpha = 0.05 power = 0.80, extremely rare find 3 significant results (0.0125%), regardless whether H0 true H1 true.alpha = 0.05 power = 0.80, 2 3 statistically significant results likely outcome overall (38.4%) H1 true.alpha = 0.05 power = 0.80, probability finding least one false positive (significant result H0 true) three studies 5%.Q2: Sometimes lines three studies, ’ll find significant effect one study, effect two related studies. Assume two related studies exactly every way (e.g., changed manipulation, procedure, questions). two studies work minor differences effect fully understand yet. single significant result Type 1 error, H0 true three studies. statement correct, assuming 5% Type 1 error rate 80% power?else equal, probability Type 1 error one three studies 5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies, 80%, substantially likely.else equal, probability Type 1 error one three studies 13.5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect), 9.6%, slightly, substantially less likely.else equal, probability Type 1 error one three studies 85.7% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect) (thus true effect), 0.8%, substantially less likely.possible know probability observe Type 1 error perform 3 studies.idea studies 80% power slightly optimistic. Examine correct answer previous question across range power values (e.g., 50% power, 30% power).Q3: Several papers suggest reasonable assumption power psychological literature might around 50%. Set number studies 4, number successes also 4, assumed power slider 50%, look table bottom app. likely observe 4 significant results 4 studies, assuming true effect?6.25%12.5%25%37.5%Imagine perform 4 studies, 3 show significant result. Change numbers online app. Leave power 50%. output text tells :observed results equally likely H0 H1, likelihood ratio 1. Benchmarks interpret Likelihood Ratios suggest 1<LR<8 weak evidence, 8<LR<32 moderate evidence, LR>32, strong evidence.data likely alternative hypothesis null hypothesis likelihood ratio 526.32These calculations show , assuming observed three significant results four studies, assuming study 50% power, 526 times likely observed data alternative hypothesis true, null hypothesis true. words, 526 times likely find significant effect three studies 50% power, find three Type 1 errors set four studies.Q4: Maybe don’t think 50% power reasonable assumption. low can power (rounded 2 digits), likelihood remain higher 32 favor H1 observing 3 4 significant results?5% power17% power34% power44% powerThe main take home message calculations understand 1) mixed results supposed happen, 2) mixed results can contain strong evidence true effect, across wide range plausible power values. app also tells much evidence, rough dichotomous way, can expect. useful educational goal. want evaluate results multiple studies, formal way performing meta-analysis.calculations make important assumption: Type 1 error rate controlled 5%. try many different tests study, report result yielded p < 0.05, calculations longer hold.Q5: Go back default settings 2 3 significant results, now set Type 1 error rate 20%, reflect modest amount p-hacking. circumstances, highest likelihood favor H1 can get explore possible values true power?Approximately 1Approximately 4.63Approximately 6.70Approximately 62.37As scenario shows, p-hacking makes studies extremely uninformative.\ninflate error rate, quickly destroy evidence data. can longer determine whether data likely effect, effect. Sometimes researchers complain people worry p-hacking try promote better Type 1 error control missing point, things (better measurement, better theory, etc.) important. fully agree aspects scientific research least important better error control. better measures theories require decades work. Better error control can accomplished today, researchers stop inflating error rates flexibly analyzing data. assignment shows, inflated rates false positives quickly make difficult learn true data collect. relative ease part scientific research can improved, can achieve today (decade) think worth stressing importance error control, publish realistic looking sets studies.Q6: ‘prestigious’ journals (, examined terms scientific quality reproducibility, reporting standards, policies concerning data material sharing, quite low quality despite prestige) publish manuscripts large number studies, statistically significant. assume average power psychology 50%, 3.125% 5 study articles contain exclusively significant results. pick random issue prestigious journal, see 10 articles, reporting 5 studies, manuscripts exclusively significant results, trust reported findings , less, articles reported mixed results? ?Q7: Unless power studies 99.99% rest career (slightly inefficient, great don’t like insecurity), observe mixed results lines research. plan deal mixed results lines research?","code":""},{"path":"bayes.html","id":"bayes","chapter":"4 Bayesian statistics","heading":"4 Bayesian statistics","text":"\"Logic!\" said Professor half . \"teach logic schools? three possibilities. Either sister telling lies, mad, telling truth. know tell lies obvious mad. moment unless evidence turns , must assume telling truth.\"Lion, Witch, Wardrobe. Story Children C. S. Lewis.children's book Lion, Witch, Wardrobe, Lucy Edmund go wardrobe country called Narnia. Lucy tells older brother sister, Peter Susan, Narnia, Edmund wants keep secret, tells Peter Susan just pretending. Peter Susan know believe - Narnia exist, ? ask Professor, lives house wardrobe, advice. Professor asks Susan Peter past experience, Lucy Edward truthful, Peter answers \"till now, said Lucy every time.\" Professor replies quote . three possible options, unlikely Lucy lying, done past, Professor says clear just talking Lucy mad. Therefore, likely option Lucy telling truth. new evidence uncovered, beliefs can updated future. approach knowledge generation, prior probability different hypotheses quantified, possible updated light new data, example Bayesian inference.Although frequentist statistics far dominant approach science, important least rudimentary exposure Bayesian statistics statistics training. Bayesian statistics especially useful inferences made cases data investigation unique, frequentist probability defined limit many trials. example, question might often Lucy lies average, whether Lucy lying specific instance existence Narnia. research, often start prior belief hypothesis true. collecting data, can use data update prior beliefs. Bayesian statistics allows update prior beliefs posterior probabilities logically consistent manner. collected data, prior odds Hypothesis 1 (H1) null-hypothesis (H0) P(H1)/P(H0), collected data, posterior odds P(H1|D)/P(H0|D), can read probability H1, given data, divided probability H0, given data. different approaches Bayesian statistics. first discuss Bayes factors, Bayesian estimation.","code":""},{"path":"bayes.html","id":"bayes-factors","chapter":"4 Bayesian statistics","heading":"4.1 Bayes factors","text":"One approach Bayesian statistics focuses comparison different models might explain data (referred model comparison). Bayesian statistics, probability data specified model (D|P(H0) number expressed sometimes referred absolute evidence, formally referred marginal likelihood. marginal likelihood uses prior probabilities average likelihood across parameter space. example, assume simple model M \nbased single parameter, can take two values, X Y, -prior believe probability values p(X) = 0.4 p(Y) = 0.6. collect data, calculate likelihood parameter values, p(D|X) = 0.02 p(D|Y) = 0.08. marginal likelihood model M P(D|M) = 0.4 × 0.02 + 0.6 × 0.08 = 0.056. often, models continuously varying parameters, marginal likelihood formula based integral, idea remains .comparison two models based relative evidence data provides models comparing. relative evidence calculated dividing marginal likelihood one model marginal likelihood another model, ratio relative evidence based marginal likelihoods called Bayes factor. Bayes factors Bayesian equivalent hypothesis tests (Dienes, 2008). Bayes factor represents much updated beliefs, based observing data. can express Bayes factors indicate much likely H1 given data compared H0 (often indicated B10) much likely H0 become compared H1 (B01), B10 = 1/B01. Similar likelihood ratios 1, Bayes factor 1 change beliefs one model compared model. large Bayes factor H1 H0 increased belief H1, Bayes Factor close H1 H0 0 increased belief H0. prior belief H1 , low (e.g., belief unicorns) even large Bayes factor supports presence unicorn might yet convince unicorns real – updated belief unicorns, now believe least likely (even still think unicorns unlikely exist). contribution Bayes Factor prior calculating posterior odds clear following formula:\\[\n\\frac{P(H1|D)}{P(H0|D)} = \\ \\frac{P(D|H1)}{P(D|H0)}\\  \\times \\ \\frac{P(H1)}{P(H0)}\n\\]\\[\nPosterior\\ Probability = \\ Bayes\\ Factor\\  \\times \\ Prior\\ Probability\n\\]Bayesian analysis data requires specifying prior. , continue example based binomial probability, coin flip. likelihood example, compared two point hypotheses (e.g., p = 0.5 vs. p = 0.8). Bayesian statistics, parameters considered random variables, uncertainty degree belief respect parameters quantified probability distributions.binomial probability lies 0 1. draw probability density want 0 1, turn prior, good reasons (simplicity, mostly) beta-prior often used binomial probabilities. shape beta-prior depends two parameters, \\(\\alpha\\) \\(\\beta\\). Note Greek letters used Type 1 error rate Type 2 error rate, purely coincidental! \\(\\alpha\\) \\(\\beta\\) binomial probabilities unrelated error rates, use letters mainly due \nlack creativity among statisticians limited choice alphabet gives us. also help \\(\\beta\\) one parameters Beta distribution. Try keep different Beta’s apart! probability density function :\\[\n\\int_{}^{}{\\left( x,\\ \\alpha,\\ \\beta \\right) = \\ \\frac{1}{B(\\alpha,\\beta)}}x^{\\alpha - 1}{(1 - x)}^{\\beta - 1}\n\\]B(\\(\\alpha\\), \\(\\beta\\)) beta function. Understanding mathematical basis function beyond scope chapter, can read Wikipedia Kruschke's book Bayesian Data Analysis (Kruschke, 2014). beta-prior variety values \\(\\alpha\\) \\(\\beta\\) can seen figure .\nFigure 4.1: Four examples Bayesian priors\nbeta densities reflect different types priors. Let’s assume approached street merchant tries sell special coin heads tails , flipped, almost always turn heads. \\(\\alpha\\) = 1, \\(\\beta\\) = 1 prior newborn baby prior, without idea expect flip coin, thus every value p equally likely. \\(\\alpha\\) = 1, \\(\\beta\\) = 1/2 prior true believer prior. sales merchant tells coin turn heads almost every time, thus, believe turn heads almost every time. \\(\\alpha\\) = 4, \\(\\beta\\) = 4, \\(\\alpha\\) = 100, \\(\\beta\\) = 100 priors slightly extremely skeptical people. \\(\\alpha\\) = 4, \\(\\beta\\) = 4 prior, expect coin fair, willing believe wide range true values possible (curve centered 0.5, curve wide, allowing high low values p). \\(\\alpha\\) = 100, \\(\\beta\\) = 100 prior really convinced coins fair, believe slight bias, (curve centered 0.5, skeptic believes p lie 0.4 0.6 – much narrower range compared slightly skeptic individual).Let’s assume newborn baby, true believer, slightly skeptic extreme skeptic buy coin, flip n = 20 times, observe x = 10 heads. outcome can plotted binomial distribution 10 heads 20 trials, Beta(11, 11) distribution.newborn baby prior Beta distribution \\(\\alpha\\) = 1 \\(\\beta\\) = 1, equals binomial likelihood distribution 0 heads 0 trials. posterior Beta distribution Beta(\\(\\alpha\\)*, \\(\\beta\\)*), :\\(\\alpha\\)* = \\(\\alpha\\) + x = 1 + 10= 11\\(\\beta\\)* = \\(\\beta\\) + n – x = 1 + 20 – 10 = 11Or calculating values directly \\(\\alpha\\) \\(\\beta\\) prior \nlikelihood:\\(\\alpha\\)* = \\(\\alpha\\)prior + \\(\\alpha\\)likelihood – 1 = 1 + 11 - 1= 11\\(\\beta\\)* = \\(\\beta\\)prior + \\(\\beta\\)likelihood - 1 = 1 + 11 – 1 = 11Thus, posterior distribution newborn Beta(11,11) distribution. equals binomial likelihood function 10 heads 20 trials, Beta(11,11) distribution. words, posterior distribution identical likelihood function uniform prior used.Take look Figure . Given 10 heads 20 coin flips, see prior distribution newborn (horizontal grey line), likelihood (blue dotted line) posterior (black line).\nFigure 4.2: Four examples different priors updated based data posterior.\ntrue believer posterior distribution centered maximum likelihood observed data, just bit direction prior. slightly skeptic strong skeptic end much stronger belief fair coin observing data, mainly already stronger prior coin fair.","code":""},{"path":"bayes.html","id":"updating-our-belief","chapter":"4 Bayesian statistics","heading":"4.2 Updating our belief","text":"Now distribution prior, distribution posterior, can see graphs values p belief increased. Everywhere black line (posterior) higher grey line (prior) belief p increased.\nFigure 4.3: Plot prior, likelihood, posterior.\nBayes Factor used quantify increase relative evidence. Let’s calculate Bayes Factor hypothesis coin fair newborn. Bayes Factor simply value posterior distribution p = 0.5, divided value prior distribution p = 0.5:BF10 = Beta(p = 0.5, 11, 11)/Beta(p = 0.5, 1, 1) = 3.70/1 = 3.70You can check online Bayes Factor calculator Jeff Rouder Richard Morey. successes, fill 10, trials, fill 20. want calculate Bayes Factor point null value p = 0.5, fill 0.5. \\(\\alpha\\) \\(\\beta\\) prior 1, given newborns prior Beta(1,1). Clicking ‘submit query’ give Bayes factor 3.70.\nFigure 1.7: Screenshot online calculator binomially distributed observations\ncan calculate plot Bayes Factor, show prior (grey), likelihood (dashed blue) posterior (black). example 20 flips, 10 heads, newborn prior, plot looks like :\nFigure 4.4: Plot prior, likelihood, posterior.\nsee newborn, p = 0.5 become probable, p = 0.4. Now let’s assume strong skeptic, believes coin fair prior Beta(100, 100), buys coin flips 100 times. Surprisingly, coin comes heads 90 100 flips. plot prior, likelihood, posterior now looks much extreme, informed prior, extremely different data. see grey prior distribution, dashed blue likelihood based data, posterior distribution black. Bayes Factor 0 represents substantial drop belief coin fair – indeed, now seems untenable hypothesis, even strong skeptic. shows data can update belief. newborn now completely believe true p coin somewhere around 0.9, strong skeptic reason believe p around 0.65, due strong prior conviction coin fair. Given enough data, even strong skeptic become convinced coin return heads time well.\nFigure 4.5: Plot prior, likelihood, posterior.\ncan now also see difference likelihood inference approach, Bayesian inference approach. likelihood inference, can compare different values p likelihood curve (e.g., p = 0.5 vs p = 0.8) calculate likelihood ratio. Bayesian inference, can compare difference prior posterior value p, calculate Bayes Factor.never seen Bayes Factors , might find difficult interpret numbers. guideline (e.g., interpreting effect sizes small, medium, large) criticism use benchmarks. hand, start somewhere getting feel Bayes Factors mean. Bayes factor 1 3 considered ‘worth bare mention’, larger 3 (smaller 1/3) considered ‘substantial’, larger 10 (smaller 1/10) considered ‘strong’. labels refer increase much believe specific hypothesis, posterior belief hypothesis. think extra-sensory perception extremely implausible, single study BF = 14 increase belief, now think extra-sensory perception pretty much extremely implausible.","code":""},{"path":"bayes.html","id":"bayesest","chapter":"4 Bayesian statistics","heading":"4.3 Bayesian Estimation","text":"posterior distribution summarizes belief expected number heads flipping coin seeing data, averaging prior beliefs data (likelihood). mean Beta distribution can calculated \\(\\alpha\\)/(\\(\\alpha\\)+\\(\\beta\\)). can thus easily calculate mean posterior distribution, expected value based prior beliefs data.can also calculate credible interval around mean, Bayesian version confidence interval slightly different interpretation. Instead Frequentist interpretation parameter one (unknown) true value, Bayesian approach considers data fixed, allow parameter vary. Bayesian approaches, probability distributions represent degree belief. calculating credible interval, one saying ‘believe 95% probable (given prior data) true parameter falls within credible interval’. 95% credible interval simply area posterior distribution 0.025 0.975 quantiles.credible interval confidence interval , uniform prior (e.g., Beta(1,1)) used. case, credible interval numerically identical confidence interval. interpretation differs. Whenever informed prior used, credible interval confidence interval differ. chosen prior representative truth, credible interval representative truth, always correct formalization beliefs. single confidence interval, probability contains true population parameter either 0 1. long run 95% confidence intervals contain true population parameter. important differences Bayesian credible intervals Frequentist confidence intervals keep mind.can plot mean posterior 10 heads 20 coin flips observed, given uniform prior.\nFigure 4.6: Plot mean posterior 10 20 heads observed given uniform prior.\ncan also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood (Albers et al., 2018). chapter shows essence Bayesian inference, decide upon prior distribution, collect data calculate marginal likelihood, use calculate posterior distribution. posterior distribution, can estimate mean 95% credible interval. specific hypothesis, can calculate relative evidence posterior model, compared prior model, Bayes Factor. many different flavors Bayesian statistics, disagreements Bayesians best approach statistical inferences , least great disagreements frequentists Bayesians, many Bayesians dislike Bayes factors (McElreath, 2016). example, Bayesians dislike subjective priors used subjective Bayesian analysis, instead prefer known objective Bayesian analysis (Berger & Bayarri, 2004). research, likely need calculations binomial example used , lot Bayesian tests now available free open source software package JASP. math priors become complex, basic idea remains . can use Bayesian statistics quantify relative evidence, can inform much believe, update beliefs, theories.","code":"\nlibrary(binom)\n\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05"},{"path":"bayes.html","id":"test-yourself-3","chapter":"4 Bayesian statistics","heading":"4.4 Test Yourself","text":"Q1: true believer prior Beta(1,0.5). observing 10 heads 20 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(10, 10)Beta(11, 10.5)Beta(10, 20)Beta(11, 20.5)Q2: strong skeptic prior Beta(100,100). observing 50 heads 100 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(50, 50)Beta(51, 51)Beta(150, 150)Beta(151, 151)Copy R script R. script requires 5 input parameters (identical Bayes Factor calculator website used ). hypothesis want examine (e.g., evaluating whether coin fair, p = 0.5), total number trials (e.g., 20 flips), number successes (e.g., 10 heads), \\(\\alpha\\) \\(\\beta\\) values Beta distribution prior (e.g., \\(\\alpha\\) = 1 \\(\\beta\\) = 1 uniform prior). Run script. calculate Bayes Factor, plot prior (grey), likelihood (dashed blue) posterior (black).see newborn, p = 0.5 become probable, p = 0.4.Q3: Change hypothesis first line 0.5 0.675, run script. testing idea coin returns 67.5% heads, statement true?belief hypothesis, given data, decreased.belief hypothesis, given data, stayed .belief hypothesis, given data, increased.Q4: Change hypothesis first line back 0.5. Let’s look increase belief hypothesis p = 0.5 strong skeptic 10 heads 20 coin flips. Change \\(\\alpha\\) prior line 4 100 \\(\\beta\\) prior line 5 100. Run script. Compare Figure R increase belief newborn (plot previous page). statement true?belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, exactly much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, decreased strong skeptic.Copy R script run . script plot mean posterior 10 heads 20 coin flips observed, given uniform prior (4.6) . script also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood.Q5: Assume outcome 20 coin flips 18 heads. Change x 18 line 2 run script. Remember mean prior Beta(1,1) distribution α/(α+β), 1/(1+1) = 0.5. Frequentist mean simply x/n, 18/20=0.9. statement true?frequentist mean higher mean posterior, mean posterior closer mean prior distribution.frequentist mean lower mean posterior, mean posterior closer mean prior distribution.frequentist mean higher mean posterior, mean posterior mean prior distribution.frequentist mean lower mean posterior, mean posterior mean prior distribution.Q6: , today, best estimate probability sun rises every day? Assume born uniform Beta(1,1) prior. sun can either rise, . Assume seen sun every day since born, means continuous string successes every day alive. ok estimate days alive just multiplying age 365 days. best estimate probability sun rise?Q7: best estimate Frequentist perspective?Q8: think goal science ? Rozeboom (1960) criticized Neyman-Pearson hypothesis testing stating:primary aim scientific experiment precipitate decisions, make appropriate adjustment degree one accepts, believes, hypothesis hypotheses tested\".Frick (1996) argued Rozeboom, stating:Rozeboom (1960) suggested scientists making decisions claims, calculating updating probability claims. However, seem practical. handful potential claims given area psychology, feasible assign probabilities, constantly updating probabilities, expect experimenters keep track ever-changing probabilities. fact, just number claims psychology overwhelming. probably impossible human beings keep track probability claim, especially probabilities constantly changing. case, scientists assign probabilities claims. Instead, scientists act like goal science collect corpus claims considered established (Giere, 1972).comes philosophy science, right wrong answers. Reflect 250 words thoughts two goals science outlines Rozeboom Frick, relate philosophy science.","code":"\nH0 <- 0.5 # Set the point null hypothesis you want to calculate the Bayes Factor for\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) #create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior)\nlikelihood <- dbeta(theta, alikelihood, blikelihood)\nposterior <- dbeta(theta, aposterior, bposterior)\n\n# Create plot\nplot(theta, posterior, ylim = c(0, 15), type = \"l\", lwd = 3, xlab = \"p\", ylab = \"Density\", las = 1)\nlines(theta, prior, col = \"grey\", lwd = 3)\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\")\nBF10 <- dbeta(H0, aposterior, bposterior) / dbeta(H0, aprior, bprior)\npoints(H0, dbeta(H0, aposterior, bposterior), pch = 19)\npoints(H0, dbeta(H0, aprior, bprior), pch = 19, col = \"grey\")\nsegments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty = 2)\ntitle(paste(\"Bayes Factor:\", round(BF10, digits = 2)))\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nymax <- 10 # set max y-axis\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) # create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior) # deterine prior distribution\nlikelihood <- dbeta(theta, alikelihood, blikelihood) # determine likelihood distribution\nposterior <- dbeta(theta, aposterior, bposterior) # determine posterior distribution\nplot(theta, posterior, ylim = c(0, ymax), type = \"l\", lwd = 3, xlab = bquote(theta), ylab = \"Density\", las = 1) # draw posterior distribution\nlines(theta, prior, col = \"grey\", lwd = 3) # draw prior distribution\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\") # draw likelihood distribution\nLL <- qbeta(.025, aposterior, bposterior) # calculate lower limit credible interval\nUL <- qbeta(.975, aposterior, bposterior) # calculate upper limit credible interval\nabline(v = aposterior / (aposterior + bposterior)) # draw line mean\nabline(v = LL, col = \"grey\", lty = 3) # draw line lower limit\nabline(v = UL, col = \"grey\", lty = 3) # draw line upper limit\npolygon(c(theta[theta < LL], rev(theta[theta < LL])), c(posterior[theta < LL], rep(0, sum(theta < LL))), col = \"lightgrey\", border = NA)\npolygon(c(theta[theta > UL], rev(theta[theta > UL])), c(posterior[theta > UL], rep(0, sum(theta > UL))), col = \"lightgrey\", border = NA)\ntitle(paste(\"Mean posterior:\", round((aposterior / (aposterior + bposterior)), digits = 5), \", 95% Credible Interval:\", round(LL, digits = 2), \";\", round(UL, digits = 2)))\nif (!require(binom)) {\n  install.packages(\"binom\")\n}\nlibrary(binom)\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05"},{"path":"questions.html","id":"questions","chapter":"5 Asking Statistical Questions","heading":"5 Asking Statistical Questions","text":"core design new study evaluation information quality: potential particular dataset achieving given analysis goal employing data analysis methods considering given utility\nKenett et al. (2016). goal data collection gain information empirical research observations collected analyzed, often statistical models.Three approaches statistical modelling can distinguished Shmueli (2010): Description, explanation, prediction. Description aims answer questions features empirical manifestation phenomenon.","code":""},{"path":"questions.html","id":"description","chapter":"5 Asking Statistical Questions","heading":"5.1 Description","text":"Description can involve unique events (e.g., case studies single patients), classes events (e.g., patients certain disease). Examples features interest duration (long), quantity (many), location (), etc.example descriptive question research Kinsey, studied sexual behavior experiences Americans time little scientific research available topic. used interviews provided statistical basis draw conclusions sexuality United States, , time, challenged conventional beliefs sexuality.Descriptive research questions answered estimation statistics. informational value estimation study determined amount observations (observations, higher precision estimates) sampling plan (representative sample, lower sample selection bias, increases ability generalize sample population), reliability measure.Descriptive research questions sometimes seen less exciting explanation prediction questions (Gerring, 2012), essential building block theory formation (Scheel, Tiokhin, et al., 2021). Although estimation question often focus mean score measure, accurate estimates variance measure extremely valuable well. variance measure essential information well-informed sample size justification, planning accuracy, performing -priori power analysis.","code":""},{"path":"questions.html","id":"prediction","chapter":"5 Asking Statistical Questions","heading":"5.2 Prediction","text":"goal predictive modeling apply algorithm statistical model predict future observations (Shmueli, 2010). example, COVID-9 pandemic large number models created combined variables estimate risk people infected COVID, people infected experience negative effects health (Wynants et al., 2020). Ideally, goal develop prediction model accurately captures regularities training data, generalizes well unseen data. bias-variance tradeoff two goals, researchers need decide much bias reduced increases variance, vice-versa (Yarkoni & Westfall, 2017). goal prediction minimize prediction error. Common methods evaluate prediction errors cross-validation, example examined whether model developed training dataset generalizes holdout dataset. development prediction models becoming increasingly popular rise machine learning approaches.","code":""},{"path":"questions.html","id":"explanation","chapter":"5 Asking Statistical Questions","heading":"5.3 Explanation","text":"use statistical models concerns tests explanatory theories. case, statistical models used test causal assumptions, explanations derive theories.\nMeehl (1990) reminds us important distinction substantive theory, statistical hypothesis, observations. Statistical inference involved drawing conclusions statistical hypothesis. Observations can lead conclusion statistical hypothesis confirmed (), conclusion directly translate corroboration theory.never test theory isolation, always include auxiliary hypotheses measures instruments used study, conditions realized experiment, ceteris paribus clause assumes things equal. Therefore, never clear failure corroborate theoretical prediction blamed theory auxiliary hypotheses. generate reliable explanatory theories, researchers therefore perform lines research auxiliary hypotheses systematically tested (Tunç & Tunç, 2020).\nFigure 5.1: Distinction theoretical hypothesis, statistical hypothesis, observations. Figure based Meehl, 1990.\n","code":""},{"path":"questions.html","id":"loosening-and-tightening","chapter":"5 Asking Statistical Questions","heading":"5.4 Loosening and Tightening","text":"three questions , can ask questions description, prediction, explanation loosening phase research, tightening phase (Fiedler, 2004). distinction relative. loosening stage, focus creating variation provides source new ideas. tightening stage, selection takes place goal distinguish useful variants less useful variants. descriptive research, unstructured interview aligned loosening phase, structured interview aligned tightening phase. prediction, building prediction model based training set loosening phase, evaluation prediction error holdout dataset tightening phase. explanation, exploratory experimentation functions generate hypotheses, hypothesis tests function distinguish theories make predictions corroborated theories predictions corroborated.important realize whether goal generate new ideas, test new ideas. Researchers often explicit stage research , runs risk trying test hypotheses prematurely (Scheel, Tiokhin, et al., 2021). Clinical trials research explicit different phases research, distinguishes Phase 1, Phase 2, Phase 3, Phase 4 trials. Phase 1 trial researchers evaluate safety new drug intervention small group non-randomized (often healthy) volunteers, examining much drug safe give, monitoring range possible side effects. phase 2 trial often performed patients participants, can focus detail finding definite dose. goal systematically explore range parameters (e.g., intensity stimulus) identify boundary conditions (Dubin, 1969). phase 3 trial large randomized controlled trial goal test effectiveness new intervention practice. Phase 3 trials require prespecified statistical analyses plan strictly controls error rates. Finally, Phase 4 trial examines long term safety generalizability. Compared Phase 3 trial, loosening, researchers explore possibility interactions drugs, moderating effects certain subgroups population. clinical trials, Phase 3 trial requires huge amount preparation, undertaken lightly.\nFigure 5.2: Four phases clinical research. Source.\n","code":""},{"path":"questions.html","id":"three-statistical-philosophies","chapter":"5 Asking Statistical Questions","heading":"5.5 Three statistical philosophies","text":"Royall (1997) distinguishes three questions one can ask:believe, now observation?, now observation?observation tell versus B? (interpret observation evidence regarding versus B?)One useful metaphor think differences look Hinduism, three ways reach enlightenment: Bhakti yoga, Path Devotion, Karma yoga, Path Action, Jnana yoga, Path Knowledge. three corresponding statistical paths Bayesian statistics, focuses updating beliefs, Neyman-Pearson statistics, focuses making decisions act, likelihood approaches, focus quantifying evidence knowledge gained data. Just like Hinduism different paths mutually exclusive, emphasis three yoga's differs individuals, scientists differ emphasis preferred approach statistics.three approaches statistical modelling (description, prediction, explanation) can examined three statistical philosophies (e.g., frequentist estimation, maximum likelihood estimation, Bayesian estimation, Neyman-Pearson hypothesis tests, likelihood ratio tests, Bayes factors). Bayesian approaches start specified prior belief, use data update belief. Frequentist procedures focus methodological procedures allow researchers make inferences control probability error long run. Likelihood approaches focus quantifying evidential value observed data. used knowledgeably, approaches often yield similar inferences (Dongen et al., 2019; Lakens et al., 2020; Tendeiro & Kiers, 2019). Jeffreys (1939), developed Bayesian hypothesis test, noted following comparing Bayesian hypothesis test frequentist methods proposed Fisher:fact struck repeatedly work, led general principles solution problem, find Fisher already grasped essentials brilliant piece common sense, results either identical mine differ cases doubtful. matter fact applied significance tests numerous applications also worked Fisher’s, yet found disagreement actual decisions reached.time, approach based different principles, allows specific inferences. example, Neyman-Pearson approach quantify evidence, Bayesian approach can lead conclusions relative support one another hypothesis, given specified priors, ignoring rate conclusion misleading. Understanding basic principles useful, criticisms statistical practices (e.g., computing p-values) always boil disagreement principles different statistical philosophies built . However, survey literature, rarely see viewpoint approaches statistical inferences, including p values, provide answers specific questions researcher might want ask. Instead, statisticians often engage call statistician’s\nfallacy — declaration believe researchers really “want know” without limiting usefulness preferred statistical question specific context (Lakens, 2021). well-known example statistician’s fallacy provided Cohen (1994) discussing null-hypothesis significance testing:’s wrong NHST? Well, among many things, tell us want know, much want know want know , desperation, nevertheless believe ! want know ‘Given data, probability H0 true?’Different statisticians argue actually \"want know\" posterior probability hypothesis, false-positive risk, effect size confidence interval, likelihood, Bayes factor, severity hypothesis tested. However, choose statistical strategy matches question want ask (Hand, 1994).","code":""},{"path":"questions.html","id":"do-you-really-want-to-test-a-hypothesis","chapter":"5 Asking Statistical Questions","heading":"5.6 Do You Really Want to Test a Hypothesis?","text":"hypothesis test specific answer specific question. can use dart game metaphor question hypothesis test aims answer. essence, dart game hypothesis test methodological procedure make directional prediction: better worse B?, dart game often compare two players, question whether act player best, player B best. hypothesis test, compare two hypotheses, question whether act null hypothesis true, whether alternative hypothesis true.Historically, researchers often interested testing hypotheses examine whether predictions derived scientific theory hold scrutiny. philosophies science () value theories able make predictions. darter wants convince good player, can make prediction (‘next arrow hit bulls-eye’), throw dart, impress hitting bulls-eye. researcher uses theory make prediction, collects data, observes can claim based predefined methodological procedure results confirm prediction, idea impressed predictive validity theory (de Groot, 1969). test supports idea theory useful starting point generate predictions reality. Philosophers science Popper call ‘verisimilitude’– theory way related truth, ‘truth-likeness’.order impressed prediction confirmed, prediction must able wrong. words, theoretical prediction needs falsifiable. predictions concerned presence absence clearly observable entities (e.g., existence black swan) relatively straightforward divide possible states world set predicted theory (e.g., swans white), set predicted theory (e.g., swans can colors white). However, many scientific questions concern probabilistic events single observations contain noise due random variation – rats certain probability develop tumor, people certain probability buy product, particles certain probability appear collision. want forbid certain outcomes test measuring probabilistic events, can divide states world based probability result observed.\nFigure 5.3: fields make black white predictions presence absence obervables, many sciences, predictions probabilistic, shades grey.\nJust hypothesis test can performed, mean interesting. hypothesis test useful 1) data generating models decided plausibility, 2) possible apply informative methodological procedure.First, two competing models good players. Just dart game little interest played Michael van Gerwen (world champion time writing) decide better dart player . Since play darts well, game two us interesting watch. Similarly, sometimes completely uninteresting compare two data generating models, one representing state world effect, another representing state world effect, cases absence effect extremely implausible.Second, hypothesis test interesting need designed informative study. designing study, need able make sure methodological rule provides severe test, likely corroborate prediction correct, time fail corroborate prediction wrong (Mayo, 2018). world champion darts stand 20 inches away dart board can just push dart location want end , possible show lack skill. blindfolded throwing darts 100 feet, possible world champion display skill. hypothesis test, statistical severity test determined error rates. Therefore, researcher needs able adequately control error rates perform test hypothesis high informational value.now hopefully clear hypothesis tests specific tool, answer specific question: applying methodological rule observed data, decision make want make incorrect decisions often? desire use methodological procedure decide competing theories, real reason report results hypothesis test. Even though might feel like test hypothesis research, carefully thinking statistical question want ask might reveal alternative statistical approaches, describing data observed, quantifying personal beliefs hypotheses, reporting relative likelihood data different hypotheses might approach answers question really want know.","code":""},{"path":"effectsizesCI.html","id":"effectsizesCI","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6 Effect Sizes and Confidence Intervals","text":"GRAB INTRO TEXT EFFECT SIZE PAPER. UPDATE EXCERSIS CALCULATE EFFECT SIZES SPREADSHEET MOTE. SPLIT CONFIDENCE INTERVAL SECTION, add BAYESIAN ESTIMATION section confidence intervals","code":""},{"path":"effectsizesCI.html","id":"effect-sizes","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.1 Effect sizes","text":"important outcome empirical study? might tempted say ’s p-value statistical test, given practically always reported articles, determines whether call something ‘significant’ . However, Cohen Cohen (1990) writes 'Things ’ve learned (far)':learned taught primary product research inquiry one measures effect size, p-values.Although want learn data different every study, rarely single thing always want know, effect sizes important part information gain data collection. measure effect size quantitative description strength phenomenon. expressed number scale, scale used depends effect size measure used. unstandardized effect sizes, can use scale people familiar . example, children grow average 6 centimeters year age 2 puberty. can interpret 6 centimeters year effect size. obvious effect size many benefits p-value. p-value gives indication unlikely children stay size become older – effect sizes tell us size clothes can expect children wear certain age, long take new clothes small.Researchers often report standardized effect sizes many psychological variables measured scale people familiar , often measured different scales. ask people happy , answer ‘5’ mean something different asked scale 1 5 asked scale 1 9. Standardized effect sizes allow researchers present magnitude reported effects standardized metric. Therefore, standardized effect sizes can understood compared regardless scale used measure dependent variable. standardized effect sizes allow researchers communicate practical significance results (practical consequences findings daily life), instead reporting statistical significance (surprising data, given assumption effect population).Standardized effect sizes also allow researchers draw meta-analytic conclusions comparing standardized effect sizes across studies. meta-analysis, researchers look results large number studies calculate average effect size across studies draw reliable conclusions. Finally, standardized effect sizes previous studies can used planning new study. -priori power analysis can provide indication average sample size study needs observe statistically significant result desired probability.important make distinction ‘statistically significant’ ‘substantially interesting’. example, might able reliably measure average, men 19 years old grow another 20 millimeters 21. difference might well statistically significant, go shopping clothes 19-year old man, something need think . important way evaluate whether effect substantially interesting look effect size.","code":""},{"path":"effectsizesCI.html","id":"the-facebook-experiment","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.1.1 The Facebook experiment","text":"summer 2014 concerns experiment Facebook performed users examine ‘emotional mood contagion’, idea people’s moods can influenced mood people around . can read article . starters, substantial concern ethical aspects study, primarily researchers performed study asked informed consent participants study (), ask permission institutional review board (ethics committee) university.One criticisms study dangerous influence people’s mood. Nancy J. Smyth, dean University Buffalo’s School Social Work wrote Social Work blog: “might even increased self-harm episodes, control anger, dare say , suicide attempts suicides resulted experimental manipulation. experiment create harm? problem , never know, protections human subjects never put place”.Facebook experiment strong effect people’s mood made people commit suicide otherwise committed suicide, obviously problematic. let us look effects manipulation Facebook used people bit closely.article, let’s see researchers manipulated:Two parallel experiments conducted positive negative emotion: One exposure friends’ positive emotional content News Feed reduced, one exposure negative emotional content News Feed reduced. conditions, person loaded News Feed, posts contained emotional content relevant emotional valence, emotional post 10% 90% chance (based User ID) omitted News Feed specific viewing.measured:experiment, two dependent variables examined pertaining emotionality expressed people’s status updates: percentage words produced given person either positive negative experimental period. total, 3 million posts analyzed, containing 122 million words, 4 million positive (3.6%) 1.8 million negative (1.6%).found:positive posts reduced News Feed, percentage positive words people’s status updates decreased B = −0.1% compared control [t(310,044) = −5.63, P < 0.001, Cohen’s d = 0.02], whereas percentage words negative increased B = 0.04% (t = 2.71, P = 0.007, d = 0.001). Conversely, negative posts reduced, percent words negative decreased B = −0.07% [t(310,541) = −5.51, P < 0.001, d = 0.02] percentage words positive, conversely, increased B = 0.06% (t = 2.19, P < 0.003, d = 0.008)., focus negative effects Facebook study (specifically, increase negative words people used) get idea whether risk increase suicide rates. Even though apparently negative effect, easy get understanding size effect numbers mentioned text. Moreover, number posts researchers analyzed really large. large sample, becomes important check size effect finding substantially interesting, large sample sizes even\nminute differences turn statistically significant (look detail ). , need better understanding “effect sizes”.Now realize effect sizes important, let us look closely commonly used effect sizes, calculated.Effect sizes can grouped two families (Rosenthal et al., 2000): d family (based standardized mean differences) r family (based measures strength association). Conceptually, d family effect sizes based comparison difference observations, divided standard deviation observations. means Cohen’s d = 1 means standardized difference two groups equals one standard deviation. r family effect sizes based proportion variance explained group membership (e.g., correlation r = 0.5 indicates 25% (r2) variance explained difference groups). Don’t worry exactly get means point. crucial issue need understand interpret size effect, different ways express size effect.","code":""},{"path":"effectsizesCI.html","id":"cohend","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.2 Cohen’s d","text":"size effect Facebook study given statistic Cohen’s d (discuss detail ). Cohen’s d (d italicized) used describe standardized mean difference effect. value can used compare effects across studies, even dependent variables measured different ways, example one study uses 7-point scales measure dependent variables, study uses 9-point scales, even completely different measures used, one study uses self-report measures, another study used physiological measurements.Cohen’s d ranges 0 infinity. get statistical details, let’s first visualize Cohen’s d 0.001 (found Facebook study) means.use vizualization http://rpsychologist.com/d3/cohend/, website made Kristoffer Magnusson, allows visualize differences two measurements (increase negative words used Facebook user number positive words timeline reduced).\nFigure 6.1: vizualization 2 groups (although difference hardly visible) representing d = 0.001.\nvizualization website, can read ways interpret Cohen’s d non-mathematical terms (summary provided number people, Facebook study, examining numbers words). says \"Moreover, order one favorable outcome treatment group compared control group, need treat 3570.4 people average.\" means Facebook study person needs type 3570 words 1 word negative instead positive. know often type amount words Facebook, think can agree effect noticeable individual level.illustrates difference statistical difference practical significance (substantial interest). effect small unlikely noticeable single individual. Hence, case, without evidence, worry much extra suicides research caused. Nevertheless, even small effects can matter kinds research. intervention makes people spend money d = 0.001, millions transactions year, small effect might well make lot money.large meta-analytic effort Richard, Bond, Stookes-Zoota (2003) estimated median effect size psychological studies Cohen’s d = 0.43. Let’s use vizualization get feeling effect size.\nFigure 6.2: vizualization 2 groups representing d = 0.43.\nAssume know people likely comply large request initial smaller request, ask large request directly (known foot---door effect), specific context effect size 0.43. Given effect size, likely random person drawn ‘small initial request condition’ likely agree larger request, compared person ‘initial small request’ condition? see Figure 6.2 probability superiority 61.9%.\nFigure 6.3: vizualization 2 groups representing d = 2.\nBased data, difference height 21-year old men women Netherlands approximately 13 centimeters (unstandardized effect size), standardized effect size d = 2. pick random man random woman walking street hometown Rotterdam, likely man taller woman? see quite (92.1%) likely. even huge effect size, much larger effects researchers study, still considerable overlap two distributions. conclude length people one group larger length people another group, mean everyone one group larger everyone group!understand Cohen’s d, let’s first look formula \nt-statistic:\\[\nt = \\frac{{\\overline{M}}_{1}{- \\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}} \\times \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\n\\]\\[{\\overline{M}}_{1}{- \\overline{M}}_{2}\\] difference means, \\[\\text{SD}_{\\text{pooled}}\\] pooled standard deviation (see Lakens, 2013), n1 n2 sample sizes two groups comparing. t-value (follows known distribution) used determine whether difference two groups t-test statistically significant. formula Cohen’s d similar:Cohen’s d = \\[\\frac{{\\overline{M}}_{1}{-\\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}}}\\]can calculate Cohen’s d hand independent samples t-value (can often convenient result section paper reading report effect sizes) :\\[d = t ⨯ \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}\\]can see, sample size part formula t-value, part formula Cohen’s d. Let’s assume difference two means observe 1, pooled standard deviation also 1. , average, happens t-value Cohen’s d, simulate studies, function sample size simulations? Given mean difference standard deviation, sample size becomes\nbigger, t-value become larger, Cohen’s d gets closer true\nvalue. , whereas t-value (corresponding p-value) increase function sample size, Cohen's d becomes accurate. makes p-values function sample size, true effect, means p-values can used make statement whether effect practically significant. Reporting interpreting effect size inform practical significance effect, therefore almost always beneficial report effect sizes alongside statistical test.","code":""},{"path":"effectsizesCI.html","id":"correcting-for-bias","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.2.1 Correcting for Bias","text":"Population effect sizes almost always estimated basis samples, measure population effect size estimate based sample averages, Cohen’s d overestimates true population effect (Cohen’s d refers population, Greek letter δ often used). Therefore, corrections bias used (even though corrections always lead completely unbiased effect size estimate). d family effect sizes, correction bias population effect size estimate Cohen’s δ known Hedges’ g (although different people use different names – d_unbiased also used). correction bias really noticeable small sample sizes, since often use software calculate effect sizes anyway, makes sense always report Hedge’s g instead Cohen’s d.commonly used interpretation Cohen’s d refer effect sizes small (d = 0.2), medium (d = 0.5), large (d = 0.8) based benchmarks suggested Cohen (1988) – note, video talk d = 0.3 small effect size, 0.2 benchmark small effect specified Cohen). However, values arbitrary interpreted rigidly. Furthermore, small effect sizes can large consequences, intervention leads reliable reduction suicide rates effect size d = 0.1. hand, start somewhere getting feeling effect sizes, benchmarks good starting point.interesting, though often used, interpretation differences groups can provided common language effect size (McGraw & Wong, 1992), also known probability superiority. expresses probability randomly sampled person one group higher observed measurement randomly sampled person group (designs) measurement (within-designs) probability individual higher value one measurement . used earlier provided website visualizes Cohen’s d.","code":""},{"path":"effectsizesCI.html","id":"r-correlations","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.3 r (correlations)","text":"second effect size widely used r. might remember r used refer correlation. correlation two continuous variables can range 0 (completely unrelated) 1 (perfect positive relationship) -1 (perfect negative relationship). Obviously, given flexibility human behavior (free lot ) correlations psychological variables rarely 1. median effect size r psychology (estimate worth) .21 (Richard et al., 2003). mentioned earlier, r family effect sizes describe proportion variance explained independent variable, \\(r^2\\).Earlier, mentioned average effect size psychology d = 0.43. might, therefore, think d = 0.43 r = .21 related somehow, :\\(r = \\frac{d_s}{\\sqrt{{d_s^{2}}^{+}\\frac{N^{2} - 2N}{n_{1} \\times n_{2}}}}\\)subscript s underneath Cohen’s d used specify Cohen’s d calculated based sample, based population. almost always case (except simulation studies, can set effect size population), N total sample size groups, whereas n1 n2 sample sizes two groups comparing. can go http://rpsychologist.com/d3/correlation/ look good visualization proportion variance explained group membership, relationship r r2.Effect sizes can implausibly large. Let’s take look study actually examines number suicides – function amount country music played radio. can find paper (free PDF version, click ). won Ig Nobel prize studies first make laugh, think. guess case study make think importance interpreting effect sizes.authors predicted following:contend themes found country music-foster suicidal mood among people already risk suicide thereby associated high suicide rate.collected data:sample comprised 49 large metropolitan areas data music available. Exposure country music measured proportion radio airtime devoted country music. Suicide data extracted annual Mortality Tapes, obtained Inter-University Consortium Political Social Research (ICPSR) University Michigan. dependent variable number suicides per 100,000 population.concluded:significant zero-order correlation found white suicide rates country music (r = .54, p < .05). greater airtime given country music, greater white suicide rate.Cohen (1988) provided benchmarks define small (r = 0.1), medium (r = 0.3), large (r = 0.5) effects. means effect listening country music suicide rates large. Remember preferable relate effect size effects literature instead benchmarks. think likelihood listening country music strongly associated higher suicide rates? country music really bad? Probably - demonstrates importance just reporting, also interpreting, effect size.doubtful possibility effect real, might surprised fact researchers able reproduce analysis original authors. likely results spurious, Type 1 error.Eta squared η² (part r family effect sizes, extension r can used two sets observations) measures proportion variation Y associated membership different groups deﬁned X, sum squares effect divided total sum squares:\\(\\eta^{2}\\) = \\(\\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\\)η² .13 means 13% total variance can accounted group membership. Although η² efficient way compare sizes effects within study (given every effect interpreted relation total variance, η² single study sum 100%), eta squared easily compared studies, total variability study (SStotal) depends design study, increases additional variables manipulated (e.g., independent variables added). Keppel (1991) recommended partial eta squared (\\(\\eta_{p}^{2}\\)) improve comparability\neffect sizes studies, expresses sum squares effect relation sum squares effect sum squares error associated effect. Partial eta squared calculated :\\(\\eta_{p}^{2}\\) = \\(\\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{effect}} + \\text{SS}_{\\text{error}}}\\)designs fixed factors (manipulated factors, factors exhaust levels independent variable, alive vs. dead), designs measured factors covariates, partial eta squared can computed F-value degrees freedom (Cohen, 1988):\\(\\eta_{p}^{2}\\) =\\(\\frac{F \\times \\text{df}_{\\text{effect}}}{{F \\times \\text{df}}_{\\text{effect}} + \\text{df}_{\\text{error}}}\\)example, F(1, 38) = 7.21, \\(\\eta_{p}^{2}\\) = 7.21 ⨯ 1/(7.21 ⨯ 1 +\n38) = 0.16.Eta squared can transformed Cohen’s d:d = 2\\(\\times f\\) \\(f^{2} = \\eta^{2}/(1 - \\eta^{2})\\)Cohen’s d, η² biased estimate true effect size \npopulation. Two less biased effect size estimates proposed, epsilon\nsquared \\(\\varepsilon^{2}\\) omega squared \\(\\omega^{2}\\). \neffect sizes less biased, always better use . Partial epsilon\nsquared partial omega squared can calculated based F-value \ndegrees freedom.\\[\n\\omega_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}} + 1}{\\text{df}_{\\text{effect}}}}\n\\]\\[\n\\varepsilon_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}}}{\\text{df}_{\\text{effect}}}}\n\\]reading effect size estimates, see practical primer \nwritten (Lakens, 2013).","code":""},{"path":"effectsizesCI.html","id":"effect-sizes-and-statistical-power","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.4 Effect Sizes and Statistical Power","text":"Based recent preprint explaining power analysis ANOVA designs, post want provide step--step mathematical overview power analysis interactions. details often make tutorial papers word limitations, good free resources available (paid resource worth money, see Maxwell, Delaney, & Kelley, 2018). post bit technical, nothing post requires knowledge multiplying dividing numbers, believe anyone willing really understand effect sizes power ANOVA designs digging details quite beneficial. three take-home messages post.power analyses ANOVA designs, always think predicted pattern means.Understanding patterns means relate effect predict essential design informative study.Always perform power analysis want test predicted interaction effect, always calculate effect size based means, sd's, correlations, instead plugging 'medium' partial eta squared.Crossover interaction effects large effects can thus studies high power smaller samples, theory can predict crossover interactions, experiments might worthwhile design.additional benefits examining interactions (risky predictions, generalizability, efficiently examining multiple main effects) shame field turned away examining interactions sometimes require large samples.","code":""},{"path":"effectsizesCI.html","id":"getting-started-comparing-two-groups","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.4.1 Getting started: Comparing two groups","text":"planning two independent group experiment. using validated measure, know standard deviation measure approximately 2. interested observing mean difference 1 , smaller effects practically meaningful. expect mean conrol condition 0, therefore want mean intervention group 1 higher.means standardized effect size mean difference, divided standard deviation, 1/2 = 0.5. Cohen's d want able detect study:\\[\\begin{equation}\nd = \\frac{m_1-m_2}{\\sigma} =  \\frac{1-0}{2} = 0.5.\n\\end{equation}\\]independent t-test mathematically identical F-test two groups. F-test, effect size used power analyses Cohen's f, generalization\nCohen’s d two groups (Cohen, 1988). calculated based standard deviation population means divided population standard deviation know measure 2), :\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma}\n\\end{equation}\\]\nequal sample sizes,\n\\[\\begin{equation}\n\\sigma _{ m } = \\sqrt { \\frac { \\sum_ { = 1 } ^ { k } ( m _ { } - m ) ^ { 2 } } { k } }.\n\\end{equation}\\]formula m grand mean, k number means, m_i mean group. formula might look bit daunting, calculating Cohen's f difficult two groups.take expected means 0 1, standard deviation 2, grand mean (m formula ) (0 + 1)/2 = 0.5. formula says subtract grand mean mean group, square value, sum . (0-0.5)^2 (1-0.5)^2, 0.25. sum values (0.25 + 0.25 = 0.5), divide number groups (0.5/2 = 0.25) take square root, find \\(\\sigma_{ m }\\) = 0.5. can now calculate Cohen's f (remember know \\(\\sigma\\) = 2 measure):\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma} = \\frac{0.5}{2} = 0.25\n\\end{equation}\\]see two groups Cohen's f half large Cohen's d, \\(f = \\frac{1}{2}d\\), always holds *F-test two independent groups.Although calculating effect sizes hand obviously incredibly enjoyable thing , might prefer using software performs calculations . , use Superpower power analysis package (developed Aaron Caldwell ). code uses function package computes power analytically one-way ANOVA conditions manipulated participants. addition effect size, function compute power sample size per condition enter. assume friend told heard someone else now need use 50 observations condition (n = 50), plan follow trustworthy advice. see code returns Cohen's f 0.25, also tells us 61.78% power use preregistered alpha level 0.03.therefore might want increase sample size planned study. Using plot_power function, can see pass 90% power 100 observations per condition.","code":"\ndesign <- Superpower::ANOVA_design(\n  design = \"2b\", \n  n = 50, \n  mu = c(1, 0), \n  sd = 2)\nSuperpower::power_oneway_between(design, alpha_level = 0.03)$Cohen_f## [1] 0.25\nSuperpower::power_oneway_between(design, alpha_level = 0.03)$power## [1] 61.78474\nSuperpower::plot_power(design, alpha_level = 0.03, min_n = 45, max_n = 150)$plot_ANOVA## Achieved Power and Sample Size for ANOVA-level effects\n##   variable                  label  n achieved_power desired_power\n## 1        a Desired Power Achieved 97          90.16            90"},{"path":"effectsizesCI.html","id":"interaction-effects","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.4.2 Interaction Effects","text":"far explained basics effect size calculations (looked statistical power) 2 group ANOVA designs. Now basis look interaction effects.One main points blog post better talk interactions ANOVAs terms pattern means, standard deviations, correlations, terms standarized effect size. reason , two groups difference means directly relates Cohen's d, widely different patterns means ANOVA Cohen's f. experience helping colleagues power analyses ANOVA designs, talking effects terms Cohen's f rarely good place start thinking hypothesis predicts. Instead, need specify predicted pattern means, knowledge standard deviation measure, calculate predicted effect size.two types interactions, visualized . ordinal interaction, mean one group (\"B1\") always higher mean group (\"B2\"). Disordinal interactions also known 'cross-' interactions, occur group larger mean switches . difference important, since another main takeaway blog post , two studies largest simple comparison effect size, study disordinal interaction much higher power study ordinal interaction. Thus, possible, want design experiments effect one condition flips around condition, instead experiment effect condition just disappears. personally never realized learned compute power interactions, never took simple important fact account. see important.","code":""},{"path":"effectsizesCI.html","id":"calculating-effect-sizes-for-interactions","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.4.3 Calculating effect sizes for interactions","text":"Mathematically interaction effect computed cell mean minus sum grand mean, marginal mean condition one factor minus grand mean, marginal mean condition factor minus grand mean (see Maxwell et al., 2017).consider two cases, one perfect disordinal interaction (means 0 1 flip around condition, 1 0) ordinal interaction (effect present one condtion, 0 1, disappears condition, means 0 0). can calcuate interaction effect follows. First, look interaction 2x2 matrix:grand mean (1 + 0 + 0 + 1) / 4 = 0.5.can compute marginal means A1, A2, B1, B2, simply averaging per row column, gets us A1 row (1+0)/2=0.5. perfect disordinal interaction, marginal means 0.5. means mean effects. main effect factor (marginal means A1 A2 exactly 0.5), main effect B.can also calculate interaction effect. cell take value cell (e.g., a1b1 1) compute difference cell mean additive effect two factors :1 - (grand mean 0.5 + (marginal mean a1 minus grand mean, 0.5 - 0.5 = 0) + (marginal mean b1 minus grand mean, 0.5 - 0.5 = 0)). Thus, cell get:a1b1: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5a1b2: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b1: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b2: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.5^2 +-0.5^2 + -0.5^2 + 0.5^2 } { 4 } }}{ 2 } = 0.25\\)R code: sqrt(((0.5)^2 +(-0.5)^2 + (-0.5)^2 + (0.5)^2)/4)/2 = 0.25.ordinal interaction grand mean (1 + 0 + 0 + 0) / 4, 0.25. marginal means a1: 0.5, a2: 0, b1: 0.5, b2: 0.Completing calculation four cells ordinal interaction gives:a1b1: 1 - (0.25 + (0.5 -0.25) + (0.5 -0.25)) = 0.25a1b2: 0 - (0.25 + (0.5 -0.25) + (0.0 -0.25)) = -0.25a2b1: 0 - (0.25 + (0.0 -0.25) + (0.5 -0.25)) = -0.25a2b2: 0 - (0.25 + (0.0 -0.25) + (0.0 -0.25)) = 0.25Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.25^2 +-0.25^2 + -0.25^2 + 0.25^2 } { 4 } }}{ 2 } = 0.125\\).R code: sqrt(((0.25)^2 +(-0.25)^2 + (-0.25)^2 + (0.25)^2)/4)/2 = 0.125.see effect size cross-interaction (f = 0.25) twice large effect size ordinal interaction (f = 0.125).math far bit much follow, easier way think effect sizes halved. disordinal interaction comparing cells a1b1 a2b2 a1b2 a2b1, (1+1)/2 vs. (0+0)/2. Thus, see t-test contrast, clear mean difference 1, simple effect started . ordinal interaction, (1+0)/2 vs. (0+0)/2, mean difference halved, namely 0.5.","code":"##    a1 a2\n## b1  1  0\n## b2  0  1"},{"path":"effectsizesCI.html","id":"power-for-interactions","chapter":"6 Effect Sizes and Confidence Intervals","heading":"6.4.4 Power for interactions","text":"obviously matters statistical power examine interaction effects experiments. use Superpower perform power analyses disordinal interaction first, collect 50 participants condition.First look Power Effect size pairwise comparisons. surprisingly, just original t-test, given 50 observations per condition, mean difference either 1, Cohen's d 0.5 (case 61.78% power) mean difference 0, power (true effect) wil observe significant results 3% time set apha level 0.03., look results ANOVA. Since main effects perfect crossover interaction, 3% Type 1 error rate. see power crossover interaction factor b 91.06%. much larger power simple effects. reason contrast equal test interaction based 200 observations. Unlike pairwise comparisons 50 vs 50 observations, contrast interaction 100 vs 100 observations. Given effect size (f = 0.25) end much higher power.heard impossible find statistically significant interaction without huge sample size, clearly see wrong. Power can higher simpe effect - depends pattern means underlying interaction. possible, design studies theory predicts perfect crossover interaction.ordinal interaction, statistical power look good based -priori power analysis. SUperpower tells us 33.99% power main effects interaction (yes, exactly power three - think three contrasts tested, effect size).heard people say careful designing studies predicting interaction patterns might low power, type pattern means warning . Maxwell, Delaney, Kelley (2018) discuss power interactions often smaller, note interactions effects often smaller real world, often examine ordinal interactions. might true. experimental psychology might possile think hypotheses predict disordinal interactions. addition fact predictions often theoretically riskier impressive (, many things can make effect go away, without theory might difficult explain effect flips around) also larger effects easier test high power.years ago blog posts Uri Simonsohn Roger Giner-Sorolla great job warning researchers need large sample sizes ordinal interactions, post repeats warning. shame researchers stop examining interaction effects. nice benefits studying interactions, 1) making riskier theoretical predictions, 2) greater generalizability (interaction effect, might show main effect operates across different conditions second factor) 3) want study two main effects efficient 2x2 design two seperate designs (Maxwell, Delaney, & Kelley, 2018). maybe blog post able highlight scenarios examining interaction effects still beneficial.","code":"\ndesign <- Superpower::ANOVA_design(\n  design = \"2b*2b\", \n  n = 50, \n  mu = c(1, 0, 0, 1), \n  sd = 2)\nSuperpower::ANOVA_exact(design, alpha_level = 0.03)## Power and Effect sizes for ANOVA tests\n##      power partial_eta_squared cohen_f non_centrality\n## a    3.000                0.00  0.0000            0.0\n## b    3.000                0.00  0.0000            0.0\n## a:b 91.055                0.06  0.2525           12.5\n## \n## Power and Effect sizes for pairwise comparisons (t-tests)\n##                       power effect_size\n## p_a_a1_b_b1_a_a1_b_b2 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b1 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b2  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b1  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b2 61.78         0.5\n## p_a_a2_b_b1_a_a2_b_b2 61.78         0.5## Power and Effect sizes for ANOVA tests\n##      power partial_eta_squared cohen_f non_centrality\n## a    3.000                0.00  0.0000            0.0\n## b    3.000                0.00  0.0000            0.0\n## a:b 91.055                0.06  0.2525           12.5\n## \n## Power and Effect sizes for pairwise comparisons (t-tests)\n##                       power effect_size\n## p_a_a1_b_b1_a_a1_b_b2 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b1 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b2  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b1  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b2 61.78         0.5\n## p_a_a2_b_b1_a_a2_b_b2 61.78         0.5\ndesign <- Superpower::ANOVA_design(\n  design = \"2b*2b\", \n  n = 50, \n  mu = c(1, 0, 0, 0), \n  sd = 2)\nSuperpower::ANOVA_exact(design, alpha_level = 0.03)## Power and Effect sizes for ANOVA tests\n##       power partial_eta_squared cohen_f non_centrality\n## a   33.9869              0.0157  0.1263          3.125\n## b   33.9869              0.0157  0.1263          3.125\n## a:b 33.9869              0.0157  0.1263          3.125\n## \n## Power and Effect sizes for pairwise comparisons (t-tests)\n##                       power effect_size\n## p_a_a1_b_b1_a_a1_b_b2 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b1 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b2 61.78        -0.5\n## p_a_a1_b_b2_a_a2_b_b1  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b2  3.00         0.0\n## p_a_a2_b_b1_a_a2_b_b2  3.00         0.0## Power and Effect sizes for ANOVA tests\n##       power partial_eta_squared cohen_f non_centrality\n## a   33.9869              0.0157  0.1263          3.125\n## b   33.9869              0.0157  0.1263          3.125\n## a:b 33.9869              0.0157  0.1263          3.125\n## \n## Power and Effect sizes for pairwise comparisons (t-tests)\n##                       power effect_size\n## p_a_a1_b_b1_a_a1_b_b2 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b1 61.78        -0.5\n## p_a_a1_b_b1_a_a2_b_b2 61.78        -0.5\n## p_a_a1_b_b2_a_a2_b_b1  3.00         0.0\n## p_a_a1_b_b2_a_a2_b_b2  3.00         0.0\n## p_a_a2_b_b1_a_a2_b_b2  3.00         0.0"},{"path":"confint.html","id":"confint","chapter":"7 Confidence Intervals","heading":"7 Confidence Intervals","text":"Kelley Rausch Kelley & Rausch (2006) explain, misleading report point estimates without illustrating uncertainty surrounding estimate. Pretending outcome statistical test final exact answer misleading, always communicate remaining uncertainty report statistical analyses. , examine question detail learning think , calculate, report confidence intervals around estimates samples.","code":""},{"path":"confint.html","id":"population-vs.-samples","chapter":"7 Confidence Intervals","heading":"7.0.1 Population vs. Samples","text":"statistics, differentiate population sample. population everyone interested , people world, elderly depressed, people buy innovative products. sample everyone able measure population interested . similarly distinguish parameter statistic. parameter characteristic population, statistic characteristic sample. Sometimes, data entire population. example, measured height people ever walked moon. \ncan calculate average height twelve individuals, know true parameter. need inferential statistics. However, know average height people ever walked earth. Therefore, need estimate parameter, using statistic based sample.addition goal observing significant difference study (example p < .05), researchers can goal estimating parameter accurately (regardless whether estimate differs null-hypothesis ). Confidence intervals can calculated around statistic data.Confidence intervals statement percentage confidence intervals contain true parameter value. behavior confidence intervals nicely visualized website Kristoffer Magnusson: http://rpsychologist.com/d3/CI/. see blue dots represent means sample, fall around red vertical line, represents true value parameter population. see blue dots always fall exactly red line. illustrates important fact always variation samples.horizontal lines around blue dots confidence intervals. default, visualization shows 95% confidence intervals. lines black, red. fact, long run, 95% horizontal bars black, 5% red.can now see meant sentence “Confidence intervals statement percentage confidence intervals contain true parameter value“. 95% samples, red line (population parameter) contained within 95% confidence interval around sample mean.see turn formulas confidence intervals, sample means confidence intervals depend sample size. larger sample size, smaller confidence intervals.","code":""},{"path":"confint.html","id":"relatCIp","chapter":"7 Confidence Intervals","heading":"7.0.2 The relation between confidence intervals and p-values","text":"direct relationship CI effect size statistical difference 0 effect. example, effect statistically different (p < 0.05) 0 two-sided t-test alpha .05, 95% CI mean difference two groups never include zero. Confidence intervals usually said informative p-values, provide information statistical difference 0 effect also communicate precision effect size estimate. 0 contained confidence interval around mean difference, effect statistically different zero – might false positive, p-value smaller 0.05.Confidence intervals often used forest plots communicate results meta-analysis. plot , see 4 rows. row shows effect size estimate one study (Hedges’ g). example, study 1 yielded effect size estimate 0.44, confidence interval around effect size 0.08 0.8. horizontal black line, similarly visualization played around , width confidence interval. touch effect size 0 (indicated black vertical line) effect statistically significant.can see, based fact confidence intervals overlap 0, studies 1, 2, 4 statistically significant.light blue diamond meta-analytic effect size. Instead using black horizontal line, upper limit lower limit confidence interval indicated left right points diamond. center diamond meta-analytic effect size estimate. meta-analysis calculates effect size combining weighing studies. confidence interval meta-analytic effect size estimate always narrower single study, combined sample size studies included meta-analysis.","code":""},{"path":"confint.html","id":"the-standard-error-and-95-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.0.3 The Standard Error and 95% Confidence Intervals","text":"calculate confidence interval, need standard error. standard error (SE) estimates variability sample means obtained taking several measurements population. easy confuse standard deviation, degree individuals within sample differ sample mean. Formally, statisticians distinguish σ \\(\\widehat{\\sigma}\\), hat means value estimated sample, lack hat means population value – ’ll leave hat, even ’ll mostly talk estimated values based sample formulas . Mathematically (σ standard\ndeviation),Standard Error (SE) = σ/√nThe standard error sample tend zero increasing sample size, estimate population mean become accurate. standard deviation sample become similar population standard deviation sample size increases, become smaller. standard deviation statistic descriptive sample, standard error describes bounds random sampling process.Standard Error used construct confidence intervals (CI) around sample estimates, mean, differences means, whatever statistics might interested . calculate confidence interval around mean (indicated Greek letter mu: μ), use t distribution corresponding degrees freedom (df : one-sample t-test, degrees freedom n-1):μ±tdf, 1-(α/2) × SEWith 95% confidence interval, α = 0.05, thus critical t-value degrees freedom 1- α /2, 0.975th quantile calculated. Remember t-distribution slightly thicker tails Z-distribution. 0.975th quantile Z-distribution 1.96, value t-distribution example df = 19 2.093. value multiplied standard error, added (upper limit confidence interval) subtracted (lower limit confidence\ninterval) mean.","code":""},{"path":"confint.html","id":"overlapping-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.0.4 Overlapping Confidence Intervals","text":"Confidence intervals often used plots. example , see three estimates (dots), surrounded three lines (95% confidence intervals). left two dots (X Y) represent means independent groups X Y scale 0 7 (see axis 0-7 left side plot). dotted lines two confidence intervals visualize overlap confidence intervals around means. two confidence intervals around means columns X Y commonly shown \nfigure scientific article. third dot, slightly larger, difference X Y, slightly thicker line visualizes confidence interval difference. difference score uses axis right (-3 3). plot , mean group X 3.3, mean group Y 5.1, difference 1.8.width confidence interval depends sample size, confidence interval level, standard error, seen . plot left , sample size 50 people group, right, sample size 500 people group. difference width confidence intervals substantial. also clear accurate estimates require large samples.mentioned earlier, 95% confidence interval contain 0, effect statistically different 0. t-test, true confidence interval around effect size, around mean difference, mean difference, standardized mean difference (effect size) directly related significance test. plots , mean difference 95% confidence interval around visible right plot. 95% confidence interval contain 0, t-test significant alpha 0.05. two confidence intervals around \nindividual means can difficult interpret relation whether means differ enough statistically significant. Open CI_Overlap.R, run code. generate plots like one . Run entire script often want (notice variability p-values due relatively low power test!), answer following question. p-value plot tell difference statistically significant, p-value .Q6: much two 95% confidence intervals around individual means \nindependent groups overlap effect just statistically\nsignificant (p ≈ 0.05) alpha 0.05?95% confidence interval around one mean contain mean \ngroup, groups differ significantly .95% confidence interval around one mean contain mean \ngroup, groups differ significantly .95% confidence interval around one mean overlap \n95% confidence interval mean group, groups differ\nsignificantly .95% confidence interval around one mean overlap \n95% confidence interval mean group, groups differ\nsignificantly .overlap two confidence intervals approximately half \none side confidence interval, groups differ significantly \n.overlap two confidence intervals approximately half \none side confidence interval, groups differ significantly \n.relationship overlap 95% confidence intervals\naround two independent means, p-value difference \ngroups.relationship overlap 95% confidence intervals\naround two independent means, p-value difference \ngroups.Note visual overlap rule can used comparison made independent groups, dependent groups! 95% confidence interval around effect sizes therefore typically easily interpretable relation significance test.","code":""},{"path":"confint.html","id":"prediction-intervals","chapter":"7 Confidence Intervals","heading":"7.0.5 Prediction Intervals","text":"Even though 95% future confidence intervals contain true parameter, 95% confidence interval contain 95% future individual observations. Sometimes, researchers want predict interval within single value fall. called prediction interval. always much wider confidence interval. reason individual observations can vary substantially, means future samples (fall within normal confidence interval 95% time) vary much less.Open file CI_mean.R. Run entire script. scripts simulate single sample population mean 100 standard deviation 15, calculate mean (M) standard deviation (sd) sample. black dotted line illustrates true mean. 95% CI contain true mean (100).orange background illustrates 95% confidence interval, calculated manually . lighter yellow background illustrates 95% prediction interval (PI). calculate , need slightly different formula standard error, namely:Standard Error (SE) = σ*√(1+1/N)rewrite formula used confidence interval σ*√(1/N), see difference confidence interval prediction interval “1+” always leads wider intervals. Prediction intervals wider, constructed contain future single value 95% time.","code":""},{"path":"confint.html","id":"capture-percentages","chapter":"7 Confidence Intervals","heading":"7.0.6 Capture Percentages","text":"One thing people find difficult understand 95% confidence interval provide us interval 95% future means fall. % means falls within single confidence interval called capture percentage. 95% confidence interval 95% capture percentage statistic (effect size) observe single sample happens exactly true parameter. situation illustrated picture . observed effect size (dot) falls exactly true effect size (vertical dotted line). case, case, 95% future means fall within 95% confidence interval.However, can’t know whether observed effect size happens exactly population effect size. case (almost never exactly case) less 95% future effect sizes fall within CI current sample. right side figure illustrates . Let’s assume observed effect size much lower true effect size. know effect sizes sample randomly distributed around true effect size. often, find effect size estimates sample fall outside 95% confidence interval single sample happen observed. , percentage future means fall within single confidence interval depends upon single confidence interval happened observe! long run, 95% CI 83.4% capture probability (Cumming & Maillardet, 2006).Let’s experience simulation. simulation R script generates large number additional samples, initial one plotted. simulation returns number CI contains mean (95% long run). simulation also returns % means future studies fall within 95% original study, capture percentage. differs (often lower, sometimes higher, ) confidence interval.Q8: Run simulations multiple times. Look output get R\nconsole. example: “95.077 % 95% confidence intervals contained \ntrue mean” “capture percentage plotted study, % values\nwithin observed confidence interval 88.17208 103.1506 : 82.377 %”.\nrunning simulations multiple times, look confidence interval\naround sample mean, relate capture percentage. \nstatement true?farther sample mean true population mean, lower \ncapture percentage.farther sample mean true population mean, lower \ncapture percentage.farther sample mean true population mean, higher \ncapture percentage.farther sample mean true population mean, higher \ncapture percentage.Q9: Simulations R randomly generated, can make specific\nsimulation reproducible setting seed random generation process.\nCopy-paste “set.seed(1000)” first line R script, run \nsimulation. sample mean 94. capture percentage? (Don’t\nforget remove set.seed command want generate random\nsimulations!).95%95%42.1%42.1%84.3%84.3%89.2%89.2%Capture percentages rarely directly used make statistical inferences. \nmain reason discuss really prevent common\nmisunderstanding 95% future means fall within single confidence\ninterval: Capture percentages clearly show true. Prediction\nintervals also rarely used psychology, common data\nscience.assignment learned important provide measure uncertainty estimates. discussed correct interpretation confidence intervals, meaning prediction intervals, difference confidence interval capture percentage.","code":""},{"path":"confint.html","id":"computing-confidence-intervals-around-effect-sizes","chapter":"7 Confidence Intervals","heading":"7.1 Computing Confidence Intervals around Effect Sizes","text":"","code":""},{"path":"confint.html","id":"mote","chapter":"7 Confidence Intervals","heading":"7.1.1 MOTE","text":"Currently easiest complete solution calculating effect sizes confidence intervals MOTE made Dr. Erin Buchanan lab. website comes full collections tutorials, comparisons software packages, demonstration videos giving incredible accessible overviews compute effect sizes wide range tests. example, video gives overview independent t-testMOTE also available R package (Buchanan et al., 2017). contains many useful functions, ways compute effect sizes summary statistics, provide output can conveniently embedded R Markdown document:Although many solutions exists compute Cohen's d, MOTE sets apart allowing researchers compute effect sizes confidence intervals many additional effect sizes, (partial) omega squared subjects ANOVA (\\(\\omega^{2}\\) \\(\\omega^{2}_p\\)), generalized omega squared ANOVA (\\(\\omega^{2}_G\\)), Epsilon squared ANOVA (\\(\\varepsilon^{2}\\)) (partial) generalized eta squared ANOVA (\\(\\eta^{2}_G\\)), well Hedges' g (bias corrected Cohen's d). want compute effect sizes confidence intervals, first resource try.","code":"\nlibrary(MOTE)\n\nres <- d.ind.t(m1 = 1.7, m2 = 2.1, sd1 = 1.01, sd2 = 0.96, n1 = 77, n2 = 78, a = .05)\nres$statistic## [1] \"$t$(153) = -2.53, $p$ = .013\"\nres$estimate## [1] \"$d_s$ = -0.41, 95\\\\% CI [-0.72, -0.09]\""},{"path":"confint.html","id":"jasp","chapter":"7 Confidence Intervals","heading":"7.1.2 JASP","text":"Free statistical software JAPS strong alternative SPSS (unlike SPSS) allows users compute Cohen's d confidence interval independent dependent ttests.\nFigure 7.1: JASP menu option allows select Cohen's d CI around .\n\nFigure 7.2: JASP output returns Cohen's d confidence interval around .\nJASP also allows compute omega squared \\(\\omega^{2}\\), less biased version \\(\\varepsilon^{2}\\) ","code":""},{"path":"confint.html","id":"esci-software","chapter":"7 Confidence Intervals","heading":"7.1.3 ESCI software","text":"people prefer use ESCI software Geoff Cumming, ESCI also option provide 95% CI around Cohen's d, independent dependent t-tests. However, option slightly hidden - need scroll right, can check box placed view.\nFigure 7.3: ESCI software somewhat hidden option compute 95% CI around Cohen's d within t-tests.\n","code":""},{"path":"confint.html","id":"mbess","chapter":"7 Confidence Intervals","heading":"7.1.4 MBESS","text":"MBESS another R package range options compute effect sizes confidence intervals (Kelley, 2007). code reproduces example MOTE .get confidence interval proportion variance (r², η², partial η²) fixed factor analysis variance need ci.pvaf function. need specify F-value, degrees freedom, sample size, confidence level.within designs, MBESS package returns error. example:error correct -subjects designs (sample size larger degrees freedom) true within-designs (sample size smaller degrees freedom many tests). Thankfully, Ken Kelley (made MBESS package) helped e-mail pointing just use R Code within ci.pvaf function adapt . Just change F-value, confidence level, df.1 df.2.","code":"\nlibrary(MBESS)\n\n# Cohen's d\nsmd(Mean.1 = 1.7, \n    Mean.2 = 2.1, \n    s.1 = 1.01, \n    s.2 = 0.96, \n    n.1 = 77, \n    n.2 = 78)## [1] -0.406028\n# Hedges' g\nsmd(Mean.1 = 1.7, \n    Mean.2 = 2.1, \n    s.1 = 1.01, \n    s.2 = 0.96, \n    n.1 = 77, \n    n.2 = 78, \n    Unbiased = TRUE)## [1] -0.4040338\nci.pvaf(F.value=5.72, df.1=1, df.2=198, N=200, conf.level=.90)## $Lower.Limit.Proportion.of.Variance.Accounted.for\n## [1] 0.002600261\n## \n## $Probability.Less.Lower.Limit\n## [1] 0.05\n## \n## $Upper.Limit.Proportion.of.Variance.Accounted.for\n## [1] 0.07563493\n## \n## $Probability.Greater.Upper.Limit\n## [1] 0.05\n## \n## $Actual.Coverage\n## [1] 0.9\nci.pvaf(F.value = 25.73, df.1 = 2, df.2 = 28, N = 18, conf.level = 0.9)## Error in ci.pvaf(F.value = 25.73, df.1 = 2, df.2 = 28, N = 18, conf.level = 0.9): N must be larger than df.1+df.2\nLims <- conf.limits.ncf(F.value = 7, conf.level = 0.90, df.1 <- 4, df.2 <- 50)\nLower.lim <- Lims$Lower.Limit/(Lims$Lower.Limit + df.1 + df.2 + 1)\nUpper.lim <- Lims$Upper.Limit/(Lims$Upper.Limit + df.1 + df.2 + 1)\nLower.lim## [1] 0.1418798\nUpper.lim## [1] 0.4630716"},{"path":"confint.html","id":"why-should-you-report-90-ci-for-eta-squared","chapter":"7 Confidence Intervals","heading":"7.1.5 Why should you report 90% CI for eta-squared?","text":"see code used 90% CI effect sizes calculated F-test. reason explained Karl Wuensch, leader field applied statistics education, gone way explain clear document, including examples, can find . don’t want read , know Cohen’s d can positive negative, r² η² squared, can therefore positive. related fact F-tests (commonly used ANOVA) one-sided. calculate 95% CI, can get situations confidence interval includes 0, test reveals statistical difference p < .05 (mathematical explanation, see Steiger (2004). means 95% CI around Cohen's d equals 90% CI around η² exactly test.final detail, eta-squared smaller zero, lower bound confidence interval can smaller 0. means confidence interval effect statistically different 0 start 0. report CI 90% CI [.00; .XX] XX upper limit CI.","code":""},{"path":"confint.html","id":"calculating-confidence-intervals-around-standard-deviations.","chapter":"7 Confidence Intervals","heading":"7.2 Calculating Confidence Intervals around Standard Deviations.","text":"calculate standard deviation (SD) sample, value \nestimate true value population. small samples, estimate can\nquite far . due law large numbers, sample size\nincreases, measuring standard deviation accurately. Since\nsample standard deviation estimate uncertainty, can calculate\n95% confidence interval around .Expressing uncertainty estimate standard deviation can \nuseful. researchers plan simulate data, perform -priori power\nanalysis, need accurate estimates standard deviation. \nsimulations, standard deviation needs accurate want \ngenerate data look like real data eventually collect. \npower analyses, often want think smallest effect size \ninterest (SESOI), can specified difference means \ncare . perform power analysis, also need specify expected\nstandard deviation data. Sometimes researchers use pilot data get\nestimate standard deviation. Since estimate population\nstandard deviation based pilot study uncertainty, confidence\nintervals might useful way quantify amount uncertainty.Open R code Confidence_Intervals_for_Standard_Deviations.R calculate\nconfidence interval around standard deviation sample. can also\nuse free GraphPad\ncalculator calculate\nconfidence intervals standard deviations. Run lines 1 3 load pwr\npackage. Lines 5 8 specify alpha level (thus 1-α confidence\ninterval), number observations (n), population standard deviations\n(st_dev), smallest effect size researcher interested (SESOI).\ncode calculates critical values upper lower confidence\ninterval (lines 11 12), calculates confidence interval \nstandard deviation (lines 15 16).Q1: run lines 5 16 see alpha level 0.05,\n100 observations, true standard deviation 1, 95% CI [0.88;\n1.16]. Change assumed population standard deviation 1 2 line\n7. Keep settings . 95% CI around standard\ndeviation 2 100 observations (run lines 5 16)?95% CI [1.38; 3.65]95% CI [1.38; 3.65]95% CI [1.76; 2.32]95% CI [1.76; 2.32]95% CI [1.82; 2.22]95% CI [1.82; 2.22]95% CI [1.84; 2.20]95% CI [1.84; 2.20]Q2: Change assumed population standard deviation back 2 1 \nline 7. Lower sample size 100 20 line 6. inform us\nwidth confidence interval standard deviation run\npilot study 20 observations. Keep settings . \n95% CI around standard deviation 1 20 observations (run lines 5\n16)?95% CI [0.91; 1.11]95% CI [0.91; 1.11]95% CI [0.82; 1.28]95% CI [0.82; 1.28]95% CI [0.76; 1.46]95% CI [0.76; 1.46]95% CI [1.52; 2.92]95% CI [1.52; 2.92]Q3: want 95% CI around standard deviation 1 \n0.05 away assumed population standard deviation, large \nnumber observations ? Note means want 95% CI fall\nwithin 0.95 1.05. notice calculations \ndistribution sample standard deviations symmetrical. Standard\ndeviations can’t smaller 0 (square rooted\nvariance). practice question : smallest number \nobservations upper 95% CI smaller 1.05? Replace n line 6\nvalues.n = 489n = 489n = 498n = 498n = 849n = 849n = 948n = 948Let’s explore consequences inaccurate estimate population\nstandard deviation -priori power analyses. Let’s imagine want \nperform -priori power analysis smallest effect size interest \nhalf scale point (scale 1-5) measure (unknown) true\npopulation standard deviation 1.2.Q4: Change number observations line 6 50. Change \nassumed population standard deviation line 7 1.2. Keep SESOI line\n8 0.5. 95% confidence interval standard deviation based \nsample 50 observation ranges 1.002 1.495 (run lines 5 16 \nconfirm). perform -priori power analysis need calculate Cohen’s d,\ndifference divided standard deviation. example, \nwant least observe difference 0.5. Cohen’s d (SESOI/SD) \nlower bound 95% confidence interval (SD = 1.002) upper\nbound (SD = 1.495)? can perform calculations running lines\n19 21.d = 0.33 d = 0.50d = 0.33 d = 0.50d = 0.40 d = 0.60d = 0.40 d = 0.60d = 0.43 d = 0.57d = 0.43 d = 0.57d = 0.29 d = 0.55d = 0.29 d = 0.55If draw sample 50 observations can happen observe value ,\ndue random variation, much smaller much larger true population\nvalue. can examine effect number observations \nthink required perform -priori power analysis.Q5: -priori power analysis performed uses estimate \nCohen’s d based lower 95% CI standard deviation. statement\ntrue?lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size smaller sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size smaller sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize smaller sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize smaller sample size really need.Q6: Let’s check answer previous question correct. still\nalpha level 0.05, n = 50, standard deviation 1.2, SESOI \n0.5. Run lines 23 24. first power analysis uses Cohen’s d based \nlower bound 95% confidence interval. second power analysis uses \nupper bound 95% confidence interval. (also third power\nanalysis based (real-life situations unknown) true standard deviation,\njust comparison, line 27). statement true?sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 86 using \nupper bound 95% CI around standard deviation.sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 86 using \nupper bound 95% CI around standard deviation.sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 189 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 189 using \nupper bound 95% CI around standard deviation.clear sample sizes -priori power analyses depend strongly \naccurate estimate standard deviation. Keep account estimates\nstandard deviation uncertainty. Use validated existing\nmeasures accurate estimates standard deviation \npopulation interest available, can rely better\nestimate standard deviation power analyses.people argue limited understanding \nmeasures using even know standard deviation \nmeasure population interest, ready use measure \ntest hypothesis. Think back lecture whether really wanted \ntest hypothesis. power analysis realize \nidea standard deviation , maybe first need spend time\nvalidating measures using.performing simulations power analyses cautionary note can \nmade estimates correlations dependent variables. \nestimating values sample, want perform simulations /\npower analyses, aware estimates uncertainty. Try get\nestimates accurate possible pre-existing data. possible,\nbit conservative sample size calculations based estimated\nparameters, just sure.http://mirrors.creativecommons.org/presskit/buttons/88x31/png/-nc-sa.eu.png© Daniel Lakens, 2019. work licensed Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0\nLicense","code":"\n#Install pwr package if needed\nif (!require(pwr)) {install.packages('pwr')}\nlibrary(pwr)\n\nalpha_level <- 0.05 #set alpha level\nn <- 100 #set number of observations\nst_dev <- 1 #set true standard deviation\nSESOI <- 0.5 #set smallest effect size of interest (raw mean difference)\n\n# calculate lower and upper critical values c_l and c_u\nc_l <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = FALSE))\nc_u <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = TRUE))\n\n# calculate lower and upper confidence interval for sd\nst_dev * c_l## [1] 0.8780068\nst_dev * c_u## [1] 1.161675\n# d based on lower bound of the 95CI around the SD\nSESOI/(st_dev * c_l)## [1] 0.5694716\n# d based on upper bound of the 95CI around the SD\nSESOI/(st_dev * c_u)## [1] 0.4304129\npwr.t.test(d = SESOI/(st_dev * c_l), power = 0.9, sig.level = 0.05)## \n##      Two-sample t test power calculation \n## \n##               n = 65.7764\n##               d = 0.5694716\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\npwr.t.test(d = SESOI/(st_dev * c_u), power = 0.9, sig.level = 0.05)## \n##      Two-sample t test power calculation \n## \n##               n = 114.4061\n##               d = 0.4304129\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n# Power analysis for true standard deviation for comparison\npwr.t.test(d = SESOI/st_dev, power = 0.9, sig.level = 0.05)## \n##      Two-sample t test power calculation \n## \n##               n = 85.03128\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group"},{"path":"references.html","id":"references","chapter":"8 References","heading":"8 References","text":"","code":""}]
