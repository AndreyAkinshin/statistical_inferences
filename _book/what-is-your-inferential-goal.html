<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 13 What is Your Inferential Goal? | Improving Your Statistical Inferences</title>

    <meta name="author" content="Daniel Lakens" />
  
   <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 13 What is Your Inferential Goal? | Improving Your Statistical Inferences" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 What is Your Inferential Goal? | Improving Your Statistical Inferences" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.3.1/transition.js"></script>
    <script src="libs/bs3compat-0.3.1/tabs.js"></script>
    <script src="libs/bs3compat-0.3.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
    <link rel="stylesheet" href="bs4_style.css" />
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="what-is-your-inferential-goal" class="section level1" number="13">
<h1><span class="header-section-number">Chapter 13</span> What is Your Inferential Goal?</h1>
<p>The inferential goal of data collection is often in some way related to the size of an effect. Therefore, to design an informative study, researchers will want to think about which effect sizes are interesting. First, it is useful to consider three effect sizes when determining the sample size. The first is the smallest effect size a researcher is interested in, the second is the smallest effect size that can be statistically significant (only in studies where a significance test will be performed), and the third is the effect size that is expected. Beyond considering these three effect sizes, it can be useful to evaluate ranges of effect sizes. This can be done by computing the width of the expected confidence interval around an effect size of interest (for example, an effect size of zero), and examine which effects could be rejected. Similarly, it can be useful to plot a sensitivity curve and evaluate the range of effect sizes the design has decent power to detect, as well as to consider the range of effects for which the design has low power. Finally, there are situations where it is useful to consider range of effects that are likely to be observed in a specific research area.</p>
<div id="what-is-the-smallest-effect-size-of-interest" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> What is the Smallest Effect Size of Interest?</h2>
<p>The strongest possible sample size justification is based on an explicit statement of the smallest effect size that is considered interesting. A smallest effect size of interest can be based on theoretical predictions or practical considerations. For a review of approaches that can be used to determine a smallest effect size of interest in randomized controlled trials, see <span class="citation">Cook et al. (<a href="references.html#ref-cook_assessing_2014" role="doc-biblioref">2014</a>)</span> and <span class="citation">Keefe et al. (<a href="references.html#ref-keefe_defining_2013" role="doc-biblioref">2013</a>)</span>, for reviews of different methods to determine a smallest effect size of interest, see <span class="citation">King (<a href="references.html#ref-king_point_2011" role="doc-biblioref">2011</a>)</span> and <span class="citation">Copay et al. (<a href="references.html#ref-copay_understanding_2007" role="doc-biblioref">2007</a>)</span>, and for a discussion focused on psychological research, see <span class="citation">Lakens, Scheel, et al. (<a href="references.html#ref-lakens_equivalence_2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>It can be challenging to determine the smallest effect size of interest whenever theories are not very developed, or when the research question is far removed from practical applications, but it is still worth thinking about which effects would be too small to matter. A first step forward is to discuss which effect sizes are considered meaningful in a specific research line with your peers. Researchers will differ in the effect sizes they consider large enough to be worthwhile <span class="citation">(<a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>. Just as not every scientist will find every research question interesting enough to study, not every scientist will consider the same effect sizes interesting enough to study, and different stakeholders will differ in which effect sizes are considered meaningful <span class="citation">(<a href="references.html#ref-kelley_effect_2012" role="doc-biblioref">Kelley &amp; Preacher, 2012</a>)</span>.</p>
<p>Even though it might be challenging, there are important benefits of being able to specify a smallest effect size of interest. The population effect size is always uncertain (indeed, estimating this is typically one of the goals of the study), and therefore whenever a study is powered for an expected effect size, there is considerable uncertainty about whether the statistical power is high enough to detect the true effect in the population. However, if the smallest effect size of interest can be specified and agreed upon after careful deliberation, it becomes possible to design a study that has sufficient power (given the inferential goal to detect or reject the smallest effect size of interest with a certain error rate). A smallest effect of interest may be subjective (one researcher might find effect sizes smaller than <em>d</em> = 0.3 meaningless, while another researcher might still be interested in effects larger than <em>d</em> = 0.1), and there might be uncertainty about the parameters required to specify the smallest effect size of interest (e.g., when performing a cost-benefit analysis), but after a smallest effect size of interest has been determined, a study can be designed with a known Type 2 error rate to detect or reject this value. For this reason an a-priori power based on a smallest effect size of interest is generally preferred, whenever researchers are able to specify one <span class="citation">(<a href="references.html#ref-aberson_applied_2019" role="doc-biblioref">Aberson, 2019</a>; <a href="references.html#ref-albers_when_2018" role="doc-biblioref">Albers &amp; Lakens, 2018</a>; <a href="references.html#ref-brown_errors_1983" role="doc-biblioref">Brown, 1983</a>; <a href="references.html#ref-cascio_open_1983" role="doc-biblioref">Cascio &amp; Zedeck, 1983</a>; <a href="references.html#ref-dienes_using_2014" role="doc-biblioref">Dienes, 2014</a>; <a href="references.html#ref-lenth_practical_2001" role="doc-biblioref">Lenth, 2001</a>)</span>.</p>
</div>
<div id="the-minimal-statistically-detectable-effect" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> The Minimal Statistically Detectable Effect</h2>
<p>The minimal statistically detectable effect, or the critical effect size, provides information about the smallest effect size that, if observed, would be statistically significant given a specified alpha level and sample size <span class="citation">(<a href="references.html#ref-cook_assessing_2014" role="doc-biblioref">Cook et al., 2014</a>)</span>. For any critical <em>t</em> value (e.g., <em>t</em> = 1.96 for <span class="math inline">\(\alpha\)</span> = 0.05, for large sample sizes) we can compute a critical mean difference <span class="citation">(<a href="references.html#ref-phillips_statistical_2001" role="doc-biblioref">Phillips et al., 2001</a>)</span>, or a critical standardized effect size. For a two-sided independent <em>t</em> test the critical mean difference is:</p>
<p><span class="math display">\[M_{crit} = t_{crit}{\sqrt{\frac{sd_1^2}{n_1} + \frac{sd_2^2}{n_2}}}\]</span></p>
<p>and the critical standardized mean difference is:</p>
<p><span class="math display">\[d_{crit} = t_{crit}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\]</span></p>
<p>In Figure <a href="what-is-your-inferential-goal.html#fig:power-effect1">13.1</a> the distribution of Cohen’s <em>d</em> is plotted for 15 participants per group when the true effect size is either <em>d</em> = 0 or <em>d</em> = 0.5. This figure is similar to Figure <a href="the-value-of-information.html#fig:power-3">12.2</a>, with the addition that the critical <em>d</em> is indicated. We see that with such a small number of observations in each group only observed effects larger than <em>d</em> = 0.75 will be statistically significant. Whether such effect sizes are interesting, and can realistically be expected, should be carefully considered and justified.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:power-effect1"></span>
<img src="08-samplesizejustification_files/figure-html/power-effect1-1.png" alt="Critical effect size for an independent *t* test with n = 15 per group and $\alpha$ = 0.05." width="100%" />
<p class="caption">
Figure 13.1: Critical effect size for an independent <em>t</em> test with n = 15 per group and <span class="math inline">\(\alpha\)</span> = 0.05.
</p>
</div>
<p>G*Power provides the critical test statistic (such as the critical <em>t</em> value) when performing a power analysis. For example, Figure <a href="what-is-your-inferential-goal.html#fig:gcrit2">13.2</a> shows that for a correlation based on a two-sided test, with <span class="math inline">\(\alpha\)</span> = 0.05, and <em>N</em> = 30, only effects larger than <em>r</em> = 0.361 or smaller than <em>r</em> = -0.361 can be statistically significant. This reveals that when the sample size is relatively small, the observed effect needs to be quite substantial to be statistically significant.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="what-is-your-inferential-goal.html#cb4-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;images/gpowcrit2.png&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gcrit2"></span>
<img src="images/gpowcrit2.png" alt="The critical correlation of a test based on a total sample size of 30 and $\alpha$ = 0.05 calculated in G*Power." width="240" />
<p class="caption">
Figure 13.2: The critical correlation of a test based on a total sample size of 30 and <span class="math inline">\(\alpha\)</span> = 0.05 calculated in G*Power.
</p>
</div>
<p>It is important to realize that due to random variation each study has a probability to yield effects larger than the critical effect size, even if the true effect size is small (or even when the true effect size is 0, in which case each significant effect is a Type I error). Computing a minimal statistically detectable effect is useful for a study where no a-priori power analysis is performed, both for studies in the published literature that do not report a sample size justification <span class="citation">(<a href="references.html#ref-lakens_equivalence_2018" role="doc-biblioref">Lakens, Scheel, et al., 2018</a>)</span>, as for researchers who rely on heuristics for their sample size justification.</p>
<p>It can be informative to ask yourself whether the critical effect size for a study design is within the range of effect sizes that can realistically be expected. If not, then whenever a significant effect is observed in a published study, either the effect size is surprisingly larger than expected, or more likely, it is an upwardly biased effect size estimate. In the latter case, given publication bias, published studies will lead to biased effect size estimates. If it is still possible to increase the sample size, for example by ignoring rules of thumb and instead performing an a-priori power analysis, then do so. If it is not possible to increase the sample size, for example due to resource constraints, then reflecting on the minimal statistically detectable effect should make it clear that an analysis of the data should not focus on <em>p</em> values, but on the effect size and the confidence interval (see Table <a href="the-value-of-information.html#tab:table-pow-rec">12.1</a>).</p>
<p>It is also useful to compute the minimal statistically detectable effect if an 'optimistic' power analysis is performed. For example, if you believe a best case scenario for the true effect size is <em>d</em> = 0.57 and use this optimistic expectation in an a-priori power analysis, effects smaller than <em>d</em> = 0.4 will not be statistically significant when you collect 50 observations in a two independent group design. If your worst case scenario for the alternative hypothesis is a true effect size of <em>d</em> = 0.35 your design would not allow you to declare a significant effect if effect size estimates close to the worst case scenario are observed. Taking into account the minimal statistically detectable effect size should make you reflect on whether a hypothesis test will yield an informative answer, and whether your current approach to sample size justification (e.g., the use of rules of thumb, or letting resource constraints determine the sample size) leads to an informative study, or not.</p>
</div>
<div id="what-is-the-expected-effect-size" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> What is the Expected Effect Size?</h2>
<p>Although the true population effect size is always unknown, there are situations where researchers have a reasonable expectation of the effect size in a study, and want to use this expected effect size in an a-priori power analysis. Even if expectations for the observed effect size are largely a guess, it is always useful to explicitly consider which effect sizes are expected. A researcher can justify a sample size based on the effect size they expect, even if such a study would not be very informative with respect to the smallest effect size of interest. In such cases a study is informative for one inferential goal (testing whether the expected effect size is present or absent), but not highly informative for the second goal (testing whether the smallest effect size of interest is present or absent).</p>
<p>There are typically three sources for expectations about the population effect size: a meta-analysis, a previous study, or a theoretical model. It is tempting for researchers to be overly optimistic about the expected effect size in an a-priori power analysis, as higher effect size estimates yield lower sample sizes, but being too optimistic increases the probability of observing a false negative result. When reviewing a sample size justification based on an a-priori power analysis, it is important to critically evaluate the justification for the expected effect size used in power analyses.</p>
</div>
<div id="using-an-estimate-from-a-meta-analysis" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Using an Estimate from a Meta-Analysis</h2>
<p>In a perfect world effect size estimates from a meta-analysis would provide researchers with the most accurate information about which effect size they could expect. Due to widespread publication bias in science, effect size estimates from meta-analyses are regrettably not always accurate. They can be biased, sometimes substantially so. Furthermore, meta-analyses typically have considerable heterogeneity, which means that the meta-analytic effect size estimate differs for subsets of studies that make up the meta-analysis. So, although it might seem useful to use a meta-analytic effect size estimate of the effect you are studying in your power analysis, you need to take great care before doing so.</p>
<p>If a researcher wants to enter a meta-analytic effect size estimate in an a-priori power analysis, they need to consider three things (see Table <a href="what-is-your-inferential-goal.html#tab:tablemetajust">13.1</a>). First, the studies included in the meta-analysis should be similar enough to the study they are performing that it is reasonable to expect a similar effect size. In essence, this requires evaluating the generalizability of the effect size estimate to the new study. It is important to carefully consider differences between the meta-analyzed studies and the planned study, with respect to the manipulation, the measure, the population, and any other relevant variables.</p>
<p>Second, researchers should check whether the effect sizes reported in the meta-analysis are homogeneous. If not, and there is considerable heterogeneity in the meta-analysis, it means not all included studies can be expected to have the same true effect size estimate. A meta-analytic estimate should be used based on the subset of studies that most closely represent the planned study. Note that heterogeneity remains a possibility (even direct replication studies can show heterogeneity when unmeasured variables moderate the effect size in each sample <span class="citation">(<a href="references.html#ref-kenny_unappreciated_2019" role="doc-biblioref">Kenny &amp; Judd, 2019</a>; <a href="references.html#ref-olsson-collentine_heterogeneity_2020" role="doc-biblioref">Olsson-Collentine et al., 2020</a>)</span>), so the main goal of selecting similar studies is to use existing data to increase the probability that your expectation is accurate, without guaranteeing it will be.</p>
<p>Third, the meta-analytic effect size estimate should not be biased. Check if the bias detection tests that are reported in the meta-analysis are state-of-the-art, or perform multiple bias detection tests yourself <span class="citation">(<a href="references.html#ref-carter_correcting_2019" role="doc-biblioref">Carter et al., 2019</a>)</span>, and consider bias corrected effect size estimates (even though these estimates might still be biased, and do not necessarily reflect the true population effect size).</p>
<table>
<caption>
<span id="tab:tablemetajust">Table 13.1: </span>Overview of recommendations when justifying the use of a meta-analytic effect size estimate for a power analysis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Type of justification
</th>
<th style="text-align:left;">
When is this justification applicable?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Measure entire population
</td>
<td style="text-align:left;">
A researcher can specify the entire population, it is finite, and it is possible
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
to measure (almost) every entity in the population.
</td>
</tr>
<tr>
<td style="text-align:left;">
Resource constraints
</td>
<td style="text-align:left;">
Limited resources are the primary reason for the choice of the sample size
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a researcher can collect.
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy
</td>
<td style="text-align:left;">
The research question focusses on the size of a parameter, and a researcher
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
collects sufficient data to have an estimate with a desired level of accuracy.
</td>
</tr>
<tr>
<td style="text-align:left;">
A-priori power analysis
</td>
<td style="text-align:left;">
The research question has the aim to test whether certain effect sizes can
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
be statistically rejected with a desired statistical power.
</td>
</tr>
<tr>
<td style="text-align:left;">
Heuristics
</td>
<td style="text-align:left;">
A researcher decides upon the sample size based on a heuristic, general rule
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
or norm that is described in the literature, or communicated orally.
</td>
</tr>
<tr>
<td style="text-align:left;">
No justification
</td>
<td style="text-align:left;">
A researcher has no reason to choose a specific sample size, or does not have
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a clearly specified inferential goal and wants to communicate this honestly.
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-an-estimate-from-a-previous-study" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Using an Estimate from a Previous Study</h2>
<p>If a meta-analysis is not available, researchers often rely on an effect size from a previous study in an a-priori power analysis. The first issue that requires careful attention is whether the two studies are sufficiently similar. Just as when using an effect size estimate from a meta-analysis, researchers should consider if there are differences between the studies in terms of the population, the design, the manipulations, the measures, or other factors that should lead one to expect a different effect size. For example, intra-individual reaction time variability increases with age, and therefore a study performed on older participants should expect a smaller standardized effect size than a study performed on younger participants. If an earlier study used a very strong manipulation, and you plan to use a more subtle manipulation, a smaller effect size should be expected. Finally, effect sizes do not generalize to studies with different designs. For example, the effect size for a comparison between two groups is most often not similar to the effect size for an interaction in a follow-up study where a second factor is added to the original design <span class="citation">(<a href="references.html#ref-lakens_simulation-based_2021" role="doc-biblioref">Lakens &amp; Caldwell, 2021</a>)</span>.</p>
<p>Even if a study is sufficiently similar, statisticians have warned against using effect size estimates from small pilot studies in power analyses. Leon, Davis, and Kraemer <span class="citation">(<a href="references.html#ref-leon_role_2011" role="doc-biblioref">2011</a>)</span> write:</p>
<blockquote>
<p>Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples.</p>
</blockquote>
<p>The two main reasons researchers should be careful when using effect sizes from studies in the published literature in power analyses is that effect size estimates from studies can differ from the true population effect size due to random variation, and that publication bias inflates effect sizes. Figure <a href="what-is-your-inferential-goal.html#fig:follow-up-bias">13.3</a> shows the distribution for <span class="math inline">\(\eta_p^2\)</span> for a study with three conditions with 25 participants in each condition when the null hypothesis is true and when there is a 'medium' true effect of <span class="math inline">\(\eta_p^2\)</span> = 0.0588 <span class="citation">(<a href="references.html#ref-richardson_eta_2011" role="doc-biblioref">Richardson, 2011</a>)</span>. As in Figure <a href="what-is-your-inferential-goal.html#fig:power-effect1">13.1</a> the critical effect size is indicated, which shows observed effects smaller than <span class="math inline">\(\eta_p^2\)</span> = 0.08 will not be significant with the given sample size. If the null hypothesis is true effects larger than <span class="math inline">\(\eta_p^2\)</span> = 0.08 will be a Type I error (the dark grey area), and when the alternative hypothesis is true effects smaller than <span class="math inline">\(\eta_p^2\)</span> = 0.08 will be a Type II error (light grey area). It is clear all significant effects are larger than the true effect size (<span class="math inline">\(\eta_p^2\)</span> = 0.0588), so power analyses based on a significant finding (e.g., because only significant results are published in the literature) will be based on an overestimate of the true effect size, introducing bias.</p>
<p>But even if we had access to all effect sizes (e.g., from pilot studies you have performed yourself) due to random variation the observed effect size will sometimes be quite small. Figure <a href="what-is-your-inferential-goal.html#fig:follow-up-bias">13.3</a> shows it is quite likely to observe an effect of <span class="math inline">\(\eta_p^2\)</span> = 0.01 in a small pilot study, even when the true effect size is 0.0588. Entering an effect size estimate of <span class="math inline">\(\eta_p^2\)</span> = 0.01 in an a-priori power analysis would suggest a total sample size of 957 observations to achieve 80% power in a follow-up study. If researchers only follow up on pilot studies when they observe an effect size in the pilot study that, when entered into a power analysis, yields a sample size that is feasible to collect for the follow-up study, these effect size estimates will be upwardly biased, and power in the follow-up study will be systematically lower than desired <span class="citation">(<a href="references.html#ref-albers_when_2018" role="doc-biblioref">Albers &amp; Lakens, 2018</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:follow-up-bias"></span>
<img src="08-samplesizejustification_files/figure-html/follow-up-bias-1.png" alt="Distribution of partial eta squared under the null hypothesis (dotted grey curve) and a medium true effect of 0.0588 (solid black curve) for 3 groups with 25 observations." width="100%" />
<p class="caption">
Figure 13.3: Distribution of partial eta squared under the null hypothesis (dotted grey curve) and a medium true effect of 0.0588 (solid black curve) for 3 groups with 25 observations.
</p>
</div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="what-is-your-inferential-goal.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert eta of 0.176 to f of 0.4621604</span></span>
<span id="cb5-2"><a href="what-is-your-inferential-goal.html#cb5-2" aria-hidden="true" tabindex="-1"></a>pwr_res <span class="ot">&lt;-</span> <span class="fu">pwr.anova.test</span>(<span class="at">k =</span> <span class="dv">3</span>,  </span>
<span id="cb5-3"><a href="what-is-your-inferential-goal.html#cb5-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">f =</span> <span class="fl">0.4621604</span>, </span>
<span id="cb5-4"><a href="what-is-your-inferential-goal.html#cb5-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">sig.level =</span> <span class="fl">0.05</span>, </span>
<span id="cb5-5"><a href="what-is-your-inferential-goal.html#cb5-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">power =</span> <span class="fl">0.8</span>)<span class="sc">$</span>n</span>
<span id="cb5-6"><a href="what-is-your-inferential-goal.html#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="what-is-your-inferential-goal.html#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="what-is-your-inferential-goal.html#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="what-is-your-inferential-goal.html#cb5-9" aria-hidden="true" tabindex="-1"></a>BUCSS_res <span class="ot">&lt;-</span> <span class="fu">ss.power.ba</span>(</span>
<span id="cb5-10"><a href="what-is-your-inferential-goal.html#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">F.observed =</span> <span class="fl">4.5</span>,</span>
<span id="cb5-11"><a href="what-is-your-inferential-goal.html#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="dv">45</span>,</span>
<span id="cb5-12"><a href="what-is-your-inferential-goal.html#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">levels.A =</span> <span class="dv">3</span>,</span>
<span id="cb5-13"><a href="what-is-your-inferential-goal.html#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">effect =</span> <span class="fu">c</span>(<span class="st">&quot;factor.A&quot;</span>),</span>
<span id="cb5-14"><a href="what-is-your-inferential-goal.html#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha.prior =</span> <span class="fl">0.05</span>,</span>
<span id="cb5-15"><a href="what-is-your-inferential-goal.html#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha.planned =</span> <span class="fl">0.05</span>,</span>
<span id="cb5-16"><a href="what-is-your-inferential-goal.html#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">assurance =</span> <span class="fl">0.5</span>,</span>
<span id="cb5-17"><a href="what-is-your-inferential-goal.html#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb5-18"><a href="what-is-your-inferential-goal.html#cb5-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> <span class="fl">0.001</span></span>
<span id="cb5-19"><a href="what-is-your-inferential-goal.html#cb5-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>In essence, the problem with using small studies to estimate the effect size that will be entered into an a-priori power analysis is that due to publication bias or follow-up bias the effect sizes researchers end up using for their power analysis do not come from a full <em>F</em> distribution, but from what is known as a <em>truncated</em> <em>F</em> distribution <span class="citation">(<a href="references.html#ref-taylor_bias_1996" role="doc-biblioref">Taylor &amp; Muller, 1996</a>)</span>. For example, imagine if there is extreme publication bias in the situation illustrated in Figure <a href="what-is-your-inferential-goal.html#fig:follow-up-bias">13.3</a>. The only studies that would be accessible to researchers would come from the part of the distribution where <span class="math inline">\(\eta_p^2\)</span> &gt; 0.08, and the test result would be statistically significant. It is possible to compute an effect size estimate that, based on certain assumptions, corrects for bias. For example, imagine we observe a result in the literature for a One-Way ANOVA with 3 conditions, reported as <em>F</em>(2, 42) = 0.017, <span class="math inline">\(\eta_p^2\)</span> = 0.176. If we would take this effect size at face value and enter it as our effect size estimate in an a-priori power analysis, the suggested sample size to achieve would suggest we need to collect 17 observations in each condition.</p>
<p>However, if we assume bias is present, we can use the <code>BUCSS</code> R package <span class="citation">(<a href="references.html#ref-anderson_sample-size_2017" role="doc-biblioref">S. F. Anderson et al., 2017</a>)</span> to perform a power analysis that attempts to correct for bias. A power analysis that takes bias into account (under a specific model of publication bias, based on a truncated <em>F</em> distribution where only significant results are published) suggests collecting 73 participants in each condition. It is possible that the bias corrected estimate of the non-centrality parameter used to compute power is zero, in which case it is not possible to correct for bias using this method. As an alternative to formally modeling a correction for publication bias whenever researchers assume an effect size estimate is biased, researchers can simply use a more conservative effect size estimate, for example by computing power based on the lower limit of 60% two-sided confidence interval around the effect size estimate, which <span class="citation">Perugini et al. (<a href="references.html#ref-perugini_safeguard_2014" role="doc-biblioref">2014</a>)</span> refer to as <em>safeguard power</em>. Both these approaches lead to a more conservative power analysis, but not necessarily a more accurate power analysis. It is simply not possible to perform an accurate power analysis on the basis of an effect size estimate from a study that might be biased and/or had a small sample size <span class="citation">(<a href="references.html#ref-teare_sample_2014" role="doc-biblioref">Teare et al., 2014</a>)</span>. If it is not possible to specify a smallest effect size of interest, and there is great uncertainty about which effect size to expect, it might be more efficient to perform a study with a sequential design (discussed below).</p>
<p>To summarize, an effect size from a previous study in an a-priori power analysis can be used if three conditions are met (see Table <a href="what-is-your-inferential-goal.html#tab:table-es-just">13.2</a>). First, the previous study is sufficiently similar to the planned study. Second, there was a low risk of bias (e.g., the effect size estimate comes from a Registered Report, or from an analysis for which results would not have impacted the likelihood of publication). Third, the sample size is large enough to yield relatively accurate effect size estimate, based on the width of a 95% CI around the observed effect size estimate. There is always uncertainty around the effect size estimate, and entering the upper and lower limit of the 95% CI around the effect size estimate might be informative about the consequences of the uncertainty in the effect size estimate for an a-priori power analysis.</p>
<table>
<caption>
<span id="tab:table-es-just">Table 13.2: </span>Overview of recommendations when justifying the use of an effect size estimate from a single study.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Type of justification
</th>
<th style="text-align:left;">
When is this justification applicable?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Measure entire population
</td>
<td style="text-align:left;">
A researcher can specify the entire population, it is finite, and it is possible
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
to measure (almost) every entity in the population.
</td>
</tr>
<tr>
<td style="text-align:left;">
Resource constraints
</td>
<td style="text-align:left;">
Limited resources are the primary reason for the choice of the sample size
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a researcher can collect.
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy
</td>
<td style="text-align:left;">
The research question focusses on the size of a parameter, and a researcher
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
collects sufficient data to have an estimate with a desired level of accuracy.
</td>
</tr>
<tr>
<td style="text-align:left;">
A-priori power analysis
</td>
<td style="text-align:left;">
The research question has the aim to test whether certain effect sizes can
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
be statistically rejected with a desired statistical power.
</td>
</tr>
<tr>
<td style="text-align:left;">
Heuristics
</td>
<td style="text-align:left;">
A researcher decides upon the sample size based on a heuristic, general rule
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
or norm that is described in the literature, or communicated orally.
</td>
</tr>
<tr>
<td style="text-align:left;">
No justification
</td>
<td style="text-align:left;">
A researcher has no reason to choose a specific sample size, or does not have
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a clearly specified inferential goal and wants to communicate this honestly.
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-an-estimate-from-a-theoretical-model" class="section level2" number="13.6">
<h2><span class="header-section-number">13.6</span> Using an Estimate from a Theoretical Model</h2>
<p>When your theoretical model is sufficiently specific such that you can build a computational model, and you have knowledge about key parameters in your model that are relevant for the data you plan to collect, it is possible to estimate an effect size based on the effect size estimate derived from a computational model. For example, if one had strong ideas about the weights for each feature stimuli share and differ on, it could be possible to compute predicted similarity judgments for pairs of stimuli based on Tversky's contrast model <span class="citation">(<a href="references.html#ref-tversky_features_1977" role="doc-biblioref">Tversky, 1977</a>)</span>, and estimate the predicted effect size for differences between experimental conditions. Although computational models that make point predictions are relatively rare, whenever they are available, they provide a strong justification of the effect size a researcher expects.</p>
</div>
<div id="compute-the-width-of-the-confidence-interval-around-the-effect-size" class="section level2" number="13.7">
<h2><span class="header-section-number">13.7</span> Compute the Width of the Confidence Interval around the Effect Size</h2>
<p>If a researcher can estimate the standard deviation of the observations that will be collected, it is possible to compute an a-priori estimate of the width of the 95% confidence interval around an effect size <span class="citation">(<a href="references.html#ref-kelley_confidence_2007" role="doc-biblioref">Kelley, 2007</a>)</span>. Confidence intervals represent a range around an estimate that is wide enough so that in the long run the true population parameter will fall inside the confidence intervals 100 - <span class="math inline">\(\alpha\)</span> percent of the time. In any single study the true population effect either falls in the confidence interval, or it doesn't, but in the long run one can <em>act</em> as if the confidence interval includes the true population effect size (while keeping the error rate in mind). Cumming <span class="citation">(<a href="references.html#ref-cumming_understanding_2013" role="doc-biblioref">2013</a>)</span> calls the difference between the observed effect size and the upper 95% confidence interval (or the lower 95% confidence interval) the margin of error.</p>
<p>If we compute the 95% CI for an effect size of <em>d</em> = 0 based on the <em>t</em> statistic and sample size <span class="citation">(<a href="references.html#ref-smithson_confidence_2003" role="doc-biblioref">Smithson, 2003</a>)</span>, we see that with 15 observations in each condition of an independent <em>t</em> test the 95% CI ranges from <em>d</em> = -0.7156777 to <em>d</em> = 0.7156777<a href="references.html#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. The margin of error is half the width of the 95% CI, 0.7156777. A Bayesian estimator who uses an uninformative prior would compute a credible interval with the same (or a very similar) upper and lower bound <span class="citation">(<a href="references.html#ref-albers_credible_2018" role="doc-biblioref">Albers et al., 2018</a>; <a href="references.html#ref-kruschke_bayesian_2011" role="doc-biblioref">Kruschke, 2011</a>)</span>, and might conclude that after collecting the data they would be left with a range of plausible values for the population effect that is too large to be informative. Regardless of the statistical philosophy you plan to rely on when analyzing the data, the evaluation of what we can conclude based on the width of our interval tells us that with 15 observation per group we will not learn a lot.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="what-is-your-inferential-goal.html#cb6-1" aria-hidden="true" tabindex="-1"></a>power_tost <span class="ot">&lt;-</span> TOSTER<span class="sc">::</span><span class="fu">powerTOSTtwo</span>(<span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">N =</span> <span class="dv">50</span>, <span class="at">low_eqbound_d =</span> <span class="sc">-</span><span class="fl">0.6</span>, <span class="at">high_eqbound_d =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<p>One useful way of interpreting the width of the confidence interval is based on the effects you would be able to reject if the true effect size is 0. In other words, if there is no effect, which effects would you have been able to reject given the collected data, and which effect sizes would not be rejected, if there was no effect? Effect sizes in the range of <em>d</em> = 0.7 are findings such as "People become aggressive when they are provoked", "People prefer their own group to other groups", and "Romantic partners resemble one another in physical attractiveness" <span class="citation">(<a href="references.html#ref-richard_one_2003" role="doc-biblioref">Richard et al., 2003</a>)</span>. The width of the confidence interval tells you that you can only reject the presence of effects that are so large, if they existed, you would probably already have noticed them. If it is true that most effects that you study are realistically much smaller than <em>d</em> = 0.7, there is a good possibility that we do not learn anything we didn't already know by performing a study with n = 15. Even without data, in most research lines we would not consider certain large effects plausible (although the effect sizes that are plausible differ between fields, as discussed below). On the other hand, in large samples where researchers can for example reject the presence of effects larger than <em>d</em> = 0.2, if the null hypothesis was true, this analysis of the width of the confidence interval would suggest that peers in many research lines would likely consider the study to be informative.</p>
<p>We see that the margin of error is almost, but not exactly, the same as the minimal statistically detectable effect (<em>d</em> = 0.7479725). The small variation is due to the fact that the 95% confidence interval is calculated based on the <em>t</em> distribution. If the true effect size is not zero, the confidence interval is calculated based on the non-central <em>t</em> distribution, and the 95% CI is asymmetric. Figure <a href="what-is-your-inferential-goal.html#fig:noncentralt">13.4</a> visualizes three <em>t</em> distributions, one symmetric at 0, and two asymmetric distributions with a noncentrality parameter (the normalized difference between the means) of 2 and 3. The asymmetry is most clearly visible in very small samples (the distributions in the plot have 5 degrees of freedom) but remains noticeable in larger samples when calculating confidence intervals and statistical power. For example, for a true effect size of <em>d</em> = 0.5 observed with 15 observations per group would yield <span class="math inline">\(d_s\)</span> = 0.50, 95% CI [-0.23, 1.22]. If we compute the 95% CI around the critical effect size, we would get <span class="math inline">\(d_s\)</span> = 0.75, 95% CI [0.00, 1.48]. We see the 95% CI ranges from exactly -2.9894746^{-8} to 1.4835407, in line with the relation between a confidence interval and a <em>p</em> value, where the 95% CI excludes zero if the test is statistically significant. As noted before, the different approaches recommended here to evaluate how informative a study is are often based on the same information.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:noncentralt"></span>
<img src="08-samplesizejustification_files/figure-html/noncentralt-1.png" alt="Central (black) and 2 non-central (darkgrey and lightgrey) *t* distributions." width="100%" />
<p class="caption">
Figure 13.4: Central (black) and 2 non-central (darkgrey and lightgrey) <em>t</em> distributions.
</p>
</div>
</div>
<div id="plot-a-sensitivity-power-analysis" class="section level2" number="13.8">
<h2><span class="header-section-number">13.8</span> Plot a Sensitivity Power Analysis</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="what-is-your-inferential-goal.html#cb7-1" aria-hidden="true" tabindex="-1"></a>pow_es <span class="ot">&lt;-</span> pwr<span class="sc">::</span><span class="fu">pwr.t.test</span>(</span>
<span id="cb7-2"><a href="what-is-your-inferential-goal.html#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb7-3"><a href="what-is-your-inferential-goal.html#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">15</span>,</span>
<span id="cb7-4"><a href="what-is-your-inferential-goal.html#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb7-5"><a href="what-is-your-inferential-goal.html#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&quot;two.sample&quot;</span>,</span>
<span id="cb7-6"><a href="what-is-your-inferential-goal.html#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>)<span class="sc">$</span>d</span></code></pre></div>
<p>A sensitivity power analysis fixes the sample size, desired power, and alpha level, and answers the question which effect size a study could detect with a desired power. A sensitivity power analysis is therefore performed when the sample size is already known. Sometimes data has already been collected to answer a different research question, or the data is retrieved from an existing database, and you want to perform a sensitivity power analysis for a new statistical analysis. Other times, you might not have carefully considered the sample size when you initially collected the data, and want to reflect on the statistical power of the study for (ranges of) effect sizes of interest when analyzing the results. Finally, it is possible that the sample size will be collected in the future, but you know that due to resource constraints the maximum sample size you can collect is limited, and you want to reflect on whether the study has sufficient power for effects that you consider plausible and interesting (such as the smallest effect size of interest, or the effect size that is expected).</p>
<p>Assume a researcher plans to perform a study where 30 observations will be collected in total, 15 in each between participant condition. Figure <a href="what-is-your-inferential-goal.html#fig:gsens0">13.5</a> shows how to perform a sensitivity power analysis in G*Power for a study where we have decided to use an alpha level of 5%, and desire 90% power. The sensitivity power analysis reveals the designed study has 90% power to detect effects of at least <em>d</em> = 1.23. Perhaps a researcher believes that a desired power of 90% is quite high, and is of the opinion that it would still be interesting to perform a study if the statistical power was lower. It can then be useful to plot a sensitivity curve across a range of smaller effect sizes.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="what-is-your-inferential-goal.html#cb8-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;images/gpow_sensitivity_1.png&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsens0"></span>
<img src="images/gpow_sensitivity_1.png" alt="Sensitivity power analysis in G*Power software." width="240" />
<p class="caption">
Figure 13.5: Sensitivity power analysis in G*Power software.
</p>
</div>
<p>The two dimensions of interest in a sensitivity power analysis are the effect sizes, and the power to observe a significant effect assuming a specific effect size. These two dimensions can be plotted against each other to create a sensitivity curve. For example, a sensitivity curve can be plotted in G*Power by clicking the 'X-Y plot for a range of values' button, as illustrated in Figure <a href="what-is-your-inferential-goal.html#fig:gsens1">13.6</a>. Researchers can examine which power they would have for an a-priori plausible range of effect sizes, or they can examine which effect sizes would provide reasonable levels of power. In simulation-based approaches to power analysis, sensitivity curves can be created by performing the power analysis for a range of possible effect sizes. Even if 50% power is deemed acceptable (in which case deciding to act as if the null hypothesis is true after a non-significant result is a relatively noisy decision procedure), Figure <a href="what-is-your-inferential-goal.html#fig:gsens1">13.6</a> shows a study design where power is extremely low for a large range of effect sizes that are reasonable to expect in most fields. Thus, a sensitivity power analysis provides an additional approach to evaluate how informative the planned study is, and can inform researchers that a specific design is unlikely to yield a significant effect for a range of effects that one might realistically expect.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsens1"></span>
<img src="images/sensitivity1.png" alt="Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05." width="100%" />
<p class="caption">
Figure 13.6: Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05.
</p>
</div>
<p>If the number of observations per group had been larger, the evaluation might have been more positive. We might not have had any specific effect size in mind, but if we had collected 150 observations per group, a sensitivity analysis could have shown that power was sufficient for a range of effects we believe is most interesting to examine, and we would still have approximately 50% power for quite small effects. For a sensitivity analysis to be meaningful, the sensitivity curve should be compared against a smallest effect size of interest, or a range of effect sizes that are expected. A sensitivity power analysis has no clear cut-offs to examine <span class="citation">(<a href="references.html#ref-bacchetti_current_2010" role="doc-biblioref">Bacchetti, 2010</a>)</span>. Instead, the idea is to make a holistic trade-off between different effect sizes one might observe or care about, and their associated statistical power.</p>
<!-- If we look at the effect size that we would have 50% power for, we see it is *d* = 0.7411272. This is very close to our critical effect size of *d* = 0.7479725 (the smallest effect size that, if observed, would be significant). The difference is due to the non-central *t*-distribution (see Figure \@ref(fig:noncentralt). If the distribution was symmetric, observing an effect size exactly on the critical value would mean half of the distribution is smaller than the critical effect size, and half of the distribution is larger, and we would have exactly 50% power for the critical effect size. Because the distribution is not symmetrical, we need to find the critical effect size for which it *is* true that half the distribution falls below it, and half the distribution falls above it. This value can't be calculated directly, and requires an iterative procedure that optimizes the values of the non-centrality parameter such that power is exactly 50% [@smithson_confidence_2003]. The `pwr` package in R can calculate this effect size in a sensitivity analysis where we enter the sample size (per group), the alpha level, and the desired power for a two-sided independent *t* test, which will return *d* = 0.7411272. We can check this true effect size would indeed give with 50% power. -->
<!-- ```{r, echo = TRUE} -->
<!-- # Sensitivity power analysis -->
<!-- pwr::pwr.t.test( -->
<!--   n = 15,  -->
<!--   sig.level = 0.05,  -->
<!--   power = 0.5,  -->
<!--   type = "two.sample", -->
<!--   alternative = "two.sided") -->
<!-- ``` -->
</div>
<div id="the-distribution-of-effect-sizes-in-a-research-area" class="section level2" number="13.9">
<h2><span class="header-section-number">13.9</span> The Distribution of Effect Sizes in a Research Area</h2>
<p>In my personal experience the most commonly entered effect size estimate in an a-priori power analysis for an independent <em>t</em> test is Cohen's benchmark for a 'medium' effect size, because of what is known as the <em>default effect</em>. When you open G*Power, a 'medium' effect is the default option for an a-priori power analysis. Cohen's benchmarks for small, medium, and large effects should not be used in an a-priori power analysis <span class="citation">(<a href="references.html#ref-cook_assessing_2014" role="doc-biblioref">Cook et al., 2014</a>; <a href="references.html#ref-correll_avoid_2020" role="doc-biblioref">Correll et al., 2020</a>)</span>, and Cohen regretted having proposed these benchmarks <span class="citation">(<a href="references.html#ref-funder_evaluating_2019" role="doc-biblioref">Funder &amp; Ozer, 2019</a>)</span>. The large variety in research topics means that any 'default' or 'heuristic' that is used to compute statistical power is not just unlikely to correspond to your actual situation, but it is also likely to lead to a sample size that is more substantially misaligned with the question you are trying to answer with the collected data.</p>
<p>Some researchers have wondered what a better default would be, if researchers have no other basis to decide upon an effect size for an a-priori power analysis. Brysbaert <span class="citation">(<a href="references.html#ref-brysbaert_how_2019-1" role="doc-biblioref">2019</a>)</span> recommends <em>d</em> = 0.4 as a default in psychology, which is the average observed in replication projects and several meta-analyses. It is impossible to know if this average effect size is realistic, but it is clear there is huge heterogeneity across fields and research questions. Any average effect size will often deviate substantially from the effect size that should be expected in a planned study. Some researchers have suggested to change Cohen's benchmarks based on the distribution of effect sizes in a specific field <span class="citation">(<a href="references.html#ref-bosco_correlational_2015" role="doc-biblioref">Bosco et al., 2015</a>; <a href="references.html#ref-funder_evaluating_2019" role="doc-biblioref">Funder &amp; Ozer, 2019</a>; <a href="references.html#ref-hill_empirical_2008" role="doc-biblioref">Hill et al., 2008</a>; <a href="references.html#ref-kraft_interpreting_2020" role="doc-biblioref">Kraft, 2020</a>; <a href="references.html#ref-lovakov_empirically_2017" role="doc-biblioref">Lovakov &amp; Agadullina, 2017</a>)</span>. As always, when effect size estimates are based on the published literature, one needs to evaluate the possibility that the effect size estimates are inflated due to publication bias. Due to the large variation in effect sizes within a specific research area, there is little use in choosing a large, medium, or small effect size benchmark based on the empirical distribution of effect sizes in a field to perform a power analysis.</p>
<p>Having some knowledge about the distribution of effect sizes in the literature can be useful when interpreting the confidence interval around an effect size. If in a specific research area almost no effects are larger than the value you could reject in an equivalence test (e.g., if the observed effect size is 0, the design would only reject effects larger than for example <em>d</em> = 0.7), then it is a-priori unlikely that collecting the data would tell you something you didn't already know.</p>
<p>It is more difficult to defend the use of a specific effect size derived from an empirical distribution of effect sizes as a justification for the effect size used in an a-priori power analysis. One might argue that the use of an effect size benchmark based on the distribution of effects in the literature will outperform a wild guess, but this is not a strong enough argument to form the basis of a sample size justification. There is a point where researchers need to admit they are not ready to perform an a-priori power analysis due to a lack of clear expectations <span class="citation">(<a href="references.html#ref-scheel_why_2020" role="doc-biblioref">Scheel et al., 2020</a>)</span>. Alternative sample size justifications, such as a justification of the sample size based on resource constraints, perhaps in combination with a sequential study design, might be more in line with the actual inferential goals of a study.</p>
</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-02-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
