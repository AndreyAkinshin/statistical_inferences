<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 14 Additional Considerations When Designing an Informative Study | Improving Your Statistical Inferences</title>

    <meta name="author" content="Daniel Lakens" />
  
   <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 14 Additional Considerations When Designing an Informative Study | Improving Your Statistical Inferences" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Additional Considerations When Designing an Informative Study | Improving Your Statistical Inferences" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.3.1/transition.js"></script>
    <script src="libs/bs3compat-0.3.1/tabs.js"></script>
    <script src="libs/bs3compat-0.3.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
    <link rel="stylesheet" href="bs4_style.css" />
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="additional-considerations-when-designing-an-informative-study" class="section level1" number="14">
<h1><span class="header-section-number">Chapter 14</span> Additional Considerations When Designing an Informative Study</h1>
<p>So far, the focus has been on justifying the sample size for quantitative studies. There are a number of related topics that can be useful to design an informative study. First, in addition to a-priori or prospective power analysis and sensitivity power analysis, it is important to discuss compromise power analysis (which is useful) and post-hoc or retrospective power analysis (which is not useful, e.g., <span class="citation">Zumbo &amp; Hubley (<a href="references.html#ref-zumbo_note_1998" role="doc-biblioref">1998</a>)</span>, <span class="citation">Lenth (<a href="references.html#ref-lenth_post_2007" role="doc-biblioref">2007</a>)</span>). When sample sizes are justified based on an a-priori power analysis it can be very efficient to collect data in sequential designs where data collection is continued or terminated based on interim analyses of the data. Furthermore, it is worthwhile to consider ways to increase the power of a test without increasing the sample size. An additional point of attention is to have a good understanding of your dependent variable, especially it's standard deviation. Finally, sample size justification is just as important in qualitative studies, and although there has been much less work on sample size justification in this domain, some proposals exist that researchers can use to design an informative study. Each of these topics is discussed in turn.</p>
<div id="compromise-power-analysis" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> Compromise Power Analysis</h2>
<p>In a compromise power analysis the sample size and the effect are fixed, and the error rates of the test are calculated, based on a desired ratio between the Type I and Type II error rate. A compromise power analysis is useful both when a very large number of observations will be collected, as when only a small number of observations can be collected.</p>
<p>In the first situation a researcher might be fortunate enough to be able to collect so many observations that the statistical power for a test is very high for all effect sizes that are deemed interesting. For example, imagine a researcher has access to 2000 employees who are all required to answer questions during a yearly evaluation in a company where they are testing an intervention that should reduce subjectively reported stress levels. You are quite confident that an effect smaller than <em>d</em> = 0.2 is not large enough to be subjectively noticeable for individuals <span class="citation">(<a href="references.html#ref-jaeschke_measurement_1989" role="doc-biblioref">Jaeschke et al., 1989</a>)</span>. With an alpha level of 0.05 the researcher would have a statistical power of 0.994, or a Type II error rate of 0.006. This means that for a smallest effect size of interest of <em>d</em> = 0.2 the researcher is 8.3 times more likely to make a Type I error than a Type II error.</p>
<p>Although the original idea of designing studies that control Type I and Type II error error rates was that researchers would need to justify their error rates <span class="citation">(<a href="references.html#ref-neyman_problem_1933" role="doc-biblioref">Neyman &amp; Pearson, 1933</a>)</span>, a common heuristic is to set the Type I error rate to 0.05 and the Type II error rate to 0.20, meaning that a Type I error is 4 times as unlikely as a Type II error. The default use of 80% power (or a 20% Type II or <span class="math inline">\(\beta\)</span> error) is based on a personal preference of <span class="citation">Cohen (<a href="references.html#ref-cohen_statistical_1988" role="doc-biblioref">1988</a>)</span>, who writes:</p>
<blockquote>
<p>It is proposed here as a convention that, when the investigator has no other basis for setting the desired power value, the value .80 be used. This means that <span class="math inline">\(\beta\)</span> is set at .20. This arbitrary but reasonable value is offered for several reasons (Cohen, 1965, pp. 98-99). The chief among them takes into consideration the implicit convention for <span class="math inline">\(\alpha\)</span> of .05. The <span class="math inline">\(\beta\)</span> of .20 is chosen with the idea that the general relative seriousness of these two kinds of errors is of the order of .20/.05, i.e., that Type I errors are of the order of four times as serious as Type II errors. This .80 desired power convention is offered with the hope that it will be ignored whenever an investigator can find a basis in his substantive concerns in his specific research investigation to choose a value ad hoc.</p>
</blockquote>
<p>We see that conventions are built on conventions: the norm to aim for 80% power is built on the norm to set the alpha level at 5%. What we should take away from Cohen is not that we should aim for 80% power, but that we should justify our error rates based on the relative seriousness of each error. This is where compromise power analysis comes in. If you share Cohen's belief that a Type I error is 4 times as serious as a Type II error, and building on our earlier study on 2000 employees, it makes sense to adjust the Type I error rate when the Type II error rate is low for all effect sizes of interest <span class="citation">(<a href="references.html#ref-cascio_open_1983" role="doc-biblioref">Cascio &amp; Zedeck, 1983</a>)</span>. Indeed, <span class="citation">Erdfelder et al. (<a href="references.html#ref-erdfelder_gpower_1996" role="doc-biblioref">1996</a>)</span> created the G*Power software in part to give researchers a tool to perform compromise power analysis.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gpowcompromise"></span>
<img src="images/compromise1.png" alt="Compromise power analysis in G*Power." width="240" />
<p class="caption">
Figure 14.1: Compromise power analysis in G*Power.
</p>
</div>
<p>Figure <a href="additional-considerations-when-designing-an-informative-study.html#fig:gpowcompromise">14.1</a> illustrates how a compromise power analysis is performed in G*Power when a Type I error is deemed to be equally costly as a Type II error, which for for a study with 1000 observations per condition would lead to a Type I error and a Type II error of 0.0179. As Faul, Erdfelder, Lang, and Buchner <span class="citation">(<a href="references.html#ref-faul_gpower_2007" role="doc-biblioref">2007</a>)</span> write:</p>
<blockquote>
<p>Of course, compromise power analyses can easily result in unconventional significance levels greater than <span class="math inline">\(\alpha\)</span> = .05 (in the case of small samples or effect sizes) or less than <span class="math inline">\(\alpha\)</span> = .001 (in the case of large samples or effect sizes). However, we believe that the benefit of balanced Type I and Type II error risks often offsets the costs of violating significance level conventions.</p>
</blockquote>
<p>This brings us to the second situation where a compromise power analysis can be useful, which is when we know the statistical power in our study is low. Although it is highly undesirable to make decisions when error rates are high, if one finds oneself in a situation where a decision must be made based on little information, <span class="citation">Winer (<a href="references.html#ref-winer_statistical_1962" role="doc-biblioref">1962</a>)</span> writes:</p>
<blockquote>
<p>The frequent use of the .05 and .01 levels of significance is a matter of convention having little scientific or logical basis. When the power of tests is likely to be low under these levels of significance, and when Type I and Type II errors are of approximately equal importance, the .30 and .20 levels of significance may be more appropriate than the .05 and .01 levels.</p>
</blockquote>
<p>For example, if we plan to perform a two-sided <em>t</em> test, can feasibly collect at most 50 observations in each independent group, and expect a population effect size of 0.5, we would have 70% power if we set our alpha level to 0.05. We can choose to weigh both types of error equally, and set the alpha level to 0.149, to end up with a statistical power for an effect of <em>d</em> = 0.5 of 0.851 (given a 0.149 Type II error rate). The choice of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> in a compromise power analysis can be extended to take prior probabilities of the null and alternative hypothesis into account <span class="citation">(<a href="references.html#ref-maier_justify_2022" role="doc-biblioref">Maier &amp; Lakens, 2022</a>; <a href="references.html#ref-miller_quest_2019" role="doc-biblioref">Miller &amp; Ulrich, 2019</a>; <a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>.</p>
<p>A compromise power analysis requires a researcher to specify the sample size. This sample size itself requires a justification, so a compromise power analysis will typically be performed together with a resource constraint justification for a sample size. It is especially important to perform a compromise power analysis if your resource constraint justification is strongly based on the need to make a decision, in which case a researcher should think carefully about the Type I and Type II error rates stakeholders are willing to accept. However, a compromise power analysis also makes sense if the sample size is very large, but a researcher did not have the freedom to set the sample size. This might happen if, for example, data collection is part of a larger international study and the sample size is based on other research questions. In designs where the Type II error rates is very small (and power is very high) some statisticians have also recommended to lower the alpha level to prevent Lindley's paradox, a situation where a significant effect (<em>p</em> &lt; <span class="math inline">\(\alpha\)</span>) is evidence for the null hypothesis <span class="citation">(<a href="references.html#ref-good_bayesnon-bayes_1992" role="doc-biblioref">Good, 1992</a>; <a href="references.html#ref-jeffreys_theory_1939" role="doc-biblioref">Jeffreys, 1939</a>)</span>. Lowering the alpha level as a function of the statistical power of the test can prevent this paradox, providing another argument for a compromise power analysis when sample sizes are large <span class="citation">(<a href="references.html#ref-maier_justify_2022" role="doc-biblioref">Maier &amp; Lakens, 2022</a>)</span>. Finally, a compromise power analysis needs a justification for the effect size, either based on a smallest effect size of interest or an effect size that is expected. Table <a href="additional-considerations-when-designing-an-informative-study.html#tab:table-compromise-just">14.1</a> lists three aspects that should be discussed alongside a reported compromise power analysis.</p>
<table>
<caption>
<span id="tab:table-compromise-just">Table 14.1: </span>Overview of recommendations when justifying error rates based on a compromise power analysis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Type of justification
</th>
<th style="text-align:left;">
When is this justification applicable?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Measure entire population
</td>
<td style="text-align:left;">
A researcher can specify the entire population, it is finite, and it is possible
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
to measure (almost) every entity in the population.
</td>
</tr>
<tr>
<td style="text-align:left;">
Resource constraints
</td>
<td style="text-align:left;">
Limited resources are the primary reason for the choice of the sample size
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a researcher can collect.
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy
</td>
<td style="text-align:left;">
The research question focusses on the size of a parameter, and a researcher
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
collects sufficient data to have an estimate with a desired level of accuracy.
</td>
</tr>
<tr>
<td style="text-align:left;">
A-priori power analysis
</td>
<td style="text-align:left;">
The research question has the aim to test whether certain effect sizes can
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
be statistically rejected with a desired statistical power.
</td>
</tr>
<tr>
<td style="text-align:left;">
Heuristics
</td>
<td style="text-align:left;">
A researcher decides upon the sample size based on a heuristic, general rule
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
or norm that is described in the literature, or communicated orally.
</td>
</tr>
<tr>
<td style="text-align:left;">
No justification
</td>
<td style="text-align:left;">
A researcher has no reason to choose a specific sample size, or does not have
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
a clearly specified inferential goal and wants to communicate this honestly.
</td>
</tr>
</tbody>
</table>
</div>
<div id="what-to-do-if-your-editor-asks-for-post-hoc-power" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> What to do if Your Editor Asks for Post-hoc Power?</h2>
<p>Post-hoc, retrospective, or observed power is used to describe the statistical power of the test that is computed assuming the effect size that has been estimated from the collected data is the true effect size <span class="citation">(<a href="references.html#ref-lenth_post_2007" role="doc-biblioref">Lenth, 2007</a>; <a href="references.html#ref-zumbo_note_1998" role="doc-biblioref">Zumbo &amp; Hubley, 1998</a>)</span>. Post-hoc power is therefore not performed before looking at the data, based on effect sizes that are deemed interesting, as in an a-priori power analysis, and it is unlike a sensitivity power analysis where a range of interesting effect sizes is evaluated. Because a post-hoc or retrospective power analysis is based on the effect size observed in the data that has been collected, it does not add any information beyond the reported <em>p</em> value, but it presents the same information in a different way. Despite this fact, editors and reviewers often ask authors to perform post-hoc power analysis to interpret non-significant results. This is not a sensible request, and whenever it is made, you should not comply with it. Instead, you should perform a sensitivity power analysis, and discuss the power for the smallest effect size of interest and a realistic range of expected effect sizes.</p>
<p>Post-hoc power is directly related to the <em>p</em> value of the statistical test <span class="citation">(<a href="references.html#ref-hoenig_abuse_2001" role="doc-biblioref">Hoenig &amp; Heisey, 2001</a>)</span>. For a <em>z</em> test where the <em>p</em> value is exactly 0.05, post-hoc power is always 50%. The reason for this relationship is that when a <em>p</em> value is observed that equals the alpha level of the test (e.g., 0.05), the observed <em>z</em> score of the test is exactly equal to the critical value of the test (e.g., <em>z</em> = 1.96 in a two-sided test with a 5% alpha level). Whenever the alternative hypothesis is centered on the critical value half the values we expect to observe if this alternative hypothesis is true fall below the critical value, and half fall above the critical value. Therefore, a test where we observed a <em>p</em> value identical to the alpha level will have exactly 50% power in a post-hoc power analysis, as the analysis assumes the observed effect size is true.</p>
<!-- ```{r obs-power-plot-1, echo = FALSE, fig.width = 8, fig.height = 8, fig.cap="Relationship between p-values and power for a Z-test."} -->
<!-- # For simplicity, take a one-sided test -->
<!-- # compute z value from p: -->
<!-- # p_val = 0.05 -->
<!-- # z <- qnorm(1-p_val) -->
<!-- # z -->
<!-- #  -->
<!-- # # compute p-value from z -->
<!-- # 1-pnorm(z) -->
<!-- # computing the Type I error rate: -->
<!-- # old, two-sided: 1-pnorm(q = 1.959964) + pnorm(q = -1.959964) -->
<!-- # Moving the mean from 0.  -->
<!-- # Because the normal distribution is symmetric, if we observe a p-value on top of the critical value (p = 0.05) we have 50% power. -->
<!-- # pnorm(q = 1.644854, mean = 1.644854) -->
<!-- # We can plot this across a range of observe p-values -->
<!-- plot_obs_pow <- (function(alpha_level, p_val) { -->
<!--   1 - pnorm(q = qnorm(1-alpha_level), mean = qnorm(1-p_val)) -->
<!-- }) -->
<!-- par(bg = "aliceblue", pty = "s") -->
<!-- plot(-10, -->
<!--   xlab = "p-value", ylab = "Observed power", axes = FALSE, -->
<!--   main = substitute(paste("Relationship between p-value and observed power")), xlim = c(0, 1), ylim = c(0, 1) -->
<!-- ) -->
<!-- abline(v = seq(0, 1, 0.1), h = seq(0, 1, 0.1), col = "lightgrey", lty = 1) -->
<!-- axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- axis(side = 2, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- curve(plot_obs_pow(alpha_level = 0.05, p_val = x), 0, 1, add = TRUE, lwd = 3) -->
<!-- points(x = 0.05, y = 0.5, cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5)) -->
<!-- abline(v = 0.05, h = 0.5, col = rgb(1, 0, 0, 0.5), lty = 1) -->
<!-- ``` -->
<p>For other statistical tests, where the alternative distribution is not symmetric (such as for the <em>t</em> test, where the alternative hypothesis follows a non-central <em>t</em> distribution, see Figure <a href="what-is-your-inferential-goal.html#fig:noncentralt">13.4</a>), a <em>p</em> = 0.05 does not directly translate to an observed power of 50%, but by plotting post-hoc power against the observed <em>p</em> value we see that the two statistics are always directly related. As Figure <a href="additional-considerations-when-designing-an-informative-study.html#fig:obs-power-plot-2">14.2</a> shows, if the <em>p</em> value is non-significant (i.e., larger than 0.05) the observed power will be less than approximately 50% in a <em>t</em> test. Lenth <span class="citation">(<a href="references.html#ref-lenth_post_2007" role="doc-biblioref">2007</a>)</span> explains how observed power is also completely determined by the observed <em>p</em> value for <em>F</em> tests, although the statement that a non-significant <em>p</em> value implies a power less than 50% no longer holds.</p>
<!-- We can compute observe power in R based on the formula below, where `alpha_level` is the alpha level, `n`is the sample size in each group, and `p_val` is the observed *p* value. In Figure \@ref(fig:obs-power-plot-2) we see that for a *p* = 0.05 observed power is ever so slightly larger than 50%, but the observed power value is very close.  -->
<!-- ```{r obs_power-plot-2, echo = TRUE, eval = FALSE} -->
<!-- 1 - pt(q = qt(1 - alpha_level/2, 2 * n - 2),  -->
<!--        df = 2 * n - 2,  -->
<!--        ncp = qt(1 - p_val/2, 2 * n - 2)) -->
<!-- ``` -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:obs-power-plot-2"></span>
<img src="08-samplesizejustification_files/figure-html/obs-power-plot-2-1.png" alt="Relationship between *p* values and power for an independent *t* test with $\alpha$ = 0.05 and n = 10." width="100%" />
<p class="caption">
Figure 14.2: Relationship between <em>p</em> values and power for an independent <em>t</em> test with <span class="math inline">\(\alpha\)</span> = 0.05 and n = 10.
</p>
</div>
<p>When editors or reviewers ask researchers to report post-hoc power analyses they would like to be able to distinguish between true negatives (concluding there is no effect, when there is no effect) and false negatives (a Type II error, concluding there is no effect, when there actually is an effect). Since reporting post-hoc power is just a different way of reporting the <em>p</em> value, reporting the post-hoc power will not provide an answer to the question editors are asking <span class="citation">(<a href="references.html#ref-hoenig_abuse_2001" role="doc-biblioref">Hoenig &amp; Heisey, 2001</a>; <a href="references.html#ref-lenth_post_2007" role="doc-biblioref">Lenth, 2007</a>; <a href="references.html#ref-schulz_sample_2005" role="doc-biblioref">Schulz &amp; Grimes, 2005</a>; <a href="references.html#ref-yuan_post_2005" role="doc-biblioref">Yuan &amp; Maxwell, 2005</a>)</span>. To be able to draw conclusions about the absence of a meaningful effect, one should perform an equivalence test, and design a study with high power to reject the smallest effect size of interest <span class="citation">(<a href="references.html#ref-lakens_equivalence_2018" role="doc-biblioref">Lakens, Scheel, et al., 2018</a>)</span>. Alternatively, if no smallest effect size of interest was specified when designing the study, researchers can report a sensitivity power analysis.</p>
</div>
<div id="sequential-analyses" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> Sequential Analyses</h2>
<p>Whenever the sample size is justified based on an a-priori power analysis it can be very efficient to collect data in a sequential design. Sequential designs control error rates across multiple looks at the data (e.g., after 50, 100, and 150 observations have been collected) and can reduce the average expected sample size that is collected compared to a fixed design where data is only analyzed after the maximum sample size is collected <span class="citation">(<a href="references.html#ref-proschan_statistical_2006" role="doc-biblioref">Proschan et al., 2006</a>; <a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">Wassmer &amp; Brannath, 2016</a>)</span>. Sequential designs have a long history <span class="citation">(<a href="references.html#ref-dodge_method_1929" role="doc-biblioref">Dodge &amp; Romig, 1929</a>)</span>, and exist in many variations, such as the Sequential Probability Ratio Test <span class="citation">(<a href="references.html#ref-wald_sequential_1945" role="doc-biblioref">Wald, 1945</a>)</span>, combining independent statistical tests <span class="citation">(<a href="references.html#ref-westberg_combining_1985" role="doc-biblioref">Westberg, 1985</a>)</span>, group sequential designs <span class="citation">(<a href="references.html#ref-jennison_group_2000" role="doc-biblioref">Jennison &amp; Turnbull, 2000</a>)</span>, sequential Bayes factors <span class="citation">(<a href="references.html#ref-schonbrodt_sequential_2017" role="doc-biblioref">Schönbrodt et al., 2017</a>)</span>, and safe testing <span class="citation">(<a href="references.html#ref-grunwald_safe_2019" role="doc-biblioref">Grünwald et al., 2019</a>)</span>. Of these approaches, the Sequential Probability Ratio Test is most efficient if data can be analyzed after every observation <span class="citation">(<a href="references.html#ref-schnuerch_controlling_2020" role="doc-biblioref">Schnuerch &amp; Erdfelder, 2020</a>)</span>. Group sequential designs, where data is collected in batches, provide more flexibility in data collection, error control, and corrections for effect size estimates <span class="citation">(<a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">Wassmer &amp; Brannath, 2016</a>)</span>. Safe tests provide optimal flexibility if there are dependencies between observations <span class="citation">(<a href="references.html#ref-ter_schure_accumulation_2019" role="doc-biblioref">ter Schure &amp; Grünwald, 2019</a>)</span>.</p>
<p>Sequential designs are especially useful when there is considerable uncertainty about the effect size, or when it is plausible that the true effect size is larger than the smallest effect size of interest the study is designed to detect <span class="citation">(<a href="references.html#ref-lakens_performing_2014" role="doc-biblioref">Lakens, 2014</a>)</span>. In such situations data collection has the possibility to terminate early if the effect size is larger than the smallest effect size of interest, but data collection can continue to the maximum sample size if needed. Sequential designs can prevent waste when testing hypotheses, both by stopping early when the null hypothesis can be rejected, as by stopping early if the presence of a smallest effect size of interest can be rejected (i.e., stopping for futility). Group sequential designs are currently the most widely used approach to sequential analyses, and can be planned and analyzed using rpact <span class="citation">(<a href="references.html#ref-wassmer_rpact_2019" role="doc-biblioref">Wassmer &amp; Pahlke, 2019</a>)</span> or gsDesign <span class="citation">(<a href="references.html#ref-anderson_group_2014" role="doc-biblioref">K. M. Anderson, 2014</a>)</span>.<a href="references.html#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
</div>
<div id="increasing-power-without-increasing-the-sample-size" class="section level2" number="14.4">
<h2><span class="header-section-number">14.4</span> Increasing Power Without Increasing the Sample Size</h2>
<p>The most straightforward approach to increase the informational value of studies is to increase the sample size. Because resources are often limited, it is also worthwhile to explore different approaches to increasing the power of a test without increasing the sample size. The first option is to use directional tests where relevant. Researchers often make directional predictions, such as ‘we predict X is larger than Y’. The statistical test that logically follows from this prediction is a directional (or one-sided) <em>t</em> test. A directional test moves the Type I error rate to one side of the tail of the distribution, which lowers the critical value, and therefore requires less observations to achieve the same statistical power.</p>
<p>Although there is some discussion about when directional tests are appropriate, they are perfectly defensible from a Neyman-Pearson perspective on hypothesis testing <span class="citation">(<a href="references.html#ref-cho_is_2013" role="doc-biblioref">Cho &amp; Abe, 2013</a>)</span>, which makes a (preregistered) directional test a straightforward approach to both increase the power of a test, as the riskiness of the prediction. However, there might be situations where you do not want to ask a directional question. Sometimes, especially in research with applied consequences, it might be important to examine if a null effect can be rejected, even if the effect is in the opposite direction as predicted. For example, when you are evaluating a recently introduced educational intervention, and you predict the intervention will increase the performance of students, you might want to explore the possibility that students perform worse, to be able to recommend abandoning the new intervention. In such cases it is also possible to distribute the error rate in a 'lop-sided' manner, for example assigning a stricter error rate to effects in the negative than in the positive direction <span class="citation">(<a href="references.html#ref-rice_heads_1994" role="doc-biblioref">Rice &amp; Gaines, 1994</a>)</span>.</p>
<p>Another approach to increase the power without increasing the sample size, is to increase the alpha level of the test, as explained in the section on compromise power analysis. Obviously, this comes at an increased probability of making a Type I error. The risk of making either type of error should be carefully weighed, which typically requires taking into account the prior probability that the null-hypothesis is true <span class="citation">(<a href="references.html#ref-cascio_open_1983" role="doc-biblioref">Cascio &amp; Zedeck, 1983</a>; <a href="references.html#ref-miller_quest_2019" role="doc-biblioref">Miller &amp; Ulrich, 2019</a>; <a href="references.html#ref-mudge_setting_2012" role="doc-biblioref">Mudge et al., 2012</a>; <a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>. If you <em>have</em> to make a decision, or want to make a claim, and the data you can feasibly collect is limited, increasing the alpha level is justified, either based on a compromise power analysis, or based on a cost-benefit analysis <span class="citation">(<a href="references.html#ref-baguley_understanding_2004" role="doc-biblioref">Baguley, 2004</a>; <a href="references.html#ref-field_minimizing_2004" role="doc-biblioref">Field et al., 2004</a>)</span>.</p>
<p>Another widely recommended approach to increase the power of a study is use a within participant design where possible. In almost all cases where a researcher is interested in detecting a difference between groups, a within participant design will require collecting less participants than a between participant design. The reason for the decrease in the sample size is explained by the equation below from <span class="citation">Maxwell et al. (<a href="references.html#ref-maxwell_designing_2017" role="doc-biblioref">2017</a>)</span>. The number of participants needed in a two group within-participants design (NW) relative to the number of participants needed in a two group between-participants design (NB), assuming normal distributions, is:</p>
<p><span class="math display">\[NW = \frac{NB (1-\rho)}{2}\]</span></p>
<p>The required number of participants is divided by two because in a within-participants design with two conditions every participant provides two data points. The extent to which this reduces the sample size compared to a between-participants design also depends on the correlation between the dependent variables (e.g., the correlation between the measure collected in a control task and an experimental task), as indicated by the (1-<span class="math inline">\(\rho\)</span>) part of the equation. If the correlation is 0, a within-participants design simply needs half as many participants as a between participant design (e.g., 64 instead 128 participants). The higher the correlation, the larger the relative benefit of within-participants designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Especially when dependent variables in within-participants designs are positively correlated, within-participants designs will greatly increase the power you can achieve given the sample size you have available. Use within-participants designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-participants design <span class="citation">(<a href="references.html#ref-maxwell_designing_2017" role="doc-biblioref">Maxwell et al., 2017</a>)</span>.<a href="references.html#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> For designs with multiple factors with multiple levels it can be difficult to specify the full correlation matrix that specifies the expected population correlation for each pair of measurements <span class="citation">(<a href="references.html#ref-lakens_simulation-based_2021" role="doc-biblioref">Lakens &amp; Caldwell, 2021</a>)</span>. In these cases sequential analyses might provide a solution.</p>
<!-- So what does the correlation do so that it increases the power of within designs, or reduces the number of observations you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots. -->
<!-- We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements on the left, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur). In the scatter plot on the right, we can see that the correlation between the measurements is indeed 0. Let's look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is $\sqrt{2}$ times as large as the standard deviation in each measurement, and indeed, 15×$\sqrt{2}$ = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent *t* test, where the correlation between measurements is not taken into account.  -->
<!-- ```{r, plot-1, fig.height = 16, fig.fullwidth = TRUE, fig.cap="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0."} -->
<!-- set.seed(419) -->
<!-- #Set color palette -->
<!-- cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") -->
<!-- n <- 10000 # set the sample size for each group -->
<!-- mx <- 100 # set the mean in group 1 -->
<!-- sdx <- 15 # set the standard deviation in group 1 -->
<!-- my <- 106 # set the mean in group 2 -->
<!-- sdy <- 15 # set the standard deviation in group 2 -->
<!-- cor.true <- 0.0 # set true correlation -->
<!-- # randomly draw data -->
<!-- cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T) -->
<!-- mu <- c(0, 0) -->
<!-- mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE) -->
<!-- x <- mat[, 1] * sdx + mx -->
<!-- y <- mat[, 2] * sdy + my -->
<!-- dif <- x - y -->
<!-- dataset <- data.frame(x, y) -->
<!-- DV <- c(x, y) # combine the two samples into a single variable -->
<!-- IV <- as.factor(c(rep("1", n), rep("2", n))) # create the independent variable (1 and 2) -->
<!-- datasetplot <- data.frame(IV, DV) # create a dataframe (to make the plot) -->
<!-- #plot graph two groups -->
<!-- p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV))) + -->
<!--   geom_histogram(alpha = 0.4, binwidth = 2, position = "identity", colour = "black", aes(y = ..density..)) + -->
<!--   scale_fill_manual(values = cbbPalette, name = "Condition") + -->
<!--   stat_function(fun = dnorm, args = c(mean = mx, sd = sdx), size = 1, color = "#E69F00", lty = 2) + -->
<!--   stat_function(fun = dnorm, args = c(mean = my, sd = sdy), size = 1, color = "#56B4E9", lty = 2) + -->
<!--   xlab("IQ") + -->
<!--   ylab("number of people") + -->
<!--   ggtitle("Data") + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   geom_vline(xintercept = mean(x), colour = "black", linetype = "dashed", size = 1) + -->
<!--   geom_vline(xintercept = mean(y), colour = "black", linetype = "dashed", size = 1) + -->
<!--   coord_cartesian(xlim = c(50, 150)) + -->
<!--   scale_x_continuous(breaks = c(60, 80, 100, 120, 140)) + -->
<!--   annotate("text", size = 6, x = 70, y = 0.02, label = paste("Mean X = ", round(mean(x)), "\n", "SD = ", round(sd(x)), sep = "")) + -->
<!--   annotate("text", size = 6, x = 130, y = 0.02, label = paste("Mean Y = ", round(mean(y)), "\n", "SD = ", round(sd(y)), sep = "")) + -->
<!--   theme(plot.title = element_text(hjust = 0.5), legend.position = "none") -->
<!-- #plot data differences -->
<!-- p2 <- ggplot(as.data.frame(dif), aes(dif)) + -->
<!--   geom_histogram(colour = "black", fill = "grey", aes(y = ..density..), binwidth = 2) + -->
<!--   #  geom_density(fill=NA, colour="black", size = 1) + -->
<!--   xlab("IQ dif") + -->
<!--   ylab("number of people") + -->
<!--   ggtitle("Data") + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   geom_vline(xintercept = mean(dif), colour = "grey20", linetype = "dashed") + -->
<!--   coord_cartesian(xlim = c(-80, 80)) + -->
<!--   scale_x_continuous(breaks = c(seq(-80, 80, 20))) + -->
<!--   annotate("text", size = 6, x = mean(dif), y = 0.01, label = paste("Mean = ", round(mean(dif)), "\n", "SD = ", round(sd(dif)), sep = "")) + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- # Plot correlation -->
<!-- p3 <- ggplot(dataset, aes(x = x, y = y)) + -->
<!--   geom_point(size = 2) + # Use hollow circles -->
<!--   geom_smooth(method = lm, colour = "#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line -->
<!--   coord_cartesian(xlim = c(40, 160), ylim = c(40, 160)) + -->
<!--   scale_x_continuous(breaks = c(seq(40, 160, 20))) + -->
<!--   scale_y_continuous(breaks = c(seq(40, 160, 20))) + -->
<!--   xlab("IQ twin 1") + -->
<!--   ylab("IQ twin 2") + -->
<!--   ggtitle(paste("Correlation = ", round(cor(x, y), digits = 2), sep = "")) + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   coord_fixed(ratio = 1) + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- # Use patchwork to combine and plot only 1 legend without title. -->
<!-- combined <- p1 / p2 / p3 + -->
<!--   theme(legend.position = "none", legend.title = element_blank()) -->
<!-- combined -->
<!-- ``` -->
<!-- ```{r, plot-4, fig.height = 16, fig.fullwidth = TRUE, fig.cap="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0.7."} -->
<!-- set.seed(419) -->
<!-- #Set color palette -->
<!-- cbbPalette<-c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") -->
<!-- n<-10000  #set the sample size for each group -->
<!-- mx<-100  #set the mean in group 1 -->
<!-- sdx<-15  #set the standard deviation in group 1 -->
<!-- my<-106  #set the mean in group 2 -->
<!-- sdy<-15  #set the standard deviation in group 2 -->
<!-- cor.true <- 0.7 #set true correlation -->
<!-- #randomly draw data -->
<!-- cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T) -->
<!-- mu <- c(0,0) -->
<!-- mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE) -->
<!-- x<-mat[,1]*sdx+mx -->
<!-- y<-mat[,2]*sdy+my -->
<!-- dif<-x-y -->
<!-- dataset<-data.frame(x,y) -->
<!-- DV<-c(x,y) #combine the two samples into a single variable -->
<!-- IV<-as.factor(c(rep("1", n), rep("2", n))) #create the independent variable (1 and 2)  -->
<!-- datasetplot<-data.frame(IV,DV) #create a dataframe (to make the plot) -->
<!-- #plot graph two groups -->
<!-- p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV))) + -->
<!--   geom_histogram(alpha = 0.4, binwidth = 2, position = "identity", colour = "black", aes(y = ..density..)) + -->
<!--   scale_fill_manual(values = cbbPalette, name = "Condition") + -->
<!--   stat_function(fun = dnorm, args = c(mean = mx, sd = sdx), size = 1, color = "#E69F00", lty = 2) + -->
<!--   stat_function(fun = dnorm, args = c(mean = my, sd = sdy), size = 1, color = "#56B4E9", lty = 2) + -->
<!--   xlab("IQ") + -->
<!--   ylab("number of people") + -->
<!--   ggtitle("Data") + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   geom_vline(xintercept = mean(x), colour = "black", linetype = "dashed", size = 1) + -->
<!--   geom_vline(xintercept = mean(y), colour = "black", linetype = "dashed", size = 1) + -->
<!--   coord_cartesian(xlim = c(50, 150)) + -->
<!--   scale_x_continuous(breaks = c(60, 80, 100, 120, 140)) + -->
<!--   annotate("text", size = 6, x = 70, y = 0.02, label = paste("Mean X = ", round(mean(x)), "\n", "SD = ", round(sd(x)), sep = "")) + -->
<!--   annotate("text", size = 6, x = 130, y = 0.02, label = paste("Mean Y = ", round(mean(y)), "\n", "SD = ", round(sd(y)), sep = "")) + -->
<!--   theme(plot.title = element_text(hjust = 0.5), legend.position = "none") -->
<!-- #plot data differences -->
<!-- p2 <- ggplot(as.data.frame(dif), aes(dif)) + -->
<!--   geom_histogram(colour = "black", fill = "grey", aes(y = ..density..), binwidth = 2) + -->
<!--   #  geom_density(fill=NA, colour="black", size = 1) + -->
<!--   xlab("IQ dif") + -->
<!--   ylab("number of people") + -->
<!--   ggtitle("Data") + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   geom_vline(xintercept = mean(dif), colour = "grey20", linetype = "dashed") + -->
<!--   coord_cartesian(xlim = c(-80, 80)) + -->
<!--   scale_x_continuous(breaks = c(seq(-80, 80, 20))) + -->
<!--   annotate("text", size = 6, x = mean(dif), y = 0.01, label = paste("Mean = ", round(mean(dif)), "\n", "SD = ", round(sd(dif)), sep = "")) + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- # Plot correlation -->
<!-- p3 <- ggplot(dataset, aes(x = x, y = y)) + -->
<!--   geom_point(size = 2) + # Use hollow circles -->
<!--   geom_smooth(method = lm, colour = "#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line -->
<!--   coord_cartesian(xlim = c(40, 160), ylim = c(40, 160)) + -->
<!--   scale_x_continuous(breaks = c(seq(40, 160, 20))) + -->
<!--   scale_y_continuous(breaks = c(seq(40, 160, 20))) + -->
<!--   xlab("IQ twin 1") + -->
<!--   ylab("IQ twin 2") + -->
<!--   ggtitle(paste("Correlation = ", round(cor(x, y), digits = 2), sep = "")) + -->
<!--   theme_bw(base_size = 20) + -->
<!--   theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) + -->
<!--   coord_fixed(ratio = 1) + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- # Use patchwork to combine and plot only 1 legend without title. -->
<!-- combined <- p1 / p2 / p3 + -->
<!--   theme(legend.position = "none", legend.title = element_blank()) -->
<!-- combined -->
<!-- ``` -->
<!-- Now let’s increase the correlation between dependent variables to 0.7. Nothing has changed when we plot the means. The correlation between measurements is now strongly positive, as we see in the plot on the right. The important difference lies in the standard deviation of the difference scores. The SD of the difference scores is 11 instead of 21 in the uncorrelated example. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above. -->
<!-- If you set the correlation to a negative value, the standard deviation of the difference scores would increase. I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor. -->
<!-- You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (*M* = 390, *SD* = 44) when they used their feet than when they used their hands (*M* = 371, *SD* = 44, *r* = .953), *t*(17) = 5.98, *p* < 0.001, Hedges' g = 0.43, M_diff = 19, 95% CI [12; 26]).  -->
<p>In general, the smaller the variation, the larger the standardized effect size (because we are dividing the raw effect by a smaller standard deviation) and thus the higher the power given the same number of observations. Some additional recommendations are provided in the literature <span class="citation">(<a href="references.html#ref-allison_power_1997" role="doc-biblioref">Allison et al., 1997</a>; <a href="references.html#ref-bausell_power_2002" role="doc-biblioref">Bausell &amp; Li, 2002</a>; <a href="references.html#ref-hallahan_statistical_1996" role="doc-biblioref">Hallahan &amp; Rosenthal, 1996</a>)</span>, such as:</p>
<ol style="list-style-type: decimal">
<li>Use better ways to screen participants for studies where participants need to be screened before participation.</li>
<li>Assign participants unequally to conditions (if data in the control condition is much cheaper to collect than data in the experimental condition, for example).</li>
<li>Use reliable measures that have low error variance <span class="citation">(<a href="references.html#ref-williams_impact_1995" role="doc-biblioref">Williams et al., 1995</a>)</span>.</li>
<li>Smart use of preregistered covariates <span class="citation">(<a href="references.html#ref-meyvis_increasing_2018" role="doc-biblioref">Meyvis &amp; Van Osselaer, 2018</a>)</span>.</li>
</ol>
<p>It is important to consider if these ways to reduce the variation in the data do not come at too large a cost for external validity. For example, in an <em>intention-to-treat</em> analysis in randomized controlled trials participants who do not comply with the protocol are maintained in the analysis such that the effect size from the study accurately represents the effect of implementing the intervention in the population, and not the effect of the intervention only on those people who perfectly follow the protocol <span class="citation">(<a href="references.html#ref-gupta_intention_2011" role="doc-biblioref">Gupta, 2011</a>)</span>. Similar trade-offs between reducing the variance and external validity exist in other research areas.</p>
</div>
<div id="know-your-measure" class="section level2" number="14.5">
<h2><span class="header-section-number">14.5</span> Know Your Measure</h2>
<p>Although it is convenient to talk about standardized effect sizes, it is generally preferable if researchers can interpret effects in the raw (unstandardized) scores, and have knowledge about the standard deviation of their measures <span class="citation">(<a href="references.html#ref-baguley_standardized_2009" role="doc-biblioref">Baguley, 2009</a>; <a href="references.html#ref-lenth_practical_2001" role="doc-biblioref">Lenth, 2001</a>)</span>. To make it possible for a research community to have realistic expectations about the standard deviation of measures they collect, it is beneficial if researchers within a research area use the same validated measures. This provides a reliable knowledge base that makes it easier to plan for a desired accuracy, and to use a smallest effect size of interest on the unstandardized scale in an a-priori power analysis.</p>
<p>In addition to knowledge about the standard deviation it is important to have knowledge about the correlations between dependent variables (for example because Cohen's d<sub>z</sub> for a dependent <em>t</em> test relies on the correlation between means). The more complex the model, the more aspects of the data-generating process need to be known to make predictions. For example, in hierarchical models researchers need knowledge about variance components to be able to perform a power analysis <span class="citation">(<a href="references.html#ref-debruine_understanding_2019" role="doc-biblioref">DeBruine &amp; Barr, 2019</a>; <a href="references.html#ref-westfall_statistical_2014" role="doc-biblioref">Westfall et al., 2014</a>)</span>. Finally, it is important to know the reliability of your measure <span class="citation">(<a href="references.html#ref-parsons_psychological_2019" role="doc-biblioref">Parsons et al., 2019</a>)</span>, especially when relying on an effect size from a published study that used a measure with different reliability, or when the same measure is used in different populations, in which case it is possible that measurement reliability differs between populations. With the increasing availability of open data, it will hopefully become easier to estimate these parameters using data from earlier studies.</p>
<p>If we calculate a standard deviation from a sample, this value is an estimate of the true value in the population. In small samples, our estimate can be quite far off, while due to the law of large numbers, as our sample size increases, we will be measuring the standard deviation more accurately. Since the sample standard deviation is an estimate with uncertainty, we can calculate a confidence interval around the estimate <span class="citation">(<a href="references.html#ref-smithson_confidence_2003" role="doc-biblioref">Smithson, 2003</a>)</span>, and design pilot studies that will yield a sufficiently reliable estimate of the standard deviation. The confidence interval for the variance <span class="math inline">\(\sigma^2\)</span> is provided in the following formula, and the confidence for the standard deviation is the square root of these limits:</p>
<p><span class="math display">\[(N - 1)s^2/\chi^2_{N-1:\alpha/2},(N - 1)s^2/\chi^2_{N-1:1-\alpha/2}\]</span></p>
<p>Whenever there is uncertainty about parameters, researchers can use sequential designs to perform an <em>internal pilot study</em> <span class="citation">(<a href="references.html#ref-wittes_role_1990" role="doc-biblioref">Wittes &amp; Brittain, 1990</a>)</span>. The idea behind an internal pilot study is that researchers specify a tentative sample size for the study, perform an interim analysis, use the data from the internal pilot study to update parameters such as the variance of the measure, and finally update the final sample size that will be collected. As long as interim looks at the data are blinded (e.g., information about the conditions is not taken into account) the sample size can be adjusted based on an updated estimate of the variance without any practical consequences for the Type I error rate <span class="citation">(<a href="references.html#ref-friede_sample_2006" role="doc-biblioref">Friede &amp; Kieser, 2006</a>; <a href="references.html#ref-proschan_two-stage_2005" role="doc-biblioref">Proschan, 2005</a>)</span>. Therefore, if researchers are interested in designing an informative study where the Type I and Type II error rates are controlled, but they lack information about the standard deviation, an internal pilot study might be an attractive approach to consider <span class="citation">(<a href="references.html#ref-chang_adaptive_2016" role="doc-biblioref">Chang, 2016</a>)</span>.</p>
</div>
<div id="conventions-as-meta-heuristics" class="section level2" number="14.6">
<h2><span class="header-section-number">14.6</span> Conventions as meta-heuristics</h2>
<p>Even when a researcher might not use a heuristic to directly determine the sample size in a study, there is an indirect way in which heuristics play a role in sample size justifications. Sample size justifications based on inferential goals such as a power analysis, accuracy, or a decision all require researchers to choose values for a desired Type I and Type II error rate, a desired accuracy, or a smallest effect size of interest. Although it is sometimes possible to justify these values as described above (e.g., based on a cost-benefit analysis), a solid justification of these values might require dedicated research lines. Performing such research lines will not always be possible, and these studies might themselves not be worth the costs (e.g., it might require less resources to perform a study with an alpha level that most peers would consider conservatively low, than to collect all the data that would be required to determine the alpha level based on a cost-benefit analysis). In these situations, researchers might use values based on a convention.</p>
<p>When it comes to a desired width of a confidence interval, a desired power, or any other input values required to perform a sample size computation, it is important to transparently report the use of a heuristic or convention (for example by using the accompanying online Shiny app). A convention such as the use of a 5% Type 1 error rate and 80% power practically functions as a lower threshold of the minimum informational value peers are expected to accept <em>without</em> any justification (whereas <em>with</em> a justification, higher error rates can also be deemed acceptable by peers). It is important to realize that none of these values are set in stone. Journals are free to specify that they desire a higher informational value in their author guidelines (e.g., Nature Human Behavior requires registered reports to be designed to achieve 95% statistical power, and my own department has required staff to submit ERB proposals where, whenever possible, the study was designed to achieve 90% power). Researchers who choose to design studies with a higher informational value than a conventional minimum should receive credit for doing so.</p>
<p>In the past some fields have changed conventions, such as the 5 sigma threshold now used in physics to declare a discovery instead of a 5% Type I error rate. In other fields such attempts have been unsuccessful (e.g., <span class="citation">Johnson (<a href="references.html#ref-johnson_revised_2013" role="doc-biblioref">2013</a>)</span>). Improved conventions should be context dependent, and it seems sensible to establish them through consensus meetings <span class="citation">(<a href="references.html#ref-mullan_town_1985" role="doc-biblioref">Mullan &amp; Jacoby, 1985</a>)</span>. Consensus meetings are common in medical research, and have been used to decide upon a smallest effect size of interest (for an example, see <span class="citation">Fried et al. (<a href="references.html#ref-fried_method_1993" role="doc-biblioref">1993</a>)</span>). In many research areas current conventions can be improved. For example, it seems peculiar to have a default alpha level of 5% both for single studies and for meta-analyses, and one could imagine a future where the default alpha level in meta-analyses is much lower than 5%. Hopefully, making the lack of an adequate justification for certain input values in specific situations more transparent will motivate fields to start a discussion about how to improve current conventions. The online Shiny app links to good examples of justifications where possible, and will continue to be updated as better justifications are developed in the future.</p>
</div>
<div id="sample-size-justification-in-qualitative-research" class="section level2" number="14.7">
<h2><span class="header-section-number">14.7</span> Sample Size Justification in Qualitative Research</h2>
<p>A value of information perspective to sample size justification also applies to qualitative research. A sample size justification in qualitative research should be based on the consideration that the cost of collecting data from additional participants does not yield new information that is valuable enough given the inferential goals. One widely used application of this idea is known as <em>saturation</em> and is indicated by the observation that new data replicates earlier observations, without adding new information <span class="citation">(<a href="references.html#ref-morse_significance_1995" role="doc-biblioref">Morse, 1995</a>)</span>. For example, let's imagine we ask people why they have a pet. Interviews might reveal reasons that are grouped into categories, but after interviewing 20 people, no new categories emerge, at which point saturation has been reached. Alternative philosophies to qualitative research exist, and not all value planning for saturation. Regrettably, principled approaches to justify sample sizes have not been developed for these alternative philosophies <span class="citation">(<a href="references.html#ref-marshall_does_2013" role="doc-biblioref">Marshall et al., 2013</a>)</span>.</p>
<p>When sampling, the goal is often not to pick a representative sample, but a sample that contains a sufficiently diverse number of subjects such that saturation is reached efficiently. Fugard and Potts <span class="citation">(<a href="references.html#ref-fugard_supporting_2015" role="doc-biblioref">2015</a>)</span> show how to move towards a more informed justification for the sample size in qualitative research based on 1) the number of codes that exist in the population (e.g., the number of reasons people have pets), 2) the probability a code can be observed in a single information source (e.g., the probability that someone you interview will mention each possible reason for having a pet), and 3) the number of times you want to observe each code. They provide an R formula based on binomial probabilities to compute a required sample size to reach a desired probability of observing codes.</p>
<p>A more advanced approach is used in Rijnsoever <span class="citation">(<a href="references.html#ref-rijnsoever_i_2017" role="doc-biblioref">2017</a>)</span>, which also explores the importance of different sampling strategies. In general, purposefully sampling information from sources you expect will yield novel information is much more efficient than random sampling, but this also requires a good overview of the expected codes, and the sub-populations in which each code can be observed. Sometimes, it is possible to identify information sources that, when interviewed, would at least yield one new code (e.g., based on informal communication before an interview). A good sample size justification in qualitative research is based on 1) an identification of the populations, including any sub-populations, 2) an estimate of the number of codes in the (sub-)population, 3) the probability a code is encountered in an information source, and 4) the sampling strategy that is used.</p>
</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-02-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
