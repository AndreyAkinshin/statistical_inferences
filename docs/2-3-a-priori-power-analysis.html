<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.3 A-priori power analysis | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-08-15" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>2.3 A-priori power analysis | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="1-pvalue.html"><span class="toc-section-number">1</span> <em>p</em>-values</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-1-what-is-a-p-value.html"><span class="toc-section-number">1.1</span> What is a <em>p</em>-value?</a>
<a href="1-2-fisher-vs-neyman.html.-neyman"><span class="toc-section-number">1.2</span> Fisher vs. Neyman</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3</span> Preventing common misconceptions about <em>p</em>-values</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3.1</span> Misunderstanding 1: A non-significant <em>p</em>-value means that the null hypothesis is true</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.2</span> Misunderstanding 2: A significant <em>p</em>-value means that the null hypothesis is false.</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3.3</span> Misunderstanding 3: A significant <em>p</em>-value means that a practically important effect has been discovered</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.4</span> Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.5</span> Misunderstanding 5: One minus the <em>p</em>-value is the probability that the effect will replicate when repeated.</a>
</div>
</li>
</ul>
<a href="1-4-which-p-values-can-you-expect.html"><span class="toc-section-number">1.4</span> Which <em>p</em>-values can you expect?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-4-which-p-values-can-you-expect.html"><span class="toc-section-number">1.4.1</span> Lindley’s paradox</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="2-power.html"><span class="toc-section-number">2</span> Sample size justification</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-1-measuring-the-entire-population.html"><span class="toc-section-number">2.1</span> Measuring the Entire Population</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2</span> Feasibility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.1</span> The smallest effect size that can be statistically significant</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.2</span> Compute the width of the confidence interval around the effect size</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.3</span> Plot a sensitivity power analysis</a>
<a href="2-2-feasibility.html."><span class="toc-section-number">2.2.4</span> Reporting a feasibility justification.</a>
</div>
</li>
</ul>
<a id="active-page" href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3</span> A-priori power analysis</a><ul class="navbar"><ul class="toc-sections">
<li class="toc"><a href="#a-priori-power-analysis"> A-priori power analysis</a></li>
</ul>
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-3-a-priori-power-analysis.html."><span class="toc-section-number">2.3.1</span> Performing a power analysis.</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.2</span> Justifying the effect size used in an a-priori power analysis</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.3</span> Justifying the error rates used in an a-priori power analysis</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.4</span> Some advice when using G*Power</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.5</span> A-priori power analysis for the absence of an effect</a>
<a href="2-3-a-priori-power-analysis.html."><span class="toc-section-number">2.3.6</span> Reporting an a-priori power analysis.</a>
</div>
</li>
</ul>
<a href="2-4-compromisepower.html"><span class="toc-section-number">2.4</span> Compromise power analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-4-compromisepower.html"><span class="toc-section-number">2.4.1</span> Reporting a compromise power analysis</a>
</div>
</li>
</ul>
<a href="2-5-observedpower.html"><span class="toc-section-number">2.5</span> Observed (post-hoc) power analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-5-observedpower.html"><span class="toc-section-number">2.5.1</span> What to do if your editor asks for post-hoc power?</a>
</div>
</li>
</ul>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6</span> Designing efficient studies</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-6-designing-efficient-studies.html."><span class="toc-section-number">2.6.1</span> Use directional tests where relevant.</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.2</span> Use sequential analysis whenever possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.3</span> Increase your alpha level</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.4</span> Use within designs where possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.5</span> Remove statistical variation where possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.6</span> Use Bayesian statistics with informed priors</a>
</div>
</li>
</ul>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7</span> What if best practices are not enough?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.1</span> Ask for more money in your grant proposals</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.2</span> Improve management</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.3</span> Change what is expected from PhD students</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.4</span> Get answers collectively</a>
</div>
</li>
</ul>
<a href="2-8-planning-for-precision.html"><span class="toc-section-number">2.8</span> Planning for precision</a>
</div>
</li>
</ul>
<a href="3-questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="3-1-do-you-really-want-to-test-a-hypothesis.html"><span class="toc-section-number">3.1</span> Do You Really Want to Test a Hypothesis?</a>
<a href="3-2-goals-of-tests.html"><span class="toc-section-number">3.2</span> Goals of tests</a>
</div>
</li>
</ul>
<a href="4-errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="4-1-why-you-dont-need-to-adjust-your-alpha-level-for-all-tests-youll-do-in-your-lifetime-.html."><span class="toc-section-number">4.1</span> Why you don’t need to adjust your alpha level for all tests you’ll do in your lifetime.</a>
<a href="4-2-why-banning-p-values-might-not-solve-our-problems-.html."><span class="toc-section-number">4.2</span> Why banning p-values might not solve our problems.</a>
<a href="4-3-error-control-in-exploratory-anovas-the-how-and-the-why.html"><span class="toc-section-number">4.3</span> Error Control in Exploratory ANOVA’s: The How and the Why</a>
</div>
</li>
</ul>
<a href="5-effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-1-effect-sizes.html"><span class="toc-section-number">5.1</span> Effect sizes</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-1-effect-sizes.html"><span class="toc-section-number">5.1.1</span> The Facebook experiment</a>
</div>
</li>
</ul>
<a href="5-2-cohend.html"><span class="toc-section-number">5.2</span> Cohen’s <em>d</em></a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-2-cohend.html"><span class="toc-section-number">5.2.1</span> Correcting for Bias</a>
</div>
</li>
</ul>
<a href="5-3-r-correlations.html"><span class="toc-section-number">5.3</span> <em>r</em> (correlations)</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4</span> Confidence Intervals</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-4-confint.html.-samples"><span class="toc-section-number">5.4.1</span> Population vs. Samples</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.2</span> The relation between confidence intervals and <em>p</em>-values</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.3</span> The Standard Error and 95% Confidence Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.4</span> Overlapping Confidence Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.5</span> Prediction Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.6</span> Capture Percentages</a>
</div>
</li>
</ul>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5</span> Computing Confidence Intervals around Effect Sizes</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.1</span> MOTE</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.2</span> JASP</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.3</span> ESCI software</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.4</span> MBESS</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.5</span> Why should you report 90% CI for eta-squared?</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="6-equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="6-1-absence-of-evidence-is-not-evidence-of-absence-.html."><span class="toc-section-number">6.1</span> Absence of evidence is not evidence of absence.</a>
<a href="6-2-justifysesoi.html"><span class="toc-section-number">6.2</span> JUstifying a smallest effect size of interest</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="6-2-justifysesoi.html"><span class="toc-section-number">6.2.1</span> Rejecting the presence of a meaningful effect</a>
</div>
</li>
</ul>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html.">Bayesian estimation using ROPE and equivalence tests.</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.2</span> 95% HDI vs 90% CI</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.3</span> Power analysis for Equivalence Tests</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.4</span> Use of prior information</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.5</span> Conclusion</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="7-severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1</span> Testing Range Predictions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.1</span> Risky Predictions</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.2</span> Systematic Noise</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.3</span> Range Predictions</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.4</span> Directional Tests</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.5</span> Minimal Effect Tests</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.6</span> Range Predictions in PRactice</a>
</div>
</li>
</ul>
<a href="7-2-verisimilitude-belief-and-progress-in-psychological-science.html"><span class="toc-section-number">7.2</span> Verisimilitude, Belief, and Progress in Psychological Science</a>
</div>
</li>
</ul>
<a href="8-sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-1-what-would-falsify-your-theory.html"><span class="toc-section-number">8.1</span> What would falsify your theory?</a>
<a href="8-2-what-would-falsify-your-theory-in-practice.html"><span class="toc-section-number">8.2</span> What would falsify your theory in practice?</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3</span> Specifying a SESOI based on theory or costs and benefits</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.1</span> Example of a theoretically predicted SESOI</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.2</span> Anchor based methods to set a SESOI</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.3</span> Cost benefit analysis</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.4</span> Setting the SESOI based on effects feasible to study</a>
</div>
</li>
</ul>
<a href="8-4-smalltelescopes.html"><span class="toc-section-number">8.4</span> The small telescopes approach</a>
<a href="8-5-setting-the-sesoi-based-on-resources-.html."><span class="toc-section-number">8.5</span> Setting the SESOI based on resources.</a>
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6</span> Setting the smallest effect size of interest in replication studies</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6.1</span> Setting the SESOI based on theoretical predictions</a>
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6.2</span> Setting the smallest effect size of interest based on resources</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="9-meta.html"><span class="toc-section-number">9</span> Meta-analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1</span> Random Variation</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1.1</span> Variation in single samples</a>
<a href="9-1-random-variation.html."><span class="toc-section-number">9.1.2</span> Variance in two groups, and their difference.</a>
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1.3</span> Correlations between two groups</a>
<a href="9-1-random-variation.html."><span class="toc-section-number">9.1.4</span> Confidence Intervals around Standard Deviations.</a>
</div>
</li>
</ul>
<a href="9-2-mixed.html"><span class="toc-section-number">9.2</span> Mixed Results</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-2-mixed.html"><span class="toc-section-number">9.2.1</span> Likelihoods of sets of studies</a>
</div>
</li>
</ul>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3</span> Introduction to Meta-Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.1</span> Single study meta-analysis</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.2</span> Simulating meta-analyses of mean standardized differences</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.3</span> Fixed Effect vs Random Effects</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.4</span> Simulating meta-analyses for dichotomous outcomes</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.5</span> Heterogeneity</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.6</span> Improving the reproducibility of meta-analyses</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="10-bias.html"><span class="toc-section-number">10</span> Bias detection</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1</span> Bias Detection</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.1</span> Funnel Plots</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.2</span> Trim and Fill</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.3</span> PET-PEESE</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.4</span> P-curve Analysis</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.5</span> TIVA</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.6</span> Let’s Detect Some Bias!</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.7</span> Introducing bias</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.8</span> Bias detection techniques</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.9</span> TIVA</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.10</span> Z-curve analysis</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">10.1.11</span> Conclusion</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="11-computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-1-step-1-setting-up-a-github-repository.html"><span class="toc-section-number">11.1</span> Step 1: Setting up a GitHub repository</a>
<a href="11-2-step-2-cloning-your-github-repository-into-rstudio.html"><span class="toc-section-number">11.2</span> Step 2: Cloning your GitHub repository into RStudio</a>
<a href="11-3-step-3-creating-an-r-markdown-file.html"><span class="toc-section-number">11.3</span> Step 3: Creating an R Markdown file</a>
<a href="11-4-step-4-reproducible-data-analysis-in-r-studio.html"><span class="toc-section-number">11.4</span> Step 4: Reproducible Data Analysis in R Studio</a>
<a href="11-5-step-5-committing-and-pushing-to-github.html"><span class="toc-section-number">11.5</span> Step 5: Committing and Pushing to GitHub</a>
<a href="11-6-step-6-reproducible-data-analysis.html"><span class="toc-section-number">11.6</span> Step 6: Reproducible Data Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-6-step-6-reproducible-data-analysis.html"><span class="toc-section-number">11.6.1</span> Extra: APA formatted manuscripts in papaja</a>
</div>
</li>
</ul>
<a href="11-7-step-7-organizing-your-data-and-code.html"><span class="toc-section-number">11.7</span> Step 7: Organizing Your Data and Code</a>
<a href="11-8-step-8-archiving-your-data-and-code.html"><span class="toc-section-number">11.8</span> Step 8: Archiving Your Data and Code</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-8-step-8-archiving-your-data-and-code.html"><span class="toc-section-number">11.8.1</span> EXTRA: Sharing Reproducible Code on Code Ocean</a>
</div>
</li>
</ul>
<a href="11-9-some-points-for-improvement-in-computational-reproducibility.html"><span class="toc-section-number">11.9</span> Some points for improvement in computational reproducibility</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">11.10</span> Conclusion</a>
</div>
</li>
</ul>
<a href="12-prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="12-1-trust-in-scientists.html"><span class="toc-section-number">12.1</span> Trust in scientists</a>
<a href="12-2-the-value-of-preregistration.html"><span class="toc-section-number">12.2</span> The value of preregistration</a>
<a href="12-3-registered-reports.html"><span class="toc-section-number">12.3</span> Registered Reports</a>
<a href="12-4-preregister-your-study.html"><span class="toc-section-number">12.4</span> Preregister your study?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="12-4-preregister-your-study.html"><span class="toc-section-number">12.4.1</span> How to preregister</a>
</div>
</li>
</ul>
<a href="12-5-what-does-a-formalized-test-of-a-prediction-look-like.html"><span class="toc-section-number">12.5</span> What Does a Formalized Test of a Prediction Look Like?</a>
<a href="12-6-are-you-ready-to-preregister-a-hypothesis-test.html"><span class="toc-section-number">12.6</span> Are you ready to preregister a hypothesis test?</a>
</div>
</li>
</ul>
<a href="13-bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="13-1-likelihoods.html"><span class="toc-section-number">13.1</span> Likelihoods</a>
<a href="13-2-bayes-factors.html"><span class="toc-section-number">13.2</span> Bayes factors</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="13-2-bayes-factors.html"><span class="toc-section-number">13.2.1</span> Updating our belief</a>
</div>
</li>
</ul>
<a href="13-3-bayesest.html"><span class="toc-section-number">13.3</span> Bayesian Estimation</a>
</div>
</li>
</ul>
<a href="14-sequential.html"><span class="toc-section-number">14</span> Sequential Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-1-choosing-alpha-levels-for-sequential-analyses-.html."><span class="toc-section-number">14.1</span> Choosing alpha levels for sequential analyses.</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-1-choosing-alpha-levels-for-sequential-analyses-.html"><span class="toc-section-number">14.1.1</span> Pocock correction</a>
</div>
</li>
</ul>
<a href="14-2-comparing-spending-functions.html"><span class="toc-section-number">14.2</span> Comparing Spending Functions</a>
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3</span> Sample Size for Sequential Designs</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3.1</span> Alpha spending functions</a>
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3.2</span> Updating Boundaries During an Experiment</a>
</div>
</li>
</ul>
<a href="14-4-test-for-non-inferiority.html"><span class="toc-section-number">14.4</span> Test for (non-)inferiority</a>
<a href="14-5-stopping-for-futility.html"><span class="toc-section-number">14.5</span> Stopping for futility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-5-stopping-for-futility.html."><span class="toc-section-number">14.5.1</span> Sequential analyses using Bayes factors.</a>
<a href="14-5-stopping-for-futility.html."><span class="toc-section-number">14.5.2</span> Reporting results after a sequential analysis.</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="15-references.html"><span class="toc-section-number">15</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="a-priori-power-analysis" class="section level2">
<h2>
<span class="header-section-number">2.3</span> A-priori power analysis</h2>
<p>When designing a study where the goal is to observe a statistically significant effect, researchers often want to make sure their sample size is large enough to have sufficient power to detect effects they expect, or effects they are interested in observing. This is done by performing an <em>a-priori</em> power analysis. Given a specified effect size, alpha level, and desired power, an a-priori power analysis will inform you about the sample size you need to collect. In Figure <a href="2-3-a-priori-power-analysis.html#fig:power-2">2.8</a> you see how the statistical power increases as the number of observations (per group) in an independent <em>t</em>-test with an alpha level of 0.05 increases.</p>
<p>It is important to highlight what the goal of a power analysis is, and what isn’t the goal. The goal of an a-priori power analysis is not to achieve sufficient power for the true effect size. The true effect size is unknown. <strong>The goal of an a-priori power analysis to achieve sufficient power given a specific assumption of the true effect size</strong>. Researchers often complain that a-priori power analysis is impossible given uncertainty about the true effect size. But when we design a study, we are uncertain about many things. We are not sure our data will be normally distributed, whether there will be a floor or ceiling effect, whether our manipulation works in a specific population, or whether our theory is true. We design studies based on assumptions. If our assumptions are wrong, we learn from our data.</p>
<p>This goal of a-priori power analysis is perhaps best illustrated by the useful recommendation to make sure your study is well powered to demonstrate the presence, <a href="2-3-a-priori-power-analysis.html#powerforabsence">and the absence</a>, of an effect. For example, you might want to perform an a-priori power analysis for an equivalence test that has has sufficient power to reject an effect size observed in an earlier study using the <a href="8-4-smalltelescopes.html#smalltelescopes">small telescopes approach</a>, while also having sufficient power for a smallest effect size that would theoretically be predicted. We compute power, assuming either of these two effect sizes are the true effect size, and aim to collect the largest sample size that either of these analyses indicate. This does not guarantee we have sufficient power for the true effect size - it simply means we have sufficient power given an assumption of what the true effect size might be. After collecting the data, we draw inferences about what we have learn, under the assumption of a true effect of a specific size. We do not draw conclusions about the (always unknown) ‘true’ effect size. Therefore, an a-priori power analysis is useful, as long as we have interesting assumptions about what effect sizes we might be interested in.</p>
<div class="figure">
<span id="fig:power-2"></span>
<p class="caption marginnote shownote">
Figure 2.8: Power curve for an independent <em>t</em>-test as a function of the sample size.
</p>
<img src="Statistical_Inferences_files/figure-html/power-2-1.png" alt="Power curve for an independent *t*-test as a function of the sample size." width="672">
</div>
<p>A-priori power calculations are performed under the assumption that there is an effect. In practice, it is of course also possible that there is no effect (e.g., d = 0). If there is no true effect, and you want to use formally correct language, power is <em>undefined</em>. Statistical power is a concept that can only be computed assuming there is a true effect. If there is no true effect, you will still observe significant effects, but these ar <em>Type 1 errors</em>, or false positives. These occur at your chosen alpha level (e.g., 5% of the time). If you perform a hypothesis test, there are four possible outcomes:</p>
<ol style="list-style-type: decimal">
<li>False positives or Type 1 errors, indicated by α (you observe a significant test result when H0 is true)</li>
<li>False negatives or Type 2 errors, indicated by β (you observe a non-significant result when H1 is true)</li>
<li>True negatives, indicated by 1-α (a non-significant result when H0 is true)</li>
<li>True positives, indicated by 1-β (a significant test result when H1 is true)</li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:NHST-outcomes"></span>
<img src="images/2.1.1.png" alt="Four possible outcomes in a null hypothesis significance test." width="961"><!--
<p class="caption marginnote">-->Figure 2.9: Four possible outcomes in a null hypothesis significance test.<!--</p>-->
<!--</div>--></span>
</p>
<p>The goal of an a-priori power analysis is to increase the sample size up to the level that the desired power, the probability of finding a significant result if there is a true effect, or 1-β, is at a desired level for an effect size one is interested in detecting, given a specific alpha level.</p>
<p>In Figure <a href="2-3-a-priori-power-analysis.html#fig:power-3">2.10</a> you see two distributions. The left (grey) distribution is centered at 0. This is our model for the null hypothesis. If the null hypothesis is true we can find statistically significant results if the effect size is extreme enough (in a two-sided test either in the positive or negative direction), but these would be Type 1 errors (the red areas under the curve).</p>
<p>The right (black) distribution is centered at an effect of d = 0.5. This is our model for the alternative hypothesis, where we expect a true effect in the population of d = 0.5. Even though there is a true effect, we will not always find a statistically significant result. This happens when, due to random variation, the observed effect size is too close to 0 to be statistically significant. These results would be false negative results (the blue area under the curve).</p>
<div class="figure">
<span id="fig:power-3"></span>
<p class="caption marginnote shownote">
Figure 2.10: Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group.
</p>
<img src="Statistical_Inferences_files/figure-html/power-3-1.png" alt="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group." width="672">
</div>
<p>If we want to increase the statistical power, we are trying to reduce the size of the blue area. One way to do this would be to increase the effect size. This would shift the entire distribution to the right, and reduce the size of the blue area. Although there are ways in controlled experiments to increase the standardized effect size (e.g., by reducing statistical variation in the data), often the effect size is what it is. All we can do is increase the sample size. This will make the distribution around 0.5 more narrow, and this reduces the blue area. You can check this in an online shiny app that <a href="http://shiny.ieis.tue.nl/d_p_power/">reproduces the plot</a>.</p>
<div id="performing-a-power-analysis." class="section level3">
<h3>
<span class="header-section-number">2.3.1</span> Performing a power analysis.</h3>
<p>There is a wide range of software tools that can help you to perform an a-priori power analysis. Sometimes statistical power can be analytically computed based on closed formulas, and in other situations power can be computed by performing simulations. If we simulate thousands of studies, and count the percentage of studies that are statistically significant, we have an estimate of the statistical power of the test. The code below computes the statistical power (using the <code>power.t.test</code> in base R) assuming an effect size of d = 0.5, and alpha level of 0.05, and a desired power of 90%.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="2-3-a-priori-power-analysis.html#cb27-1"></a><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="fl">0.5</span>, </span>
<span id="cb27-2"><a href="2-3-a-priori-power-analysis.html#cb27-2"></a>             <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb27-3"><a href="2-3-a-priori-power-analysis.html#cb27-3"></a>             <span class="dt">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb27-4"><a href="2-3-a-priori-power-analysis.html#cb27-4"></a>             <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb27-5"><a href="2-3-a-priori-power-analysis.html#cb27-5"></a>             <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 85.03129
##           delta = 0.5
##              sd = 1
##       sig.level = 0.05
##           power = 0.9
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>The output tells us we need to collect 85.03 observations. Because observations do not come in decimals (we can hardly cut 0.03 from a participant) we need to collect 86 observations. We could also get this answer through simulations.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="2-3-a-priori-power-analysis.html#cb29-1"></a><span class="kw">set.seed</span>(<span class="dv">600746</span>) <span class="co">#set a seed for reproducible simulations</span></span>
<span id="cb29-2"><a href="2-3-a-priori-power-analysis.html#cb29-2"></a>p &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">100000</span>) <span class="co">#set up empty variable to store all simulated p-values</span></span>
<span id="cb29-3"><a href="2-3-a-priori-power-analysis.html#cb29-3"></a></span>
<span id="cb29-4"><a href="2-3-a-priori-power-analysis.html#cb29-4"></a><span class="co">#Run simulation</span></span>
<span id="cb29-5"><a href="2-3-a-priori-power-analysis.html#cb29-5"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100000</span>){ <span class="co">#for each simulated experiment</span></span>
<span id="cb29-6"><a href="2-3-a-priori-power-analysis.html#cb29-6"></a>  x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 1</span></span>
<span id="cb29-7"><a href="2-3-a-priori-power-analysis.html#cb29-7"></a>  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="fl">0.5</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 2</span></span>
<span id="cb29-8"><a href="2-3-a-priori-power-analysis.html#cb29-8"></a>  z &lt;-<span class="st"> </span><span class="kw">t.test</span>(x, y) <span class="co">#perform the t-test</span></span>
<span id="cb29-9"><a href="2-3-a-priori-power-analysis.html#cb29-9"></a>  p[i] &lt;-<span class="st"> </span>z<span class="op">$</span>p.value <span class="co">#get the p-value and store it</span></span>
<span id="cb29-10"><a href="2-3-a-priori-power-analysis.html#cb29-10"></a>}</span>
<span id="cb29-11"><a href="2-3-a-priori-power-analysis.html#cb29-11"></a></span>
<span id="cb29-12"><a href="2-3-a-priori-power-analysis.html#cb29-12"></a><span class="co">#Calculate power: sum significant p-values, divide by 100000 simulations</span></span>
<span id="cb29-13"><a href="2-3-a-priori-power-analysis.html#cb29-13"></a>(<span class="kw">sum</span>(p <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>)<span class="op">/</span><span class="dv">100000</span>) <span class="co">#power</span></span></code></pre></div>
<pre><code>## [1] 0.90015</code></pre>
<p>We see that after 100000 simulations with 85 participants, the estimated power is 0.90015 with 85 participants in each group. The analytic solution tells us power is 0.899894. The more simulations you perform, the more accurate the power estimate will be (as we see, 100000 simulations gives very accurate results).</p>
<p>The way you perform the power analysis depends on the software you use. There are excellent software packages for power analysis, such as <a href="https://www.ncss.com/software/pass/">PASS</a> or <a href="https://stats.idre.ucla.edu/sas/seminars/proc-power/">SAS</a>, but these solutions are rather expensive. There are also freeware solutions, such as the widely used <a href="https://gpower.hhu.de">G*Power</a>, <a href="https://wiki.usask.ca/pages/viewpageattachments.action?pageId=420413544">MorePower</a> by Campbell and Thompson, and <a href="https://jakewestfall.shinyapps.io/pangea/">PANGEA</a> by Jake Westfall.
There is also a range of options in R. The default <code>stats</code> package has power functions for <em>t</em>-tests, proportions, and ANOVA, the <code>pwr</code> package has a wider range of options, <code>pwr2ppl</code> by Chris Aberson accompanies his excellent book on power analyses <span class="citation">(Aberson, <label for="tufte-mn-20" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle">2019<span class="marginnote">Aberson, C. L. (2019). <em>Applied Power Analysis for the Behavioral Sciences: 2nd Edition</em> (2 edition). Routledge.</span>)</span>, <code>powerlmm</code> by Kristoffer Magnusson performs power analyses for <a href="https://github.com/rpsychologist/powerlmm">two- and three-level linear mixed models</a>, and <a href="https://aaroncaldwell.us/SuperpowerBook/">Superpower</a> created by Aaron Caldwell and myself does power analyses for complex ANOVA designs.</p>
<p>It takes time to learn to use this software correctly. You will need to sit down for a few hours and go through vignettes or read the accompanying publications, before you know what to do. In our online manual for Superpower, we compare Superpower against G*Power, <code>pwr</code>, <code>pwr2ppl</code> and MorePower. Each software package will have slightly different design philosophies. In G*Power you are expected to enter the standardized effect size d, while in MorePower we enter the mean difference and the standard deviation, and in Superpower we need to enter the means and standard deviations for each condition. In general I recommend to always think about the raw pattern of means you expect, before you perform a power analysis. This typically leads to more realistic effect sizes estimates than the use of standardized effect sizes (where researchers all too often ‘expect’ a d = 0.5 just because that is the default value in G*Power).</p>
<div class="figure">
<span id="fig:powerex1"></span>
<p class="caption marginnote shownote">
Figure 2.11: Example of a power analysis in G*Power.
</p>
<img src="images/powerex1.png" alt="Example of a power analysis in G\*Power." width="378">
</div>
<div class="figure">
<span id="fig:powerex2"></span>
<p class="caption marginnote shownote">
Figure 2.12: Example of a power analysis in MorePower.
</p>
<img src="images/powerex2.png" alt="Example of a power analysis in MorePower." width="222">
</div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="2-3-a-priori-power-analysis.html#cb31-1"></a>pwr<span class="op">::</span><span class="kw">pwr.t.test</span>(<span class="dt">d =</span> <span class="fl">0.5</span>, </span>
<span id="cb31-2"><a href="2-3-a-priori-power-analysis.html#cb31-2"></a>                <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb31-3"><a href="2-3-a-priori-power-analysis.html#cb31-3"></a>                <span class="dt">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb31-4"><a href="2-3-a-priori-power-analysis.html#cb31-4"></a>                <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb31-5"><a href="2-3-a-priori-power-analysis.html#cb31-5"></a>                <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 63.76561
##               d = 0.5
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="2-3-a-priori-power-analysis.html#cb33-1"></a>design_result &lt;-<span class="st"> </span>Superpower<span class="op">::</span><span class="kw">ANOVA_design</span>(<span class="dt">design =</span> <span class="st">"2b"</span>,</span>
<span id="cb33-2"><a href="2-3-a-priori-power-analysis.html#cb33-2"></a>                                          <span class="dt">n =</span> <span class="dv">86</span>,</span>
<span id="cb33-3"><a href="2-3-a-priori-power-analysis.html#cb33-3"></a>                                          <span class="dt">mu =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>),</span>
<span id="cb33-4"><a href="2-3-a-priori-power-analysis.html#cb33-4"></a>                                          <span class="dt">sd =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb33-5"><a href="2-3-a-priori-power-analysis.html#cb33-5"></a>                                          <span class="dt">plot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb33-6"><a href="2-3-a-priori-power-analysis.html#cb33-6"></a></span>
<span id="cb33-7"><a href="2-3-a-priori-power-analysis.html#cb33-7"></a><span class="co"># Plot power curve (from 5 to 200)</span></span>
<span id="cb33-8"><a href="2-3-a-priori-power-analysis.html#cb33-8"></a>Superpower<span class="op">::</span><span class="kw">plot_power</span>(design_result, <span class="dt">max_n =</span> <span class="dv">200</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:powerex3"></span>
<img src="Statistical_Inferences_files/figure-html/powerex3-1.png" alt="Example of a power analysis plot in Superpower." width="672"><p class="caption marginnote shownote">
Figure 2.13: Example of a power analysis plot in Superpower.
</p>
</div>
</div>
<div id="justifyeffectsize" class="section level3">
<h3>
<span class="header-section-number">2.3.2</span> Justifying the effect size used in an a-priori power analysis</h3>
<p>One challenge in power analysis is that you never know the true effect size. This leads to the ‘sample size samba’ <span class="citation">(Schulz &amp; Grimes, <label for="tufte-mn-21" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle">2005<span class="marginnote">Schulz, K. F., &amp; Grimes, D. A. (2005). Sample size calculations in randomised trials: Mandatory and mystical. <em>The Lancet</em>, <em>365</em>(9467), 1348–1353.</span>)</span>. Researchers go back and forth between the effect size they expect, and the sample size they are willing to collect, until they ‘expect’ the effect size that, in an a-priori power analysis, leads to the sample size they are willing to collect. This practice obviously makes a power analysis a useless procedure. If you perform an a-priori power analysis, it is essential that you have a good justification for the effect size you rely on, or the result of the power analysis has no value.</p>
<div id="using-a-smallest-effect-size-of-interest" class="section level4">
<h4>
<span class="header-section-number">2.3.2.1</span> Using a smallest effect size of interest</h4>
<p>Best practice is to justify an a-priori power analysis based on a smallest effect size of interest. Because the population effect size is always uncertain (indeed, estimating this is typically one of the goals of the study) this means there is considerable uncertainty about the achieved power in any study (see Figure <a href="2-2-feasibility.html#fig:power-1">2.2</a>). However, there the smallest effect size of interest is not uncertain. A smallest effect of interest may be subjective (one researcher might find effect sizes smaller than d = 0.3 meaningless, while another researcher might still be interested in effects up to d = 0.1), but it does not have uncertainty. This means that if you enter it in an a-priori power analyses, you will be guaranteed to have the desired power (or higher power) for the smallest effect size you care about. The topic of how to justify a smallest effect size of interest is discussed in a <a href="8-sesoi.html#sesoi">dedicated chapter</a>.</p>
</div>
<div id="using-an-estimate-of-a-meta-analysis" class="section level4">
<h4>
<span class="header-section-number">2.3.2.2</span> Using an estimate of a meta-analysis</h4>
<p>As we will see in the chapter on <a href="10-bias.html#bias">bias</a> it is regrettably not true that effect size estimates from meta-analyses are always accurate. They can be biased, sometimes substantially so. Furthermore, meta-analyses typically have considerable <a href="9-3-introduction-to-meta-analysis.html#heterogeneity">heterogeneity</a>, which means that the meta-analytic effect size estimate differs for subsets of studies that make up the meta-analysis. So, although it might seem useful to enter a meta-analytic effect size estimate in your power analysis, you need to take great care before doing so.</p>
<p>If you want to enter a meta-analytic effect size estimate in an a-priori power analysis, you need to consider three things. First, the studies included in the meta-analysis should be similar enough to the study you are performing that it is reasonable to expect a similar effect size. In essence, this requires evaluating the generalizability of the effect size estimate to your study. Carefully consider differences between the meta-analysed studies and your study, with respect to the manipulation, the measure, the population, and any other relevant variables. Second, check whether the effect sizes reported in the meta-analysis are homogeneous. If not, and there is considerable heterogeneity in the meta-analysis, it means not all included studies can be expected to have the same true effect size estimate. This is almost always true. Choose the meta-analytic estimate for the subset of studies that most closely represent your planned study (this is why it is important to make sure others can <a href="9-3-introduction-to-meta-analysis.html#metareporting">reproduce your meta-analysis</a>. Third, the meta-analytic effect size estimate should not be biased. Check the bias detection tests that are reported in the meta-analysis, and consider bias corrected effect size estimates (even though these estimates might still be biased, and not reflect the true population effect size).</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:tablemetajust">Table 2.3: </span>Overview of recommendations when justifying the use of a meta-analytic effect size estimate for a power analysis.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to take into account
</th>
<th style="text-align:left;">
How to take it into account?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Are the studies in the meta-analysis similar?
</td>
<td style="text-align:left;">
Are the studies in the meta-analyses very similar in design, measure, an population to the study you are planning? Evalaute the generalizability of the effect size estimate to your study.
</td>
</tr>
<tr>
<td style="text-align:left;">
Are the studies in the meta-analysis homogeneous?
</td>
<td style="text-align:left;">
Is there heterogeneity in the meta-analysis? If so, use the meta-analytic effect size estimate of the most relevant homogenous subsample.
</td>
</tr>
<tr>
<td style="text-align:left;">
Is the effect size estimate unbiased?
</td>
<td style="text-align:left;">
Did the original study report bias detection tests, and was there bias? If so, it might be wise to use a more conservative effect size estimate, based on bias correction techniques (e.g., PET-PEESE) while acknowledging these corrected effect size estimates might not represent the true meta-analytic effect size estimate.
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-an-estimate-from-a-previous-study" class="section level4">
<h4>
<span class="header-section-number">2.3.2.3</span> Using an estimate from a previous study</h4>
<p>Statisticians have warned against using effect size estimates from small samples in power analyses. Leon, Davis, and Kraemer <span class="citation">(<label for="tufte-mn-22" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-22" class="margin-toggle">2011<span class="marginnote">Leon, A. C., Davis, L. L., &amp; Kraemer, H. C. (2011). The Role and Interpretation of Pilot Studies in Clinical Research. <em>Journal of Psychiatric Research</em>, <em>45</em>(5), 626–629. <a href="https://doi.org/10.1016/j.jpsychires.2010.10.008">https://doi.org/10.1016/j.jpsychires.2010.10.008</a></span>)</span> write:</p>
<blockquote>
<p>Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples.</p>
</blockquote>
<p>The two main reasons researchers should be careful when using effect sizes from the published literature in power analyses is that effect size estimates from small studies are inaccurate, and that publication bias inflates effect sizes. Bias can even emerge when researchers do not take an effect size estimate from the literature, but perform the pilot study themselves, due to <strong>follow-up bias</strong> <span class="citation">(Albers &amp; Lakens, <label for="tufte-mn-23" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-23" class="margin-toggle">2018<span class="marginnote">Albers, C. J., &amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. <em>Journal of Experimental Social Psychology</em>, <em>74</em>, 187–195. <a href="https://doi.org/10.1016/j.jesp.2017.09.004">https://doi.org/10.1016/j.jesp.2017.09.004</a></span>)</span>. Figure <a href="2-3-a-priori-power-analysis.html#fig:follow-up-bias">2.14</a> illustrates that for a 3 group ANOVA with 15 participants in each group, only effects with a partial eta squared larger than 0.133 will be statistically significant. But even if researchers are willing to follow up on smaller effect sizes, entering an effect size estimate of <span class="math inline">\(\eta_p^2\)</span> = 0.01 in an a-priori power analysis would require a total sample size of 957 observations for 80% power. If researchers only follow up on pilot studies when they leads to feasible sample sizes, their effect size estimates will be biased.</p>
<div class="figure">
<span id="fig:follow-up-bias"></span>
<p class="caption marginnote shownote">
Figure 2.14: Distribution of partial eta squared under H0 (grey curve) and a true effect of 0.0588 (black curve).
</p>
<img src="Statistical_Inferences_files/figure-html/follow-up-bias-1.png" alt="Distribution of partial eta squared under H0 (grey curve) and a true effect of 0.0588 (black curve)." width="672">
</div>
<p>In essence, the problem illustrated in Figure <a href="2-3-a-priori-power-analysis.html#fig:follow-up-bias">2.14</a> is that the effect sizes we end up using four our power analysis do not come from a full <em>f</em>-distribution, but that we ignore small effect sizes, and thus perform power analyses based on a <strong>truncated <em>F</em>-distribution</strong>. It is possible to correct for bias, using some assumptions about bias. For example, imagine we observe a result in the literature for a One-Way ANOVA with 3 conditions, reported as <em>F</em>(2, 42) = 0.017, <span class="math inline">\(\eta_p^2\)</span> = 0.176. Taking this effect size at face value, entering it in an a-priori power analysis would suggest we need to collect 27 observations in each condition. However, using the <code>BUCSS</code> R package <span class="citation">(Anderson et al., <label for="tufte-mn-24" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-24" class="margin-toggle">2017<span class="marginnote">Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. <em>Psychological Science</em>, <em>28</em>(11), 1547–1562.</span>)</span>, we can perform a power analysis that attempts to correct for bias.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="2-3-a-priori-power-analysis.html#cb34-1"></a><span class="kw">library</span>(BUCSS)</span>
<span id="cb34-2"><a href="2-3-a-priori-power-analysis.html#cb34-2"></a><span class="kw">ss.power.ba</span>(</span>
<span id="cb34-3"><a href="2-3-a-priori-power-analysis.html#cb34-3"></a>  <span class="dt">F.observed =</span> <span class="fl">4.5</span>,</span>
<span id="cb34-4"><a href="2-3-a-priori-power-analysis.html#cb34-4"></a>  <span class="dt">N =</span> <span class="dv">45</span>,</span>
<span id="cb34-5"><a href="2-3-a-priori-power-analysis.html#cb34-5"></a>  <span class="dt">levels.A =</span> <span class="dv">3</span>,</span>
<span id="cb34-6"><a href="2-3-a-priori-power-analysis.html#cb34-6"></a>  <span class="dt">effect =</span> <span class="kw">c</span>(<span class="st">"factor.A"</span>),</span>
<span id="cb34-7"><a href="2-3-a-priori-power-analysis.html#cb34-7"></a>  <span class="dt">alpha.prior =</span> <span class="fl">0.05</span>,</span>
<span id="cb34-8"><a href="2-3-a-priori-power-analysis.html#cb34-8"></a>  <span class="dt">alpha.planned =</span> <span class="fl">0.05</span>,</span>
<span id="cb34-9"><a href="2-3-a-priori-power-analysis.html#cb34-9"></a>  <span class="dt">assurance =</span> <span class="fl">0.5</span>,</span>
<span id="cb34-10"><a href="2-3-a-priori-power-analysis.html#cb34-10"></a>  <span class="dt">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb34-11"><a href="2-3-a-priori-power-analysis.html#cb34-11"></a>  <span class="dt">step =</span> <span class="fl">0.001</span></span>
<span id="cb34-12"><a href="2-3-a-priori-power-analysis.html#cb34-12"></a>)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 73
## 
## [[2]]
## [1] 2.012</code></pre>
<p>This analysis suggests collecting 73 participants in each condition, based on a bias corrected (under a specific model of publication bias assuming only significant effects ( with <em>p</em> &lt; 0.05) being published) non-centrality parameter of 2.012. It is possible that the bias corrected non-centrality parameter is zero, in which case it is not possible to correct for bias (or the true effect size might be 0).</p>
<p>Instead of formally modeling a correction for publication bias, researchers can simply use a more conservative effect size estimate, for example by computing power based on the lower limit of 60% two-sided confidence interval around the effect size estimate <span class="citation">(Perugini et al., <label for="tufte-mn-25" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-25" class="margin-toggle">2014<span class="marginnote">Perugini, M., Gallucci, M., &amp; Costantini, G. (2014). Safeguard power as a protection against imprecise power estimates. <em>Perspectives on Psychological Science</em>, <em>9</em>(3), 319–332.</span>)</span>, which they refer to as <strong>safeguard power</strong>.</p>
<p>Both these approaches lead to a more conservative power analysis, but not necessarily a more accurate power analysis. It is simply not possible to perform an accurate power analysis on the basis of an effect size estimate from a study that might be biased and had a small sample size.</p>
<p>A sample size from a single study can be used if two conditions are met. First, there was a low risk of bias (e.g., the effect size estimate comes from a Registered Report, or from an analysis which results would not have impacted the likelihood of publication). Second, the sample size is large enough to yield relatively accurate effect size estimate, based on the width of a 95% CI around the observed effect size estimate. There is always uncertainty around the effect size estimate, and entering the upper and lower limit of the 95% CI around the effect size estimate might be educational about the consequences of the uncertainty that is present for an a-priori power analysis.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-es-just">Table 2.4: </span>Overview of recommendations when justifying the use of an effect size estimate from a single study.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to take into account
</th>
<th style="text-align:left;">
How to take it into account?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Is there a risk of bias?
</td>
<td style="text-align:left;">
Evaluate the possibility that you would not have used, or had access to, the effect size estimate if had been smaller. Examine the difference when entering the reported, and a bias corrected, effect sizes estimate in a power analysis.
</td>
</tr>
<tr>
<td style="text-align:left;">
How large is the uncertainty?
</td>
<td style="text-align:left;">
Studies with a small number of observations have large uncertainty. Consider the possibility of using a more conservative effect sizes estimate to reduce the possibility of an underpowered study for the true effect size (such as a safeguard power analysis).
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-a-heuristic" class="section level4">
<h4>
<span class="header-section-number">2.3.2.4</span> Using a heuristic</h4>
<p>The most commonly entered effect size estimate in an a-priori power analysis for an independent <em>t</em>-test is d = 0.5. The reason is that it is the default in G*Power. It is also an effect size that Cohen <span class="citation">(<label for="tufte-mn-26" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-26" class="margin-toggle">1988<span class="marginnote">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed). L. Erlbaum Associates.</span>)</span> referred to as a ‘medium’ effect size, which seems less pretentious than expecting a ‘large’ effect size (d = 0.8) and leads to a required sample size that is substantially more manageable than when entering a ‘small’ effect size (d = 0.2). Cohen would probably not have chosen d = 0.5 as a default option for a power analysis tool designed for psychologists, as he wrote (1988, p. 13):</p>
<blockquote>
<p>Many effects sought in personality, social, and clinical-psychological research are likely to be small effects as here defined, both because of the attenutation in validity of the measures employed and the subtlety of the issues frequently involved.</p>
</blockquote>
<p>The large variety in research topics in psychology means that any ‘default’ or ‘heuristic’ you use in an a-priori power analysis is wrong, and more importantly, it is likely to be more substantially wrong than using any of the other strategies outlines here. Cohen’s benchmarks should not be used in a-priori power analysis <span class="citation">(Correll et al., <label for="tufte-mn-27" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-27" class="margin-toggle">2020<span class="marginnote">Correll, J., Mellinger, C., McClelland, G. H., &amp; Judd, C. M. (2020). Avoid Cohen’s “Small”, “Medium”, and “Large” for Power Analysis. <em>Trends in Cognitive Sciences</em>, <em>24</em>(3), 200–207. <a href="https://doi.org/10.1016/j.tics.2019.12.009">https://doi.org/10.1016/j.tics.2019.12.009</a></span>)</span>.</p>
<p>Some researchers have wondered what a better default would be, if researchers have no other basis to decide upon an effect size for an a-priori power analysis. For example, sometimes it is recommended to perform a power analysis based on an average effect size in psychology. For example, Brysbaert <span class="citation">(<label for="tufte-mn-28" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-28" class="margin-toggle">2019<span class="marginnote">Brysbaert, M. (2019). How many participants do we have to include in properly powered experiments? A tutorial of power analysis with reference tables. <em>Journal of Cognition</em>, <em>2</em>(1), 16. <a href="https://doi.org/10.5334/joc.72">https://doi.org/10.5334/joc.72</a></span>)</span> recommends d = 0.4 as a default, which is approximately the average effect size in meta-meta-analyses, analysing effect sizes in entire scientific disciplines. However, such meta-analytic effect sizes estimated might be inflated due to publication bias, and there is huge heterogeneity in each field, so the average will often not be close to the effect size in the research line your are performing. As such, the use of such estimates is also not recommended.</p>
</div>
</div>
<div id="justifying-the-error-rates-used-in-an-a-priori-power-analysis" class="section level3">
<h3>
<span class="header-section-number">2.3.3</span> Justifying the error rates used in an a-priori power analysis</h3>
<p>TOO BE COMPLETED</p>
</div>
<div id="some-advice-when-using-gpower" class="section level3">
<h3>
<span class="header-section-number">2.3.4</span> Some advice when using G*Power</h3>
<p>G*Power is one of the most widely used software tools for power analysis <span class="citation">(Bakker et al., <label for="tufte-mn-29" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-29" class="margin-toggle">2020<span class="marginnote">Bakker, M., Veldkamp, C. L. S., Akker, O. R. van den, Assen, M. A. L. M. van, Crompvoets, E., Ong, H. H., &amp; Wicherts, J. M. (2020). Recommendations in pre-registrations and internal review board proposals promote formal power analyses but do not increase sample size. <em>PLOS ONE</em>, <em>15</em>(7), e0236079. <a href="https://doi.org/10.1371/journal.pone.0236079">https://doi.org/10.1371/journal.pone.0236079</a></span>)</span>. The option for power analysis for a Pearson’s correlation coefficient is under the Exact test family (Correlation: Bivariate normal model). The “Correlation: Point biserial model” option under the <em>t</em>-tests family is for correlations where one variable is dichotomous. The difference is small, but the required sample size is typically a few observations larger for Pearson’s correlation coefficient.</p>
<div class="figure">
<span id="fig:gpowcor"></span>
<p class="caption marginnote shownote">
Figure 2.15: The options for a power analysis for Pearson’s correlation(above) and the point biserial correlation (when one variable is dichotomous).
</p>
<img src="images/gpowcor.png" alt="The options for a power analysis for Pearson's correlation(above) and the point biserial correlation (when one variable is dichotomous)." width="377">
</div>
<p>Although the effect size for an independent <em>t</em>-test and dependent <em>t</em>-test are often both referred to as Cohen’s d, they differ, and are calculated in different ways. Cohen himself distinguished between <span class="math inline">\(d_s\)</span> and <span class="math inline">\(d_z\)</span> <span class="citation">(Lakens, <label for="tufte-mn-30" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-30" class="margin-toggle">2013<span class="marginnote">Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. <em>Frontiers in Psychology</em>, <em>4</em>. <a href="https://doi.org/10.3389/fpsyg.2013.00863">https://doi.org/10.3389/fpsyg.2013.00863</a></span>)</span>. You should not enter Cohen’s d for a power analysis for a dependent <em>t</em>-test (nor should you directly compared Cohen’s d from a between design with Cohen’s d for a within design, nor should you use the benchmarks Cohen provided for a small (0.2), medium (0.5), and large (0.8) effect be used for Cohen’s <span class="math inline">\(d_z\)</span>). Make sure you are entering the correct effect size. Cohen’s <span class="math inline">\(d_z\)</span> can be calculated from the <em>t</em>-value for a dependent <em>t</em>-test and the sample size as follows:</p>
<p><span class="math display">\[d_z = {\frac{t}{\sqrt{n}}} \]</span></p>
<div class="figure">
<span id="fig:gpowdzd"></span>
<p class="caption marginnote shownote">
Figure 2.16: Power for a dependent and independent <em>t</em>-test require entering Cohen’s dz and d, respectively.
</p>
<img src="images/gpowdzd.png" alt="Power for a dependent and independent *t*-test require entering Cohen's dz and d, respectively." width="377">
</div>
<p>The third issue that researchers often miss is that G*Power has a very unfortunate default setting for power analyses for within subject ANOVA tests. In Figure <a href="2-3-a-priori-power-analysis.html#fig:gpowdwithin2">2.17</a> we see on the left how we can directly calculate Cohen’s <span class="math inline">\(f\)</span> (the effect size one needs to enter for ANOVA power analyses) from partial eta squared. A medium effect size for <span class="math inline">\(\eta^2_p\)</span> of 0.588 equals a Cohen’s <span class="math inline">\(f\)</span> of 0.25. If we specify an expected correlation between dependent variables of 0.8, have 3 repeated measurements, one condition, and want to achieve 95% power with an alpha level of 0.05, we need a sample size of 19. However, if out value for <span class="math inline">\(\eta^2_p\)</span> comes from the scientific literature or statistical software such as SPSS, this effect size measure already has the correlation between observation incorporated. If we would simply enter it in G*Power, we take into account the correlation twice, which leads to a massively smaller sample size. Instead, we need to click on the “Options” button and check the radiobutton before “As in SPSS”. We see that “Effect size f” changes into “Effect size f(U)”, and the box “Corr among rep measures” has disappeared on the right. This is because the correlation no longer needs to be entered separately - it is already taken into account in the <span class="math inline">\(\eta^2_p\)</span> as SPSS computes it. Most importantly, we now see that the sample size we need to achieve 95% power has changed to 127. This is a big difference, and I have seen people make this mistake very often. If you use G*power for power analyses for ANOVA designs, I recommend always double checking the Options setting (and maybe repeat your analysis in software such as Superpower just to double check).</p>
<div class="figure">
<span id="fig:gpowdwithin2"></span>
<p class="caption marginnote shownote">
Figure 2.17: Power analysis for repeated ANOVA in G*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account.
</p>
<img src="images/gpowwithin2.png" alt="Power analysis for repeated ANOVA in G\*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account." width="956">
</div>
</div>
<div id="powerforabsence" class="section level3">
<h3>
<span class="header-section-number">2.3.5</span> A-priori power analysis for the absence of an effect</h3>
<p>It is</p>
</div>
<div id="reporting-an-a-priori-power-analysis." class="section level3">
<h3>
<span class="header-section-number">2.3.6</span> Reporting an a-priori power analysis.</h3>
<p>As with all the analyses you rely on in your work, make sure the power analysis is computationally reproducible. If you performed power analyses in R, you can store the script. In G*Power, you can save the several power analyses you performed consecutively in the app under the ‘protocol of power analysis’ tab by saving them as a .rtf file or printing them as a pdf file.</p>
<div class="figure">
<span id="fig:gpowprotocol"></span>
<p class="caption marginnote shownote">
Figure 2.18: G*Power allows you to easily save all details about the power analysis you performed.
</p>
<img src="images/gpowprotocol.png" alt="G\*Power allows you to easily save all details about the power analysis you performed." width="375">
</div>
<p>If you use the <a href="https://arcstats.io/shiny/anova-exact/">Superpower Shiny app</a> for more complex ANOVA designs, you can print a pdf file with all detailed about the power analysis you performed. Sharing the code or a printout of the power analysis will capture all required information about the power analysis you performed, which is often preferable over attempting to communicate all this information verbally.</p>
<div class="figure">
<span id="fig:superpowerreport"></span>
<p class="caption marginnote shownote">
Figure 2.19: Superpower allows you to print a report with all information about the power analysis you performed.
</p>
<img src="images/superpowerreport.png" alt="Superpower allows you to print a report with all information about the power analysis you performed." width="738">
</div>
<p>Providing a reproducible report is sufficient to clarify the power calculation, but not the reasons behind the power calculations. Your decision for the desired Type 1 and Type 2 error rate needs a justification, and most importantly, the effect size you have powered for needs to be justified. If your effect size estimate is based on the existing literature, provide a full citation, and preferably a direct quote from the article. If your effect size is based on an empirical estimate from the scientific literature, you will need to address 1) uncertainty, and 2) bias <span class="citation">(Anderson et al., <label for="tufte-mn-31" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-31" class="margin-toggle">2017<span class="marginnote">Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. <em>Psychological Science</em>, <em>28</em>(11), 1547–1562.</span>; Taylor &amp; Muller, <label for="tufte-mn-32" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-32" class="margin-toggle">1996<span class="marginnote">Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and sample size calculation due to estimating noncentrality. <em>Communications in Statistics-Theory and Methods</em>, <em>25</em>(7), 1595–1610.</span>)</span>. If your effect size is based on a smallest effect size of interest, this value should not just be stated, but <a href="6-2-justifysesoi.html#justifysesoi">justified</a> (e.g., based on theoretical predictions, practical implications, or effect sizes observed or detectable in the literature).</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-pow-rec-2">Table 2.5: </span>Overview of recommendations when reporting an a-prior power analysis.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to address?
</th>
<th style="text-align:left;">
How to address it?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
List all primary analyses you plan to do.
</td>
<td style="text-align:left;">
Following JARS guidelines, specify all primary analyses for which you want to control Type 1 and Type 2 error rates
</td>
</tr>
<tr>
<td style="text-align:left;">
Specify the alpha level for each analysis
</td>
<td style="text-align:left;">
List and justify the chosen alpha level for each analysis. Make sure to correct for multiple comparisons where needed.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the desired power?
</td>
<td style="text-align:left;">
List and justify the desired power for each analysis
</td>
</tr>
<tr>
<td style="text-align:left;">
For each power analysis, specify the effect size metric, the effect size, and the justification for powering for the effect size.
</td>
<td style="text-align:left;">
Report the effect size metric (e.g., Cohen’s <span class="math inline">\(d_z\)</span>, Cohen’s f), the effect size (e.g., 0.3). and the justification for the effect size, whether it is based on a smallest effect size of interest, a meta-analytic effect size estimate, or the estimate of a single previous study.
</td>
</tr>
<tr>
<td style="text-align:left;">
Make sure the power analysis is reproducible.
</td>
<td style="text-align:left;">
Include the code used to run the power analysis, or print a report containing the details about the power analyses you performed.
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="2-2-feasibility.html"><button class="btn btn-default">Previous</button></a>
<a href="2-4-compromisepower.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-15
</p>
</div>
</div>



</body>
</html>
