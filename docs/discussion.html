<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 15 Discussion | Improving Your Statistical Inferences</title>

    <meta name="author" content="Daniel Lakens" />
  
   <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 15 Discussion | Improving Your Statistical Inferences" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Discussion | Improving Your Statistical Inferences" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::bs4_book,
set in the _output.yml file.</p>" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.3.1/transition.js"></script>
    <script src="libs/bs3compat-0.3.1/tabs.js"></script>
    <script src="libs/bs3compat-0.3.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
    <link rel="stylesheet" href="bs4_style.css" />
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="discussion" class="section level1" number="15">
<h1><span class="header-section-number">Chapter 15</span> Discussion</h1>
<p>Providing a coherent sample size justification is an essential step in designing an informative study. There are multiple approaches to justifying the sample size in a study, depending on the goal of the data collection, the resources that are available, and the statistical approach that is used to analyze the data. An overarching principle in all these approaches is that researchers consider the value of the information they collect in relation to their inferential goals.</p>
<p>The process of justifying a sample size when designing a study should sometimes lead to the conclusion that it is not worthwhile to collect the data, because the study does not have sufficient informational value to justify the costs. There will be cases where it is unlikely there will ever be enough data to perform a meta-analysis (for example because of a lack of general interest in the topic), the information will not be used to make a decision or claim, and the statistical tests do not allow you to test a hypothesis with reasonable error rates or to estimate an effect size with sufficient accuracy. If there is no good justification to collect the maximum number of observations that one can feasibly collect, performing the study anyway is a waste of time and/or money <span class="citation">(<a href="references.html#ref-brown_errors_1983" role="doc-biblioref">Brown, 1983</a>; <a href="references.html#ref-button_power_2013" role="doc-biblioref">Button et al., 2013</a>; <a href="references.html#ref-halpern_continuing_2002" role="doc-biblioref">S. D. Halpern et al., 2002</a>)</span>.</p>
<p>The awareness that sample sizes in past studies were often too small to meet any realistic inferential goals is growing among psychologists <span class="citation">(<a href="references.html#ref-button_power_2013" role="doc-biblioref">Button et al., 2013</a>; <a href="references.html#ref-fraley_n-pact_2014" role="doc-biblioref">Fraley &amp; Vazire, 2014</a>; <a href="references.html#ref-lindsay_replication_2015" role="doc-biblioref">Lindsay, 2015</a>; <a href="references.html#ref-sedlmeier_studies_1989" role="doc-biblioref">Sedlmeier &amp; Gigerenzer, 1989</a>)</span>. As an increasing number of journals start to require sample size justifications, some researchers will realize they need to collect larger samples than they were used to. This means researchers will need to request more money for participant payment in grant proposals, or that researchers will need to increasingly collaborate <span class="citation">(<a href="references.html#ref-moshontz_psychological_2018" role="doc-biblioref">Moshontz et al., 2018</a>)</span>. If you believe your research question is important enough to be answered, but you are not able to answer the question with your current resources, one approach to consider is to organize a research collaboration with peers, and pursue an answer to this question collectively.</p>
<p>A sample size justification should not be seen as a hurdle that researchers need to pass before they can submit a grant, ethical review board proposal, or manuscript for publication. When a sample size is simply stated, instead of carefully justified, it can be difficult to evaluate whether the value of the information a researcher aims to collect outweighs the costs of data collection. Being able to report a solid sample size justification means a researchers knows what they want to learn from a study, and makes it possible to design a study that can provide an informative answer to a scientific question.</p>
<div id="designing-efficient-studies" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Designing efficient studies</h2>
<p>So far, we have mainly focused on how to perform a power analysis based on assumptions about what the true effect size is. Although the true effect size is unknown, there is a true effect size in the population, and it has an impact on our statistical power. It is therefore useful to consider how we can design studies that will be optimally efficient to answer our research question. There are some ways in which we can increase the power of our test that are worth exploring.</p>
<div id="use-directional-tests-where-relevant." class="section level3" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> Use directional tests where relevant.</h3>
<p>Researchers often make directional predictions, such as ‘we predict X is larger than Y’. The statistical test that logically follows from this prediction is a directional (or one-sided) <em>t</em>-test. A directional test moves the Type 1 error rate to one side of the tail, which lowers the critical value, and requires less observations to achieve the same statistical power. Although there is some discussion about when directional tests are appropriate, they are perfectly defensible from a Neyman-Pearson perspective on hypothesis testing. However, there might be situations where you do not want to ask a directional test. Sometimes it might be important for an effect opposite to your prediction to also reach statistical significance. For example, when you are evaluating a recently introduced educational intervention, and you predict the intervention will increase the performence of students, you might want to remain open to the possibility that students perform worse, to be able to recommend abandoning the new intervention. It is also possible to distribute the error rate in a 'lop-sided' manner, for example assigning a stricter error rate to effects in the negative than in the positive direction <span class="citation">(<a href="references.html#ref-rice_heads_1994" role="doc-biblioref">Rice &amp; Gaines, 1994</a>)</span>.</p>
</div>
<div id="use-sequential-analysis-whenever-possible" class="section level3" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> Use sequential analysis whenever possible</h3>
<p><a href="##sequential">Sequential analyses</a> greatly increase the efficiency of tests <span class="citation">(<a href="references.html#ref-lakens_performing_2014" role="doc-biblioref">Lakens, 2014</a>)</span>. For any of the ways in which you determined the sample size of your study that have been discussed here, a sequential design, where error rates are controlled as you repeatedly analyze the data that comes in, will on average decrease the sample size you will collect.</p>
</div>
<div id="increase-your-alpha-level" class="section level3" number="15.1.3">
<h3><span class="header-section-number">15.1.3</span> Increase your alpha level</h3>
<p>As explained in the section on <a href="#compromisepower">compromise power analysis</a> balancing error rates by increasing your alpha level also increases the statistical power (or reduces the Type 2 error rate). Obviously this comes at an increased probability of fooling yourself. The risk of doing so should be carefully weighed, which typically requires taking into account the prior probability that the null-hypothesis is true <span class="citation">(<a href="references.html#ref-miller_quest_2019" role="doc-biblioref">Miller &amp; Ulrich, 2019</a>)</span>. If you <em>have</em> to make a decision, and the data you can feasibly collect is limited, a compromise power analysis is justified, and so is increasing the alpha level.</p>
</div>
<div id="use-within-designs-where-possible" class="section level3" number="15.1.4">
<h3><span class="header-section-number">15.1.4</span> Use within designs where possible</h3>
<p>One widely recommended approach to increase power is using a within subject design. Indeed, you need fewer observations to detect a mean difference between two conditions in a within-subjects design (in a dependent <em>t</em>-test) than in a between-subjects design (in an independent t-test). The reason is straightforward, but not always explained, and even less often expressed in the easy equation below. The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell &amp; Delaney, 2004, p. 561, formula 45):</p>
<p><span class="math display">\[NW = \frac{NB (1-\rho)}{2}\]</span></p>
<p>We divide by two due to the fact that in a two-condition within design every participant provides two data-points. The extent to which this reduces the sample size compared to a between-subject design depends on the correlation between the two dependent variables, as indicated by the (1-<span class="math inline">\(\rho\)</span>) part of the equation. If the correlation is 0, a within-subject design simply needs half as many observations as a between-subject design (e.g., 64 instead 128 observations). The higher the correlation, the larger the relative benefit of within designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Note than when the correlation is -1, you need 128 observations in a within-design and 128 observations in a between-design, but in a within-design you will need to collect two measurements from each participant, making a within design more work than a between-design. However, negative correlations between dependent variables in psychology are rare, and perfectly negative correlations will probably never occur.</p>
<p>So what does the correlation do so that it increases the power of within designs, or reduces the number of observations you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots.</p>
<p>We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements on the left, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur). In the scatter plot on the right, we can see that the correlation between the measurements is indeed 0. Let's look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is <span class="math inline">\(\sqrt{2}\)</span> times as large as the standard deviation in each measurement, and indeed, 15×<span class="math inline">\(\sqrt{2}\)</span> = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent <em>t</em>-test, where the correlation between measurements is not taken into account.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plot-1"></span>
<img src="08-samplesizejustification_files/figure-html/plot-1-1.png" alt="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0." width="100%" />
<p class="caption">
Figure 15.1: Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0.
</p>
</div>
<p>Now let’s increase the correlation between dependent variables to 0.7. Nothing has changed when we plot the means. The correlation between measurements is now strongly positive, as we see in the plot on the right. The important difference lies in the standard deviation of the difference scores. The SD of the difference scores is 11 instead of 21 in the uncorrelated example. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plot-4"></span>
<img src="08-samplesizejustification_files/figure-html/plot-4-1.png" alt="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0.7." width="100%" />
<p class="caption">
Figure 15.2: Distributions of two independent groups with means 100 and 106 and a standard deviation of 15, distribution of the differences, and correlation of 0.7.
</p>
</div>
<p>If you set the correlation to a negative value, the standard deviation of the difference scores would increase. I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor.</p>
<p>You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (<em>M</em> = 390, <em>SD</em> = 44) when they used their feet than when they used their hands (<em>M</em> = 371, <em>SD</em> = 44, <em>r</em> = .953), <em>t</em>(17) = 5.98, <em>p</em> &lt; 0.001, Hedges' g = 0.43, M_diff = 19, 95% CI [12; 26]).</p>
<p>Since most dependent variables in within designs in psychology are positively correlated, within designs will greatly increase the power you can achieve given the sample size you have available. Use within-designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-subject design <span class="citation">(<a href="references.html#ref-maxwell_designing_2017" role="doc-biblioref">Maxwell et al., 2017</a>)</span>.</p>
<p>You can use this <a href="http://shiny.ieis.tue.nl/within_between/">Shiny app</a> to play around with different means, sd's, and correlations, and see the effect of the distribution of the difference scores.</p>
<!-- ```{r shiny2, echo=F} -->
<!-- knitr::include_app('http://shiny.ieis.tue.nl/within_between/', height = '4000px') -->
<!-- ``` -->
</div>
<div id="remove-statistical-variation-where-possible" class="section level3" number="15.1.5">
<h3><span class="header-section-number">15.1.5</span> Remove statistical variation where possible</h3>
<p>The smaller the variation, the larger the standardized effect size (because we are dividing the raw effect by a smaller denominator) and thus the higher the power given the same number of observations. For an overview of different approaches to reduce the variance, see <span class="citation">Allison et al. (<a href="references.html#ref-allison_power_1997" role="doc-biblioref">1997</a>)</span>. They discuss:</p>
<ol style="list-style-type: decimal">
<li>Better ways to screen participants for studies where participants need to be screened before participation.</li>
<li>Assigning participants unequally to conditions (if the control condition is much cheaper than the experimental condition, for example).</li>
<li>Using multiple measurements to increase measurement reliability (or use well-validated measures, if I may add).</li>
<li>Smart use of (preregistered, I’d recommend) covariates.</li>
</ol>
<p>Another approach they do not mention is to, where possible, collect multiple observations from the same participant. This can also increase power, especially if there is variation at the individual level, and you analyze data with hierarchical models.</p>
</div>
<div id="use-bayesian-statistics-with-informed-priors" class="section level3" number="15.1.6">
<h3><span class="header-section-number">15.1.6</span> Use Bayesian statistics with informed priors</h3>
<p>Regrettably, almost all approaches to statistical inferences become very limited when the number of observations is small. If you are very confident in your predictions (and your peers agree), incorporating prior information will give you a benefit. For a discussion of the benefits and risks of such an approach, see <span class="citation">van de Schoot et al. (<a href="references.html#ref-van_de_schoot_analyzing_2015" role="doc-biblioref">2015</a>)</span>.</p>
</div>
</div>
<div id="what-if-best-practices-are-not-enough" class="section level2" number="15.2">
<h2><span class="header-section-number">15.2</span> What if best practices are not enough?</h2>
<p>In the end, getting an informative answer requires designing an informative experiment, and sometimes no matter how efficient you design your experiment, you will need to collect more data. Sometimes. there are no secret tricks or magical solutions. So what should we do then? The solutions below are, regrettably, a lot more work than making a small change to the design of your study. But it is about time we start to take them seriously.</p>
<div id="ask-for-more-money-in-your-grant-proposals" class="section level3" number="15.2.1">
<h3><span class="header-section-number">15.2.1</span> Ask for more money in your grant proposals</h3>
<p>Some grant organizations distribute funds to be awarded as a function of how much money is requested. If you need more money to collect informative data, ask for it. Obviously grants are incredibly difficult to get, but if you ask for money, include a budget that acknowledges that data collection is not as cheap as you hoped some years ago. In my experience, psychologists are often asking for much less money to collect data than other scientists. Increasing the requested funds for participant payment by a factor of 10 is often reasonable, given the requirements of journals to provide a solid sample size justification, and the more realistic effect size estimates that are emerging from preregistered studies.</p>
</div>
<div id="improve-management" class="section level3" number="15.2.2">
<h3><span class="header-section-number">15.2.2</span> Improve management</h3>
<p>If the implicit or explicit goals that you should meet are still the same now as they were 10 years ago, and you did not receive a miraculous increase in money and time to do research, then an update of the evaluation criteria is long overdue. I sincerely hope your manager is capable of this, but some ‘upward management’ might be needed. In the coda of Lakens &amp; Evers (2014) we wrote:</p>
<blockquote>
<p>All else being equal, a researcher running properly powered studies will clearly contribute more to cumulative science than a researcher running underpowered studies, and if researchers take their science seriously, it should be the former who is rewarded in tenure systems and reward procedures, not the latter.</p>
</blockquote>
</div>
<div id="change-what-is-expected-from-phd-students" class="section level3" number="15.2.3">
<h3><span class="header-section-number">15.2.3</span> Change what is expected from PhD students</h3>
<p>When I did my PhD, there was the assumption that you performed enough research in the 4 years you are employed as a full-time researcher to write a thesis with 3 to 5 empirical chapters (with some chapters having multiple studies). These studies were ideally published, but at least publishable. If we consider it important for PhD students to produce multiple publishable scientific articles during their PhD’s, this will greatly limit the types of research they can do. Instead of evaluating PhD students based on their publications, we can see the PhD as a time where researchers learn skills to become an independent researcher, and evaluate them not based on publishable units, but in terms of clearly identifiable skills. I personally doubt data collection is particularly educational after the 20th participant, and I would probably prefer to hire a post-doc who had well-developed skills in programming, statistics, and who broadly read the literature, then someone who used that time to collect participant 21 to 200. If we make it easier for PhD students to demonstrate their skills level we can evaluate what they have learned in a more sensible manner than by counting the number of publications they wrote. Currently, difference in the resources PhD students have at their disposal are a huge confound as we try to judge their skill based on their resume. Researchers at rich universities obviously have more resources – it should not be difficult to develop tools that allow us to judge the skills of people where resources are much less of a confound.</p>
</div>
<div id="get-answers-collectively" class="section level3" number="15.2.4">
<h3><span class="header-section-number">15.2.4</span> Get answers collectively</h3>
<p>Our society has some serious issues that psychologists can help address. These questions are incredibly complex. I have long lost faith in the idea that a bottom-up organized scientific discipline that rewards individual scientists will manage to generate reliable and useful knowledge that can help to solve these societal issues. For some of these questions we need well-coordinated research lines where hundreds of scholars work together, pool their resources and skills, and collectively pursuit answers to these important questions. And if we are going to limit ourselves in our research to the questions we can answer in our own small labs, these big societal challenges are not going to be solved. Call me a pessimist.</p>
<p>There is a reason we resort to forming unions and organizations that have to goal to collectively coordinate what we do. If you greatly dislike team science, don’t worry – there will always be options to make scientific contributions by yourself. But now, there are almost no ways for scientists who want to pursue huge challenges in large well-organized collectives of hundreds or thousands of scholars (for a recent exception that proves my rule by remaining unfunded: see the <a href="https://psysciacc.org/">Psychological Science Accelerator</a>). If you honestly believe your research question is important enough to be answered, then get together with everyone who also thinks so, and pursue answers collectively.</p>

</div>
</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-02-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
