<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Asking Statistical Questions | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniel Lakens">
<meta name="description" content="At the core of the design of a new study is the evaluation of its information quality: the potential of a particular dataset for achieving a given analysis goal by employing data analysis methods...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 5 Asking Statistical Questions | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:description" content="At the core of the design of a new study is the evaluation of its information quality: the potential of a particular dataset for achieving a given analysis goal by employing data analysis methods...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Asking Statistical Questions | Improving Your Statistical Inferences">
<meta name="twitter:description" content="At the core of the design of a new study is the evaluation of its information quality: the potential of a particular dataset for achieving a given analysis goal by employing data analysis methods...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="active" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="" href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="" href="prereg.html"><span class="header-section-number">10</span> Preregistration and Transparency</a></li>
<li><a class="" href="references.html"><span class="header-section-number">11</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="questions" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Asking Statistical Questions<a class="anchor" aria-label="anchor" href="#questions"><i class="fas fa-link"></i></a>
</h1>
<p>At the core of the design of a new study is the evaluation of its <strong>information quality</strong>: the potential of a particular dataset for achieving a given analysis goal by employing data analysis methods and considering a given utility
<span class="citation">(<a href="references.html#ref-kenett_information_2016" role="doc-biblioref">Kenett et al., 2016</a>)</span>. The goal of data collection is to gain information through <strong>empirical research</strong> where observations are collected and analyzed, often through statistical models. Three approaches to statistical modelling can be distinguished <span class="citation">Shmueli (<a href="references.html#ref-shmueli_explain_2010" role="doc-biblioref">2010</a>)</span>: Description, explanation, and prediction, which are discussed below. The utility often depends on which effects are deemed interesting. A thorough evaluation of the informatin quality of a study therefore depends on clearly specifying the goal of data collection, the statistical modelling approach that is chosen, and the usefulness of the data to draw conclusions about effects of interest with the chosen analysis method. A study with low information quality might not be worth performing, as the data that will be collected has low potential to achieve the analysis goal.</p>
<div id="description" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Description<a class="anchor" aria-label="anchor" href="#description"><i class="fas fa-link"></i></a>
</h2>
<p>Description aims to answer questions about features of the empirical manifestation of some phenomenon. Description can involve unique events (e.g., case studies of single patients), classes of events (e.g., patients with a certain disease). Examples of features of interest are duration (how long), quantity (how many), location (where), etc.</p>
<p>An example of a descriptive question is research by <a href="https://en.wikipedia.org/wiki/Kinsey_Reports">Kinsey</a>, who studied the sexual behavior and experiences of Americans in a time that very little scientific research was available on this topic. He used interviews that provided the statistical basis to draw conclusions about sexuality in the United States, which, at the time, challenged conventional beliefs about sexuality.</p>
<p>Descriptive research questions are answered through <strong>estimation statistics</strong>. The informational value of an estimation study is determined by the amount of observations (the more observations, the higher the <strong>precision</strong> of the estimates) and the sampling plan (the more representative the sample, the lower the <strong>sample selection bias</strong>, which increases the ability to generalize from the sample to the population), and the reliability of the measure.</p>
<p>Descriptive research questions are sometimes seen as less exciting than explanation or prediction questions <span class="citation">(<a href="references.html#ref-gerring_mere_2012" role="doc-biblioref">Gerring, 2012</a>)</span>, but they are essential building block of theory formation <span class="citation">(<a href="references.html#ref-scheel_why_2021" role="doc-biblioref">Scheel, Tiokhin, et al., 2021</a>)</span>. Although estimation question often focus on the mean score of a measure, accurate estimates of the variance of a measure are extremely valuable as well. The variance of a measure is essential information in a well-informed sample size justification, both when planning for accuracy, as when performing an a-priori power analysis.</p>
</div>
<div id="prediction" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h2>
<p>The goal in predictive modeling is to apply an algorithm or a statistical model to predict future observations <span class="citation">(<a href="references.html#ref-shmueli_explain_2010" role="doc-biblioref">Shmueli, 2010</a>)</span>. For example, during the COVID-9 pandemic a large number of models were created that combined variables to estimate the risk that people would be infected with COVID, or that people who were infected would experience negative effects on their health <span class="citation">(<a href="references.html#ref-wynants_prediction_2020" role="doc-biblioref">Wynants et al., 2020</a>)</span>. Ideally, the goal is to develop a prediction model that accurately captures the regularities in its training data, and that generalizes well to unseen data. There is a <strong>bias-variance tradeoff</strong> between these two goals, and researchers need to decide how much bias should be reduced which increases the variance, or vice-versa <span class="citation">(<a href="references.html#ref-yarkoni_choosing_2017" role="doc-biblioref">Yarkoni &amp; Westfall, 2017</a>)</span>. The goal in prediction is to minimize prediction error. Common methods to evaluate prediction errors is <strong>cross-validation</strong>, where for example it is examined whether a model developed on a training dataset generalizes to a holdout dataset. The development of prediction models is becoming increasingly popular with the rise of machine learning approaches.</p>
</div>
<div id="explanation" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Explanation<a class="anchor" aria-label="anchor" href="#explanation"><i class="fas fa-link"></i></a>
</h2>
<p>The use of statistical models concerns tests of explanatory theories. In this case, statistical models are used to test causal assumptions, or explanations that we derive from theories.
Meehl <span class="citation">(<a href="references.html#ref-meehl_appraising_1990" role="doc-biblioref">1990</a>)</span> reminds us of the important distinction between a substantive theory, a statistical hypothesis, and observations. Statistical inference is only involved in drawing conclusions about the statistical hypothesis. Observations can lead to the conclusion that the statistical hypothesis is is confirmed (or not), but this conclusion does not directly translate into corroboration for the theory.</p>
<p>We never test a theory in isolation, but always include auxiliary hypotheses about the measures and instruments that are used in a study, conditions realized in the experiment, to the <strong>ceteris paribus</strong> clause that assumes all other things are equal. Therefore, it is never clear if a failure to corroborate a theoretical prediction should be blamed on the theory or the auxiliary hypotheses. To generate reliable explanatory theories, researchers therefore have to perform lines of research in which auxiliary hypotheses are systematically tested <span class="citation">(<a href="references.html#ref-tunc_falsificationist_2020" role="doc-biblioref">Tunç &amp; Tunç, 2020</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:meehl1990"></span>
<img src="images/meehl1990.png" alt="Distinction between a theoretical hypothesis, a statistical hypothesis, and observations. Figure based on Meehl, 1990." width="100%"><p class="caption">
Figure 5.1: Distinction between a theoretical hypothesis, a statistical hypothesis, and observations. Figure based on Meehl, 1990.
</p>
</div>
</div>
<div id="loosening-and-tightening" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Loosening and Tightening<a class="anchor" aria-label="anchor" href="#loosening-and-tightening"><i class="fas fa-link"></i></a>
</h2>
<p>For each of the three questions above, we can ask questions about description, prediction, and explanation during a <strong>loosening</strong> phase when doing research, or during a <strong>tightening</strong> phase <span class="citation">(<a href="references.html#ref-fiedler_tools_2004" role="doc-biblioref">Fiedler, 2004</a>)</span>. The distinction is relative. During the loosening stage, the focus is on creating variation that provides the source for new ideas. During the tightening stage, selection takes place with the goal to distinguish useful variants from less useful variants. In descriptive research, an unstructured interview is more aligned with the loosening phase, while a structured interview is more aligned with the tightening phase. In prediction, building a prediction model based on the training set is the loosening phase, while evaluation the prediction error in the holdout dataset is the tightening phase. In explanation, exploratory experimentation functions to generate hypotheses, while hypothesis tests function to distinguish theories that make predictions that are corroborated from those theories which predictions are not corroborated.</p>
<p>It is important to realize whether your goal is to generate new ideas, or to test new ideas. Researchers are often not explicit about the stage their research is in, which runs the risk of trying to test hypotheses prematurely <span class="citation">(<a href="references.html#ref-scheel_why_2021" role="doc-biblioref">Scheel, Tiokhin, et al., 2021</a>)</span>. Clinical trials research is more explicit about the different phases of research, and distinguishes Phase 1, Phase 2, Phase 3, and Phase 4 trials. In a Phase 1 trial researchers evaluate the safety of a new drug or intervention in a small group of non-randomized (often healthy) volunteers, by examining how much of a drug is safe to give, while monitoring a range of possible side effects. A phase 2 trial are often performed with patients as participants, and can focus in more detail on finding the definite dose. The goal is to systematically explore a range of parameters (e.g., the intensity of a stimulus) to identify boundary conditions <span class="citation">(<a href="references.html#ref-dubin_theory_1969" role="doc-biblioref">Dubin, 1969</a>)</span>. A phase 3 trial is a large randomized controlled trial with the goal to test the effectiveness of the new intervention in practice. Phase 3 trials require a prespecified statistical analyses plan that strictly controls error rates. Finally, a Phase 4 trial examines long term safety and generalizability. Compared to a Phase 3 trial, there is more loosening, as researchers explore the possibility of interactions with other drugs, or moderating effects in certain subgroups of the population. In clinical trials, a Phase 3 trial requires a huge amount of preparation, and is not undertaken lightly.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:trialphase"></span>
<img src="images/trialphase.jpg" alt='Four phases of clinical research. &lt;a href="https://clinicalinfo.hiv.gov/en/glossary/phase-1-trial"&gt;Source&lt;/a&gt;.' width="100%"><p class="caption">
Figure 5.2: Four phases of clinical research. <a href="https://clinicalinfo.hiv.gov/en/glossary/phase-1-trial">Source</a>.
</p>
</div>
</div>
<div id="three-statistical-philosophies" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Three statistical philosophies<a class="anchor" aria-label="anchor" href="#three-statistical-philosophies"><i class="fas fa-link"></i></a>
</h2>
<p>Royall <span class="citation">(<a href="references.html#ref-royall_statistical_1997" role="doc-biblioref">1997</a>)</span> distinguishes three questions one can ask:</p>
<ol style="list-style-type: decimal">
<li>What do I believe, now that I have this observation?</li>
<li>What should I do, now that I have this observation?</li>
<li>What does this observation tell me about A versus B? (How should I interpret this observation as evidence regarding A versus B?)</li>
</ol>
<p>One useful metaphor that think about these differences is if we look at Hinduism, where there are three ways to reach enlightenment: The Bhakti yoga, or the Path of Devotion, the Karma yoga, or the Path of Action, the Jnana yoga, or the Path of Knowledge. The three corresponding statistical paths are Bayesian statistics, which focuses on updating beliefs, Neyman-Pearson statistics, which focuses on making decisions about how to act, and likelihood approaches, which focus on quantifying the evidence or knowledge gained from the data. Just like in Hinduism the different paths are not mutually exclusive, and the emphasis on these three yoga's differs between individuals, so will scientists differ in their emphasis of their preferred approach to statistics.</p>
<p>The three approaches to statistical modelling (description, prediction, and explanation) can be examined from each the three statistical philosophies (e.g., frequentist estimation, maximum likelihood estimation, and Bayesian estimation, or Neyman-Pearson hypothesis tests, likelihood ratio tests, and Bayes factors). Bayesian approaches start from a specified prior belief, and use the data to update their belief. Frequentist procedures focus on methodological procedures that allow researchers to make inferences that control the probability of error in the long run. Likelihood approaches focus on quantifying the evidential value in the observed data. When used knowledgeably, these approaches often yield very similar inferences <span class="citation">(<a href="references.html#ref-dongen_multiple_2019" role="doc-biblioref">Dongen et al., 2019</a>; <a href="references.html#ref-lakens_improving_2020" role="doc-biblioref">Lakens et al., 2020</a>; <a href="references.html#ref-tendeiro_review_2019" role="doc-biblioref">Tendeiro &amp; Kiers, 2019</a>)</span>. Jeffreys <span class="citation">(<a href="references.html#ref-jeffreys_theory_1939" role="doc-biblioref">1939</a>)</span>, who developed a Bayesian hypothesis test, noted the following when comparing his Bayesian hypothesis test against frequentist methods proposed by Fisher:</p>
<blockquote>
<p>I have in fact been struck repeatedly in my own work, after being led on general principles to a solution of a problem, to find that Fisher had already grasped the essentials by some brilliant piece of common sense, and that his results would be either identical with mine or would differ only in cases where we should both be very doubtful. As a matter of fact I have applied my significance tests to numerous applications that have also been worked out by Fisher’s, and have not yet found a disagreement in the actual decisions reached.</p>
</blockquote>
<p>At the same time, each approach is based on different principles, and allows for specific inferences. For example, a Neyman-Pearson approach does not quantify evidence, and a Bayesian approach can lead conclusions about the relative support for one over another hypothesis, given specified priors, while ignoring the rate at which such a conclusion would be misleading. Understanding these basic principles is useful, as criticisms on statistical practices (e.g., computing <em>p</em>-values) always boil down to a disagreement about the principles that different statistical philosophies are built on. However, when we survey the literature, we rarely see the viewpoint that all approaches to statistical inferences, including p values, provide answers to specific questions a researcher might want to ask. Instead, statisticians often engage in what I call the <strong>statistician’s
fallacy</strong> — a declaration of what they believe researchers really “want to know” without limiting the usefulness of their preferred statistical question to a specific context <span class="citation">(<a href="references.html#ref-lakens_practical_2021" role="doc-biblioref">Lakens, 2021</a>)</span>. The most well-known example of the statistician’s fallacy is provided by Cohen <span class="citation">(<a href="references.html#ref-cohen_earth_1994" role="doc-biblioref">1994</a>)</span> when discussing null-hypothesis significance testing:</p>
<blockquote>
<p>What’s wrong with NHST? Well, among many other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does! What we want to know is ‘Given these data, what is the probability that H0 is true?’</p>
</blockquote>
<p>Different statisticians will argue what you actually "want to know" is the posterior probability of a hypothesis, the false-positive risk, the effect size and its confidence interval, the likelihood, the Bayes factor, or the severity with which a hypothesis has been tested. However, it is up to you to choose a statistical strategy that matches the question you want the ask <span class="citation">(<a href="references.html#ref-hand_deconstructing_1994" role="doc-biblioref">Hand, 1994</a>)</span>.</p>
</div>
<div id="do-you-really-want-to-test-a-hypothesis" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> Do You Really Want to Test a Hypothesis?<a class="anchor" aria-label="anchor" href="#do-you-really-want-to-test-a-hypothesis"><i class="fas fa-link"></i></a>
</h2>
<p>A hypothesis test is a very specific answer to a very specific question. We can use a dart game as a metaphor for the question a hypothesis test aims to answer. In essence, both a dart game and a hypothesis test are a methodological procedure to make a directional prediction: Is A better or worse than B?, In a dart game we very often compare two players, and the question is whether we should act as is player A is the best, or player B is the best. In a hypothesis test, we compare two hypotheses, and the question is whether we should act as if the null hypothesis is true, or whether the alternative hypothesis is true.</p>
<p>Historically, researchers have often been interested in testing hypotheses to examine whether predictions that are derived from a scientific theory hold up under scrutiny. Some philosophies of science (but not all) value theories that are able to make predictions. If a darter wants to convince you they are a good player, they can make a prediction (‘the next arrow will hit the bulls-eye’), throw a dart, and impress you by hitting the bulls-eye. When a researcher uses a theory to make a prediction, collects data, and observes can claim based on a predefined methodological procedure that the results confirm their prediction, the idea is you are impressed by the <strong>predictive validity of a theory</strong> <span class="citation">(<a href="references.html#ref-de_groot_methodology_1969" role="doc-biblioref">de Groot, 1969</a>)</span>. The test supports the idea that the theory is a useful starting point to generate predictions about reality. Philosophers of science such as Popper call this ‘verisimilitude’– the theory is in some way related to the truth, and it has some ‘truth-likeness’.</p>
<p>In order to be impressed when a prediction is confirmed, the prediction must be able to be wrong. In other words, a theoretical prediction needs to be falsifiable. If our predictions concerned the presence of absence of clearly observable entities (e.g., the existence of a black swan) it is relatively straightforward to divide all possible states of the world into a set that is predicted by our theory (e.g., all swans are white), and a set that is not predicted by our theory (e.g., swans can have other colors than white). However, many scientific questions concern probabilistic events where single observations contain noise due to random variation – rats have a certain probability to develop a tumor, people have a certain probability to buy a product, or particles have a certain probability to appear after a collision. If we want to forbid certain outcomes of our test when measuring probabilistic events, we can divide the states of the world based on the probability that some result will be observed.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:blackwhite"></span>
<img src="images/blackwhite.png" alt="Some fields make black and white predictions about the presence or absence of obervables, but in many sciences, predictions are probabilistic, and shades of grey." width="100%"><p class="caption">
Figure 5.3: Some fields make black and white predictions about the presence or absence of obervables, but in many sciences, predictions are probabilistic, and shades of grey.
</p>
</div>
<p>Just because a hypothesis test can be performed, does not mean it is interesting. A hypothesis test is most useful when 1) both data generating models that are decided between have some plausibility, and 2) it is possible to apply an informative methodological procedure.</p>
<p>First, the two competing models should both be good players. Just as in a dart game there would be very little interest if I played Michael van Gerwen (the world champion at the time of writing) to decide who the better dart player is. Since I do not play darts very well, a game between the two of us would not be interesting to watch. Similarly, it is sometimes completely uninteresting to compare two data generating models, one representing the state of the world when there is no effect, and another representing the state of the world when there is some effect, because in some cases the absence of an effect is extremely implausible.</p>
<p>Second, for a hypothesis test to be interesting you need to have designed an informative study. When designing a study, you need to be able to make sure that the methodological rule provides a severe test, where you are likely to corroborate a prediction if it is correct, while at the same time fail to corroborate a prediction when it is wrong <span class="citation">(<a href="references.html#ref-mayo_statistical_2018" role="doc-biblioref">Mayo, 2018</a>)</span>. If the world champion in darts and I stand 20 inches away from a dart board and can just push the dart in the location where we want it to end up, it is not possible to show my lack of skill. If we are both are blindfolded and throwing the darts from 100 feet, it is not possible for the world champion to display their skill. In a hypothesis test, the statistical severity of a test is determined by the error rates. Therefore, a researcher needs to be able to adequately control error rates to perform a test of a hypothesis with high informational value.</p>
<p>By now it is hopefully clear that a hypothesis tests are a very specific tool, that answer a very specific question: After applying a methodological rule to observed data, which decision should I make if I do not want to make incorrect decisions too often? If you have no desire to use a methodological procedure to decide between competing theories, there is no real reason to report the results of a hypothesis test. Even though it might feel like you should test a hypothesis when doing research, carefully thinking about the statistical question you want to ask might reveal that alternative statistical approaches, such as describing the data you have observed, quantifying your personal beliefs about hypotheses, or reporting the relative likelihood of data under different hypotheses might be the approach that answers the question you really want to know.</p>
</div>
<div id="directional-one-sided-versus-non-directional-two-sided-tests" class="section level2" number="5.7">
<h2>
<span class="header-section-number">5.7</span> Directional (One-Sided) versus Non-Directional (Two-Sided) Tests<a class="anchor" aria-label="anchor" href="#directional-one-sided-versus-non-directional-two-sided-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Interestingly, there is quite some disagreement about whether the statistical question you ask in a study should be <strong>directional</strong> (meaning that only effects in a predicted direction will lead to rejection of the null hypothesis) or <strong>non-directional</strong> (meaning that effects in either direction will lead to the rejection of the null-hypothesis). For example, <span class="citation">Baguley (<a href="references.html#ref-baguley_serious_2012" role="doc-biblioref">2012</a>)</span> writes "one-sided tests should typically be avoided" because researchers are rarely willing to claim an effect in the non-predicted direction is non-significant, regardless of how large it is. At the same time, <span class="citation">Jones (<a href="references.html#ref-jones_test_1952" role="doc-biblioref">1952</a>)</span> has stated: “Since the test of the null hypothesis against a one-sided alternative is the most powerful test for all directional hypotheses, it is strongly recommended that the one-tailed model be adopted wherever its use is appropriate”, and <span class="citation">Cho &amp; Abe (<a href="references.html#ref-cho_is_2013" role="doc-biblioref">2013</a>)</span> complain about the "widespread overuse of two-tailed testing for directional research hypotheses tests". Let's reflect on some arguments for or against the choice to perform a one-sided test.</p>
<p>First, it is clear that a directional test provides a clear advantage in statistical power. As Figure <a href="questions.html#fig:onesidedtwosidedratio">5.4</a> shows, the ratio of the sample for a non-directional versus a directional test means that approximately 80% of the sample size of a non-directional test is required to achieve the same power in a directional test (the exact benefit depends on the power and effect size, as seen in the figure below).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:onesidedtwosidedratio"></span>
<img src="05-questions_files/figure-html/onesidedtwosidedratio-1.png" alt="Ratio of the required sample size for a one-sample *t*-test for a non-directional/direactional test to achieve 50%, 80% or 95% power." width="100%"><p class="caption">
Figure 5.4: Ratio of the required sample size for a one-sample <em>t</em>-test for a non-directional/direactional test to achieve 50%, 80% or 95% power.
</p>
</div>
<p>Because in a directional test the alpha level is used for only one tail in the distribution</p>
<p>Note that despite the title of this section, there is a subtle distinction between a directional and a one-sided test <span class="citation">(<a href="references.html#ref-baguley_serious_2012" role="doc-biblioref">Baguley, 2012</a>)</span>. Although the two terms overlap when performing a <em>t</em>-test, they do not overlap for an <em>F</em>-test. The <em>F</em>-value and the <em>t</em>-value are related: <span class="math inline">\(t^2 = F\)</span>. This holds as long as the df1 = 1 (e.g., F(1, 100), or in other words as long as only two groups are compared. The critical <em>t</em>-value, squared, of a non-directional <em>t</em>-test with a 5% error rate equals the critical <em>F</em>-value for an <em>F</em>-test, which is always one-sided, with a 5% error rate. Because an <em>F</em>-test is always non-directional, and based on a one-sided test, you can not halve the <em>p</em>-value in an <em>F</em>-test to perform a 'one-sided' test. It already was a one-sided <em>F</em>-test with a 5% error rate.</p>
<pre><code>## [1] 3.936143</code></pre>
<pre><code>## [1] 3.936143</code></pre>
<pre><code>## [1] 1.983972</code></pre>
<div class="inline-figure"><img src="05-questions_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;"></div>
<!-- ## Statistical Decision Theory -->

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></div>
<div class="next"><a href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#questions"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="nav-link" href="#description"><span class="header-section-number">5.1</span> Description</a></li>
<li><a class="nav-link" href="#prediction"><span class="header-section-number">5.2</span> Prediction</a></li>
<li><a class="nav-link" href="#explanation"><span class="header-section-number">5.3</span> Explanation</a></li>
<li><a class="nav-link" href="#loosening-and-tightening"><span class="header-section-number">5.4</span> Loosening and Tightening</a></li>
<li><a class="nav-link" href="#three-statistical-philosophies"><span class="header-section-number">5.5</span> Three statistical philosophies</a></li>
<li><a class="nav-link" href="#do-you-really-want-to-test-a-hypothesis"><span class="header-section-number">5.6</span> Do You Really Want to Test a Hypothesis?</a></li>
<li><a class="nav-link" href="#directional-one-sided-versus-non-directional-two-sided-tests"><span class="header-section-number">5.7</span> Directional (One-Sided) versus Non-Directional (Two-Sided) Tests</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/05-questions.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/05-questions.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-03-10.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
