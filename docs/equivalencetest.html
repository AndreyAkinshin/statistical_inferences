<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Interval Hypotheses and Equivalence Testing | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniel Lakens">
<meta name="description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 9 Interval Hypotheses and Equivalence Testing | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Interval Hypotheses and Equivalence Testing | Improving Your Statistical Inferences">
<meta name="twitter:description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="active" href="equivalencetest.html"><span class="header-section-number">9</span> Interval Hypotheses and Equivalence Testing</a></li>
<li><a class="" href="references.html"><span class="header-section-number">10</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="equivalencetest" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Interval Hypotheses and Equivalence Testing<a class="anchor" aria-label="anchor" href="#equivalencetest"><i class="fas fa-link"></i></a>
</h1>
<p>Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are analyzed with a commonly analyzed with a null-hypothesis significance test. When a statistically significant <em>p</em>-value is observed, the null-hypothesis can be rejected, and researchers claim that the intervention works, or that there is a relationship between two variables. But if the <em>p</em>-value is not statistically significant, researcher very often draw a logically incorrect conclusion: They conclude there is no effect based on <em>p</em> &gt; 0.05.</p>
<p>Open a result section of an article you are writing, or the result section of articles you have recently read. Search for <em>p</em> &gt; 0.05, and look carefully what you or the scientists concluded. If you see the conclusion that there was 'no effect' or there was 'no association between variables', you have found an example of forgetting that <em>absence of evidence is not evidence of absence</em> <span class="citation">(<a href="references.html#ref-altman_statistics_1995" role="doc-biblioref">Altman &amp; Bland, 1995</a>)</span>. A non-significant result in itself only tells us that we can not reject the null hypothesis. It is tempting to ask after <em>p</em> &gt; 0.05 'so, is the true effect zero'? But the <em>p</em>-value from a null-hypothesis significance test can not answer that question. It might be useful to think of the answer to the question whether an effect is absent after observing <em>p</em> &gt; 0.05 as 無 (<a href="https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning">mu</a>), used as a non-dualistic answer, neither yes nor no, or 'unasking the question', because it is not possible to answer the question whether an effect is absent based on <em>p</em> &gt; 0.05.</p>
<p>There should be many situations where researchers are interested in examining whether an effect is absent. For example, it can be important to show two groups do not differ on factors that might be a confound in the experimental design (e.g., examining whether a manipulation intended to increase fatigue did not affect the mood of the participants by showing that positive and negative affect did not differ between the groups). Researchers might be interested in whether two interventions work equally well, especially when the newer intervention costs less or requires less effort (e.g., is online therapy just as efficient as in person therapy?). And other times we might be interested to demonstrate the absence of an effect because a theoretical model predicts there is no effect, or because we believe a previously published study was a false positive, and we expect to show the absence of an effect in a replication study.</p>
<p>Statistical approaches have been developed to allow researchers to make claims about the absence of effects. However, it is never possible to show an effect is <em>exactly</em> 0. Even if you would collect data from every person in the world, the effect in any single study will randomly vary around the true effect size of 0, and there will be a tiny observed difference. Hodges and Lehman <span class="citation">(<a href="references.html#ref-hodges_testing_1954" role="doc-biblioref">Hodges &amp; Lehmann, 1954</a>)</span> were the first to discuss the statistical problem of testing whether two populations have the same mean. They suggest (p. 264) to: “test that their means do not differ by more than an amount specified to represent the smallest difference of practical interest.” Nunnaly <span class="citation">(<a href="references.html#ref-nunnally_place_1960" role="doc-biblioref">Nunnally, 1960</a>)</span> similarly proposed a ‘fixed-increment’ hypothesis where researcher compare an observed effect against a range of values that is deemed too small to be meaningful. Defining a range of values considered practically equivalent to the absence of an effect is known as an <strong>equivalence range</strong> <span class="citation">(<a href="references.html#ref-bauer_unifying_1996" role="doc-biblioref">Bauer &amp; Kieser, 1996</a>)</span> or a <strong>region of practical equivalence</strong> <span class="citation">(<a href="references.html#ref-kruschke_bayesian_2013" role="doc-biblioref">Kruschke, 2013</a>)</span>. The equivalence range should be specified in advance, and requires careful consideration of which effects are too small to be meaningful.</p>
<p>Although researchers have repeatedly attempted to introduce test against an equivalence range in the social sciences <span class="citation">(<a href="references.html#ref-cribbie_recommendations_2004" role="doc-biblioref">Cribbie et al., 2004</a>; <a href="references.html#ref-hoenig_abuse_2001" role="doc-biblioref">Hoenig &amp; Heisey, 2001</a>; <a href="references.html#ref-levine_communication_2008" role="doc-biblioref">Levine et al., 2008</a>; <a href="references.html#ref-quertemont_how_2011" role="doc-biblioref">Quertemont, 2011</a>; <a href="references.html#ref-rogers_using_1993" role="doc-biblioref">Rogers et al., 1993</a>)</span>, this statistical approach became popular during the replication crisis as researchers searched for tools to interpret null-results when performing replication studies. Researchers wanted to be able to publish informative null results when replicating findings in the literature that suspected of being false positives. One notable example were the studies on pre-cognition by Daryl Bem, which ostensibly showed that participants were able to predict the future <span class="citation">(<a href="references.html#ref-bem_feeling_2011" role="doc-biblioref">Bem, 2011</a>)</span>. Equivalence tests were proposed as a statistical approach to answer the question whether an observed effect is small enough to conclude that a previous study could not be replicated <span class="citation">(<a href="references.html#ref-anderson_theres_2016" role="doc-biblioref">S. F. Anderson &amp; Maxwell, 2016</a>; <a href="references.html#ref-lakens_equivalence_2017" role="doc-biblioref">Lakens, 2017</a>; <a href="references.html#ref-simonsohn_small_2015" role="doc-biblioref">Simonsohn, 2015</a>)</span>. Researchers specify a smallest effect size of interest (for example an effect of 0.5, so for a two-sided test any value outside a range from -0.5 to 0.5) and test whether effects more extreme than this range can be rejected. If so, they can reject the presence effects that are large enough to be meaningful.</p>
<p>Equivalence tests are a specific implementation of <strong>interval hypothesis tests</strong>, where instead of testing against a null hypothesis of no effect (or an effect size of 0), an effect is tested against a null hypothesis that represents a range of non-zero effect sizes. One can distinguish a <strong>nil null hypothesis</strong>, where the null hypothesis is an effect of 0, from a <strong>non-nil null hypothesis</strong>, where the null hypothesis is any other effect that 0, for example effects more extreme than the smallest effect size of interest <span class="citation">(<a href="references.html#ref-nickerson_null_2000" role="doc-biblioref">Nickerson, 2000</a>)</span>. As Nickerson writes:</p>
<blockquote>
<p>The distinction is an important one, especially relative to the controversy regarding the merits or shortcomings of NHST inasmuch as criticisms that may be valid when applied to nil hypothesis testing are not necessarily valid when directed at null hypothesis testing in the more general sense.</p>
</blockquote>
<p>Indeed, one of the most widely suggested improvements that mitigates the most important limitations of null-hypothesis significance testing is to replace the nil null hypothesis with the test of a range prediction (by specifying a non-nil null hypothesis) in an interval hypothesis test <span class="citation">(<a href="references.html#ref-lakens_practical_2021" role="doc-biblioref">Lakens, 2021</a>)</span>. To illustrate the difference, panel A in Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a> visualizes the results that are predicted in a two-sided null hypothesis test with a nil hypothesis, where the test examines whether an effect of 0 can be rejected. Panel B shows an interval hypothesis where an effect between 0.5 and 2.5 is predicted, where the non-nill null hypothesis consists of values smaller than 0.5 or larger than 2.5, and the interval hypothesis tests examines whether values in these ranges can be rejected. Panel C illustrates an equivalence test, which is basically identical to an interval hypothesis test, but the predicted effects are located in a range around 0, and contain effects that are deemed too small to be meaningful.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:intervaltest"></span>
<img src="09-equivalencetest_files/figure-html/intervaltest-1.png" alt="Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D)." width="100%"><p class="caption">
Figure 9.1: Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D).
</p>
</div>
<p>When an equivalence test is reversed, an a researcher designs a study to reject effect less extreme than a smallest effect size of interest (see Panel D in Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a>), it is called a <strong>minimum effect test</strong> <span class="citation">(<a href="references.html#ref-murphy_testing_1999" role="doc-biblioref">Murphy &amp; Myors, 1999</a>)</span>. A researchers might not just be interested in rejecting an effect of 0 (as in a null hypothesis significance test) but in rejecting effects that are too small to be meaningful. All else equal, a study designed to have high power for a minimum effect requires more observations than if the goal had been to reject an effect of zero. As the confidence interval needs to reject a value that is closer to the observed effect size (e.g., 0.1 instead of 0) it needs to be more narrow, which requires more observations.</p>
<p>One benefit of a minimum effect test compared to a null-hypothesis test is that there is no distinction between statistical significance and practical significance. As the test value is chosen to represent the minimum effect of interest, whenever it is rejected, the effect is both statistically and practically significant <span class="citation">(<a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>. Another benefit of minimum effect tests is that, especially in correlational studies in the social sciences, variables are often connected through causal structures that result in real but theoretically uninteresting nonzero correlations between variables, which has been labeled the 'crud factor' <span class="citation">(<a href="references.html#ref-meehl_appraising_1990" role="doc-biblioref">Meehl, 1990</a>; <a href="references.html#ref-orben_crud_2020" role="doc-biblioref">Orben &amp; Lakens, 2020</a>)</span>. Because an effect of zero is unlikely to be true in large correlational datasets, rejecting a nil null hypothesis is not a severe test. Even if the hypothesis is incorrect, it is likely that an effect of 0 will be rejected due to 'crud'. For this reason, some researchers have suggested to test against a minimum effect of <em>r</em> = 0.1, as correlations below this threshold are quite common due to theoretically irrelevant correlations between variables <span class="citation">(<a href="references.html#ref-ferguson_providing_2021" role="doc-biblioref">Ferguson et al., 2021</a>)</span>.</p>
<p>Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a> illustrates two-sided tests, but it is often more intuitive and logical to perform one-sided tests. In that case, a minimum effect test would, for example, aim to reject effects smaller than 0.1, and an equivalence test would aim to reject effects larger than for example 0.1. Instead of specifying an upper and lower bound of a range, it is sufficient to specify a single value for one-sided tests. A final variation of a one-sided non-nil null hypothesis test is known as a test for <strong>non-inferiority</strong>, which examines of an effect is larger than the lower bound of an equivalence range. Such a test is for example performed when a novel intervention should not be noticeable worse than an existing intervention - but it can be a tiny bit worse. For example, if a difference between a novel and existing intervention is not smaller than -0.1, and effects smaller than -0.1 can be rejected, one can conclude an effect is non-inferior <span class="citation">(<a href="references.html#ref-mazzolari_myths_2022" role="doc-biblioref">Mazzolari et al., 2022</a>; <a href="references.html#ref-schumi_through_2011" role="doc-biblioref">Schumi &amp; Wittes, 2011</a>)</span>. We see that extending nil null hypothesis tests to non-nil null hypotheses allow researchers to ask questions that might be more interesting.</p>
<div id="equivalence-tests" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Equivalence tests<a class="anchor" aria-label="anchor" href="#equivalence-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Equivalence tests were first developed in pharmaceutical sciences <span class="citation">(<a href="references.html#ref-hauck_new_1984" role="doc-biblioref">Hauck &amp; Anderson, 1984</a>; <a href="references.html#ref-westlake_use_1972" role="doc-biblioref">Westlake, 1972</a>)</span> and later formalized as the <strong>two one-sided tests (TOST)</strong> approach to equivalence testing <span class="citation">(<a href="references.html#ref-schuirmann_comparison_1987" role="doc-biblioref">Schuirmann, 1987</a>; <a href="references.html#ref-seaman_equivalence_1998" role="doc-biblioref">Seaman &amp; Serlin, 1998</a>; <a href="references.html#ref-wellek_testing_2010" role="doc-biblioref">Wellek, 2010</a>)</span>. In the TOST procedure one performs two one-sided tests, one examining if we can reject effects smaller than the lower bound of the equivalence range, and one examining if we can reject effect larger than the upper bound of the equivalence range. If both one-sided tests are significant, we can reject the presence of effects large enough to be meaningful. To perform an equivalence test, you don't need to learn any new statistical tests, as it is just the well-known <em>t</em>-test against a different value than 0. It is somewhat surprising that the use of <em>t</em>tests to perform equivalence tests is not taught alongside their use in null-hypothesis significance tests, as there is some indication that this could prevent common misunderstandings of <em>p</em>-values <span class="citation">(<a href="references.html#ref-parkhurst_statistical_2001" role="doc-biblioref">Parkhurst, 2001</a>)</span>. Let's look at an example of an equivalence test using the TOST procedure.</p>
<p>In a study where researchers are manipulating fatigue by asking participants to carry heavy boxes around, the researchers want to ensure the manipulation does not inadvertently alter participants’ moods. The researchers assess positive and negative emotions in both ocnditions, and want to claim there are no differences in positive mood. Let’s assume that positive mood in the experimental fatigue condition (m1 = 4.55, sd1 = 1.05, n1 = 15) did not differ from the mood in the the control condition (m2 = 4.87, sd2 = 1.11, n2 = 15). The researchers conclude: “Mood did not differ between conditions, <em>t</em> = -0.81, <em>p</em> = .42”. Of course, mood did differ between conditions, as 4.55 - 4.87 = -0.32. The claim is that there was no <em>meaningful</em> difference in mood, but to make such a claim in a correct manner, we first need to specify which difference in mood is large enough to be meaningful. For now, let's assume the researcher consider any effect less extreme half a scale point too small to be meaningful. We test now test if the observed mean difference of -0.32 is small enough such that we can reject the presence of effects that are large enough to matter.</p>
<p>The TOSTER package (originally created by myself but recently redesigned by Aaron Caldwell) can be used to plot two <em>t</em>-distributions and their critical regions indicating when we can reject the presence of effects smaller than -0.5 and larger than 0.5. It can take some time to get used to the idea that we are rejecting values more extreme than the equivalence bounds. Try to consistently ask in any hypothesis test: Which values can the test reject? In a nil null hypothesis test, we can reject an effect of 0, and in the equivalence test in the Figure below, we can reject values lower than -0.5 and higher than 0.5. In Figure <a href="equivalencetest.html#fig:tdistequivalence">9.2</a> we see two <em>t</em>-distributions centered on the upper and lower bound of the specified equivalence range (-0.5 and 0.5).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tdistequivalence"></span>
<img src="09-equivalencetest_files/figure-html/tdistequivalence-1.png" alt="The mean difference and its confidence interval plotted below the *t*-distributions used to perform the two-one-sided tests against -0.5 and 0.5." width="100%"><p class="caption">
Figure 9.2: The mean difference and its confidence interval plotted below the <em>t</em>-distributions used to perform the two-one-sided tests against -0.5 and 0.5.
</p>
</div>
<p>Let's first look at the left curve. We see the green highlighted area in the tails that highlights which observed mean differences would be extreme enough to statistically reject an effect of -0.5. Our observed mean difference of -0.32 lies very close to 0.5, and if we look at the left distribution, the mean is not far enough away from 0.5 to fall in the green area that indicates when observed differences would be statistically significant. We can also perform the equivalence test using the TOSTER package, and look at the results.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/tsum_TOST.html">tsum_TOST</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="fl">4.55</span>, m2 <span class="op">=</span> <span class="fl">4.87</span>, sd1 <span class="op">=</span> <span class="fl">1.05</span>, sd2 <span class="op">=</span> <span class="fl">1.11</span>,
                  n1 <span class="op">=</span> <span class="fl">15</span>, n2 <span class="op">=</span> <span class="fl">15</span>, low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Equivalence
## Equivalence Bounds (raw):-0.500 &amp; 0.500
## Alpha Level:0.05
## The equivalence test was non-significant, t(27.91) = 0.456, p = 3.26e-01
## The null hypothesis test was non-significant, t(27.91) = -0.811, p = 4.24e-01
## NHST: don't reject null significance hypothesis that the effect is equal to zero 
## TOST: don't reject null equivalence hypothesis
## 
## TOST Results 
##                     t        SE       df    p.value
## t-test     -0.8111280 0.3945124 27.91398 0.42415467
## TOST Lower  0.4562595 0.3945124 27.91398 0.32586680
## TOST Upper -2.0785154 0.3945124 27.91398 0.02348582
## 
## Effect Sizes 
##                 estimate        SE   lower.ci  upper.ci conf.level
## Raw           -0.3200000 0.3945124 -0.9911879 0.3511879        0.9
## Hedges' g(av) -0.2881401 0.3812249 -0.9301965 0.3193638        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<p>In the line 't-test' the output shows the traditional nil null hypothesis significance test (which we already knew was not statistically significant: <em>t</em> = -0.81, <em>p</em> = 0.42). We also see a test indicated by TOST Lower. This is the first one-sided test examining if we can reject effects lower than -0.5. From the test result, we see this is not the case: <em>t</em> = 0.46, <em>p</em> = 0.36. This is an ordinary <em>t</em>-test, just against an effect of -0.5. Because we can not reject differences more extreme than -0.5, it is possible that a different we consider meaningful (e.g., a difference of -0.60) is present. When we look at the one-sided test against the upper bound of the equivalence range (0.5) we see that we can statistically reject the presence of mood effects larger than 0.5, as in the line TOST Upper we see <em>t</em> = -2.08, <em>p</em> = 0.02. Our final conclusion is therefore that, even thought we can reject effects more extreme than 0.5 based on the observed mean difference of -0.32, we can not reject effects more extreme than -0.5. Therefore, we can not completely reject the presence of meaningful mood effects. As the data does not allow us to claim the effect is different from 0, nor that the effect is, if anything, too small to matter (based on an equivalence range from -0.5 to 0.5), the data are <strong>inconclusive</strong>. We can not distinguish between a Type 2 error (there is an effect, but in this study we just did not detect it) or a true negative (there really is no effect large enough to matter).</p>
<p>One way to reduce the probability of an inconclusive effect is to collect sufficient data. Let's imagine the researchers had not collected 15 participants in each condition, but 200 participants. They otherwise observe exactly the same data. As explained in the chapter on <a href="confint.html#confint">confidence intervals</a>, as the sample size increases, the confidence interval becomes more narrow. For a TOST equivalence test to be able to reject both the upper and lower bound of the equivalence range, the confidence interval needs to fall completely within the equivalence range. In Figure <a href="equivalencetest.html#fig:ciequivalence">9.3</a> we see the observed mean difference (the black dot) and it's confidence interval (the horizontal black line) for a sample size of 200 per group. The coloourful plot on top of the mean difference and confidence interval is a confidence density plot <span class="citation">(<a href="references.html#ref-schweder_confidence_2016" role="doc-biblioref">Schweder &amp; Hjort, 2016</a>)</span>, which is a graphical summary of the distribution of confidence. NUMBER BELOW DO NOT MATCH TEXT YET</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ciequivalence"></span>
<img src="09-equivalencetest_files/figure-html/ciequivalence-1.png" alt="The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5." width="100%"><p class="caption">
Figure 9.3: The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.
</p>
</div>
<p>If we wanted to have sufficient power to detect an effect of 0, which sample size would we need to collect?</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 87.26261
##           delta = 0
##              sd = 1
##          bounds = -0.5, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/tsum_TOST.html">tsum_TOST</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="fl">4.55</span>, m2 <span class="op">=</span> <span class="fl">4.87</span>, sd1 <span class="op">=</span> <span class="fl">1.05</span>, sd2 <span class="op">=</span> <span class="fl">1.11</span>,
                  n1 <span class="op">=</span> <span class="fl">200</span>, n2 <span class="op">=</span> <span class="fl">200</span>, low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Equivalence
## Equivalence Bounds (raw):-0.500 &amp; 0.500
## Alpha Level:0.05
## The equivalence test was significant, t(396.78) = 1.666, p = 4.82e-02
## The null hypothesis test was significant, t(396.78) = -2.962, p = 3.24e-03
## NHST: reject null significance hypothesis that the effect is equal to zero 
## TOST: reject null equivalence hypothesis
## 
## TOST Results 
##                    t        SE       df      p.value
## t-test     -2.961821 0.1080417 396.7773 3.242190e-03
## TOST Lower  1.666024 0.1080417 396.7773 4.824893e-02
## TOST Upper -7.589665 0.1080417 396.7773 1.156039e-13
## 
## Effect Sizes 
##                 estimate        SE   lower.ci   upper.ci conf.level
## Raw           -0.3200000 0.1080417 -0.4981286 -0.1418714        0.9
## Hedges' g(av) -0.2956218 0.1008059 -0.4625958 -0.1310411        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<p>You might not expect a true difference of exactly zero of the mood manipulation. For example, you might deem it reasonable to expect a true effect of d = 0.2. You are still interested in showing this effect is too small too matter, which you defined as a difference of 0.5. If the true effect size is indeed d = 0.2, we should expect to observe effects much closer to the equivalence bound 0.5. To be able to reliable exclude effects larger than 0.5, we will need a larger sample size, similar to how we need a larger sample size for a null-hypothesis test powered to detect a d = 0.3, than a null-hypothesis test powered to detect a d = 0.5.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># New TOSTER power functions allows power for expected non-zero effect.</span>
<span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0.2</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 190.988
##           delta = 0.2
##              sd = 1
##          bounds = -0.5, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
<p>It is better to specify the equivalence bounds in terms of raw effect sizes. Setting them in terms of Cohen's d leads to bias in the statistical test (as we have to use the observed standard deviation to translate Cohen's d into a raw effect size). This is in practice not too problematic in any single equivalence test, but as equivalence testing becomes more popular, and fields establish smallest effect sizes of interest, they should do so in raw effect size differences, not in terms of standardized effect size differences.</p>
<p>There is no need to set an upper and lower bound for an equivalence test. Your question might be directional: Can I reject an effect larger than 0.5? In such a question, any effect smaller than 0.5, even an effect of -0.8, would reject effects you deem meaningful. An example of a situation where such a test is approrpiate is a replication study. If a previous study observed an effect of d = 0.48, and you perform a replication study, you might decide to consider any effect smaller than d = 0.2 a failure to replicate - including any effect in the opposite direction. Although most software for equivalence tests requires you to specify an upper and lower bound, you can mimick a one-sided test by setting the other equivalence bound to an extreme value for the research question you are asking. In the analysis below, the lower bound is set to d = -5. We see the data is very far removed from this bound, and of course the one-sided test against d = -5 will always be statistically significant, so it does not influence the final decision based on the TOST procedure.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># New TOSTER function</span>
<span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/tsum_TOST.html">tsum_TOST</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="fl">4.55</span>, m2 <span class="op">=</span> <span class="fl">4.87</span>, sd1 <span class="op">=</span> <span class="fl">1.05</span>, sd2 <span class="op">=</span> <span class="fl">1.11</span>,
                  n1 <span class="op">=</span> <span class="fl">15</span>, n2 <span class="op">=</span> <span class="fl">15</span>, low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span>,
                  eqbound_type <span class="op">=</span> <span class="st">"SMD"</span>, bias_correction <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">res</span></code></pre></div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Equivalence
## Equivalence Bounds (raw):-5.402 &amp; 0.540
## Alpha Level:0.05
## The equivalence test was significant, t(27.91) = -2.180, p = 1.89e-02
## The null hypothesis test was non-significant, t(27.91) = -0.811, p = 4.24e-01
## NHST: don't reject null significance hypothesis that the effect is equal to zero 
## TOST: reject null equivalence hypothesis
## 
## TOST Results 
##                    t        SE       df      p.value
## t-test     -0.811128 0.3945124 27.91398 4.241547e-01
## TOST Lower 12.881936 0.3945124 27.91398 1.439959e-13
## TOST Upper -2.180434 0.3945124 27.91398 1.890746e-02
## 
## Effect Sizes 
##                 estimate        SE   lower.ci  upper.ci conf.level
## Raw           -0.3200000 0.3945124 -0.9911879 0.3511879        0.9
## Hedges' g(av) -0.2881401 0.3812249 -0.9301965 0.3193638        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="09-equivalencetest_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Old TOSTER power function do not take one-sided tests into account</span>
<span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/powerTOSTtwo.html">powerTOSTtwo</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.05</span>, low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span>, statistical_power <span class="op">=</span> <span class="fl">0.9</span><span class="op">)</span></code></pre></div>
<pre><code>## The required sample size to achieve 90 % power with equivalence bounds of -5 and 0.5 is 87 per group, or 174 in total.</code></pre>
<pre><code>## [1] 86.57739</code></pre>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># New TOSTER power functions takes one-sided bounds into account</span>
<span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0.0</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 69.19784
##           delta = 0
##              sd = 1
##          bounds = -5.0, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
</div>
<div id="equivalence-tests-for-anova" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Equivalence tests for ANOVA<a class="anchor" aria-label="anchor" href="#equivalence-tests-for-anova"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Get Data</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"InsectSprays"</span><span class="op">)</span>
<span class="co"># Build ANOVA</span>
<span class="va">aovtest</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">count</span> <span class="op">~</span> <span class="va">spray</span>,
              data <span class="op">=</span> <span class="va">InsectSprays</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">aovtest</span><span class="op">)</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## spray        5   2669   533.8    34.7 &lt;2e-16 ***
## Residuals   66   1015    15.4                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>We can use this summary to perform an equivalence test for an <em>F</em>-test:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/equ_ftest.html">equ_ftest</a></span><span class="op">(</span>Fstat <span class="op">=</span> <span class="fl">34.70228</span>,
          df1 <span class="op">=</span> <span class="fl">5</span>,
          df2 <span class="op">=</span> <span class="fl">66</span>,
          eqbound <span class="op">=</span> <span class="fl">0.35</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##  Equivalence Test from F-test
## 
## data:  Summary Statistics
## F = 34.702, df1 = 5, df2 = 66, p-value = 1
## 95 percent confidence interval:
##  0.5806263 0.7804439
## sample estimates:
## [1] 0.724439</code></pre>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/equ_anova.html">equ_anova</a></span><span class="op">(</span><span class="va">aovtest</span>,
          eqbound <span class="op">=</span> <span class="fl">0.35</span><span class="op">)</span></code></pre></div>
<pre><code>##        effect df1 df2  F.value       p.null      pes eqbound     p.equ
## 1 spray         5  66 34.70228 3.182584e-17 0.724439    0.35 0.9999965</code></pre>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/plot_pes.html">plot_pes</a></span><span class="op">(</span>Fstat <span class="op">=</span> <span class="fl">34.70228</span>,
         df1 <span class="op">=</span> <span class="fl">5</span>,
         df2 <span class="op">=</span> <span class="fl">66</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="09-equivalencetest_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_eq_f.html">power_eq_f</a></span><span class="op">(</span>df1 <span class="op">=</span> <span class="fl">2</span>, 
            df2 <span class="op">=</span> <span class="fl">60</span>,
            eqbound <span class="op">=</span> <span class="fl">.15</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Power for Non-Inferiority F-test 
## 
##             df1 = 2
##             df2 = 60
##         eqbound = 0.15
##       sig.level = 0.05
##           power = 0.8188512</code></pre>
</div>
<div id="reporting-equivalence-tests" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Reporting equivalence tests<a class="anchor" aria-label="anchor" href="#reporting-equivalence-tests"><i class="fas fa-link"></i></a>
</h2>
<p>It is common practice to only report the test yielding the higher <em>p</em>-value of the two one-sided tests when reporting an equivalence test. Because both one-sided tests need to be statistically significant to reject the null hypothesis in an equivalence test (i.e., the presence of effects large enough to matter), when the larger of the two hypothesis tests rejects the equivalence bound, so does the other test. Unlike in null-hypothesis significance tests it is not common to report standardized effect sizes for equivalence tests, but there can be situations where researchers might want to discuss how far the effect is removed from the equivalence bounds on the raw scale. Prevent the erroneous interpretation to claim there is 'no effect', that an effect is 'absent', that the true effect size is 'zero', or vague verbal descriptions, such as that two groups yielded 'similar' or 'comparable' data. A significant equivalence test rejects effects more extreme that the equivalence bounds. Smaller true effects have not been rejected, and thus it remains possible that there is a true effect. Because a TOST procedure is a frequentist test based on a <em>p</em>-value, all other <a href="pvalue.html#misconceptions">misconceptions of <em>p</em>-values</a> should be prevented as well.</p>
<p>When summarizing the main result of an equivalence test, for example in an abstract, always report the equivalence range that the data is tested against. Reading 'based on an equivalence test we concluded the absence of a meaningful effect' means something very different if the equivalence bounds were d = -0.9 to 0.9 than when the bounds were d = -0.2 to d = 0.2. So instead, write "based on an equivalence test with an equivalence range of d = -0.2 to 0.2, we conclude the absence of a meaningful effect. Even more neutral would be statement such as: 'based on an equivalence test, we rejected the presence of effects more extreme than -0.2 to 0.2, so we can act (with an error rate of alpha) as if the effect, is any, is less extreme than our equivalence range'. If both a null-hypothesis test and a significant test are non-significant, the findings is best described as 'inconclusive': There is not enough data to reject the null, or the smallest effect size of interest. If both the null-hypothesis test and the equivalence test are statistically significant, you can claim there is an effect, but at the same time it is too small to matter (given the justification for the equivalence range).</p>
</div>
<div id="ROPE" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> The Bayesian ROPE procedure<a class="anchor" aria-label="anchor" href="#ROPE"><i class="fas fa-link"></i></a>
</h2>
<p>In Bayesian estimation, one way to argue for the absence of a meaningful effect is the region of practical equivalence (ROPE) procedure (Kruschke, 2014, chapter 12), which is “somewhat analogous to frequentist equivalence testing” (Kruschke &amp; Liddell, 2017). In the ROPE procedure, an equivalence range is specified, just as in equivalence testing, but the Bayesian highest density interval based on a posterior distribution (as explained in the chapter on <a href="bayes.html#bayes">Bayesian statistics</a>) is used to make decisions. Kruschke suggests that: “if the 95 % HDI falls entirely inside the ROPE then we decide to accept the ROPE’d value for practical purposes”.</p>
<p>If the prior used by Kruschke was perfectly uniform, ROPE and equivalence testing would identical (if the same confidence interval width was used, e.g., 90%), barring philosophical differences in how the numbers should be interpreted. The BEST package by default uses a ‘broad’ prior, and therefore the 95% CI and 95% HDI are not exactly the same, but they are very close, and one might even argue they are 'practically equivalent'.</p>
<p>In the code below, I randomly generate random normally distributed data (with means of 0 and a sd of 1) and perform the ROPE procedure and the TOST. The 95% HDI is from -0.10 to 0.42, and the 95% CI is from -0.11 to 0.41, with mean differences of 0.17 or 0.15.</p>
<div id="which-interval-width-should-be-used" class="section level3" number="9.4.1">
<h3>
<span class="header-section-number">9.4.1</span> Which interval width should be used?<a class="anchor" aria-label="anchor" href="#which-interval-width-should-be-used"><i class="fas fa-link"></i></a>
</h3>
<p>Because the TOST procedure is based on two one-sided tests, a 90% confidence interval is used when the one-sided tests are performed at an alpha level of 5%. Because both the test against the upper bound and the test against the lower bound needs to be statistically significant to declare equivalence (which as explained in the chapter on <a href="errorcontrol.html#multiplecomparisons">error control</a> is an intersection-union approach to multiple testing) it is not necessary to correct for the fact that two tests are performed. If the alpha level is adjusted for multiple comparisons, or if the alpha level is justified instead of relying on the default 5% level (or both), the corresponding confidence interval should be used, where CI = 100 - (2 * <span class="math inline">\(\alpha\)</span>). Thus, the width of the confidence interval is directly related to the choice for the alpha level, as we are making decisions based on whether we can reject the effect size of interest or not, with a desired maximum Type 1 error rate.</p>
<p>When using a Highest Density Interval from a Bayesian perspective, such as the ROPE procedure, the choice for a width of a confidence interval does not follow logically from a desired error rate, or any other principle. Kruschke (2014, Chapter 5) writes: “How should we define 'reasonably credible'? One way is by saying that any points within the 95% HDI are reasonably credible.” McElreath has recommended the use of 67%, 89%, and 97%, because "No reason. They are prime numbers, which makes them easy to remember.". There are only two principled solutions. First, if a highest density interval width is used to make claims, these claims will be made with certain error rates, and researchers should quantify the risk of erroneous claim by incorporating frequentist error control. This would make the ROPE procedure a Bayesian/Frequentist compromise procedure, where the computation of a posterior distribution allows for Bayesian interpretations of which parameters values are believed to be most probable, while decisions based on whether or not the HDI falls within an equivalence range have a formally controlled error rate. Not that when using an informative prior, a HDI does not match a CI, and the error rate when using a HDI can only be derived through simulations. The second solution is to not make any claims, present the full posterior distribution, and let readers draw their own conclusions.</p>
</div>
<div id="powerequiv" class="section level3" number="9.4.2">
<h3>
<span class="header-section-number">9.4.2</span> Power analysis for Equivalence Tests<a class="anchor" aria-label="anchor" href="#powerequiv"><i class="fas fa-link"></i></a>
</h3>
<p>For an equivalence test, power analysis can be performed based on closed functions, and the calculations take just a fraction of a second. I find that useful, for example in my role in our ethics board, where we evaluate proposals that have to justify their sample size, and we often check power calculations. Kruschke has an excellent R package (BEST) that can do power analyses for the ROPE procedure. This is great work – but the simulations take a while (a little bit over an hour for 1000 simulations).</p>
<p>Because the BESTpower function relies on simulations, you need to specify the sample size, and it will calculate the power. That’s actually the reverse of what you typically want in a power analysis (you want to input the desired power, and see which sample size you need). This means you most likely need to run multiple simulations in BESTpower, before you have determined the sample size that will yield good power. Furthermore, the software requires your to specify the expected means and standard deviations, instead of simply an expected effect size. Instead of Frequentist power analysis, where the hypothesized effect size is a point value (e.g., d = 0.4), Bayesian power analysis models the alternative as a distribution, acknowledging there is uncertainty.</p>
<p>In the end, however, the result of a power analysis for ROPE and for TOST is actually remarkably similar. Using the code below to perform the power analysis for ROPE, we see that 100 participants in each group give us approximately 88.4% power (with 2000 simulations, this estimate is still a bit uncertain) to get a 95% HDI that falls within our ROPE of -0.5 to 0.5, assuming standard deviations of 1.</p>
<p>We can use the powerTOSTtwo.raw function in the TOSTER package (using an alpha of 0.025 instead of 0.05, to mirror to 95% HDI) to calculate the sample size we would need to achieve 88.4% power for independent t-test (using equivalence bounds of -0.5 and 0.5, and standard deviations of 1):</p>
<p>powerTOSTtwo.raw(alpha=0.025,statistical_power=0.875,low_eqbound=-0.5,high_eqbound=0.5,sdpooled=1)</p>
<p>The outcome is 100 as well. So if you use a broad prior, it seems you can save yourself some time by using the power analysis for equivalence tests, without severe consequences.</p>
</div>
<div id="use-of-prior-information" class="section level3" number="9.4.3">
<h3>
<span class="header-section-number">9.4.3</span> Use of prior information<a class="anchor" aria-label="anchor" href="#use-of-prior-information"><i class="fas fa-link"></i></a>
</h3>
<p>The biggest benefit of ROPE over TOST is that is allows you to incorporate prior information in your data analysis. If you have reliable prior information, ROPE can use this information, which is especially useful if you don’t have a lot of data. If you use priors, it is typically advised to check the robustness of the posterior against reasonable changes in the prior (Kruschke, 2013).</p>
</div>
<div id="conclusion" class="section level3" number="9.4.4">
<h3>
<span class="header-section-number">9.4.4</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h3>
<p>Using the ROPE procedure or the TOST procedure will most likely lead to very similar inferences. For all practical purposes, the differences are small. It’s quite a lot easier to perform a power analysis for TOST, and by default, TOST has greater statistical power because it uses 90% CI. But power analysis is possible for ROPE (which is a rare pleasure to see for Bayesian analyses), and you could choose to use a 90% HDI, or any other value that matches your goals. TOST will be easier and more familiar because it is just a twist on the classic t-test, but ROPE might be a great way to dip your toes in Bayesian waters and explore the many more things you can do with Bayesian posterior distributions.</p>
<!-- ```{r} -->
<!-- # Data is loaded because BEST analysis takes so very long -->
<!-- load("data/BESTout.Rdata") -->
<!-- plot(BESTout) -->
<!-- #TOST test -->
<!-- TOSTtwo.raw(m1=mean(x),m2=mean(y),sd1=sd(x),sd2=sd(y),n1=length(x),n2=length(y),low_eqbound=-0.5,high_eqbound=0.5, alpha=0.025) -->
<!-- powerPro -->
<!-- #TOST power analysis -->
<!-- powerTOSTtwo.raw(alpha=0.025,statistical_power=0.875,low_eqbound=-0.5,high_eqbound=0.5,sdpooled=1) -->
<!-- ``` -->
<p>```</p>
</div>
</div>
<div id="absence-of-evidence-is-not-evidence-of-absence." class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> Absence of evidence is not evidence of absence.<a class="anchor" aria-label="anchor" href="#absence-of-evidence-is-not-evidence-of-absence."><i class="fas fa-link"></i></a>
</h2>
<p>Take a look at the last paper you have written. Search for a <em>p</em>-value larger than 0.05. How did you interpret this result?
It is common that researchers interpret a <em>p</em>-value larger than 0.05 as the absence of an effect. However, this is a misinterpretation of <em>p</em>-values.</p>
<p>When you find <em>p</em> &gt; 0.05, you did not observe surprising data, assuming there is no true effect. You can often read in the literature how <em>p</em> &gt; 0.05 is interpreted as ‘no effect’ but due to a lack of power the data might not be surprising if there was an effect. In this blog I’ll explain how to test for equivalence, or the lack of a meaningful effect, using equivalence hypothesis testing. I’ve created easy to use R functions that allow you to perform equivalence hypothesis tests. Warning: If you read beyond this paragraph, you will never again be able to write “as predicted, the interaction revealed there was an effect for participants in the experimental condition (p &lt; 0.05) but there was no effect in the control condition (F &lt; 1).” If you prefer the veil of ignorance, here’s a nice site with cute baby animals to spend the next 9 minutes on instead.</p>
<p>Any science that wants to be taken seriously needs to be able to provide support for the null-hypothesis. I often see people switching over from Frequentist statistics when effects are significant, to the use of Bayes Factors to be able to provide support for the null hypothesis. But it is possible to test if there is a lack of an effect using p-values (why no one ever told me this in the 11 years I worked in science is beyond me). It’s as easy as doing a t-test, or more precisely, as doing two t-tests.</p>
<p>The practice of Equivalence Hypothesis Testing (EHT) is used in medicine, for example to test whether a new cheaper drug isn’t worse (or better) than the existing more expensive option. A very simple EHT approach is the ‘two-one-sided t-tests’ (TOST) procedure (Schuirmann, 1987). Its simplicity makes it wonderfully easy to use.</p>
<p>The basic idea of the test is to flip things around: In Equivalence Hypothesis Testing the null hypothesis is that there is a true effect larger than a Smallest Effect Size of Interest (SESOI; Lakens, 2014). That’s right – the null-hypothesis is now that there IS an effect, and we are going to try to reject it (with a p &lt; 0.05). The alternative hypothesis is that the effect is smaller than a SESOI, anywhere in the equivalence range - any effect you think is too small to matter, or too small to feasibly examine. For example, a Cohen’s d of 0.5 is a medium effect, so you might set d = 0.5 as your SESOI, and the equivalence range goes from d = -0.5 to d = 0.5 In the TOST procedure, you first decide upon your SESOI: anything smaller than your smallest effect size of interest is considered smaller than small, and will allow you to reject the null-hypothesis that there is a true effect. You perform two t-tests, one testing if the effect is smaller than the upper bound of the equivalence range, and one testing whether the effect is larger than the lower bound of the equivalence range. Yes, it’s that simple.</p>
<p>Let’s visualize this. Below on the left axis is a scale for the effect size measure Cohen’s d. On the left is a single 90% confidence interval (the crossed circles indicate the endpoints of the 95% confidence interval) with an effect size of d = 0.13. On the right is the equivalence range. It is centered on 0, and ranges from d = -0.5 to d = 0.5.</p>
<p>We see from the 95% confidence interval around d = 0.13 (again, the endpoints of which are indicated by the crossed circles) that the lower bound overlaps with 0. This means the effect (d = 0.13, from an independent t-test with two conditions of 90 participants each) is not statistically different from 0 at an alpha of 5%, and the p-value of the normal NHST is 0.384 (the title provides the exact numbers for the 95% CI around the effect size). But is this effect statistically smaller than my smallest effect size of interest?</p>
</div>
<div id="justifysesoi" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> JUstifying a smallest effect size of interest<a class="anchor" aria-label="anchor" href="#justifysesoi"><i class="fas fa-link"></i></a>
</h2>
<div id="rejecting-the-presence-of-a-meaningful-effect" class="section level3" number="9.6.1">
<h3>
<span class="header-section-number">9.6.1</span> Rejecting the presence of a meaningful effect<a class="anchor" aria-label="anchor" href="#rejecting-the-presence-of-a-meaningful-effect"><i class="fas fa-link"></i></a>
</h3>
<p>There are two ways to test the lack of a meaningful effect that yield the same result. The first is to perform two one sided t-tests, testing the observed effect size against the ‘null’ of your SESOI (0.5 and -0.5). These t-tests show the d = 0.13 is significantly larger than d = -0.5, and significantly smaller than d = 0.5. The highest of these two p-values is p = 0.007. We can conclude that there is support for the lack of a meaningful effect (where meaningful is defined by your choice of the SESOI). The second approach (which is easier to visualize) is to calculate a 90% CI around the effect (indicated by the vertical line in the figure), and check whether this 90% CI falls completely within the equivalence range. You can see a line from the upper and lower limit of the 90% CI around d = 0.13 on the left to the equivalence range on the right, and the 90% CI is completely contained within the equivalence range. This means we can reject the null of an effect that is larger than d = 0.5 or smaller than d = -0.5 and conclude this effect is smaller than what we find meaningful (and you’ll be right 95% of the time, in the long run).</p>
<p>[Technical note: The reason we are using a 90% confidence interval, and not a 95% confidence interval, is because the two one-sided tests are completely dependent. You could actually just perform one test, if the effect size is positive against the upper bound of the equivalence range, and if the effect size is negative against the lower bound of the equivalence range. If this one test is significant, so is the other. Therefore, we can use a 90% confidence interval, even though we perform two one-sided tests. This is also why the crossed circles are used to mark the 95% CI.].</p>
<p>So why were we not using these tests in the psychological literature? It’s the same old, same old. Your statistics teacher didn’t tell you about them. SPSS doesn’t allow you to do an equivalence test. Your editors and reviewers were always OK with your statements such as “as predicted, the interaction revealed there was an effect for participants in the experimental condition (p &lt; 0.05) but there was no effect in the control condition (F &lt; 1).” Well, I just ruined that for you. Absence of evidence is not evidence of absence!</p>
<p>We can’t use p &gt; 0.05 as evidence of a lack of an effect. You can switch to Bayesian statistics if you want to support the null, but the default priors are only useful of in research areas where very large effects are examined (e.g., some areas of cognitive psychology), and are not appropriate for most other areas in psychology, so you will have to be able to quantify your prior belief yourself. You can teach yourself how, but there might be researchers who prefer to provide support for the lack of an effect within a Frequentist framework. Given that most people think about the effect size they expect when designing their study, defining the SESOI at this moment is straightforward. After choosing the SESOI, you can even design your study to have sufficient power to reject the presence of a meaningful effect. Controlling your error rates is thus straightforward in equivalence hypothesis tests, while it is not that easy in Bayesian statistics (although it can be done through simulations).</p>
<p>One thing I noticed while reading this literature is that TOST procedures, and power analyses for TOST, are not created to match the way psychologists design studies and think about meaningful effects. In medicine, equivalence is based on the raw data (a decrease of 10% compared to the default medicine), while we are more used to think in terms of standardized effect sizes (correlations or Cohen’s d). Biostatisticians are fine with estimating the pooled standard deviation for a future study when performing power analysis for TOST, but psychologists use standardized effect sizes to perform power analyses. Finally, the packages that exist in R (e.g., equivalence) or the software that does equivalence hypothesis tests (e.g., Minitab, which has TOST for t-tests, but not correlations) requires that you use the raw data. In my experience (Lakens, 2013) researchers find it easier to use their own preferred software to handle their data, and then calculate additional statistics not provided by the software they use by typing in summary statistics in a spreadsheet (means, standard deviations, and sample sizes per condition). So my functions don’t require access to the raw data (which is good for reviewers as well). Finally, the functions make a nice picture such as the one above so you can see what you are doing.</p>
<!-- ## Using CI for equivalence tests -->
<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- x<-rnorm(100) #Generate 100 random normally distributed observations -->
<!-- y<-rnorm(100) #Generate 100 random normally distributed observations -->
<!-- #ROPE test -->
<!-- BESTout<-BESTmcmc(x,y) -->
<!-- plot(BESTout, ROPE = c(-0.5, 0.5), showCurve = TRUE, xlim = c(-1,1)) -->
<!-- #TOST test -->
<!-- TOSTout <- TOSTER::t_TOST(x = x, y = y, low_eqbound = -0.5, high_eqbound = 0.5, alpha = 0.05) -->
<!-- plot(TOSTout, estimates = "raw") -->
<!-- ``` -->

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="power.html"><span class="header-section-number">8</span> Sample size justification</a></div>
<div class="next"><a href="references.html"><span class="header-section-number">10</span> References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#equivalencetest"><span class="header-section-number">9</span> Interval Hypotheses and Equivalence Testing</a></li>
<li><a class="nav-link" href="#equivalence-tests"><span class="header-section-number">9.1</span> Equivalence tests</a></li>
<li><a class="nav-link" href="#equivalence-tests-for-anova"><span class="header-section-number">9.2</span> Equivalence tests for ANOVA</a></li>
<li><a class="nav-link" href="#reporting-equivalence-tests"><span class="header-section-number">9.3</span> Reporting equivalence tests</a></li>
<li>
<a class="nav-link" href="#ROPE"><span class="header-section-number">9.4</span> The Bayesian ROPE procedure</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#which-interval-width-should-be-used"><span class="header-section-number">9.4.1</span> Which interval width should be used?</a></li>
<li><a class="nav-link" href="#powerequiv"><span class="header-section-number">9.4.2</span> Power analysis for Equivalence Tests</a></li>
<li><a class="nav-link" href="#use-of-prior-information"><span class="header-section-number">9.4.3</span> Use of prior information</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">9.4.4</span> Conclusion</a></li>
</ul>
</li>
<li><a class="nav-link" href="#absence-of-evidence-is-not-evidence-of-absence."><span class="header-section-number">9.5</span> Absence of evidence is not evidence of absence.</a></li>
<li>
<a class="nav-link" href="#justifysesoi"><span class="header-section-number">9.6</span> JUstifying a smallest effect size of interest</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#rejecting-the-presence-of-a-meaningful-effect"><span class="header-section-number">9.6.1</span> Rejecting the presence of a meaningful effect</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/09-equivalencetest.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/09-equivalencetest.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-03-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
