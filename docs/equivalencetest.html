<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Equivalence Testing and Interval Hypotheses | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniel Lakens">
<meta name="description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 9 Equivalence Testing and Interval Hypotheses | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Equivalence Testing and Interval Hypotheses | Improving Your Statistical Inferences">
<meta name="twitter:description" content="Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="active" href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="" href="prereg.html"><span class="header-section-number">10</span> Preregistration and Transparency</a></li>
<li><a class="" href="meta.html"><span class="header-section-number">11</span> Meta-analysis</a></li>
<li><a class="" href="bias.html"><span class="header-section-number">12</span> Bias detection</a></li>
<li><a class="" href="references.html"><span class="header-section-number">13</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="equivalencetest" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses<a class="anchor" aria-label="anchor" href="#equivalencetest"><i class="fas fa-link"></i></a>
</h1>
<p>Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are analyzed with a commonly analyzed with a null-hypothesis significance test. When a statistically significant <em>p</em>-value is observed, the null-hypothesis can be rejected, and researchers claim that the intervention works, or that there is a relationship between two variables. But if the <em>p</em>-value is not statistically significant, researcher very often draw a logically incorrect conclusion: They conclude there is no effect based on <em>p</em> &gt; 0.05.</p>
<p>Open a result section of an article you are writing, or the result section of articles you have recently read. Search for <em>p</em> &gt; 0.05, and look carefully what you or the scientists concluded. If you see the conclusion that there was 'no effect' or there was 'no association between variables', you have found an example of forgetting that <em>absence of evidence is not evidence of absence</em> <span class="citation">(<a href="references.html#ref-altman_statistics_1995" role="doc-biblioref">Altman &amp; Bland, 1995</a>)</span>. A non-significant result in itself only tells us that we can not reject the null hypothesis. It is tempting to ask after <em>p</em> &gt; 0.05 'so, is the true effect zero'? But the <em>p</em>-value from a null-hypothesis significance test can not answer that question. It might be useful to think of the answer to the question whether an effect is absent after observing <em>p</em> &gt; 0.05 as 無 (<a href="https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning">mu</a>), used as a non-dualistic answer, neither yes nor no, or 'unasking the question', because it is not possible to answer the question whether an effect is absent based on <em>p</em> &gt; 0.05.</p>
<p>There should be many situations where researchers are interested in examining whether an effect is absent. For example, it can be important to show two groups do not differ on factors that might be a confound in the experimental design (e.g., examining whether a manipulation intended to increase fatigue did not affect the mood of the participants by showing that positive and negative affect did not differ between the groups). Researchers might be interested in whether two interventions work equally well, especially when the newer intervention costs less or requires less effort (e.g., is online therapy just as efficient as in person therapy?). And other times we might be interested to demonstrate the absence of an effect because a theoretical model predicts there is no effect, or because we believe a previously published study was a false positive, and we expect to show the absence of an effect in a replication study.</p>
<p>Statistical approaches have been developed to allow researchers to make claims about the absence of effects. However, it is never possible to show an effect is <em>exactly</em> 0. Even if you would collect data from every person in the world, the effect in any single study will randomly vary around the true effect size of 0, and there will be a tiny observed difference. Hodges and Lehman <span class="citation">(<a href="references.html#ref-hodges_testing_1954" role="doc-biblioref">Hodges &amp; Lehmann, 1954</a>)</span> were the first to discuss the statistical problem of testing whether two populations have the same mean. They suggest (p. 264) to: “test that their means do not differ by more than an amount specified to represent the smallest difference of practical interest.” Nunnaly <span class="citation">(<a href="references.html#ref-nunnally_place_1960" role="doc-biblioref">Nunnally, 1960</a>)</span> similarly proposed a ‘fixed-increment’ hypothesis where researcher compare an observed effect against a range of values that is deemed too small to be meaningful. Defining a range of values considered practically equivalent to the absence of an effect is known as an <strong>equivalence range</strong> <span class="citation">(<a href="references.html#ref-bauer_unifying_1996" role="doc-biblioref">Bauer &amp; Kieser, 1996</a>)</span> or a <strong>region of practical equivalence</strong> <span class="citation">(<a href="references.html#ref-kruschke_bayesian_2013" role="doc-biblioref">Kruschke, 2013</a>)</span>. The equivalence range should be specified in advance, and requires careful consideration of which effects are too small to be meaningful.</p>
<p>Although researchers have repeatedly attempted to introduce test against an equivalence range in the social sciences <span class="citation">(<a href="references.html#ref-cribbie_recommendations_2004" role="doc-biblioref">Cribbie et al., 2004</a>; <a href="references.html#ref-hoenig_abuse_2001" role="doc-biblioref">Hoenig &amp; Heisey, 2001</a>; <a href="references.html#ref-levine_communication_2008" role="doc-biblioref">Levine et al., 2008</a>; <a href="references.html#ref-quertemont_how_2011" role="doc-biblioref">Quertemont, 2011</a>; <a href="references.html#ref-rogers_using_1993" role="doc-biblioref">Rogers et al., 1993</a>)</span>, this statistical approach became popular during the replication crisis as researchers searched for tools to interpret null-results when performing replication studies. Researchers wanted to be able to publish informative null results when replicating findings in the literature that suspected of being false positives. One notable example were the studies on pre-cognition by Daryl Bem, which ostensibly showed that participants were able to predict the future <span class="citation">(<a href="references.html#ref-bem_feeling_2011" role="doc-biblioref">Bem, 2011</a>)</span>. Equivalence tests were proposed as a statistical approach to answer the question whether an observed effect is small enough to conclude that a previous study could not be replicated <span class="citation">(<a href="references.html#ref-anderson_theres_2016" role="doc-biblioref">S. F. Anderson &amp; Maxwell, 2016</a>; <a href="references.html#ref-lakens_equivalence_2017" role="doc-biblioref">Lakens, 2017</a>; <a href="references.html#ref-simonsohn_small_2015" role="doc-biblioref">Simonsohn, 2015</a>)</span>. Researchers specify a smallest effect size of interest (for example an effect of 0.5, so for a two-sided test any value outside a range from -0.5 to 0.5) and test whether effects more extreme than this range can be rejected. If so, they can reject the presence effects that are large enough to be meaningful.</p>
<p>Equivalence tests are a specific implementation of <strong>interval hypothesis tests</strong>, where instead of testing against a null hypothesis of no effect (or an effect size of 0), an effect is tested against a null hypothesis that represents a range of non-zero effect sizes. One can distinguish a <strong>nil null hypothesis</strong>, where the null hypothesis is an effect of 0, from a <strong>non-nil null hypothesis</strong>, where the null hypothesis is any other effect that 0, for example effects more extreme than the smallest effect size of interest <span class="citation">(<a href="references.html#ref-nickerson_null_2000" role="doc-biblioref">Nickerson, 2000</a>)</span>. As Nickerson writes:</p>
<blockquote>
<p>The distinction is an important one, especially relative to the controversy regarding the merits or shortcomings of NHST inasmuch as criticisms that may be valid when applied to nil hypothesis testing are not necessarily valid when directed at null hypothesis testing in the more general sense.</p>
</blockquote>
<p>Indeed, one of the most widely suggested improvements that mitigates the most important limitations of null-hypothesis significance testing is to replace the nil null hypothesis with the test of a range prediction (by specifying a non-nil null hypothesis) in an interval hypothesis test <span class="citation">(<a href="references.html#ref-lakens_practical_2021" role="doc-biblioref">Lakens, 2021</a>)</span>. To illustrate the difference, panel A in Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a> visualizes the results that are predicted in a two-sided null hypothesis test with a nil hypothesis, where the test examines whether an effect of 0 can be rejected. Panel B shows an interval hypothesis where an effect between 0.5 and 2.5 is predicted, where the non-nill null hypothesis consists of values smaller than 0.5 or larger than 2.5, and the interval hypothesis tests examines whether values in these ranges can be rejected. Panel C illustrates an equivalence test, which is basically identical to an interval hypothesis test, but the predicted effects are located in a range around 0, and contain effects that are deemed too small to be meaningful.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:intervaltest"></span>
<img src="09-equivalencetest_files/figure-html/intervaltest-1.png" alt="Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D)." width="100%"><p class="caption">
Figure 9.1: Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D).
</p>
</div>
<p>When an equivalence test is reversed, an a researcher designs a study to reject effect less extreme than a smallest effect size of interest (see Panel D in Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a>), it is called a <strong>minimum effect test</strong> <span class="citation">(<a href="references.html#ref-murphy_testing_1999" role="doc-biblioref">Murphy &amp; Myors, 1999</a>)</span>. A researchers might not just be interested in rejecting an effect of 0 (as in a null hypothesis significance test) but in rejecting effects that are too small to be meaningful. All else equal, a study designed to have high power for a minimum effect requires more observations than if the goal had been to reject an effect of zero. As the confidence interval needs to reject a value that is closer to the observed effect size (e.g., 0.1 instead of 0) it needs to be more narrow, which requires more observations.</p>
<p>One benefit of a minimum effect test compared to a null-hypothesis test is that there is no distinction between statistical significance and practical significance. As the test value is chosen to represent the minimum effect of interest, whenever it is rejected, the effect is both statistically and practically significant <span class="citation">(<a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>. Another benefit of minimum effect tests is that, especially in correlational studies in the social sciences, variables are often connected through causal structures that result in real but theoretically uninteresting nonzero correlations between variables, which has been labeled the 'crud factor' <span class="citation">(<a href="references.html#ref-meehl_appraising_1990" role="doc-biblioref">Meehl, 1990</a>; <a href="references.html#ref-orben_crud_2020" role="doc-biblioref">Orben &amp; Lakens, 2020</a>)</span>. Because an effect of zero is unlikely to be true in large correlational datasets, rejecting a nil null hypothesis is not a severe test. Even if the hypothesis is incorrect, it is likely that an effect of 0 will be rejected due to 'crud'. For this reason, some researchers have suggested to test against a minimum effect of <em>r</em> = 0.1, as correlations below this threshold are quite common due to theoretically irrelevant correlations between variables <span class="citation">(<a href="references.html#ref-ferguson_providing_2021" role="doc-biblioref">Ferguson et al., 2021</a>)</span>.</p>
<p>Figure <a href="equivalencetest.html#fig:intervaltest">9.1</a> illustrates two-sided tests, but it is often more intuitive and logical to perform one-sided tests. In that case, a minimum effect test would, for example, aim to reject effects smaller than 0.1, and an equivalence test would aim to reject effects larger than for example 0.1. Instead of specifying an upper and lower bound of a range, it is sufficient to specify a single value for one-sided tests. A final variation of a one-sided non-nil null hypothesis test is known as a test for <strong>non-inferiority</strong>, which examines of an effect is larger than the lower bound of an equivalence range. Such a test is for example performed when a novel intervention should not be noticeable worse than an existing intervention - but it can be a tiny bit worse. For example, if a difference between a novel and existing intervention is not smaller than -0.1, and effects smaller than -0.1 can be rejected, one can conclude an effect is non-inferior <span class="citation">(<a href="references.html#ref-mazzolari_myths_2022" role="doc-biblioref">Mazzolari et al., 2022</a>; <a href="references.html#ref-schumi_through_2011" role="doc-biblioref">Schumi &amp; Wittes, 2011</a>)</span>. We see that extending nil null hypothesis tests to non-nil null hypotheses allow researchers to ask questions that might be more interesting.</p>
<div id="equivalence-tests" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Equivalence tests<a class="anchor" aria-label="anchor" href="#equivalence-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Equivalence tests were first developed in pharmaceutical sciences <span class="citation">(<a href="references.html#ref-hauck_new_1984" role="doc-biblioref">Hauck &amp; Anderson, 1984</a>; <a href="references.html#ref-westlake_use_1972" role="doc-biblioref">Westlake, 1972</a>)</span> and later formalized as the <strong>two one-sided tests (TOST)</strong> approach to equivalence testing <span class="citation">(<a href="references.html#ref-schuirmann_comparison_1987" role="doc-biblioref">Schuirmann, 1987</a>; <a href="references.html#ref-seaman_equivalence_1998" role="doc-biblioref">Seaman &amp; Serlin, 1998</a>; <a href="references.html#ref-wellek_testing_2010" role="doc-biblioref">Wellek, 2010</a>)</span>. The TOST procedure entails performing two one-sided tests to examine whether the observed data is surprisingly larger than a lower equivalence boundary (<span class="math inline">\(\Delta_{L}\)</span>), or surprisingly smaller than an upper equivalence boundary (<span class="math inline">\(\Delta_{U}\)</span>):</p>
<p><span class="math display">\[
t_{L} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2} - \Delta_{L}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
t_{U} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2}{- \Delta}_{U}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
\]</span></p>
<p>where <em>M</em> indicates the means of each sample, <em>n</em> is the sample size, and σ is
the pooled standard deviation:</p>
<p><span class="math display">\[
\sigma = \sqrt{\frac{\left( n_{1} - 1 \right)\text{sd}_{1}^{2} + \left( n_{2} - 1 \right)\text{sd}_{2}^{2}}{n_{1} + \ n_{2} - 2}}
\]</span></p>
<p>If both one-sided tests are significant, we can reject the presence of effects large enough to be meaningful. The formulas are highly similar to the normal formula for the <em>t</em>-statistic. The difference between a NHST <em>t</em>-test and the TOST procedure is that the lower equivalence boundary <span class="math inline">\(\Delta_{L}\)</span> and the upper equivalence boundary <span class="math inline">\(\Delta_{U}\)</span> are subtracted from the mean difference between groups (in a normal <em>t</em>-test, we compare the mean difference against 0, and thus the delta drops out of the formula because it is 0).</p>
<p>To perform an equivalence test, you don't need to learn any new statistical tests, as it is just the well-known <em>t</em>-test against a different value than 0. It is somewhat surprising that the use of <em>t</em>tests to perform equivalence tests is not taught alongside their use in null-hypothesis significance tests, as there is some indication that this could prevent common misunderstandings of <em>p</em>-values <span class="citation">(<a href="references.html#ref-parkhurst_statistical_2001" role="doc-biblioref">Parkhurst, 2001</a>)</span>. Let's look at an example of an equivalence test using the TOST procedure.</p>
<p>In a study where researchers are manipulating fatigue by asking participants to carry heavy boxes around, the researchers want to ensure the manipulation does not inadvertently alter participants’ moods. The researchers assess positive and negative emotions in both ocnditions, and want to claim there are no differences in positive mood. Let’s assume that positive mood in the experimental fatigue condition (m1 = 4.55, sd1 = 1.05, n1 = 15) did not differ from the mood in the the control condition (m2 = 4.87, sd2 = 1.11, n2 = 15). The researchers conclude: “Mood did not differ between conditions, <em>t</em> = -0.81, <em>p</em> = .42”. Of course, mood did differ between conditions, as 4.55 - 4.87 = -0.32. The claim is that there was no <em>meaningful</em> difference in mood, but to make such a claim in a correct manner, we first need to specify which difference in mood is large enough to be meaningful. For now, let's assume the researcher consider any effect less extreme half a scale point too small to be meaningful. We test now test if the observed mean difference of -0.32 is small enough such that we can reject the presence of effects that are large enough to matter.</p>
<p>The TOSTER package (originally created by myself but recently redesigned by Aaron Caldwell) can be used to plot two <em>t</em>-distributions and their critical regions indicating when we can reject the presence of effects smaller than -0.5 and larger than 0.5. It can take some time to get used to the idea that we are rejecting values more extreme than the equivalence bounds. Try to consistently ask in any hypothesis test: Which values can the test reject? In a nil null hypothesis test, we can reject an effect of 0, and in the equivalence test in the Figure below, we can reject values lower than -0.5 and higher than 0.5. In Figure <a href="equivalencetest.html#fig:tdistequivalence">9.2</a> we see two <em>t</em>-distributions centered on the upper and lower bound of the specified equivalence range (-0.5 and 0.5).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tdistequivalence"></span>
<img src="09-equivalencetest_files/figure-html/tdistequivalence-1.png" alt="The mean difference and its confidence interval plotted below the *t*-distributions used to perform the two-one-sided tests against -0.5 and 0.5." width="100%"><p class="caption">
Figure 9.2: The mean difference and its confidence interval plotted below the <em>t</em>-distributions used to perform the two-one-sided tests against -0.5 and 0.5.
</p>
</div>
<p>Below the two curves we see a line that represents the confidence interval ranging from -0.99 to 0.35, and a dot on the line that indicates the observed mean difference of -0.32. Let's first look at the left curve. We see the green highlighted area in the tails that highlights which observed mean differences would be extreme enough to statistically reject an effect of -0.5. Our observed mean difference of -0.32 lies very close to 0.5, and if we look at the left distribution, the mean is not far enough away from 0.5 to fall in the green area that indicates when observed differences would be statistically significant. We can also perform the equivalence test using the TOSTER package, and look at the results.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/tsum_TOST.html">tsum_TOST</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="fl">4.55</span>, m2 <span class="op">=</span> <span class="fl">4.87</span>, sd1 <span class="op">=</span> <span class="fl">1.05</span>, sd2 <span class="op">=</span> <span class="fl">1.11</span>,
                  n1 <span class="op">=</span> <span class="fl">15</span>, n2 <span class="op">=</span> <span class="fl">15</span>, low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Equivalence
## Equivalence Bounds (raw):-0.500 &amp; 0.500
## Alpha Level:0.05
## The equivalence test was non-significant, t(27.91) = 0.456, p = 3.26e-01
## The null hypothesis test was non-significant, t(27.91) = -0.811, p = 4.24e-01
## NHST: don't reject null significance hypothesis that the effect is equal to zero 
## TOST: don't reject null equivalence hypothesis
## 
## TOST Results 
##                     t        SE       df    p.value
## t-test     -0.8111280 0.3945124 27.91398 0.42415467
## TOST Lower  0.4562595 0.3945124 27.91398 0.32586680
## TOST Upper -2.0785154 0.3945124 27.91398 0.02348582
## 
## Effect Sizes 
##                 estimate        SE   lower.ci  upper.ci conf.level
## Raw           -0.3200000 0.3945124 -0.9911879 0.3511879        0.9
## Hedges' g(av) -0.2881401 0.3812249 -0.9301965 0.3193638        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span><span class="op">$</span><span class="va">TOST</span><span class="op">$</span><span class="va">p</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></code></pre></div>
<pre><code>## [1] 0.4241547</code></pre>
<p>In the line 't-test' the output shows the traditional nil null hypothesis significance test (which we already knew was not statistically significant: <em>t</em> = 0.46, <em>p</em> = 0.42. Just like the default <em>t</em>-test in R, the tsum_TOST function will by default calculate Welch’s <em>t</em>-test (instead of Student’s <em>t</em>-test), which is a better default <span class="citation">(<a href="references.html#ref-delacre_why_2017" role="doc-biblioref">Delacre et al., 2017</a>)</span>, but you can request Student’s <em>t</em>-test by adding <code>var.equal = TRUE</code> as an argument to the function.</p>
<p>We also see a test indicated by TOST Lower. This is the first one-sided test examining if we can reject effects lower than -0.5. From the test result, we see this is not the case: <em>t</em> = 0.46, <em>p</em> = 0.33. This is an ordinary <em>t</em>-test, just against an effect of -0.5. Because we can not reject differences more extreme than -0.5, it is possible that a different we consider meaningful (e.g., a difference of -0.60) is present. When we look at the one-sided test against the upper bound of the equivalence range (0.5) we see that we can statistically reject the presence of mood effects larger than 0.5, as in the line TOST Upper we see <em>t</em> = -2.08, <em>p</em> = 0.02. Our final conclusion is therefore that, even thought we can reject effects more extreme than 0.5 based on the observed mean difference of -0.32, we can not reject effects more extreme than -0.5. Therefore, we can not completely reject the presence of meaningful mood effects. As the data does not allow us to claim the effect is different from 0, nor that the effect is, if anything, too small to matter (based on an equivalence range from -0.5 to 0.5), the data are <strong>inconclusive</strong>. We can not distinguish between a Type 2 error (there is an effect, but in this study we just did not detect it) or a true negative (there really is no effect large enough to matter).</p>
<p>Note that because we fail to reject the one-sided test against the lower equivalence bound, the possibility remains that there is a true effect size that is large enough to be considered meaningful. This statement is true, even when the effect size we have observed (-0.32) is closer to zero than the equivalence bound of -0.5. One might think the observed effect size needs to be more extreme (i.e., further away from 0) than the equivalence bound to maintain the possibility that there is an effect that is large enough to be considered meaningful. But that is not required. The 90% CI indicates that some values below -0.5 can not be rejected. As we can expect that 90% of confidence intervals in the long run capture the true population parameter, it is perfectly possible that the true effect size is more extreme than -0.5. And, the effect might even be more extreme than the values captured by this confidence interval, as 10% of the time, the computed confidence interval is expected to not contain the true effect size. Therefore, when we fail to reject the smallest effect size of interest, we retain the possibility that an effect of interest exists. If we can reject the nil null hypothesis, but fail to reject values more extreme than the equivalence bounds, then we can claim there is an effect, and it might be large enough to be meaningful.</p>
<p>One way to reduce the probability of an inconclusive effect is to collect sufficient data. Let's imagine the researchers had not collected 15 participants in each condition, but 200 participants. They otherwise observe exactly the same data. As explained in the chapter on <a href="confint.html#confint">confidence intervals</a>, as the sample size increases, the confidence interval becomes more narrow. For a TOST equivalence test to be able to reject both the upper and lower bound of the equivalence range, the confidence interval needs to fall completely within the equivalence range. In Figure <a href="equivalencetest.html#fig:ciequivalence">9.3</a> we see the observed mean difference (the black dot) and it's confidence interval (the horizontal black line) for a sample size of 200 per group. The colourful plot on top of the mean difference and confidence interval is a confidence density plot <span class="citation">(<a href="references.html#ref-schweder_confidence_2016" role="doc-biblioref">Schweder &amp; Hjort, 2016</a>)</span>, which is a graphical summary of the distribution of confidence.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ciequivalence"></span>
<img src="09-equivalencetest_files/figure-html/ciequivalence-1.png" alt="The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5." width="100%"><p class="caption">
Figure 9.3: The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.
</p>
</div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Equivalence
## Equivalence Bounds (raw):-0.500 &amp; 0.500
## Alpha Level:0.05
## The equivalence test was significant, t(396.78) = 1.666, p = 4.82e-02
## The null hypothesis test was significant, t(396.78) = -2.962, p = 3.24e-03
## NHST: reject null significance hypothesis that the effect is equal to zero 
## TOST: reject null equivalence hypothesis
## 
## TOST Results 
##                    t        SE       df      p.value
## t-test     -2.961821 0.1080417 396.7773 3.242190e-03
## TOST Lower  1.666024 0.1080417 396.7773 4.824893e-02
## TOST Upper -7.589665 0.1080417 396.7773 1.156039e-13
## 
## Effect Sizes 
##                 estimate        SE   lower.ci   upper.ci conf.level
## Raw           -0.3200000 0.1080417 -0.4981286 -0.1418714        0.9
## Hedges' g(av) -0.2956218 0.1008059 -0.4625958 -0.1310411        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<p>Because of the larger sample size, the confidence is more narrow, and we see it excludes both the upper and lower equivalence bound, which means we can reject effects outside of the equivalence range (even though with a <em>p</em> = 0.048 the one-sided test against the lower equivalence bound is only just statistically significant). If we look at the traditional nil null hypothesis test we see we can also reject an effect of 0. In other words, both the null hypothesis test as the equivalence test have yielded significant results. This means we can claim that the observed effect is statistically different from zero, and that the effect is statistically smaller than effects we deemed large enough to matter when we specified the equivalence range from -0.5 to 0.5. This illustrates how combining equivalence tests and nil null hypothesis tests can prevent us from mistaking statistically significant effects for practically significant effects. Yes, we can reject an effect of 0, but no, the effect is not large enough to be meaningful.</p>
</div>
<div id="reporting-equivalence-tests" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Reporting Equivalence Tests<a class="anchor" aria-label="anchor" href="#reporting-equivalence-tests"><i class="fas fa-link"></i></a>
</h2>
<p>It is common practice to only report the test yielding the higher <em>p</em>-value of the two one-sided tests when reporting an equivalence test. Because both one-sided tests need to be statistically significant to reject the null hypothesis in an equivalence test (i.e., the presence of effects large enough to matter), when the larger of the two hypothesis tests rejects the equivalence bound, so does the other test. Unlike in null-hypothesis significance tests it is not common to report standardized effect sizes for equivalence tests, but there can be situations where researchers might want to discuss how far the effect is removed from the equivalence bounds on the raw scale. Prevent the erroneous interpretation to claim there is 'no effect', that an effect is 'absent', that the true effect size is 'zero', or vague verbal descriptions, such as that two groups yielded 'similar' or 'comparable' data. A significant equivalence test rejects effects more extreme that the equivalence bounds. Smaller true effects have not been rejected, and thus it remains possible that there is a true effect. Because a TOST procedure is a frequentist test based on a <em>p</em>-value, all other <a href="pvalue.html#misconceptions">misconceptions of <em>p</em>-values</a> should be prevented as well.</p>
<p>When summarizing the main result of an equivalence test, for example in an abstract, always report the equivalence range that the data is tested against. Reading 'based on an equivalence test we concluded the absence of a meaningful effect' means something very different if the equivalence bounds were d = -0.9 to 0.9 than when the bounds were d = -0.2 to d = 0.2. So instead, write "based on an equivalence test with an equivalence range of d = -0.2 to 0.2, we conclude the absence of a meaningful effect. Of course, whether peers agree you have correctly concluded the absence of a meaningful effect depends on whether they agree with your justification for a smallest effect of interest! A more neutral conclusion would be a statement such as: 'based on an equivalence test, we rejected the presence of effects more extreme than -0.2 to 0.2, so we can act (with an error rate of alpha) as if the effect, is any, is less extreme than our equivalence range'. Here, you do not use value-laden terms such as 'meaningful'. If both a null-hypothesis test and a significant test are non-significant, the findings is best described as 'inconclusive': There is not enough data to reject the null, or the smallest effect size of interest. If both the null-hypothesis test and the equivalence test are statistically significant, you can claim there is an effect, but at the same time claim the effect is too small to be of interest (given your justification for the equivalence range).</p>
<p>Equivalence bounds can be specified in raw effect sizes, or in standardized mean differences. It is better to specify the equivalence bounds in terms of raw effect sizes. Setting them in terms of Cohen's <em>d</em> leads to bias in the statistical test, as the observed standard deviation has to be used to translate the specified Cohen's <em>d</em> into a raw effect size for the equivalence test (and when you set equivalence bounds in standardized mean differences, TOSTER will warn: "Warning: setting bound type to SMD produces biased results!"). The bias is in practice not too problematic in any single equivalence test, and being able to specify the equivalence bounds in standardized mean differences lowers the threshold to perform an equivalence test when they do not know the standard deviation of their measure. But as equivalence testing becomes more popular, and fields establish smallest effect sizes of interest, they should do so in raw effect size differences, not in standardized effect size differences.</p>
</div>
<div id="minimum-effect-tests" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Minimum Effect Tests<a class="anchor" aria-label="anchor" href="#minimum-effect-tests"><i class="fas fa-link"></i></a>
</h2>
<p>If a researcher has specified a smallest effect size of interest, and is interested in testing whether the effect in the population is larger than this smallest effect of interest, a minimum effect test can be performed. As with any hypothesis test, we can reject the smallest effect of interest whenever the confidence interval around the observed effect does not overlap with it. In the case of a minimum effect test, however, the confidence interval should be fall completely beyond the smallest effect size of interest. For example, let's assume a researcher performs a minimum effect test with 200 observations per condition against a smallest effect size of interest of a mean difference of 0.5.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tmet"></span>
<img src="09-equivalencetest_files/figure-html/tmet-1.png" alt="The mean difference and its confidence interval plotted below the *t*-distributions used to perform the two-one-sided tests against -0.5 and 0.5 when performing a minimum effect test." width="100%"><p class="caption">
Figure 9.4: The mean difference and its confidence interval plotted below the <em>t</em>-distributions used to perform the two-one-sided tests against -0.5 and 0.5 when performing a minimum effect test.
</p>
</div>
<pre><code>## 
## Welch Modified Two-Sample t-Test
## Hypothesis Tested: Minimal Effect
## Equivalence Bounds (raw):-0.500 &amp; 0.500
## Alpha Level:0.05
## The minimal effect test was significant, t(396.78) = 12.588, p = 4.71e-04
## The null hypothesis test was significant, t(396.78) = 7.960, p = 1.83e-14
## NHST: reject null significance hypothesis that the effect is equal to zero 
## TOST: reject null MET hypothesis
## 
## TOST Results 
##                    t        SE       df      p.value
## t-test      7.959893 0.1080417 396.7773 1.827800e-14
## TOST Lower 12.587737 0.1080417 396.7773 1.000000e+00
## TOST Upper  3.332048 0.1080417 396.7773 4.714941e-04
## 
## Effect Sizes 
##                estimate        SE  lower.ci  upper.ci conf.level
## Raw           0.8600000 0.1080417 0.6818714 1.0381286        0.9
## Hedges' g(av) 0.7944836 0.1041808 0.6263676 0.9689959        0.9
## 
## Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
<p>Below the two curves we again see a line that represents the confidence interval ranging from 0.68 to 1.04, and a dot on the line that indicates the observed mean difference of 0.86. The entire confidence interval lies well above the minimum effect of 0.5, and we can therefore not just reject the nil null hypothesis, but also effects smaller than the minimum effect of interest. Therefore, we can claim that the effect is large enough to be not just statistically significant, but also practically significant (as long as we have justified our smallest effect size of interest well). Because we have performed a two-sided minimum effect test, the minimum effect test would also have been significant if the confidence interval had been completely on the opposite side of -0.5.</p>
<p>Earlier we discussed how combining traditional NHST and an equivalence test lead to more informative results, and it is also possible to combine a minimum effect test and an equivalence test. One might even say that such a combination is most informative test of a prediction whenever a smallest effect size of interest can be specified. In principle, this is true. As long as we are able to collect enough data, we will always get an informative and straightforward answer when we combine a minimum effect test with an equivalence test: Either we can reject all effects that are too small to be of interest, or we can reject all effects that are large enough to be of interest. As we will see below in the section on power analysis for interval hypotheses, whenever the true effect size is close to the smallest effect size of interest, a large amount of observations will need to be collected. And if the true effect size happens to be identical to the smallest effect size of interest, neither the minimum effect test nor the equivalence test can be correctly rejected (and any significant test would be a Type 1 error). If a researcher can collect sufficient data (so that the test has high statistical power), and is relatively confident that the true effect size will be larger or smaller than the smallest effect of interest, then the combination of a minimum effect test and an equivalence test can be attractive as such a hypothesis test is likely yield a clear answer to the research question.</p>
</div>
<div id="power-analysis-for-interval-hypothesis-tests" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Power Analysis for Interval Hypothesis Tests<a class="anchor" aria-label="anchor" href="#power-analysis-for-interval-hypothesis-tests"><i class="fas fa-link"></i></a>
</h2>
<p>When designing a study it is a sensible strategy to always plan for both the presence, as the absence, of an effect. Several scientific journals require a sample size justification for Registered Reports where the statistical power to reject the null hypothesis is high, but where the study is also capable of demonstrating the absence of an effect, for example by also performing a power analysis for an equivalence test. As we saw in the chapter on <a href="errorcontrol.html#errorcontrol">error control</a> and <a href="likelihoods.html#likelihoods">likelihoods</a> null results are to be expected, and if you only think about the possibility of observing a null effect when the data has been collected, it is often too late.</p>
<p>The statistical power for interval hypotheses depend on the alpha level, the sample size, the smallest effect of interest you decide to test against, and the true effect size. For an equivalence test, it is common to perform a power analysis assuming the true effect size is 0, but this might not always be realistic. The closer the expected effect size is to the smallest effect size of interest, the larger the sample size needed to reach a desired power. Don't be tempted to assume a true effect size of 0, if you have good reason to expect a small but non-zero true effect size. The sample size that the power analysis indicates you need to collect might be smaller, but in reality you also have a higher probability of an inconclusive result. Earlier versions of TOSTER only enabled researchers to perform power analyses for equivalence tests assuming a true effect size of 0, but a new power function by Aaron Caldwell allows users to specify <code>delta</code>, the expected effect size.</p>
<p>Assume a researchers desired to achieve 90% power for an equivalence test with an equivalence range from -0.5 to 0.5, with an alpha level of 0.05, and assuming a population effect size of 0. A power analysis for an equivalence test can be performed to examine the required sample size.</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 87.26261
##           delta = 0
##              sd = 1
##          bounds = -0.5, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
<p>We see that the required sample size is 88 participants in each condition for the independent <em>t</em>-test, Let's compare this power analysis to a situation where the researcher expects a true effect of d = 0.1, instead of a true effect of 0. To be able to reliable reject effects larger than 0.5, we will need a larger sample size, just as how we need a larger sample size for a null-hypothesis test powered to detect a d = 0.4 than a null-hypothesis test powered to detect a d = 0.5.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0.1</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 108.9187
##           delta = 0.1
##              sd = 1
##          bounds = -0.5, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
<p>We see the sample size has now increased to 109 participants in each condition. As mentioned before, it is not necessary to perform a two-sided equivalence test. It is also possible to perform a one-sided equivalence test. An example of a situation where such a directional test is appropriate is a replication study. If a previous study observed an effect of d = 0.48, and you perform a replication study, you might decide to consider any effect smaller than d = 0.2 a failure to replicate - including any effect in the opposite direction, such as an effect of d = -0.3. Although most software for equivalence tests requires you to specify an upper and lower bound for an equivalence range, you can mimic a one-sided test by setting the equivalence bound in the direction you want to ignore to a low value so that the one-sided test against this value will always be statistically significant. This can also be used to perform a power analysis for a minimum effect test, where one bound is the minimum effect of interest, and the other bound is set to an extreme value on the other side of the expected effect size.</p>
<p>In the power analysis for an equivalence test example below, the lower bound is set to -5 (it should be set low enough such that lowering it even further has no noticeable effect). We see that the new power function in the TOSTER package takes the directional prediction into account, and just as with directional predictions in a nil null hypothesis test, a directional prediction in an equivalence test is more efficient, and only 70 observations are needed to achieve 90% power.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># New TOSTER power functions allows power for expected non-zero effect.</span>
<span class="fu">TOSTER</span><span class="fu">::</span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/power_t_TOST.html">power_t_TOST</a></span><span class="op">(</span>power <span class="op">=</span> <span class="fl">0.9</span>, delta <span class="op">=</span> <span class="fl">0</span>,
                     alpha <span class="op">=</span> <span class="fl">0.05</span>, type <span class="op">=</span> <span class="st">"two.sample"</span>,
                     low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, high_eqbound <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<pre><code>## 
##      Two-sample TOST power calculation 
## 
##           power = 0.9
##            beta = 0.1
##           alpha = 0.05
##               n = 69.19784
##           delta = 0
##              sd = 1
##          bounds = -5.0, 0.5
## 
## NOTE: n is number in *each* group</code></pre>
<p>Statistical software offers options for power analyses for some statistical tests, but not for all tests. Just as with power analysis for a nil null hypothesis test, it can be necessary to use a simulation-based approach to power analysis.</p>
</div>
<div id="ROPE" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> The Bayesian ROPE procedure<a class="anchor" aria-label="anchor" href="#ROPE"><i class="fas fa-link"></i></a>
</h2>
<p>In Bayesian estimation, one way to argue for the absence of a meaningful effect is the <strong>region of practical equivalence</strong> (ROPE) procedure (<span class="citation">Kruschke (<a href="references.html#ref-kruschke_bayesian_2013" role="doc-biblioref">2013</a>)</span>), which is “somewhat analogous to frequentist equivalence testing” (<span class="citation">Kruschke &amp; Liddell (<a href="references.html#ref-kruschke_bayesian_2017" role="doc-biblioref">2017</a>)</span>). In the ROPE procedure, an equivalence range is specified, just as in equivalence testing, but the Bayesian highest density interval based on a posterior distribution (as explained in the chapter on <a href="bayes.html#bayes">Bayesian statistics</a>) is used instead of the confidence interval.</p>
<p>If the prior used by Kruschke was perfectly uniform, and the ROPE procedure and an equivalence test used the same confidence interval (e.g., 90%), the two tests would yield identical results. There would only be philosophical differences in how the numbers are interpreted. The BEST package in R that can be used to perform the ROPE procedure by default uses a ‘broad’ prior, and therefore results of the ROPE procedure and an equivalence test are not exactly the same, but they are very close. One might even argue the two tests are 'practically equivalent'. In the R code below random normally distributed data for two conditions is generated (with means of 0 and a standard deviation of 1) and the ROPE procedure and a TOST equivalence test are performed.</p>
<p><img src="09-equivalencetest_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;"><img src="09-equivalencetest_files/figure-html/unnamed-chunk-7-2.png" width="100%" style="display: block; margin: auto;">
The 90% HDI ranges from -0.06 to 0.39, with an estimated mean based on the prior and the data of 0.164. The HDI falls completely between the upper and the lower bound of the equivalence range, and therefore values more extreme than -0.5 or 0.5 are deemed implausible. The 95% CI ranges from -0.07 to 0.36 with an observed mean difference of 0.15. We see that the numbers are not identical, because in Bayesian estimation the observed values are combined with a prior, and the mean estimate is not purely based on the data. But the results are very similar, and will in most cases lead to similar inferences. The BEST R package also enables researchers to perform simulation based power analyses, which take a long time but, when using a broad prior, yield a result that is basically identical to the sample size from a power analysis for an equivalence test. The biggest benefit of ROPE over TOST is that it allows you to incorporate prior information. If you have reliable prior information, ROPE can use this information, which is especially useful if you don’t have a lot of data. If you use informed priors, check the robustness of the posterior against reasonable changes in the prior in sensitivity analyses.</p>
</div>
<div id="which-interval-width-should-be-used" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> Which interval width should be used?<a class="anchor" aria-label="anchor" href="#which-interval-width-should-be-used"><i class="fas fa-link"></i></a>
</h2>
<p>Because the TOST procedure is based on two one-sided tests, a 90% confidence interval is used when the one-sided tests are performed at an alpha level of 5%. Because both the test against the upper bound and the test against the lower bound needs to be statistically significant to declare equivalence (which as explained in the chapter on <a href="#multiplecomparisons">error control</a> is an intersection-union approach to multiple testing) it is not necessary to correct for the fact that two tests are performed. If the alpha level is adjusted for multiple comparisons, or if the alpha level is justified instead of relying on the default 5% level (or both), the corresponding confidence interval should be used, where CI = 100 - (2 * <span class="math inline">\(\alpha\)</span>). Thus, the width of the confidence interval is directly related to the choice for the alpha level, as we are making decisions to reject the smallest effect size of interest, or not, based on whether the confidence interval excluded the effect that is tested against.</p>
<p>When using a Highest Density Interval from a Bayesian perspective, such as the ROPE procedure, the choice for a width of a confidence interval does not follow logically from a desired error rate, or any other principle. Kruschke (2014, Chapter 5) writes: “How should we define 'reasonably credible'? One way is by saying that any points within the 95% HDI are reasonably credible.” McElreath has recommended the use of 67%, 89%, and 97%, because "No reason. They are prime numbers, which makes them easy to remember.". There are only two principled solutions. First, if a highest density interval width is used to make claims, these claims will be made with certain error rates, and researchers should quantify the risk of erroneous claim by computing frequentist error rates. This would make the ROPE procedure a Bayesian/Frequentist compromise procedure, where the computation of a posterior distribution allows for Bayesian interpretations of which parameters values are believed to be most probable, while decisions based on whether or not the HDI falls within an equivalence range have a formally controlled error rate. Note that when using an informative prior, a HDI does not match a CI, and the error rate when using a HDI can only be derived through simulations. The second solution is to not make any claims, present the full posterior distribution, and let readers draw their own conclusions.</p>
</div>
<div id="test-yourself-6" class="section level2" number="9.7">
<h2>
<span class="header-section-number">9.7</span> Test Yourself<a class="anchor" aria-label="anchor" href="#test-yourself-6"><i class="fas fa-link"></i></a>
</h2>
<!-- **Q1**: What is the correct interpretation of the following result if a researchers has performed a nil null hypothesis and an equivalence test (the equivalence bounds are indicated by the vertical dashed lines)?  -->
<!-- ```{r, eq_q_1, echo = FALSE} -->
<!-- res <- TOSTER::tsum_TOST(m1 = 0, m2 = 0.2, sd1 = 1, sd2 = 1, -->
<!--                   n1 = 100, n2 = 100, low_eqbound = -0.34, high_eqbound = 0.34) -->
<!-- plot(res, type = "tnull") -->
<!-- ``` -->
<!-- A) We can reject an effect of 0, and we fail to reject the smallest effect size of interest. Therefore we can claim there is an effect  -->

<!-- UPDATE TIER PROTOCOL https://osf.io/4cxed/ -->
<!-- ADD INFO TO PREREG SO PEOPLE CAN BE FOUND-->
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="power.html"><span class="header-section-number">8</span> Sample size justification</a></div>
<div class="next"><a href="prereg.html"><span class="header-section-number">10</span> Preregistration and Transparency</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#equivalencetest"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="nav-link" href="#equivalence-tests"><span class="header-section-number">9.1</span> Equivalence tests</a></li>
<li><a class="nav-link" href="#reporting-equivalence-tests"><span class="header-section-number">9.2</span> Reporting Equivalence Tests</a></li>
<li><a class="nav-link" href="#minimum-effect-tests"><span class="header-section-number">9.3</span> Minimum Effect Tests</a></li>
<li><a class="nav-link" href="#power-analysis-for-interval-hypothesis-tests"><span class="header-section-number">9.4</span> Power Analysis for Interval Hypothesis Tests</a></li>
<li><a class="nav-link" href="#ROPE"><span class="header-section-number">9.5</span> The Bayesian ROPE procedure</a></li>
<li><a class="nav-link" href="#which-interval-width-should-be-used"><span class="header-section-number">9.6</span> Which interval width should be used?</a></li>
<li><a class="nav-link" href="#test-yourself-6"><span class="header-section-number">9.7</span> Test Yourself</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/09-equivalencetest.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/09-equivalencetest.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-03-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
