<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.2 Verisimilitude, Belief, and Progress in Psychological Science | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-08-15" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>7.2 Verisimilitude, Belief, and Progress in Psychological Science | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="1-pvalue.html"><span class="toc-section-number">1</span> <em>p</em>-values</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-1-what-is-a-p-value.html"><span class="toc-section-number">1.1</span> What is a <em>p</em>-value?</a>
<a href="1-2-fisher-vs-neyman.html.-neyman"><span class="toc-section-number">1.2</span> Fisher vs. Neyman</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3</span> Preventing common misconceptions about <em>p</em>-values</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3.1</span> Misunderstanding 1: A non-significant <em>p</em>-value means that the null hypothesis is true</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.2</span> Misunderstanding 2: A significant <em>p</em>-value means that the null hypothesis is false.</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html"><span class="toc-section-number">1.3.3</span> Misunderstanding 3: A significant <em>p</em>-value means that a practically important effect has been discovered</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.4</span> Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.</a>
<a href="1-3-preventing-common-misconceptions-about-p-values.html."><span class="toc-section-number">1.3.5</span> Misunderstanding 5: One minus the <em>p</em>-value is the probability that the effect will replicate when repeated.</a>
</div>
</li>
</ul>
<a href="1-4-which-p-values-can-you-expect.html"><span class="toc-section-number">1.4</span> Which <em>p</em>-values can you expect?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="1-4-which-p-values-can-you-expect.html"><span class="toc-section-number">1.4.1</span> Lindley’s paradox</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="2-power.html"><span class="toc-section-number">2</span> Sample size justification</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-1-measuring-the-entire-population.html"><span class="toc-section-number">2.1</span> Measuring the Entire Population</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2</span> Feasibility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.1</span> The smallest effect size that can be statistically significant</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.2</span> Compute the width of the confidence interval around the effect size</a>
<a href="2-2-feasibility.html"><span class="toc-section-number">2.2.3</span> Plot a sensitivity power analysis</a>
<a href="2-2-feasibility.html."><span class="toc-section-number">2.2.4</span> Reporting a feasibility justification.</a>
</div>
</li>
</ul>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3</span> A-priori power analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-3-a-priori-power-analysis.html."><span class="toc-section-number">2.3.1</span> Performing a power analysis.</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.2</span> Justifying the effect size used in an a-priori power analysis</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.3</span> Justifying the error rates used in an a-priori power analysis</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.4</span> Some advice when using G*Power</a>
<a href="2-3-a-priori-power-analysis.html"><span class="toc-section-number">2.3.5</span> A-priori power analysis for the absence of an effect</a>
<a href="2-3-a-priori-power-analysis.html."><span class="toc-section-number">2.3.6</span> Reporting an a-priori power analysis.</a>
</div>
</li>
</ul>
<a href="2-4-compromisepower.html"><span class="toc-section-number">2.4</span> Compromise power analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-4-compromisepower.html"><span class="toc-section-number">2.4.1</span> Reporting a compromise power analysis</a>
</div>
</li>
</ul>
<a href="2-5-observedpower.html"><span class="toc-section-number">2.5</span> Observed (post-hoc) power analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-5-observedpower.html"><span class="toc-section-number">2.5.1</span> What to do if your editor asks for post-hoc power?</a>
</div>
</li>
</ul>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6</span> Designing efficient studies</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-6-designing-efficient-studies.html."><span class="toc-section-number">2.6.1</span> Use directional tests where relevant.</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.2</span> Use sequential analysis whenever possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.3</span> Increase your alpha level</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.4</span> Use within designs where possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.5</span> Remove statistical variation where possible</a>
<a href="2-6-designing-efficient-studies.html"><span class="toc-section-number">2.6.6</span> Use Bayesian statistics with informed priors</a>
</div>
</li>
</ul>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7</span> What if best practices are not enough?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.1</span> Ask for more money in your grant proposals</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.2</span> Improve management</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.3</span> Change what is expected from PhD students</a>
<a href="2-7-what-if-best-practices-are-not-enough.html"><span class="toc-section-number">2.7.4</span> Get answers collectively</a>
</div>
</li>
</ul>
<a href="2-8-planning-for-precision.html"><span class="toc-section-number">2.8</span> Planning for precision</a>
</div>
</li>
</ul>
<a href="3-questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="3-1-do-you-really-want-to-test-a-hypothesis.html"><span class="toc-section-number">3.1</span> Do You Really Want to Test a Hypothesis?</a>
<a href="3-2-goals-of-tests.html"><span class="toc-section-number">3.2</span> Goals of tests</a>
</div>
</li>
</ul>
<a href="4-errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="4-1-why-you-dont-need-to-adjust-your-alpha-level-for-all-tests-youll-do-in-your-lifetime-.html."><span class="toc-section-number">4.1</span> Why you don’t need to adjust your alpha level for all tests you’ll do in your lifetime.</a>
<a href="4-2-why-banning-p-values-might-not-solve-our-problems-.html."><span class="toc-section-number">4.2</span> Why banning p-values might not solve our problems.</a>
<a href="4-3-error-control-in-exploratory-anovas-the-how-and-the-why.html"><span class="toc-section-number">4.3</span> Error Control in Exploratory ANOVA’s: The How and the Why</a>
</div>
</li>
</ul>
<a href="5-effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-1-effect-sizes.html"><span class="toc-section-number">5.1</span> Effect sizes</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-1-effect-sizes.html"><span class="toc-section-number">5.1.1</span> The Facebook experiment</a>
</div>
</li>
</ul>
<a href="5-2-cohend.html"><span class="toc-section-number">5.2</span> Cohen’s <em>d</em></a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-2-cohend.html"><span class="toc-section-number">5.2.1</span> Correcting for Bias</a>
</div>
</li>
</ul>
<a href="5-3-r-correlations.html"><span class="toc-section-number">5.3</span> <em>r</em> (correlations)</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4</span> Confidence Intervals</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-4-confint.html.-samples"><span class="toc-section-number">5.4.1</span> Population vs. Samples</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.2</span> The relation between confidence intervals and <em>p</em>-values</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.3</span> The Standard Error and 95% Confidence Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.4</span> Overlapping Confidence Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.5</span> Prediction Intervals</a>
<a href="5-4-confint.html"><span class="toc-section-number">5.4.6</span> Capture Percentages</a>
</div>
</li>
</ul>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5</span> Computing Confidence Intervals around Effect Sizes</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.1</span> MOTE</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.2</span> JASP</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.3</span> ESCI software</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.4</span> MBESS</a>
<a href="5-5-computing-confidence-intervals-around-effect-sizes.html"><span class="toc-section-number">5.5.5</span> Why should you report 90% CI for eta-squared?</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="6-equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="6-1-absence-of-evidence-is-not-evidence-of-absence-.html."><span class="toc-section-number">6.1</span> Absence of evidence is not evidence of absence.</a>
<a href="6-2-justifysesoi.html"><span class="toc-section-number">6.2</span> JUstifying a smallest effect size of interest</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="6-2-justifysesoi.html"><span class="toc-section-number">6.2.1</span> Rejecting the presence of a meaningful effect</a>
</div>
</li>
</ul>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html.">Bayesian estimation using ROPE and equivalence tests.</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.2</span> 95% HDI vs 90% CI</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.3</span> Power analysis for Equivalence Tests</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.4</span> Use of prior information</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">6.2.5</span> Conclusion</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="7-severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1</span> Testing Range Predictions</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.1</span> Risky Predictions</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.2</span> Systematic Noise</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.3</span> Range Predictions</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.4</span> Directional Tests</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.5</span> Minimal Effect Tests</a>
<a href="7-1-testing-range-predictions.html"><span class="toc-section-number">7.1.6</span> Range Predictions in PRactice</a>
</div>
</li>
</ul>
<a id="active-page" href="7-2-verisimilitude-belief-and-progress-in-psychological-science.html"><span class="toc-section-number">7.2</span> Verisimilitude, Belief, and Progress in Psychological Science</a><ul class="toc-sections">
<li class="toc"><a href="#verisimilitude-belief-and-progress-in-psychological-science"> Verisimilitude, Belief, and Progress in Psychological Science</a></li>
</ul>
</div>
</li>
</ul>
<a href="8-sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-1-what-would-falsify-your-theory.html"><span class="toc-section-number">8.1</span> What would falsify your theory?</a>
<a href="8-2-what-would-falsify-your-theory-in-practice.html"><span class="toc-section-number">8.2</span> What would falsify your theory in practice?</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3</span> Specifying a SESOI based on theory or costs and benefits</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.1</span> Example of a theoretically predicted SESOI</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.2</span> Anchor based methods to set a SESOI</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.3</span> Cost benefit analysis</a>
<a href="8-3-specifying-a-sesoi-based-on-theory-or-costs-and-benefits.html"><span class="toc-section-number">8.3.4</span> Setting the SESOI based on effects feasible to study</a>
</div>
</li>
</ul>
<a href="8-4-smalltelescopes.html"><span class="toc-section-number">8.4</span> The small telescopes approach</a>
<a href="8-5-setting-the-sesoi-based-on-resources-.html."><span class="toc-section-number">8.5</span> Setting the SESOI based on resources.</a>
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6</span> Setting the smallest effect size of interest in replication studies</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6.1</span> Setting the SESOI based on theoretical predictions</a>
<a href="8-6-setting-the-smallest-effect-size-of-interest-in-replication-studies.html"><span class="toc-section-number">8.6.2</span> Setting the smallest effect size of interest based on resources</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="9-meta.html"><span class="toc-section-number">9</span> Meta-analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1</span> Random Variation</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1.1</span> Variation in single samples</a>
<a href="9-1-random-variation.html."><span class="toc-section-number">9.1.2</span> Variance in two groups, and their difference.</a>
<a href="9-1-random-variation.html"><span class="toc-section-number">9.1.3</span> Correlations between two groups</a>
<a href="9-1-random-variation.html."><span class="toc-section-number">9.1.4</span> Confidence Intervals around Standard Deviations.</a>
</div>
</li>
</ul>
<a href="9-2-mixed.html"><span class="toc-section-number">9.2</span> Mixed Results</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-2-mixed.html"><span class="toc-section-number">9.2.1</span> Likelihoods of sets of studies</a>
</div>
</li>
</ul>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3</span> Introduction to Meta-Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.1</span> Single study meta-analysis</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.2</span> Simulating meta-analyses of mean standardized differences</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.3</span> Fixed Effect vs Random Effects</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.4</span> Simulating meta-analyses for dichotomous outcomes</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.5</span> Heterogeneity</a>
<a href="9-3-introduction-to-meta-analysis.html"><span class="toc-section-number">9.3.6</span> Improving the reproducibility of meta-analyses</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="10-bias.html"><span class="toc-section-number">10</span> Bias detection</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1</span> Bias Detection</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.1</span> Funnel Plots</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.2</span> Trim and Fill</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.3</span> PET-PEESE</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.4</span> P-curve Analysis</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.5</span> TIVA</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.6</span> Let’s Detect Some Bias!</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.7</span> Introducing bias</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.8</span> Bias detection techniques</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.9</span> TIVA</a>
<a href="10-1-bias-detection.html"><span class="toc-section-number">10.1.10</span> Z-curve analysis</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">10.1.11</span> Conclusion</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="11-computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-1-step-1-setting-up-a-github-repository.html"><span class="toc-section-number">11.1</span> Step 1: Setting up a GitHub repository</a>
<a href="11-2-step-2-cloning-your-github-repository-into-rstudio.html"><span class="toc-section-number">11.2</span> Step 2: Cloning your GitHub repository into RStudio</a>
<a href="11-3-step-3-creating-an-r-markdown-file.html"><span class="toc-section-number">11.3</span> Step 3: Creating an R Markdown file</a>
<a href="11-4-step-4-reproducible-data-analysis-in-r-studio.html"><span class="toc-section-number">11.4</span> Step 4: Reproducible Data Analysis in R Studio</a>
<a href="11-5-step-5-committing-and-pushing-to-github.html"><span class="toc-section-number">11.5</span> Step 5: Committing and Pushing to GitHub</a>
<a href="11-6-step-6-reproducible-data-analysis.html"><span class="toc-section-number">11.6</span> Step 6: Reproducible Data Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-6-step-6-reproducible-data-analysis.html"><span class="toc-section-number">11.6.1</span> Extra: APA formatted manuscripts in papaja</a>
</div>
</li>
</ul>
<a href="11-7-step-7-organizing-your-data-and-code.html"><span class="toc-section-number">11.7</span> Step 7: Organizing Your Data and Code</a>
<a href="11-8-step-8-archiving-your-data-and-code.html"><span class="toc-section-number">11.8</span> Step 8: Archiving Your Data and Code</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="11-8-step-8-archiving-your-data-and-code.html"><span class="toc-section-number">11.8.1</span> EXTRA: Sharing Reproducible Code on Code Ocean</a>
</div>
</li>
</ul>
<a href="11-9-some-points-for-improvement-in-computational-reproducibility.html"><span class="toc-section-number">11.9</span> Some points for improvement in computational reproducibility</a>
<a href="bayesian-estimation-using-rope-and-equivalence-tests-.html"><span class="toc-section-number">11.10</span> Conclusion</a>
</div>
</li>
</ul>
<a href="12-prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="12-1-trust-in-scientists.html"><span class="toc-section-number">12.1</span> Trust in scientists</a>
<a href="12-2-the-value-of-preregistration.html"><span class="toc-section-number">12.2</span> The value of preregistration</a>
<a href="12-3-registered-reports.html"><span class="toc-section-number">12.3</span> Registered Reports</a>
<a href="12-4-preregister-your-study.html"><span class="toc-section-number">12.4</span> Preregister your study?</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="12-4-preregister-your-study.html"><span class="toc-section-number">12.4.1</span> How to preregister</a>
</div>
</li>
</ul>
<a href="12-5-what-does-a-formalized-test-of-a-prediction-look-like.html"><span class="toc-section-number">12.5</span> What Does a Formalized Test of a Prediction Look Like?</a>
<a href="12-6-are-you-ready-to-preregister-a-hypothesis-test.html"><span class="toc-section-number">12.6</span> Are you ready to preregister a hypothesis test?</a>
</div>
</li>
</ul>
<a href="13-bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="13-1-likelihoods.html"><span class="toc-section-number">13.1</span> Likelihoods</a>
<a href="13-2-bayes-factors.html"><span class="toc-section-number">13.2</span> Bayes factors</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="13-2-bayes-factors.html"><span class="toc-section-number">13.2.1</span> Updating our belief</a>
</div>
</li>
</ul>
<a href="13-3-bayesest.html"><span class="toc-section-number">13.3</span> Bayesian Estimation</a>
</div>
</li>
</ul>
<a href="14-sequential.html"><span class="toc-section-number">14</span> Sequential Analysis</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-1-choosing-alpha-levels-for-sequential-analyses-.html."><span class="toc-section-number">14.1</span> Choosing alpha levels for sequential analyses.</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-1-choosing-alpha-levels-for-sequential-analyses-.html"><span class="toc-section-number">14.1.1</span> Pocock correction</a>
</div>
</li>
</ul>
<a href="14-2-comparing-spending-functions.html"><span class="toc-section-number">14.2</span> Comparing Spending Functions</a>
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3</span> Sample Size for Sequential Designs</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3.1</span> Alpha spending functions</a>
<a href="14-3-sample-size-for-sequential-designs.html"><span class="toc-section-number">14.3.2</span> Updating Boundaries During an Experiment</a>
</div>
</li>
</ul>
<a href="14-4-test-for-non-inferiority.html"><span class="toc-section-number">14.4</span> Test for (non-)inferiority</a>
<a href="14-5-stopping-for-futility.html"><span class="toc-section-number">14.5</span> Stopping for futility</a><ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="14-5-stopping-for-futility.html."><span class="toc-section-number">14.5.1</span> Sequential analyses using Bayes factors.</a>
<a href="14-5-stopping-for-futility.html."><span class="toc-section-number">14.5.2</span> Reporting results after a sequential analysis.</a>
</div>
</li>
</ul>
</div>
</li>
</ul>
<a href="15-references.html"><span class="toc-section-number">15</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<div id="verisimilitude-belief-and-progress-in-psychological-science" class="section level2">
<h2>
<span class="header-section-number">7.2</span> Verisimilitude, Belief, and Progress in Psychological Science</h2>
<p>Does science offer a way to learn what is true about our world? According to the perspective in philosophy of science known as <em>scientific realism</em>, the answer is ‘yes’. Scientific realism is the idea that successful scientific theories that have made novel predictions give us a good reason to believe these theories make statements about the world that are at least partially true. Known as the <em>no miracle argument</em>, only realism can explain the success of science, which consists of repeatedly making successful predictions (Duhem, 1906), without requiring us to believe in miracles.</p>
<p>Not everyone thinks that it matters whether scientific theories make true statements about the world, as scientific realists do. Laudan (1981) argues against scientific realism based on a pessimistic meta-induction: If theories that were deemed successful in the past turn out to be false, then we can reasonably expect all our current successful theories to be false as well. Van Fraassen (1980) believes it is sufficient for a theory to be ‘empirically adequate’, and make true predictions about things we can observe, irrespective of whether these predictions are derived from a theory that describes how the
unobservable world is in reality. This viewpoint is known as <em>constructive empiricism</em>. As Van Fraassen summarizes the constructive empiricist perspective (1980, p.12): “Science aims to give us theories which are empirically adequate; and acceptance of a theory involves as belief only that it is empirically
adequate”.</p>
<p>The idea that we should ‘believe’ scientific hypotheses is not something scientific realists can get behind. Either they think theories make true statements about things in the world, but we will have to remain completely agnostic about when they do (Feyerabend, 1993), or they think that corroborating novel and risky predictions makes it reasonable to believe that a theory has some ‘truth-likeness’, or <em>verisimilitude</em>. The concept of verisimilitude is based on the intuition that a theory is closer to a true statement when the
theory allows us to make more true predictions, and less false predictions. When data is in line with predictions, a theory gains verisimilitude, when data are not in line with predictions, a theory loses verisimilitude (Meehl, 1978). Popper clearly intended verisimilitude to be different from belief (Niiniluoto, 1998). Importantly, verisimilitude refers to how close a theory is to the truth, which makes it an ontological, not epistemological question. That is, verisimilitude is a function of the degree to which a theory is similar to the truth, but it is not a function of the degree of belief in, or the evidence for,
a theory (Meehl, 1978, 1990). It is also not necessary for a scientific realist that we ever know what is true – we just need to be of the opinion that we can move closer to the truth (known as comparative scientific realism, Kuipers, 2016).</p>
<p>Attempts to formalize verisimilitude have been a challenge, and from the perspective of an empirical scientist, the abstract nature of this ongoing discussion does not really make me optimistic it will be extremely useful in everyday practice. On a more intuitive level, verisimilitude can be regarded as the extent to which a theory makes the most correct (and least incorrect) statements about specific features in the world. One way to think about this is using the ‘possible worlds’ approach (Niiniluoto, 1999), where for each basic state of the world one can predict, there is a possible world that contains each unique combination of states.</p>
<p>For example, consider the experiments by Stroop (1935), where color related words (e.g., RED, BLUE) are printed either in congruent colors (i.e., the word RED in red ink) or incongruent colors (i.e., the word RED in blue ink). We might have a very simple theory predicting that people automatically process irrelevant information in a task. When we do two versions of a Stroop experiment, one where people are asked to read the words, and one where people are asked to name the colors, this simple theory would predict slower responses on incongruent trials, compared to congruent trials. A slightly more advanced theory predicts that congruency effects are dependent upon the salience of the word dimension and color dimension (Melara &amp; Algom, 2003). Because in the standard Stroop experiment the <em>word</em> dimension is much more salient in both
tasks than the <em>color</em> dimension, this theory predicts slower responses on incongruent trials, but only in the color naming condition. We have four possible worlds, two of which represent predictions from either of the two theories, and two that are not in line with either theory.</p>
<table>
<thead><tr class="header">
<th></th>
<th>Responses Color Naming</th>
<th>Responses Word Naming</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>World 1</td>
<td>Slower</td>
<td>Slower</td>
</tr>
<tr class="even">
<td>World 2</td>
<td>Slower</td>
<td>Not Slower</td>
</tr>
<tr class="odd">
<td>World 3</td>
<td>Not Slower</td>
<td>Slower</td>
</tr>
<tr class="even">
<td>World 4</td>
<td>Not Slower</td>
<td>Not Slower</td>
</tr>
</tbody>
</table>
<p>In an unpublished working paper, Meehl (1990b) discusses a ‘box score’ of the number of successfully predicted features, which he acknowledges is too simplistic. No widely accepted formalized measure of verisimilitude is available to express the similarity between the successfully predicted features by a
theory, although several proposals have been put forward (Niiniluoto, 1998; Oddie, 2013, for an example based on Tversky’s (1977) contrast model, see Cevolani, Crupi, &amp; Festa, 2011). However, even if formal measures of verisimilitude are not available, it remains a useful concept to describe theories that are assumed to be closer to the truth because they make novel predictions (Psillos, 1999).</p>
<p>As empirical scientists, our main job is to decide which features are present in our world. Therefore, we need to know if predictions made by theories are corroborated or falsified in experiments. To be able to falsify a theory, it needs to forbid certain states of the world (Lakatos, 1978). This is not easy, especially for probabilistic statements, which is the bread and butter of psychological science. Where a single black swan is clearly observable, probabilistic statements only reach their true predicted value in infinity, and every finite sample will have some variation around the predicted value.
However, according to Popper, probabilistic statements can be <em>made</em> falsifiable by interpreting probability as the relative frequency of a result in a specified hypothetical series of observations, and decide that reproducible regularities are not attributed to randomness (Popper, 2002). Even though any finite sample
will show some variation, we can decide upon a limit of the variation. Researchers can use the limit of variation that is allowed as a <em>methodological rule</em>, and decide whether a set of observations falls in a ‘forbidden’ state of the world, or in a ‘permitted’ state of the world, according to some theoretical
prediction.</p>
<p>This <em>methodological falsification</em> (Lakatos, 1978) is clearly inspired by a Neyman-Pearson perspective on statistical inferences. Popper (2002, p. 168) acknowledges feedback from the statistician Abraham Wald, who developed statistical decision theory based on the work by Neyman and Pearson (Wald, 1992). Lakatos (1978, p. 25) writes how we can make predictions falsifiable by “specifying certain rejection rules which may render statistically interpreted evidence ‘inconsistent’ with the probabilistic theory” and notes: “this
methodological falsificationism is the philosophical basis of some of the most interesting developments in modern statistics. The Neyman-Pearson approach rests completely on methodological falsificationism”. To use methodological falsification, Popper describes how empirical researchers need to decide upon an interval within which the predicted value will fall. We can then calculate for any number of observations the probability that our value will indeed fall within this range, and design a study such that this probability is very high, or that it’s complementary probability, which Popper denotes by ε, is small. We can recognize this procedure as a Neyman-Pearson hypothesis test, where ε is the Type 2 error rate. In other words, high statistical power, or when the null is true, a very low alpha level, can corroborate a hypothesis.</p>
<p>Popper distinguishes between subjective probabilities (where the degree of probability is expressed as feelings of certainty, or, belief), and objective probabilities (where probabilities are relative frequencies with which an event occurs in a specified range of observations. Popper strongly believed that the corroboration of tests was based on Frequentist, not Bayesian, probabilities (Popper, p. 434): “As to degree of corroboration, it is nothing but a measure of the degree to which a hypothesis h has been tested, and of the degree to which it has stood up to tests. It must not be interpreted, therefore, as a degree of
the rationality of our belief in the truth of h [the hypothesis]”. For a scientific realist, who believes the main goal of scientists is to identify features of the world that corroborate or falsify theories, what matters is whether theories are truthlike, not whether you <em>believe</em> they are truthlike. As Taper and Lele (2011) express this viewpoint: “It is not that we believe that Bayes’ rule or Bayesian mathematics is flawed, but that from the axiomatic foundational definition of probability Bayesianism is doomed to answer questions irrelevant to science. We do not care what you believe, we barely care what we believe, what we are interested in is what you can show.” Indeed, if the goal is to identify the presence or absence of features in the world to develop more truth-like theories, we mainly need procedures that allow us to make choices about the presence or absence of these features with high accuracy. Subjective belief plays no role in these procedures.</p>
<p>To identify the presence or absence of features with high accuracy, we need a statistical procedure that allows us to make decisions while controlling the probability we make an error. This idea is translated into practice in hypothesis testing procedures put forward by Neyman and Pearson (1933): “We are inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis. But we may look at the purpose of tests from another view-point. Without hoping to know. whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong.” Any procedure with good error control can be used (although Popper stresses that these findings should also be replicable). Some authors prefer likelihood ratios where error rates have maximum bounds (Royall, 1997; Taper &amp; Ponciano, 2016), but in general, frequentists hypothesis tests are used where both the Type 1 error rate and the Type 2 error rate are controlled.</p>
<p>Meehl (1978) believes “the almost universal reliance on merely refuting the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, is basically unsound, poor scientific strategy, and one of the worst things that ever happened in the history of psychology”. Meehl is of this opinion, not because hypothesis tests are not useful, but because they are not used to test risky predictions. Meehl remarks that “When I was a rat psychologist, I unabashedly employed significance testing in latent-learning experiments; looking back I see no reason to fault myself for having done so in the light of my present methodological views” (Meehl, 1990a). When one theory predicts rats learn nothing, and another theory predicts rats learn <em>something</em>, even Meehl believed testing the difference between an experimental and control group was a useful test of a theoretical prediction. However, Meehl believes that many hypothesis tests are used in a way such that they actually do not increase the verisimilitude of theories are all. If you predict gender differences, you will find them more often than not in a large enough sample. Because people can not be randomly assigned to gender conditions, the null hypothesis is most likely false, not predicted by any theory, and therefore rejecting the null hypothesis does not increase the verisimilitude of any theory. But as a scientific realist, Meehl believes accepting or rejecting predictions is a sound procedure, as long as you test risky predictions in procedures with low error rates. Using such procedures, we have observed an asymmetry in the Stroop experiments, where the interference effect is much
greater in the color naming task than in the word naming task, which leads us to believe the theory that takes into account the salience of the word and color dimensions has higher truth-likeness.</p>
<p>From a scientific realist perspective, Bayes Factors or Bayesian posteriors do not provide an answer to the main question of interest, which is the verisimilitude of scientific theories. Belief can be used to decide which questions to examine, but it can not be used to determine the truth-likeness of a theory. Obviously, if you reject realism, and follow anti-realist philosophical viewpoints such as Fraassen’s constructive empiricism, then you also reject verisimilitude, or the idea that theories can be closer to an unobservable and unknowable truth. I understand most psychologists do not choose their statistical approaches to follow logically from their philosophy on science, and instead follow norms or hypes. But I think it is useful to at least reflect upon basic questions. What is the goal of science? Can we approach the truth, or can we only <em>believe</em> in hypotheses? There should be some correspondence between your choice of statistical inferences, and your philosophy of science.</p>
<p>References</p>
<p>Cevolani, G., Crupi, V., &amp; Festa, R. (2011). Verisimilitude and belief change
for conjunctive theories. <em>Erkenntnis</em>, <em>75</em>(2), 183.</p>
<p>Feyerabend, P. (1993). <em>Against method</em> (3rd ed). London ; New York: Verso.</p>
<p>Kuipers, T. A. F. (2016). Models, postulates, and generalized nomic truth
approximation. <em>Synthese</em>, <em>193</em>(10), 3057–3077.
<a href="https://doi.org/10.1007/s11229-015-0916-9" class="uri">https://doi.org/10.1007/s11229-015-0916-9</a></p>
<p>Lakatos, I. (1978). <em>The methodology of scientific research programmes: Volume
1: Philosophical papers</em> (Vol. 1). Cambridge University Press.</p>
<p>Laudan, L. (1981). A confutation of convergent realism. <em>Philosophy of Science</em>,
<em>48</em>(1), 19–49.</p>
<p>Meehl, P. E. (1978). Theoretical Risks and Tabular Asterisks: Sir Karl, Sir
Ronald, and the Slow Progress of Soft Psychology. <em>Journal of Consulting and
Clinical Psychology</em>, <em>46</em>, 806–834.</p>
<p>Meehl, P. E. (1990a). Appraising and amending theories: The strategy of
Lakatosian defense and two principles that warrant it. <em>Psychological Inquiry</em>,
<em>1</em>(2), 108–141.</p>
<p>Meehl, P. E. (1990b). <em>Corroboration and verisimilitude: Against Lakatos’ “sheer
leap of faith.”</em> Working Paper, MCPS-90-01). Minneapolis: University of
Minnesota, Center for Philosophy of Science. Retrieved from
<a href="http://meehl.umn.edu/sites/g/files/pua1696/f/146corroborationverisimilitude.pdf" class="uri">http://meehl.umn.edu/sites/g/files/pua1696/f/146corroborationverisimilitude.pdf</a></p>
<p>Melara, R. D., &amp; Algom, D. (2003). Driven by information: A tectonic theory of
Stroop effects. <em>Psychological Review</em>, <em>110</em>(3), 422–471.
<a href="https://doi.org/10.1037/0033-295X.110.3.422" class="uri">https://doi.org/10.1037/0033-295X.110.3.422</a></p>
<p>Neyman, J., &amp; Pearson, E. S. (1933). On the Problem of the Most Efficient Tests
of Statistical Hypotheses. <em>Philosophical Transactions of the Royal Society of
London A: Mathematical, Physical and Engineering Sciences</em>, <em>231</em>(694–706),
289–337. <a href="https://doi.org/10.1098/rsta.1933.0009" class="uri">https://doi.org/10.1098/rsta.1933.0009</a></p>
<p>Niiniluoto, I. (1998). Verisimilitude: The Third Period. <em>The British Journal
for the Philosophy of Science</em>, <em>49</em>, 1–29.</p>
<p>Niiniluoto, I. (1999). <em>Critical Scientific Realism</em>. Oxford University Press.</p>
<p>Oddie, G. (2013). The content, consequence and likeness approaches to
verisimilitude: compatibility, trivialization, and underdetermination.
<em>Synthese</em>, <em>190</em>(9), 1647–1687. <a href="https://doi.org/10.1007/s11229-011-9930-8" class="uri">https://doi.org/10.1007/s11229-011-9930-8</a></p>
<p>Popper, K. R. (2002). <em>The logic of scientific discovery</em>. London; New York:
Routledge.</p>
<p>Psillos, S. (1999). <em>Scientific realism: how science tracks truth</em>. London; New
York: Routledge.</p>
<p>Royall, R. (1997). <em>Statistical Evidence: A Likelihood Paradigm</em>. London ; New
York: Chapman and Hall/CRC.</p>
<p>Stroop, J. R. (1935). Studies of interference in serial verbal reactions.
<em>Journal of Experimental Psychology</em>, <em>18</em>(6), 643.</p>
<p>Taper, M. L., &amp; Lele, S. R. (2011). Philosophy of Statistics. In P. S.
Bandyophadhyay &amp; M. R. Forster (Eds.), <em>Evidence, evidence functions, and error
probabilities</em> (pp. 513–531). Elsevier, USA.</p>
<p>Taper, M. L., &amp; Ponciano, J. M. (2016). Evidential statistics as a statistical
modern synthesis to support 21st century science. <em>Population Ecology</em>, <em>58</em>(1),
9–29.</p>
<p>Tversky, A. (1977). Features of similarity. <em>Psychological Review</em>, <em>84</em>(4),
327.</p>
<p>Van Fraassen, B. C. (1980). <em>The scientific image</em>. Oxford : New York: Clarendon
Press ; Oxford University Press.</p>
<p>Wald, A. (1992). Statistical Decision Functions. In S. Kotz &amp; N. L. Johnson
(Eds.), <em>Breakthroughs in Statistics</em> (pp. 342–357). Springer New York.
<a href="https://doi.org/10.1007/978-1-4612-0919-5_22" class="uri">https://doi.org/10.1007/978-1-4612-0919-5_22</a></p>

</div>
<!-- </div> -->
</body></html>

<p style="text-align: center;">
<a href="7-1-testing-range-predictions.html"><button class="btn btn-default">Previous</button></a>
<a href="8-sesoi.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-15
</p>
</div>
</div>



</body>
</html>
