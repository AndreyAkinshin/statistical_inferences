[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"open educational resource integrates information blog, MOOCs Improving Statistical Inferences Improving Statistical Questions, scientific work one place. goal make information findable, accessible, make easier update material based recent scientific developments new statistical software.used right re-use adapt open access articles, without adding quotation marks citing . work shared Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. Thanks collaborators Casper Albers, Farid Anvari, Aaron Caldwell, Harlan Cambell, Nicholas Coles, Lisa DeBruine, Marie Delacre, Zoltan Dienes, Noah van Dongen, Alexander Etz, Ellen Evers, Jaroslav Gottfriend, Seth Green, Christopher Harms, Arianne Herrera-Bennett, Joe Hilgard, Peder Isager, Maximilian Maier, Neil McLatchie, Brian Nosek, Pepijn Obels, Amy Orben, Anne Scheel, Janneke Staaks, Leo Tiokhin, Mehmet Tunç, Duygu Uygun Tunç, contributed work done past part formed basis material.find mistakes, suggestions improvement, can submit issue GitHub page open educational resource. hope material use .Dr. Daniël Lakens","code":""},{"path":"pvalue.html","id":"pvalue","chapter":"1 Using p-values to test a hypothesis","heading":"1 Using p-values to test a hypothesis","text":"One question interests scientists whether differences exist measurements collected different conditions. answer question ordinal claim. example, researcher might hypothesize time students spend learning new information dedicated retrieval information tests (condition ), compared spending time studying (condition B). collecting data, observing mean grade higher students student spend part time tests, researcher can make ordinal claim student performence better condition condition B. Ordinal claims quantify size effect. Ordinal claims can used state difference conditions. non-directional (two-sided) test question whether difference either direction, directional (one-sided) test question whether effect specific direction, cases alternative difference.make ordinal claims, researchers typically rely methodological procedure known hypothesis test. One part hypothesis test consists computing p-value examining whether statistically significant difference. 'Significant' means something worthy attention. hypothesis test used distinguish signal (worth paying attention ) random noise empirical data. worth distinguishing statistical significance, used claim whether observed effect signal noise, practical significance, depends whether size effect large enough worthwhile consequences real life. use methodological procedure decide whether ordinal claim can made functions safeguard confirmation bias. Depending desires, scientists might tempted interpret data support hypothesis, even . hypothesis test, used correctly, controls amount time researchers fool make ordinal claims.","code":""},{"path":"pvalue.html","id":"philosophical-approaches-to-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.1 Philosophical approaches to p-values","text":"look p-values computed, important examine supposed help us make ordinal claims testing hypotheses. definition p-value probability observing sample data, extreme data, assuming null hypothesis true. definition tell us much interpret p-value.interpretation p-value depends statistical philosophy one subscribes . Ronald Fisher published 'Statistical Methods Research Workers' 1925 popularized p-values. Fisherian framework p-value interpreted descriptive continuous measure compatibility observed data null-hypothesis (Greenland et al., 2016). compatibility observed data null model falls 1 (perfectly compatible) 0 (extremely incompatible), every individual can interpret p-value “statistical thoughtfulness\". According Fisher (1956), p-values \"generally lead probability statement real world, rational well-defined measure reluctance accept hypotheses test\". Fisher tried formalize philosophy approach called 'fiducial inference', recevied widespread adoption approaches, decision theory, likelihoods, Bayesian inference. Indeed, Zabell (Zabell, 1992) writes \"fiducial argument stands Fisher's one great failure\", although others expressed hope might developed useful approach future (Schweder & Hjort, 2016). Fisherian p-value describes incompatibility data single hypothesis, known significance testsing. main reason significance test limited, researchers specify null-hypothesis, specify alternative hypothesis.Neyman Pearson built insights p-values William Gosset (inventor Student's t-test) Ronald Fisher, developed approached called statistical hypothesis testing. main difference significance testing approach developed Fisher statistical hypothesis test null-hypothesis alternative hypothesis specified. Neyman-Pearson framework goal statistical tests guide behavior researchers respect two hypotheses. Based results statistical test, without ever knowing whether hypothesis true , researchers choose tentatively act null hypothesis alternative hypothesis true. psychology, researchers often use imperfect hybrid Fisherian Neyman-Pearson frameworks, Neyman-Pearson approach , according Dienes Dienes (2008) “logic underlying statistics see professional journals psychology”.Neyman-Pearson hypothesis test performed observed p-value used check smaller chosen alpha level, matter much smaller . example, alpha level 0.01 used, p = 0.006 p = 0.000001 lead researchers decide act state world best described alternative hypothesis. differs Fisherian approach p-values, lower p-value, greater psychological reluctance researcher accept null-hypothesis testing. Neyman-Pearson hypothesis test see goal inference quantifying continuous measure compatibility evidence. Instead, Neyman (1957) writes:content concept inductive behavior recognition purpose every piece serious research provide grounds selection one several contemplated courses action.Intuitively, one might feel like decisions act based results single statistical test, point often raised criticism Neyman-Pearson approach statistical inferences. However, criticisms rarely use definition ‘act’ Neyman used. true , example, decision implement new government policy based single study result. However, Neyman considered making scientific claim ‘act’ well, wrote (1957, p. 10) concluding phase study involves:act decision take particular action, perhaps assume particular attitude towards various sets hypothesesCox (1958) writes:might argued making inference 'deciding' make statement certain type populations therefore, provided word decision interpreted narrowly, study statistical decisions embraces inference. point one main general problems statistical inference consists deciding types statement can usefully made exactly mean.Thus, Neyman-Pearson approach, p-values form basis decisions claims make. science, claims underly novel experiments form auxiliary hypotheses, assumptions underlying hypotheses assumed accurate order test work planned. example, important participants can see color planned experiment, assume true Ishihara test successfully identifies participants colorblind.","code":""},{"path":"pvalue.html","id":"creating-a-null-model","chapter":"1 Using p-values to test a hypothesis","heading":"1.2 Creating a null model","text":"\nFigure 1.1: Scientists tendency worship p-values value 0.05.\nAssume ask two groups 10 people much liked extended directors cut Lord Rings (LOTR) trilogy. means total sample size (N) 20, sample size group (n) 10. first group consists friends, second groups consists friends wife. friends rate trilogy score 1 10. can calculate average rating friends, 8.7, average rating wife’s friends, 7.7. can compare scores groups looking raw data, plotting data.\nTable 1.1: Ratings Lord Rings extended trilogy two groups friends.\ncan see groups overlap mean ratings differ 1 whole point. question faced following: difference two groups just random variation, can claim friends like extended directors cut Lord Rings (LOTR) trilogy wife’s friends?null-hypothesis significance test try answer question calculating probability observed difference (case, mean difference 1) extreme difference, assumption real difference much friends wife’s friends like extended directors cut LOTR, just looking random noise. probability called p-value. probability low enough, decide claim difference. probability low enough, refrain making claim difference.null-hypothesis assumes ask infinite number friends infinite number wife’s friends much like LOTR, difference huge groups exactly 0. However, sample drawn draw population, random variation likely lead difference somewhat larger smaller 0. can create null model quantifies expected variation observed data, just due random noise, tell us constitutes reasonable expectation much differences groups can vary difference population.practical create null model terms standardized distribution, makes easier calculate probability specific values occur, regardless scale used collect measurements. One version null model differences t-distribution, can used describe differences expected drawing samples population. null model built assumptions. case t-distribution, assumption scores normally distributed. reality, assumptions upon statistical methods built never met perfectly, statisticians examine impact violations assumptions methodological procedures. Statistical tests still useful practice impact violations statistical inferences small enough.can quantify distribution t-values expected difference population probability density function. plot probability density function t-distribution 18 degrees freedom (df), corresponds example collect data 20 friends (df = N - 2 two independent groups). continuous distribution, probabilities defined infinite number points, probability observing single point (e.g., t = 2.5) always zero. Probabilities measured intervals. reason, p-value computed, defined 'probability observing data', 'probability observed data, extreme data'. creates interval (tail distribution) probability can calculated.","code":""},{"path":"pvalue.html","id":"calculating-a-p-value","chapter":"1 Using p-values to test a hypothesis","heading":"1.3 Calculating a p-value","text":"t-value can computed mean sample, mean population, standard deviation sample, sample size. computing probability observing t-value extreme extreme one observed, get p-value. comparison movie ratings two groups friends , performing two-sided Student's t-test yields t-value 2.5175 p-value 0.02151.can graph t-distribution (df = 18) highlight two tail areas start t-values 2.5175 -2.5175.\nFigure 1.2: t-distribution 18 degrees freedom.\n","code":"\nt.test(df_long$rating ~ df_long$`Friend Group`, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  df_long$rating by df_long$`Friend Group`\n## t = 2.5175, df = 18, p-value = 0.02151\n## alternative hypothesis: true difference in means between group Friends Daniel and group Friends Kyra is not equal to 0\n## 95 percent confidence interval:\n##  0.1654875 1.8345125\n## sample estimates:\n## mean in group Friends Daniel   mean in group Friends Kyra \n##                          8.7                          7.7"},{"path":"pvalue.html","id":"which-p-values-can-you-expect","chapter":"1 Using p-values to test a hypothesis","heading":"1.4 Which p-values can you expect?","text":"educational video 'Dance p-values', Geoff Cumming explains p-values vary experiment experiment. However, reason 'trust p' mentions video. Instead, important clearly understand p-value distributions prevent misconceptions. p-values part frequentist statistics, need examine can expect long run. never experiment hundreds times, limited number studies lifetime, best way learn expect long run computer simulations.Take moment try answer following two questions. p-values can expect observe true effect, repeat study one-hundred thousand times? p-values can expect true effect, repeat study one-hundred thousand times? know answer, worry - learn now. know answer, worth reflecting know answer essential aspect p-values. like , simply never taught . see, essential solid understanding interpret p-values.p-values can expect completely determined statistical power study, probability observe significant effect, true effect. statistical power ranges 0 1. can illustrate simulating one-sample t-tests. idea simulate IQ scores group people. know standard deviation IQ scores 15. now, set mean IQ score simulated group 106, compare average IQ score people (known 100 – ’s IQ tests normalized). testing people simulated sample IQ differs average (know correct answer ‘yes’, made simulation).simulation, generate n = 71 normally distributed IQ scores mean M (106 default) standard deviation 15. perform one-sample t-test store p-value. plot distribution \nFigure 1.3: Distribution p-values power = 50%.\nx-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power.simulation result illustrates probability density function p-values. probability density function provides probability random variable specific value (Figure 1.2 t-distribution). p-value random variable, can use probability density function plot p-value distribution (Hung et al., 1997; Ulrich & Miller, 2018), Figure 1.4. can vary sample size, effect size, alpha level online Shiny app. Increasing sample size effect size increase steepness p-value distribution, means probability observe small p-values increases. p-value distribution function statistical power test.\nFigure 1.4: Probability density function p-values two-sided t-test.\ntrue effect, p-values uniformly distributed. means every p-value equally likely observed null hypothesis true. words, true effect, p-value 0.08 just likely p-value 0.98. remember thinking counterintuitive first learned (well completing PhD), makes sense think goal guarantee H0 true, alpha % p-values fall alpha level. set alpha 0.01, 1% observed p-values fall 0.01, set alpha 0.12, 12% observed p-values fall 0.12. can happen p-values uniformly distributed null-hypothesis true.\nFigure 1.5: Distribution p-values power = 50%.\n","code":"\np <- numeric(100000) # store all simulated *p*-values\n\nfor (i in 1:100000) { # for each simulated experiment\n  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data\n  y <- rnorm(n = 71, mean = 105, sd = 15) # Simulate data\n  p[i] <- t.test(x, y)$p.value # store the *p*-value\n}\n\n(sum(p < 0.05) / 100000) # compute power\nhist(p, breaks = 20) # plot a histogram"},{"path":"pvalue.html","id":"lindley","chapter":"1 Using p-values to test a hypothesis","heading":"1.5 Lindley's paradox","text":"statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. known Lindley's paradox (Lindley, 1957), sometimes Jeffreys-Lindley paradox (Spanos, 2013). distribution p-values function statistical power (Cumming, 2008), higher power, right-skewed distribution becomes (.e., likely becomes small p-values observed). true effect p-values uniformly distributed, 1% observed p-values fall 0.04 0.05. statistical power extremely high, p-values fall 0.05, p-values fall 0.01. Figure 1.6 see high power small p-values (e.g., 0.001) likely observed effect effect (e.g., dotted black curve representing 99% power falls grey horizontal line representing uniform distribution null true p-value 0.01).Yet perhaps surprisingly, observing p-value 0.04 likely null hypothesis (H0) true alternative hypothesis (H1) true high power, illustrated fact Figure 1.6 density p-value distribution higher null true, test 99% power, 0.04. Lindley's paradox shows p-value example 0.04 can statistically significant, time evidence null hypothesis. Neyman-Pearson approach made claim maximum error rate 5%, likelihood Bayesian approach, conclude data supports null. Lindley's paradox illustrates different statistical philosophies reach different conclusions, p-value can directly interpreted measure evidence, without taking power test account. Although necessary, researchers might desire prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis. can achieved lowering alpha level function sample size (Good, 1992; Leamer, 1978; Maier & Lakens, 2022), explained chapter error control.\nFigure 1.6: P-value distribution 0 (grey horizontal line, 50% power (black solid curve), 99% power (black dotted curve, p-values just 0.05 likely H0 true H1 true).\n","code":""},{"path":"pvalue.html","id":"correctly-reporting-and-interpreting-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.6 Correctly reporting and interpreting p-values","text":"Although strict Neyman-Pearson perspective sufficient report p < \\(\\alpha\\) p > \\(\\alpha\\), researchers report exact p-values. facilitates re-use results secondary analyses (Appelbaum et al., 2018), allows researchers compare p-value alpha level preferred use (Lehmann & Romano, 2005). claims made using methodological procedure known maximum error rates, p-value never allows state anything certainty. Even set alpha level 0.000001 single claim can error, Fisher (1935) reminds us, '“one chance million” undoubtedly occur, less appropriate frequency, however surprised may occur us”. uncertainty sometimes reflected academic writing, researchers can seen using words 'prove', 'show', 'known'. slightly longer accurate statement hypothesis test read:claim /effect, acknowledging scientists make claims using methodological procedure, misled, long run, alpha % beta % time, deem acceptable. foreseeable future, new data information emerges proves us wrong, assume claim correct.Remember Neyman-Pearson framework researchers make claims, necessarily believe truth claims. example, OPERA collaboration reported 2011 observed neutrinos traveled faster speed light. claim made 0.2---million Type 1 error rate, assuming error purely due random noise. However, none researchers actually believed claim true, theoretically impossible neutrinos move faster speed light. Indeed, later confirmed equipment failures cause anomalous data: fiber optic cable attached improperly, clock oscillator ticking fast. Nevertheless, claim made explicit invitation scientific community provide new data information prove claim wrong.researchers “accept” “reject” hypothesis Neyman-Pearson approach statistical inferences, communicate belief conclusion substantive hypothesis. Instead, utter Popperian basic statement based prespecified decision rule observed data reflect certain state world. Basic statements describe observation made (e.g., \"observed black swan\") event occurred (e.g., \"students performed better exam trained spaced practice, \").claim data observed, theory used make predictions. Data never 'proves' theory true false. basic statement can corroborate prediction derived theory, . many predictions deduced theory corroborated, can become increasingly convinced theory close truth. 'truth-likeness' theories called verisimilitude. shorter statement hypothesis test presented therefore read 'p = .xx, corroborates prediction, alpha level y%', 'p = .xx, corroborate prediction, statistical power y% effect size interest'. Often, alpha level statistical power mentioned experimental design section article, repeating might remind readers error rates associated claims.Even made correct claims, underlying theory can false. Popper (2002, p. 94) reminds us “empirical basis objective science thus nothing ‘absolute’ basis ”. argues science built solid bedrock, piles driven swamp notes “simply stop satisfied piles firm enough carry structure, least time .” Hacking (1965) writes: “Rejection refutation. Plenty rejections must tentative.” reject null model, tentatively, aware fact might done error, without necessarily believing null model false, without believing theory used make predictions true. Neyman (1960, p. 13) inferential behavior : “act behave future (perhaps new experiments performed) particular manner, conforming outcome experiment”. knowledge science provisional.statisticians recommend interpreting p-values measures evidence. example, Bland (2015) teaches p-values can interpreted 'rough ready' guide strength evidence, p > 0.1 indicates 'little evidence', 0.01 < p < 0.05 indicates 'evidence', p < 0.001 'strong evidence'. incorrect (Lakens, 2022), clear previous sections Lindley's paradox, uniform p-value distributions. want quantify evidence, see chapters [likelihoods][#likelihoods] [Bayesian statistics][#bayes].","code":""},{"path":"pvalue.html","id":"misconceptions","chapter":"1 Using p-values to test a hypothesis","heading":"1.7 Preventing common misconceptions about p-values","text":"p-value probability observed data, extreme data, assumption null hypothesis true. understand means, might especially useful know doesn’t mean. First, need know ‘assumption null hypothesis true’ looks like, data expect null hypothesis true. Although null hypothesis can value, assignment assume null hypothesis specified mean difference 0. example, might interested calculating difference control condition experimental condition dependent variable.useful distinguish null hypothesis (prediction mean difference population exactly 0) null model (model data expect collect data null hypothesis true). null hypothesis point 0, null model distribution. visualized textbooks power analysis software using pictures can see , t-values horizontal axis, critical t-value somewhere 1.96 – 2.00 (depending sample size). done statistical test comparing two groups based t-distribution, p-value statistically significant t-value larger critical t-value.personally find things become lot clearer plot null model mean differences instead t-values. , can see null model mean differences can expect compare two groups 50 observations true difference two groups 0, standard deviation group . standard deviation 1, can also interpret mean differences Cohen’s d effect size. also distribution can expect Cohen's d 0, collecting 50 observations per group independent t-test.\n(#fig:fig.1.3.1)Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test\nfirst thing notice expect mean null model 0. Looking x-axis, see plotted distribution centered 0. even mean difference population 0 imply every sample draw population give mean difference exactly zero. variation around population value, function standard deviation sample size.y-axis graph represents density, provides indication relative likelihood measuring particular value continuous distribution. can see likely mean difference true population value zero, larger differences zero become increasingly less likely. graph two areas colored red. areas represent 2.5% extreme values left tail distribution, 2.5% extreme values right tail distribution. Together, make 5% extreme mean differences expect observe, given number observations, true mean difference exactly 0. mean difference red area observed, corresponding statistical test statistically significant 5% alpha level. words, 5% observed mean differences far enough away 0 considered surprising. null hypothesis true, observing ‘surprising’ mean difference red areas Type 1 error.Let’s assume null model Figure true, observe mean difference 0.5 two groups. observed difference falls red area right tail distribution. means observed mean difference relatively surprising, assumption true mean difference 0. true mean difference 0, probability density functions shows expect mean difference 0.5 often. calculate p-value observation, lower 5%. probability observing mean difference least far away 0 0.5 (either left mean, right, two-tailed test) less 5%.One reason prefer plot null model raw scores instead t-values can see null model changes sample size increases. collect 5000 instead 50 observations, see null model still centered 0 – null model now expect values fall close around 0.\n(#fig:fig.1.3.2)Distribution observed Cohen's d effect sizes collecting 5000 observations per group independent t-test d = 0.\ndistribution much narrower distribution mean differences based standard error difference means. value calculated based standard deviation sample size, follows:\\(\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+\\frac{\\sigma_{2}^{2}}{n_{2}}}\\)formula shows standard deviations group (σ) squared divided sample size group, added together, square root taken. larger sample size bigger number divide , thus smaller standard error difference means. n = 50 example :\\(\\sqrt{\\frac{1^{2}}{50}+\\frac{1^{2}}{50}}\\)standard error differences means thus 0.2 n = 50 group, n = 5000 0.02. Assuming normal distribution 95% observations fall 1.96 SE. 50 samples per group, mean differences fall -1.96 * 0.2 = -0.392, +1.96 * 0.2 = 0.392, can see red areas start approximately -0.392 0.392 n = 50. 5000 samples per group, mean differences fall -1.96 * 0.02, +1.96 * 0.02; words -0.0392 0.0392 n = 5000. Due larger sample size n = 5000 observations per group, expect observe mean differences sample closer 0 compared null model 50 observations.collected n = 5000, observe mean difference 0.5, clear difference even surprising collected 50 observations. now almost ready address common misconceptions p-values, can , need introduce model data null true. sampling data model true mean difference 0, alternative model look like? software (G*power, see Figure 1.7) visualize null model (red curve) alternative model (blue curve) output:\nFigure 1.7: Screenshot Gpower software\nstudy, rarely already know true mean difference (already knew, study?). let’s assume -knowing entity. Following Paul Meehl, call -knowing entity ‘Omniscient Jones’. collect sample 50 observations, Omniscient Jones already knows true mean difference population 0.5. , expect variation around 0.5 alternative model. figure shows expected data pattern null hypothesis true (now indicated grey line) shows alternative model, assuming true mean difference 0.5 exists population (indicated black line).\n(#fig:fig.1.3.4)Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0.\nOmniscient Jones said true difference much larger. Let’s assume another study, now collect 50 observations, Omniscient Jones tells us true mean difference 1.5. null model change, alternative model now moves right.can play around alternative null models online app: http://shiny.ieis.tue.nl/d_p_power/. app allows specify sample size group independent t-test (2 infinity), mean difference (0 2), alpha level. plot, red areas visualize Type 1 errors. blue area visualizes Type 2 error rate (discuss ). app also tells critical value: vertical line (n = 50 line falls mean difference 0.4) verbal label says: “Effects larger 0.4 statistically significant”. Note true effects smaller -0.4, even though second label , app shows situation two-sided independent t-test.can see left vertical line indicates critical mean difference blue area part alternative model. Type 2 error rate (1 - power study). study 80% power, 80% mean differences observe fall right critical value indicated line. alternative model true, observe effect smaller critical value, observed p-value larger 0.05, even true effect. can check app larger sample size, right entire alternative distribution falls, thus higher power. can also see larger sample size, narrower distribution, less distribution fall critical value (long true population mean larger critical value). Finally, larger alpha level, left critical mean difference moves, smaller area alternative distribution falls critical value.app also plots 3 graphs illustrate power curves function different alpha levels, sample sizes, true mean differences. Play around app changing values. Get feel variable impacts null- alternative models, mean difference statistically significant, Type 1 Type 2 error rates.far, several aspects null models become clear. First , population value traditional null hypothesis value 0, sample draw, observed difference falls distribution centered 0, thus often slightly larger smaller 0. Second, width distribution depends sample size standard deviation. larger sample size study, narrower distribution around 0. Finally, mean difference observed falls tails null model, can considered surprising. away null-value, surprising result . null model true, surprising values happen probability specified alpha level (called Type 1 errors). Remember Type 1 error occurs researcher concludes difference population, true mean difference population zero.now finally ready address common misconceptions p-values. Let’s go list common misconceptions reported scientific literature. examples might sounds like semantics. easy first glance think statement communicates right idea, even written version formally correct. However, statement formally correct, wrong. exactly people often misunderstand p-values, worth formally correct interpreted.","code":""},{"path":"pvalue.html","id":"misconception1","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.1 Misunderstanding 1: A non-significant p-value means that the null hypothesis is true","text":"common version misconception reading sentence ‘p > 0.05 can conclude effect’. Another version sentence ‘difference, (p > 0.05)’.look misconception detail, want remind one fact easy remember, enable recognize many misconceptions p-values: p-values statement probability data, statement probability hypothesis probability theory. Whenever see p-values interpreted probability theory hypothesis, know something right. Examples statements hypothesis ‘null hypothesis true’, ‘alternative hypothesis true’, statements say probability null alternative model true 100%. subtler version statement ‘observed difference due chance’. observed difference ‘due chance’ (instead due presence real difference) null hypothesis true, , statement implies 100% probable null hypothesis true.conclude ‘effect’ ‘difference’ similarly claiming 100% probable null hypothesis true. since p-values statements probability data, refrain making statements probability theory solely based p-value. ’s ok. p-values designed help identify surprising results noisy data generation process (aka real world). designed quantify probability hypothesis true.Let’s take concrete example illustrate non-significant result mean null hypothesis true. figure , Omniscient Jones tells us true mean difference 0.5. can see , alternative distribution visualized probability mean differences expect null hypothesis true centered 0.5. observed mean difference 0.35. value extreme enough statistically different 0. can see , value fall within red area null model (hence, p-value smaller alpha level).Nevertheless, see observing mean difference 0.35 quite likely given true mean difference 0.5, observing mean difference 0.35 much likely alternative model, null model. can see comparing height density curve difference 0.35 null model, approximately 0.5, height density curve alternative model, approximately 1.5. See chapter likelihoods details.\n(#fig:fig.1.3.6)Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0 d = 0.5 observing d = 0.35.\np-value tells us mean difference 0.35 extremely surprising, assume null hypothesis true. can many reasons . real world, Omniscient Jones tell us true mean difference, possible true effect, illustrated figure .say instead? solution subtle, important. Let’s revisit two examples incorrect statements made earlier. First, ‘p > 0.05 can conclude effect’ incorrect, might well effect (remember p-values statements data, probability effect effect). Fisher’s interpretation p-value can conclude rare event happened, null hypothesis false (writes literally: “Either exceptionally rare chance occurred, theory random distribution true”). might sound like statement probability theory, really just stating two possible scenarios low p-values occur (made Type 1 error, alternative hypothesis true). remain possible, quantify probability either possible reality (e.g., saying 95% probable null hypothesis false). Neyman-Pearson perspective p > .05 means can act null hypothesis can rejected, without maintaining desired error rate 5%.interested concluding effect absent, null hypothesis testing tool use. null hypothesis test answers question ‘can reject null hypothesis desired error rate’. can , p > 0.05, conclusion can drawn based p-value (remember concept 無 ‘mu’: answer neither yes ). Luckily, statistical approaches developed ask questions absence effect equivalence testing, Bayes factors, Bayesian estimation (see Harms & Lakens, 2018, overview). assignment week 6 learn equivalence tests detail.second incorrect statement ‘difference’’. statement somewhat easier correct. can instead write ‘statistically significant difference’. Granted, bit tautological, basically saying p-value larger alpha level two different ways, least statement formally correct. difference ‘difference’ ‘statistically significant difference’ might sound like semantics, first case formally saying ‘difference 0’ second saying ‘difference large enough yield p < .05’. Although never seen anyone , informative message might ‘given sample size 50 per group, alpha level 0.05, observed differences extreme 0.4 statistically significant, observed mean difference 0.35, reject null hypothesis’. feels like unsatisfactory conclusion, remember null hypothesis test designed draw interesting conclusions absence effects – need learn equivalence tests get satisfactory answers null effects.","code":""},{"path":"pvalue.html","id":"misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.2 Misunderstanding 2: A significant p-value means that the null hypothesis is false.","text":"opposite misconception one discussed previously. Examples incorrect statements based misconception ‘p < .05, therefore effect’, ‘difference two groups, p < .05’. , statements imply 100% probable null model false, alternative model true.simple example extreme statements incorrect, imagine generate series numbers R using following command:command generates 50 random observations distribution mean 0 standard deviation 1 (long run – mean standard deviation vary sample generated). Imagine run command , observe mean 0.5. figure visualizes scenario. can perform one-sample t-test 0, test tells us, p < .05, data observed surprisingly different 0, assuming random number generator R functions generates data true mean 0.\n(#fig:fig.1.3.7)Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0 observing d = 0.5.\nsignificant p-value allow us conclude null hypothesis (“random number generator works”) false. true mean 50 samples generated surprisingly extreme. low p-value simply tells us observation surprising. observe surprising observations low probability null hypothesis true – still happen. Therefore, significant result mean alternative hypothesis true – result can also Type 1 error, example , Omniscient Jones knows case.Let’s revisit incorrect statement ‘p < .05, therefore effect’. correct interpretation significant p-value requires us acknowledge possibility significant result might Type 1 error. Remember Fisher conclude “Either exceptionally rare chance occurred, theory random distribution true”. correct interpretation terms Neyman Pearson statistics : “can act null hypothesis false, wrong 5% time long run”. Note specific use word ‘act’, imply anything whether specific hypothesis true false, merely states act null-hypothesis false time observe p < alpha, make error alpha percent time.formally correct statements bit long. scientific articles, often read shorter statement : ‘can reject null hypothesis’, ‘can accept alternative hypothesis’. statements might made assumption readers add ‘5% probability wrong, long run’. might useful add ‘5% long run error rate’ least first time make statement article remind readers.example strong subjective prior probability random number generator R works. Alternative statistical procedures incorporate prior beliefs Bayesian statistics (week 2) false positive report probabilities (week 3). frequentist statistics, idea need replicate study several times. observe Type 1 error every now , unlikely observe Type 1 error three times row. Alternatively, can lower alpha level single study reduce probability Type 1 error rate.","code":"\nrnorm(n = 50, mean = 0, sd = 1)##  [1] -0.10542606  1.01505935 -0.64066722 -1.07093346 -1.13728239  0.95253861\n##  [7]  0.72452512  1.17600867  0.17391640  0.57650315 -0.90940472 -0.54306443\n## [13] -0.29076605 -2.70983196 -0.04853286  1.07959391 -1.28578314  1.01988326\n## [19] -0.69331320 -0.37169394 -0.30048825  0.01559035  0.77903180  0.86122009\n## [25] -0.42054593 -0.81477953  0.69985855  1.39973526  0.50799504 -1.68467655\n## [31]  1.04332806 -0.74935607 -0.13098121  0.96468287  2.11325246  1.40058737\n## [37] -0.18515209  1.32335820 -0.80310917  0.70879827 -0.82634514  0.92633710\n## [43] -0.30604728 -0.10622924 -0.13763026  0.43023054 -0.29327920 -1.93906352\n## [49]  0.41581406  1.16765945"},{"path":"pvalue.html","id":"misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.3 Misunderstanding 3: A significant p-value means that a practically important effect has been discovered","text":"common concern interpreting p-values ‘significant’ normal language implies ‘important’, thus ‘significant’ effect interpreted ‘important’ effect. However, question whether effect important completely orthogonal question whether different zero, even large effect . effects practical impact. smaller effect, less likely effects noticed individuals, effects might still large impact societal level. Therefore, general take home message statistical significance answer question whether effect matters practice, ‘practically important’. answer question whether effect matters, need present cost-benefit analysis.issue practical significance often comes studies large sample size. seen , increasing sample size, width density distribution around null-value becomes narrow, values considered surprising fall closer closer zero.plot null model large sample size (e.g., n = 10000 per group) see even small mean differences (differences extreme mean difference 0.04) considered ‘surprising’. still means really difference population, observe differences larger 0.04 less 5% time, long run, 95% observed differences smaller mean difference 0.04. becomes difficult argue practical significance effects. Imagine specific intervention successful changing people’s spending behavior, implementing intervention people save 12 cents per year. difficult argue effect make individual happier. However, money combined, yield 2 million, used treat diseases developing countries, real impact. cost intervention might considered high goal make individuals happier, might consider worthwhile goal raise 2 million charity.effects psychology additive (can combine transfer increase happiness 0.04 scale points), often difficult argue importance small effects subjective feelings. cost-benefit analysis might show small effects matter lot, whether case can inferred p-value.Note nothing problem interpretation p-value per se: p < 0.05 still correctly indicates , null hypothesis true, observed data considered surprising. However, just data surprising, mean need care . mainly verbal label ‘significant’ causes confusion – perhaps less confusing think ‘significant’ effect ‘surprising’ effect, necessarily ‘important’ effect.","code":""},{"path":"pvalue.html","id":"misconception4","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.4 Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.","text":"misinterpretation one possible explanation incorrect statement p-value ‘probability data observed chance.’ Assume collect 20 observations, Omniscient Jones tells us null hypothesis true (example generated random numbers R). means sampling distribution figure .\n(#fig:fig.1.3.8)Distribution observed Cohen's d effect sizes collecting 20 observations per group independent t-test d = 0.\nreality, means 100% time observe significant result, false positive (Type error). Thus, 100% significant results Type 1 errors.important distinguish probabilities collecting data analyzing result, probabilities collecting data analyzing results. Type 1 error rate controls, studies perform future null hypothesis true, 5% observed mean differences fall red tail areas. seen data falls tail areas p < alpha, know null hypothesis true, observed significant effects always Type 1 error. read carefully, notice misunderstanding cause differences question asked. \"observed p < .05, probability null hypothesis true?\" different question \"null hypothesis true, probability observing (extreme) data”. latter question answered p-value. first question can answered without making subjective judgment probability null hypothesis true prior collecting data (see lectures Bayesian statistics week 2).","code":""},{"path":"pvalue.html","id":"misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.5 Misunderstanding 5: One minus the p-value is the probability that the effect will replicate when repeated.","text":"impossible calculate probability effect replicate, based p-value. main reason know true mean difference. Omniscient Jones, knew true mean difference (e.g., difference two groups 0.5 scale points) know statistical power test. statistical power probability find significant result, alternative model true (.e. true effect). example, reading text left bar app, see N = 50 per group, alpha level 0.05, true mean difference 0.5, probability finding significant result (statistical power) 69.69%. observe significant effect scenario (e.g., p = 0.03) true 97% probability exact replication study (sample size) yield significant effect. probability study yields significant effect determined statistical power - p-value previous study.\ncan generally take away last misunderstanding fact probability replication depends presence versus absence true effect. words, stated , true effect exists level statistical power informs us frequently yield significant result (e.g., 80% power means observe significant result 80% time). hand, effect null (non-existent) significant results observed 5% time long run (.e. Type 1 error rate given alpha 0.05). Therefore, either statistical power alpha level equals probability replication, depending isn’t true effect.","code":""},{"path":"pvalue.html","id":"test-yourself","chapter":"1 Using p-values to test a hypothesis","heading":"1.8 Test Yourself","text":"","code":""},{"path":"pvalue.html","id":"questions-about-which-p-values-you-can-expect","chapter":"1 Using p-values to test a hypothesis","heading":"1.8.1 Questions about which p-values you can expect","text":"Copy code R run code.x-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power (minor variations simulation).Q1: Since statistical power probability observing statistically significant result, true effect, can also see power figure . ?can calculate number p-values larger 0.5, divide number simulations.can calculate number p-values first bar (contains ‘significant’ p-values 0.00 0.05) divide p-values bar total number simulations.can calculate difference p-values 0.5 minus p-values 0.5, divide number total number simulations.can calculate difference p-values 0.5 minus p-values 0.05, divide number number simulations.Q2: Change sample size n <- 26 n <- 51. Run simulation selecting lines pressing CTRL+Enter. power simulation now increased sample size 26 people 51 people?55%60%80%95%Q3: look distribution p-values, notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution much flatter 50% powerThe p-value distribution much normally distributed 50% powerFeel free increase decrease sample size see happens run simulation. done exploring, make sure n <- 51 .Q4: happen true difference simulated samples average IQ score? situation, probability observe effect, might say ‘0 power’. Formally, power defined true effect. However, can casually refer 0 power. Change mean sample 100 (set m <- 106 m <- 100) now difference mean sample, population value testing one-sample t-test. Run script . notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution basically completely flat (ignoring minor variation due random noise simulation)p-value distribution normally distributedThe question builds simulation true difference groups.Q5: Look leftmost bar plot produced Q4, look frequency p-values bar formal name bar?power (true positives)true negativesThe Type 1 error (false positives)Type 2 error (false negatives)Let’s take look just p-values 0.05. Bear next steps – worth . Find variable determines many bars , statement bars <- 20. Change bars <- 100. now get 1 bar p-values 0 0.01, one bar p-values 0.01 0.02, 100 bars total. red dotted line now indicate frequency p-values null hypothesis true, every bar contains 1% total number p-values. want look p-values 0.05, cut plot 0.05. Change xlim = c(0, 1) xlim = c(0, 0.05). Instead seeing p-values 0 1, see p-values 0 0.05. Re-run simulation (still m <- 100). see uniform distribution, now every bar contains 1% p-values, p-value distribution flat almost impossible see (zoom y-axis later assignment). red line now clearly gives frequency bar, assuming null hypothesis true.Change mean simulation line 9 m <- 107 (remember n still 51). Re-run simulation. ’s clear high power. p-values left-bar, contains p-values 0.00 0.01.Q6: plot last simulation tells 90.5% power. power use alpha 5%. can also use alpha 1%. statistical power simulated studies use alpha 1%, looking graph? Pick answer closest answer simulations.~90%~75%~50%~5%able look p-values around 0.03 0.04, zoom y-axis well. part code plot draw, change ylim = c(0, nSims) ylim = c(0, 10000). Re-run script.Change mean sample 108 m <- 108), leave sample size 51. Run simulation. Look distribution changed compared graph .Look fifth bar left. bar now contains p-values 0.04 0.05. notice something peculiar. Remember red dotted line indicates frequency bar, assuming null hypothesis true. See bar p-values 0.04 0.05 lower red line. simulated studies 96% power. power high, p-values 0.04 0.05 rare – occur less 1% time (p-values smaller 0.01). null hypothesis true, p-values 0.04 0.05 occur exactly 1% time (p-values uniformly distributed). Now ask : high power, observe p-value 0.04 0.05, likely null-hypothesis true, alternative hypothesis true? Given likely observe p-values 0.04 0.05 null hypothesis true, alternative hypothesis true, interpret p-value significant alpha 0.05 likely null hypothesis true, alternative hypothesis true.simulations, know true effect , real world, don’t know. high power, use alpha level 0.05, find p-value p = .045, data surprising, assuming null hypothesis true, even surprising, assuming alternative hypothesis true. shows significant p-value always evidence alternative hypothesis.Q7: know high (e.g., 98%) power smallest effect size care , observe p-value 0.045, correct conclusion?effect significant, provides strong support alternative hypothesis.effect significant, without doubt Type 1 error.high power, use alpha level smaller 0.05, therefore, effect can considered significant.effect significant, data likely null hypothesis alternative hypothesis.Q8: Play around sample size (n) mean (m) changing numerical values (thus, vary statistical power simulated studies). Look simulation result bar contains p-values 0.04 0.05. red line indicates many p-values found bar null-hypothesis true (always 1%). best, much likely p-value 0.04 0.05 come p-value distribution representing true effect, come p-value distribution effect? can answer question seeing much higher bar p-values 0.04 0.05 can become. best bar simulation five times high red line (bar shows 5% p-values end 0.04 0.05, red line remains 1%), best p-values 0.04 0.05 five times likely true effect true effect.best, p-values 0.04 0.05 equally likely \nalternative hypothesis, null hypothesis.best, p-values 0.04 0.05 approximately 4 times \nlikely alternative hypothesis, null hypothesis.best, p-values 0.04 0.05 ~10 times likely alternative hypothesis, null hypothesis.best, p-values 0.04 0.05 ~30 times likely alternative hypothesis, null hypothesis.reason, statisticians warn p-values just 0.05 (e.g.,\n0.04 0.05) best weak support alternative\nhypothesis. find p-values range, consider replicating \nstudy, ’s possible, interpret result least bit\ncautiously.","code":"\nnsims <- 100000 # number of simulations\n\nm <- 106 # mean sample\nn <- 26 # set sample size\nsd <- 15 # SD of the simulated data\n\np <- numeric(nsims) # set up empty vector\nbars <- 20\n\nfor (i in 1:nsims) { # for each simulated experiment\n  x <- rnorm(n = n, mean = m, sd = sd)\n  z <- t.test(x, mu = 100) # perform the t-test\n  p[i] <- z$p.value # get the p-value\n}\npower <- round((sum(p < 0.05) / nsims), 2) # power\n\n# Plot figure\nhist(p,\n  breaks = bars, xlab = \"P-values\", ylab = \"number of p-values\\n\", \n  axes = FALSE,  main = paste(\"P-value Distribution with\", \n                              round(power * 100, digits = 1), \"% Power\"),\n  col = \"grey\", xlim = c(0, 1), ylim = c(0, nsims))\naxis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))\naxis(side = 2, at = seq(0, nsims, nsims / 4), \n     labels = seq(0, nsims, nsims / 4), las = 2)\nabline(h = nsims / bars, col = \"red\", lty = 3)"},{"path":"pvalue.html","id":"questions-about-p-value-misconceptions","chapter":"1 Using p-values to test a hypothesis","heading":"1.8.2 Questions about p-value misconceptions","text":"Q1: sample size group independent t-test 50\nobservations (see Figure @ref(fig:fig.1.3.1)), statement correct?mean differences observe two groups always 0.mean differences observe two groups always different 0.Observing mean difference +0.5 -0.5 considered surprising, assuming null hypothesis true.Observing mean difference +0.1 -0.1 considered surprising, assuming null hypothesis true.Q2: sense null models figures (Figure @ref(fig:fig.1.3.1) @ref(fig:fig.1.3.2)) similar, sense different?cases, distributions centered zero, critical\nt-value 1.96 2 (two-sided test, depending sample size). larger sample size, closer 0 mean differences fall considered ‘surprising’.cases, t-value 0 likely outcome, critical t-value around 0.4 n = 50, around 0.05 n = 5000.cases, means vary exactly way around 0, Type 1 error rate much smaller n = 5000 n = 50.standard error much larger n = 50 n = 5000, much likely null hypothesis true n = 50.Q3: can play around alternative null models online app: http://shiny.ieis.tue.nl/d_p_power/. app allows specify sample size group independent t-test (2 infinity), mean difference (0 2), alpha level. plot, red areas visualize Type 1 errors. blue area visualizes Type 2 error rate (discuss ). app also tells critical value: vertical line (n = 50 line falls mean difference 0.4) verbal label says: “Effects larger 0.4 statistically significant”. Note true effects smaller -0.4, even though second label , app shows situation two-sided independent t-test.can see left vertical line indicates critical mean difference blue area part alternative model. Type 2 error rate (1 - power study). study 80% power, 80% mean differences observe fall right critical value indicated line. alternative model true, observe effect smaller critical value, observed p-value larger 0.05, even true effect. can check app larger sample size, right entire alternative distribution falls, thus higher power. can also see larger sample size, narrower distribution, less distribution fall critical value (long true population mean larger critical value). Finally, larger alpha level, left critical mean difference moves, smaller area alternative distribution falls critical value.app also plots 3 graphs illustrate power curves function different alpha levels, sample sizes, true mean differences. Play around app changing values. Get feel variable impacts null- alternative models, mean difference statistically significant, Type 1 Type 2 error rates.Open app, make sure set default settings \nsample size 50 alpha level 0.05. Look distribution null model. Set sample size 2. Set sample size 5000. app allow plot data ‘group’ size 1, n = 2 get pretty good idea range values can expect true effect 0, collect single observations (n = 1). Given experiences app change different parameters, statement true?null hypothesis true standard deviation 1, \nrandomly take 1 observation group calculate difference score, differences fall -0.4 0.4 95% pairs observations draw.null hypothesis true standard deviation 1, n = 50 per group, 95% studies data collected observe long run mean difference -0.4 0.4.study n = 50 per group, even SD unknown known null hypothesis true, rarely observe mean difference extreme -0.4 0.4.sample size increases, expected distribution means become narrower null model, alternative model.Q4: Open app default settings. Set slider alpha level 0.01 (keeping mean difference 0.5 sample size 50). Compared critical value alpha = 0.05, statement true?Compared alpha 0.05, less extreme values considered surprising alpha 0.01 used, differences larger 0.53 scale points (smaller -0.53) now statistically significant.Compared alpha 0.05, less extreme values considered surprising alpha 0.01 used, differences larger 0.33 scale points (smaller -0.33) now statistically significant.Compared alpha 0.05, extreme values considered surprising alpha 0.01 used, differences larger 0.53 scale points (smaller -0.53) statistically significant.Compared alpha 0.05, extreme values considered surprising alpha 0.01 used, differences larger 0.33 scale points (smaller -0.33) now statistically significant.Q5: can’t conclude null hypothesis true, observe statistically non-significant p-value (p > alpha)?calculating p-values always need take prior probability account.need acknowledge probability observed Type 1 error.null hypothesis never true.need acknowledge probability observed Type 2 error.Q6: can’t conclude alternative hypothesis true, observe statistically significant p-value (p < alpha)?calculating p-values always need take prior probability account.need acknowledge probability observed Type 1 error.alternative hypothesis never true.need acknowledge probability observed Type 2 error.Q7: common concern interpreting p-values ‘significant’ normal language implies ‘important’, thus ‘significant’ effect interpreted ‘important’ effect. However, question whether effect important completely orthogonal question whether different zero, even large effect . effects practical impact. smaller effect, less likely effects noticed individuals, effects might still large impact societal level. Therefore, general take home message statistical significance answer question whether effect matters practice, ‘practically important’. answer question whether effect matters, need present cost-benefit analysis.Go app: http://shiny.ieis.tue.nl/d_p_power/. Set sample size 50000, mean difference 0.5, alpha level 0.05. effects , observed, statistically different 0?Effects extreme -0.01 0.01Effects extreme -0.04 0.04Effects extreme -0.05 0.05Effects extreme -0.12 0.12If plot null model large sample size (e.g., n = 10000 per group) see even small mean differences (differences extreme mean difference 0.04) considered ‘surprising’. still means really difference population, observe differences larger 0.04 less 5% time, long run, 95% observed differences smaller mean difference 0.04. becomes difficult argue practical significance effects. Imagine specific intervention successful changing people’s spending behavior, implementing intervention people save 12 cents per year. difficult argue effect make individual happier. However, money combined, yield 2 million, used treat diseases developing countries, real impact. cost intervention might considered high goal make individuals happier, might consider worthwhile goal raise 2 million charity.effects psychology additive (can combine transfer increase happiness 0.04 scale points), often difficult argue importance small effects subjective feelings. cost-benefit analysis might show small effects matter lot, whether case can inferred p-value. Instead, need report interpret effect size,Q8: Let’s assume random number generator R works, use rnorm(n = 50, mean = 0, sd = 1) generate 50 observations, mean observations 0.5, one-sample t-test yields p-value 0.03, smaller alpha level (set 0.05). probability observed significant difference (p < alpha) just chance?3%5%95%100%Q9: statement true?probability replication study yield significant result 1-p.probability replication study yield significant result 1-p multiplied probability null hypothesis true.probability replication study yield significant result equal statistical power replication study (true effect), alpha level (true effect).probability replication study yield significant result equal statistical power replication study + alpha level.Q10: non-significant p-value (.e., p = 0.65) mean null hypothesis true?- result Type 2 error, false negative.Yes, true negative.Yes, p-value larger alpha level null hypothesis true., need least two non-significant p-values conclude null hypothesis true.Q11: correct way present non-significant p-value (e.g., p = 0.34 assuming alpha level 0.05 used independent t-test)?null hypothesis confirmed, p > 0.05There difference two conditions, p > 0.05The observed difference statistically different 0.null hypothesis true.Q12: observing significant p-value (p < .05) mean null hypothesis false?, p < .05 means alternative true, null hypothesis wrong., p-values never statement probability hypothesis theory.Yes, exceptionally rare event occurred.Yes, difference statistically significant.Q13: statistically significant effect always practically important\neffect?, extremely large samples, extremely small effects can statistically significant, small effects never practically important., alpha level theory set 0.20, case significant effect practically important., important effect depends cost-benefit analysis, surprising data null hypothesis.true.Q14: correct definition p-value?p-value probability null hypothesis true, given data extreme extreme data observed.p-value probability alternative hypothesis true, given data extreme extreme data observed.p-value probability observing data extreme extreme data observed, assuming alternative hypothesis true.p-value probability observing data extreme extreme data observed, assuming null hypothesis true.","code":""},{"path":"errorcontrol.html","id":"errorcontrol","chapter":"2 Error control","heading":"2 Error control","text":"previous chapter p-values learned Neyman-Pearson approach hypothesis testing goal make scientific claims controlling often make fool long run. Benjamini (2016) notes, p-value \"offers first line defense fooled randomness, separating signal noise\". indications banning use p-values increases ability researchers present erroneous claims. Based qualitative analyses scientific articles published null-hypothesis significance ban journal Basic Applied Social Psychology Fricker et al. (2019) conclude: “researchers employ descriptive statistics found likely overinterpret /overstate results compared researcher uses hypothesis testing p < 0.05 threshold”. Researchers often control error rates make claims, sometimes intentionally use flexibility data analysis 'p-hack' cherry-pick one many performed analyses shows results wanted see. error-statistical approach statistical inferences, problematic behavior, Mayo (2018) writes:problem cherry picking, hunting significance, host biasing selection effects – main source handwringing behind statistics crisis science – wreak havoc method’s error probabilities. becomes easy arrive findings severely tested.## outcome can expect perform study?perform study plan make claim based statistical test plan perform, long run probability making correct claim erroneous claim determined three factors, namely Type 1 error rate, Type 2 error rate, probability null-hypothesis true. four possible outcomes statistical test, depending whether result statistically significant , whether null hypothesis true, .False Positive (FP): Concluding true effect, true effect (H0 true). also referred Type 1 error, indicated α.False Negative (FN): Concluding true effect, true effect (H1 true). also referred Type 2 error, indicated β.True Negative (TN): Concluding true effect, true effect (H0 true). complement False Positives, thus indicated by1-α.True Positive (TP): Concluding true effect, true effect (H1 true). complement False Negatives, thus indicated 1-β.probability observing true positive true effect , long run, equal statistical power study. probability observing false positive null hypothesis true , long run, equal alpha level set, Type 1 error rate.\nFigure 2.1: Difference Type 1 Type 2 errors. Figure made Paul Ellis\n, next study perform, four possible outcomes likely? First, assume set alpha level 5%. Furthermore, assume designed study 80% power (example, assume Omniscient Jones knows indeed exactly 80% power). last thing specify probability null hypothesis true. Let’s assume next study idea null hypothesis true , equally likely null hypothesis true, alternative hypothesis true (probability 50%). can now calculate likely outcome study .perform calculation, take moment think know answer. might designed studies 5% alpha level 80% power, believed equally likely H0 H1 true. Surely, useful reasonable expectations result expect, perform study? Yet experience, many researchers perform without thinking probabilities . often hope observe true positive, even situation described , likely outcome true negative. now calculate probabilities.assume perform 200 studies 5% alpha level, 80% power, 50% probability H0 true. many false positives, true positives, false negatives, true negatives expect long run?table see 2.5% studies false positive (5% Type 1 error rate,\nmultiplied 50% probability H0 true). 40% studies true positive (80% power multiplied 50% probability H1 true). probability false negative 10% (20% Type 2 error rate multiplied 50% probability H1 true). likely outcome true negative, 47.5% (95% probability observing non-significant result, multiplied 50% probability H0 true).might enthusiastic outlook, like perform studies higher probability observing true positive. ? can reduce alpha level, increase power, increase probability H1 true. probability observing true positive depends power, multiplied probability H1 true, design studies values high. Statistical power can increased changes design study (e.g., increasing sample size). probability H1 true depends hypothesis testing. probability H1 true high outset, risk testing hypothesis already established enough certainty. solution, might happen often career, come test hypothesis trivial, explaining peers makes lot sense. words, come idea , explaining , think extremely plausible. creative research ideas likely rare academic career, ever . research needs ground-breaking. also extremely valuable perform replication extension studies relatively likely H1 true, scientific community still benefits knowing findings generalize different circumstances.","code":""},{"path":"errorcontrol.html","id":"positive-predictive-value","chapter":"2 Error control","heading":"2.1 Positive predictive value","text":"John Ioannides wrote well known article titled \"Published Research Findings False\" (J. P. . Ioannidis, 2005). time, learned set alpha 5%, Type 1 error rate higher 5% (long run). two statements related? aren’t 95% published research findings true? key understanding difference two different probabilities calculated. Type 1 error rate probability saying effect, effect. Ioannides calculates positive predictive value (PPV), conditional probability study turns show statistically significant result, actually true effect. probability useful understand, people often selectively focus significant results, due publication bias, research areas significant results published.real-life example useful understand concept positive predictive value concerned number vaccinated vaccinated people admitted hospital COVID symptoms. places equal numbers patients vaccinated unvaccinated. understand concept positive predictive value, might believe reveals equally likely end hospital, whether vaccinated . incorrect. Figure 2.2 nicely vizualizes, probability person vaccinated high, probability vaccinated person ends hospital much lower probability unvaccinated person ends hospital. However, select individuals end hospital, computing probability conditional hospital.\nFigure 2.2: vaccinated people hospital.\nuseful understand probability , observed significant result experiment, result actually true positive. words, long run, many true positives can expect, among positive results (true positives false positives)? known Positive Predictive Value (PPV). can also calculate many false positives can expect, among positive results (, true positives false positives). known False Positive Report Probability (Wacholder et al., 2004), sometimes also referred False Positive Risk (Colquhoun, 2019).\\[PPV = \\frac{\\text{True}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives} +\n                                                \\text{False}\\ \\text{Positives})}\\]\\[FPRP = \\frac{\\text{False}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives}\n                                                  + \\text{False}\\ \\text{Positives})}\\]PPV FPRP combine classic Frequentist concepts statistical power alpha levels prior probabilities H0 H1 true. depend proportion studies effect (H1 true), effect (H0 true), addition statistical power, alpha level. , can observe false positive null hypothesis true, can observe true positive alternative hypothesis true. Whenever perform study, either operating reality true effect, operating reality effect – don’t know reality .perform studies, aware outcomes studies (significant non-significant findings). read literature, publication bias, often access significant results. thinking PPV (FPRP) becomes important. set alpha level 5%, long run 5% studies H0 true (FP + TN) significant. literature significant results, access true negatives, \npossible proportion false positives literature much larger 5%.continue example , see 85 positive results (80 + 5) 200 studies. false positive report probability 5/85 = 0.0588. time, alpha 5% guarantees (long run) 5% 100 studies null hypothesis true Type 1 errors: 5%*100 = 0.05. also true. 200 studies, 0.05*200 = 10 possibly false positives (H0 true experiments). 200 studies performed (H0 true 50% studies), proportion false positives experiments 2.5%. Thus, experiments , proportion false positives , long run, never higher Type error rate set researcher (e.g., 5% H0 true experiments), can lower (H0 true less 100% experiments).\nFigure 2.3: Screenshot output results PPV Shiny app Michael Zehetleitner Felix Schönbrodt \n(Note: FDR FPRP different abbreviations thing)People often say something like: “Well, know 1 20 results published literature Type 1 errors”. able understand true practice, learning positive predictive value. also explains common p-value misconception \"observed significant finding, probability made Type 1 error (false positive) 5%.\" correct. Even use 5% alpha level, quite reasonable assume much 5% significant findings published literature false positives. 100% studies perform, null hypothesis true, studies published, 1 20 studies, long run, false positives (rest correctly reveal statistically significant difference). scientific literature, positive predictive value can quite high, specific circumstances, might even high published research findings false. happen researchers examine mostly studies null-hypothesis true, low power, Type 1 error rate inflated due p-hacking types bias.","code":""},{"path":"errorcontrol.html","id":"type-1-error-inflation","chapter":"2 Error control","heading":"2.2 Type 1 error inflation","text":"\nFigure 2.4: Quote 1830 book Babbage Reflections Decline Science England Causes available \nperform multiple comparisons, risk Type 1 error rate inflates. multiple comparisons planned, cases possible control Type 1 error rate lowering alpha level individual analysis. widely known approach control multiple comparisons Bonferroni correction alpha level divided number tests performed (Dunn, 1961). However, researchers also often use informal data analysis strategies inflate Type 1 error rate. Babbage (1830) already complained problematic practices 1830, two centuries later, still common. Barber (1976) provides depth discussion range approaches, eyeballing data decide hypotheses test (sometimes called 'double dipping'), selectively reporting analyses confirm predictions, ignoring non-significant results, collecting many variables performing multitudes tests, performing sub-group analyses planned analysis yields nonsignificant results, nonsignificant prediction derive new hypothesis supported data, test hypothesis data hypothesis derived (sometimes called HARKing (Kerr, 1998)). Many researchers admit used practices inflate error rates (Chin et al., 2021; Fiedler & Schwarz, 2015; John et al., 2012; Makel et al., 2021; van de Schoot et al., 2021). used practices first scientific article published, fully aware problematic (Jostmann et al., 2016).paradigms, researchers lot flexibility compute main dependent variable. Elson colleagues examined 130 publications use Competitive Reaction Time Task, participants select duration intensity blasts delivered competitor (Elson et al., 2014). task used measure 'aggressive behavior' ethical manner. compute score, researchers can use duration noise blast, intensity, combination therefore, averaged number trials, several possible transformations data. 130 publications examined reported 157 different quantification strategies total, showing calculations dependent variable unique, used single article. One might wonder authors sometimes use different computations across articles. One possible explanation used flexibility data analysis find statistically significant results.\nFigure 2.5: Plot publications using CRTT (blue) unique quantifications meaure (red). Figure FlexibleMeasures.com Malte Elson\n","code":""},{"path":"errorcontrol.html","id":"optional-stopping","chapter":"2 Error control","heading":"2.3 Optional stopping","text":"\nFigure 2.6: Screenshot scientific paper explicitly admitting using optional stopping\nOne practice inflates Type 1 error rate known optional stopping. optional stopping, researcher repeatedly analyzes data, continues data collection test result statistically significant, stops significant effect observed. quote published article figure example researchers transparently report used optional stopping, commonly people disclose use optional stopping methods sections. last years, many researchers learned optional stopping problematic. lead general idea collect data, look whether results significant, stop data collection result significant, , continue data collection. correct conclusion, example becoming inflexible. correct approach collect data batches, called sequential analysis, extensively developed statisticians, used many studies. example, safety efficacy Pfizer–BioNTech COVID-19 vaccine used experimental design planned analyze data 5 times, controlled overall Type 1 error rate lowering alpha level interim analysis.\nFigure 2.7: Screenshot planned interim analyses examining safety Efficacy BNT162b2 mRNA Covid-19 Vaccine.\nmain lesson certain research practices can increase flexibility efficiency studies perform, done right, practices can inflate Type 1 error rate done wrong. Let’s therefore try get better understanding inflating Type 1 error rate optional stopping, correctly using sequential analysis.Copy code R run . script simulate ongoing data collection. 10 participants condition, p-value calculated performing independent t-test, t-test repeated every additional participant collected. , p-values plotted function increasing sample size.example, Figure , see p-value plotted y-axis (0 1) sample size plotted x-axis (0 200). simulation, true effect size d = 0, meaning true effect. can thus observe true negatives false positives. sample size increases, p-value slowly moves (remember chapter p-values true effect, p-values uniformly distributed). Figure 2.8, p-value drops grey line (indicating alpha level 0.05) collecting 83 participants condition, drift back upwards larger p-values. figure, becomes clear often look data, larger total sample size, higher probability one analyses yield p < \\(\\alpha\\). resources infinite, Type 1 error rate 1, researcher can always find significant result optional stopping.\nFigure 2.8: Simulated p-values additional observation null true.\ntrue effect, see p-values also vary, eventually drop alpha level. Due variation, just know exactly . perform -priori power analysis, can compute probability looking specific sample size yield significant p-value. Figure 2.9 see simulation, now true small effect d = 0.3. 200 observations per condition, sensitivity power analysis reveals 85% power. analyze data interim analysis (e.g., 150 observations) often already find statistically significant effect (74% power). illustrates benefit sequential analyses, control error rates, can stop early interim analysis. Sequential analyses especially useful large expensive studies uncertainty true effect size.\nFigure 2.9: Simulated p-values additional observation d = 0.3.\nformally examine inflation Type 1 error rate optional stopping simulation study. Copy code R run code. Note 50000 simulations (needed get error rates reasonably accurate) take time run.simulation perform multiple independent t-tests simulated data, looking multiple times maximum sample size reached. first four lines, can set important parameters simulation. First, maximum sample size condition (e.g., 100). , number looks (e.g., 5). best, can look data every participant (e.g., 100 participants, can look 100 times – actually 98 times, need 2 participants condition t-test!). can set number simulations (, clearer pattern , longer simulation takes), alpha level (e.g., 0.05). Since can make Type 1 error true effect, effect size set 0 simulations.perform single test, Type 1 error rate probability finding p-value lower alpha level, effect. optional stopping scenario look data twice, Type 1 error rate probability finding p-value lower alpha level first look, probability finding p-value lower alpha level first look, finding p-value lower alpha level second look. conditional probability, makes error control little bit complex multiple looks completely independent.much optional stopping inflate Type 1 error rate? p-values can expect optional stopping?Start running simulation without changing values, simulating 100 participants condition, looking 5 times data, alpha 0.05. Note 50.000 simulations take ! see something similar Figure 2.10 (based 500.000 simulations make pattern clear).\nFigure 2.10: Simulation 500000 studies performing 5 interim analyses alpha level 5%\nsee 100 bars, one % (one p-values 0.00 0.01, one p-values 0.01 0.02, etc.). horizontal line indicates p-values fall, uniformly distributed (true effect, explained chapter p-values).distribution p-values peculiar. see compared uniform distributions, bunch results just alpha threshold 0.05 missing, seem pulled just 0.05, much higher frequency outcomes compared data analyzed multiple times comes . Notice relatively high p-values (e.g., p = 0.04) common lower p-values (e.g., 0.01). see chapter bias detection statistical techniques p-curve analysis can pick pattern.using alpha level 5% 5 looks data, overall Type 1 error rate inflated 14%. lower alpha level interim analysis, overall Type 1 error rate can controlled. shape p-value distribution still look peculiar, total number significant test results controlled desired alpha level. well-known Bonferroni correction (.e., using alpha level \\(\\alpha\\) / number looks), Pocock correction slighlty efficient. information perform interim analyses controlling error rates, see dedicated chapter sequential analysis.","code":"\nn <- 200 # total number of datapoints (per condition) after initial 10\nd <- 0.0 # effect size d\n\np <- numeric(n) # store p-values\nx <- numeric(n) # store x-values\ny <- numeric(n) # store y-values\n\nn <- n + 10 # add 10 to number of datapoints\n\nfor (i in 10:n) { # for each simulated participants after the first 10\n  x[i] <- rnorm(n = 1, mean = 0, sd = 1)\n  y[i] <- rnorm(n = 1, mean = d, sd = 1)\n  p[i] <- t.test(x[1:i], y[1:i], var.equal = TRUE)$p.value\n}\n\np <- p[10:n] # Remove first 10 empty p-values\n\n# Create the plot\nplot(0, col = \"red\", lty = 1, lwd = 3, ylim = c(0, 1), xlim = c(10, n), \n     type = \"l\", xlab = \"sample size\", ylab = \"p-value\")\nlines(p, lwd = 2)\nabline(h = 0.05, col = \"darkgrey\", lty = 2, lwd = 2) # draw line at p = 0.05\n\nmin(p) # Return lowest p-value from all looks\ncat(\"The lowest p-value was observed at sample size\", which.min(p) + 10) \ncat(\"The p-value dropped below 0.05 for the first time at sample size:\", \n    ifelse(is.na(which(p < 0.05)[1] + 10), \"NEVER\", which(p < 0.05)[1] + 10)) \nN <- 100 # total datapoints (per condition)\nlooks <- 5 # set number of looks at the data\nnsims <- 50000 # number of simulated studies\nalphalevel <- 0.05 # set alphalevel\n\nlook_at_n <- ceiling(seq(N / looks, N, (N - (N / looks)) / (looks-1))) # looks\nlook_at_n<-look_at_n[look_at_n > 2] #Remove looks at N of 1 or 2\nlooks<-length(look_at_n) #if looks are removed, update number of looks\n\nmatp <- matrix(NA, nrow = nsims, ncol = looks) # Matrix for p-values l tests\np <- numeric(nsims) # Variable to save pvalues\n\n# Loop data generation for each study, then loop to perform a test for each N\nfor (i in 1:nsims) {\n  x <- rnorm(n = N, mean = 0, sd = 1)\n  y <- rnorm(n = N, mean = 0, sd = 1)\n  for (j in 1:looks) {\n    matp[i, j] <- t.test(x[1:look_at_n[j]], y[1:look_at_n[j]], \n                         var.equal = TRUE)$p.value # perform the t-test, store\n  }\n  cat(\"Loop\", i, \"of\", nsims, \"\\n\")\n}\n\n# Save Type 1 error rate smallest p at all looks\nfor (i in 1:nsims) {\n  p[i] <- ifelse(length(matp[i,which(matp[i,] < alphalevel)]) == 0, \n                 matp[i,looks], matp[i,which(matp[i,] < alphalevel)])\n}\n\nhist(p, breaks = 100, col = \"grey\") # create plot\nabline(h = nsims / 100, col = \"red\", lty = 3)\n\ncat(\"Type 1 error rates for look 1 to\", looks, \":\", \n    colSums(matp < alphalevel) / nsims)\ncat(\"Type 1 error rate when only the lowest p-value for all looks is reported:\", \n    sum(p < alphalevel) / nsims)"},{"path":"errorcontrol.html","id":"justifyerrorrate","chapter":"2 Error control","heading":"2.4 Justifying Error Rates","text":"reject H0 , may reject true; accept H0 , may accepting false, say, really alternative Bt true. two sources error can rarely eliminated completely; cases important avoid first, others second. reminded old problem considered LaplaceE number votes court judges needed convict prisoner. serious convict innocent man acquit guilty? depend upon consequences error ; punishment death fine ; danger community released criminals ; current ethical views punishment? point view mathematical theory can show risk errors may controlled minimised. use statistical tools given case, determining just balance struck, must left investigator.Even though theory Type 1 Type 2 error rate justified researcher, Neyman Pearson (1933) write , practice researchers tend imitate others. default use alpha level 0.05 can already found work} Gosset t-distribution (Cowles & Davis, 1982; Kennedy-Shaffer, 2019), believed difference two standard deviations (z-score 2) sufficiently rare. default use 80% power (20% Type 2 error rate) similarly based personal preferences Cohen (1988), writes:proposed convention , investigator basis setting desired power value, value .80 used. means beta set .20. value offered several reasons (Cohen, 1965, pp. 98-99). chief among takes consideration implicit convention alpha .05. beta .20 chosen idea general relative seriousness two kinds errors order .20/.05, .e., Type errors order four times serious Type II errors. .80 desired power convention offered hope ignored whenever investigator can find basis substantive concerns specific research investigation choose value ad hoc.see conventions built conventions: norm aim 80% power built norm set alpha level 5%. Although nothing special alpha level 5%, interesting reflect become widely established. Uygun Tunç et al. (2021) argue one possible reason , far conventions go, alpha level 5% might low enough peers take claims made error rate seriously, time high enough peers motivated perform independent replication study increase decrease confidence claim. Although lower error rates establish claims convincingly, also require resources. One might speculate\nresearch areas every claim important enough careful justification costs benefits, 5% pragmatic function facilitating conjectures refutations fields otherwise lack coordinated approach knowledge generation, faced limited resources.Nevertheless, researchers proposed move away default use 5% alpha level. example, Johnson (2013) proposes default significance level 0.005 0.001. Others cautioned blanket recommendation additional resources required reduce Type 1 error rate might worth costs (Lakens, Adolfi, et al., 2018). lower alpha lever requires larger sample size achieve sstatistical power. sample size can increased, lower alpha level reduces statistical power, increases Type 2 error rate. Whether desireable evaluated case case basis.two main reasons abandon universal use 5% alpha level. first reason carefully choose alpha level decision-making becomes efficient (Mudge et al., 2012). researchers use hypothesis tests make dichotomous decisions methodological falsificationist approach statistical inferences, certain maximum sample size willing able collect, typically possible make decisions efficiently choosing error rates combined cost Type 1 Type 2 errors minimized. aim either minimize balance Type 1 Type 2 error rates given sample size effect size, alpha level set based convention, weighting relative cost types errors (Maier & Lakens, 2022).example, imagine researcher plans collect 64 participants per condition detect d = 0.5 effect, weighs cost Type 1 errors 4 times much Type 2 errors. exactly scenario Cohen (1988) described, 64 participants per condition relative weigfht Type 1 Type 2 errors yields 5% Type 1 error rate 20% Type 2 error rate. Now imagine researchers realizes resources collect 80 observations instead just 64. interest effect size d = 0.5, relative weight Type 1 Type 2 errors 4 satisfied set alpha level 0.037 Type 2 error rate 0.147. Alternatively, researcher might decided collect 64 observations, balance error rates, set alpha level weighted combined error rate minimized, achieved alpha level set 0.033, vizualized Figure 2.11 (information, see Maier & Lakens (2022)).\nFigure 2.11: Weighted combined error rate, minimized alpha = 0.037.\nJustifying error rates can leads situations alpha level increased 0.05, leads optimal decision making. Winer (1962) writes: “frequent use .05 .01 levels significance \n197 matter convention little scientific logical basis. power tests likely low levels significance, Type 1 Type 2 errors approximately equal importance, .30 .20 levels significance may appropriate .05 .01 levels.” reasoning design 70% power smallest effect size interest balance Type 1 Type 2 error rates sensible manner. course, increase increase alpha level deemed acceptable authors can justify costs increase Type 1 error rate sufficiently compensated benefit decreased Type 2 error rate. encompass cases (1) study practical implications require decision making, (2) cost-benefit analysis provided gives clear rationale relatively high costs Type 2 error, (3) probability H1 false relatively low, (4) feasible blackuce overall error rates collecting data.One also carefully reflect choice alpha level experiment achieves high statistical power effect sizes considered meaningful. study 99% power effect sizes interest, thus 1% Type 2 error rate, uses default 5% alpha level, also suffers lack balance, use lower alpha level lead balanced decision, increase severity test.second reason relevant large data sets, related Lindley's paradox. statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis, recommended lower alpha level function sample size. need discussed Leamer (1978), writes \"rule thumb quite popular now, , setting significance level arbitrarily .05, shown deficient sense every reasonable viewpoint significance level decreasing function sample size.\" idea approach reduce alpha level Bayes factor likelihood computed significant results never evidence null hypothesis (online Shiny app perform calculations, see app ).","code":""},{"path":"errorcontrol.html","id":"multiplecomparisons","chapter":"2 Error control","heading":"2.5 Why you don't need to adjust your alpha level for all tests you'll do in your lifetime.","text":"researchers criticize corrections multiple comparisons one might well correct tests lifetime [Perneger (1998). choose use Neyman-Pearson approasch statistics reason correct tests perform lifetime work done life tests single theory, use last words decide accept reject theory, long one individual tests performed yielded p < α. Researchers rarely work like .Instead, Neyman-Pearson approach hypothesis testing, goal use data make decisions act. Neyman (Neyman, 1957) calls approach inductive behavior. outcome experiment leads one take different possible actions, can either practical (e.g., implement new procedure, abandon research line) scientific (e.g., claim effect). error-statistical approach (Mayo, 2018) inflated Type 1 error rates mean become likely able claim support hypothesis, even hypothesis wrong. reduces severity test. prevent , need control error rate level claim.useful distinction literature multiple testing union-intersection testing approach, intersection-union testing approach (Dmitrienko & D’Agostino Sr, 2013). union-intersection approach, claim made -least-one test significant. cases, correction multiple comparisons required control error rate. intersection-union approach, claim made performed tests statistically significant, correction multiple comparisons required (indeed, assumptions researchers even increase alpha level intersection-union approach).might seem researchers can get corrections multiple comparisons formulating hypothesis every possible test perform. Indeed, can. ten ten correlation matrix, researcher might state testing 45 unique predictions, uncorrected alpha level. However, readers might reasonably question whether 45 tests predicted theory, 45 tests show significant result, meager track record predictions make us doubt theory derived . different ways control error rates, easiest Bonferroni correction ever--slightly less conservative Holm-Bonferroni sequential procedure. number statistical tests becomes substantial, sometimes preferable control false discovery rates, instead error rates (Benjamini & Hochberg, 1995).","code":""},{"path":"errorcontrol.html","id":"power-analysis","chapter":"2 Error control","heading":"2.6 Power Analysis","text":"far largely focused Type 1 error control. clear Figure 2.9, true effect p-values eventually become smaller alpha level sample size becomes large enough. designing experiment goal choose sample size provides desired Type 2 error rate effect size interest. can achieved performing -priori power analysis. important highlight goal -priori power analysis achieve sufficient power true effect size. true effect size always unknown designing study. goal -priori power analysis achieve sufficient power, given specific assumption effect size researcher wants detect. Just like Type error rate maximum probability making Type error conditional assumption null hypothesis true, -priori power analysis computed assumption specific effect size. unknown assumption correct. researcher can make sure assumptions well justified. Statistical inferences based test Type II error controlled conditional assumption specific effect size. allow inference , assuming true effect size least large used -priori power analysis, maximum Type II error rate study larger desired value.Figure 2.12 see expected distribution observed standardized effect sizes (Cohen's d) independent t-test 50 observations condition. bell-shaped curve left represents expectations null true, red areas tail represent Type 1 errors. bell-shaped curve right represents expectations alternative hypothesis true, d= 0.5. vertical line d = 0.4 represents critical effect size. sample size alpha level 0.05, observed effect sizes smaller d = 0.4 statistically significant. true effect, outcomes Type 2 errors, illustrated blue shaded area. remainder curve reflects true positives, true effect, observed effect sizes statistically significant. power test percentages distribution right larger critical value.\nFigure 2.12: Distribution d = 0 d = 0.5 independent t-test n = 50.\nissue Type 2 error control discussed detail nthe chapter sample size justification. Even thought topic Type 2 error control briefly discussed , least important Type 1 error control. informative study high probability observing effect effect. Indeed, default recommendation aim 80% power leaves surprisingly large (20%) probability Type 2 error. researcher cares making decision error, researcher care whether decision error false positive false negative, argument made Type 1 Type 2 errors weighed equally. Therefore, desiging study balanced error rates (e.g., 5% Type 1 error 95% power) make sense.","code":""},{"path":"errorcontrol.html","id":"test-yourself-1","chapter":"2 Error control","heading":"2.7 Test Yourself","text":"","code":""},{"path":"errorcontrol.html","id":"questions-about-the-positive-predictive-value","chapter":"2 Error control","heading":"2.7.1 Questions about the positive predictive value","text":"Q1: example start chapter, see control Type 1 error rate 5% using alpha 0.05. Still, 50% probability H0 true, proportion false positives experiments performed turns much lower, namely 2.5%, 0.025. ?proportion false positives experiments performed variable distribution around true error rate – sometimes ’s higher, sometimes ’s lower, due random variation.proportion false positives experiments performed 5% H0 true 200 studies.proportion false positives experiments performed 5% 50% power – power increases 50%, proportion false positives experiments performed becomes smaller.proportion false positives experiments performed 5% 100% power, becomes smaller power lower 100%.Q2: FWhat make biggest difference improving probability find true positive? Check answer shifting sliders online PPV app.Increase % -priori true hypothesesDecrease % -priori true hypothesesIncrease alpha levelDecrease alpha levelIncrease powerDecrease powerIncreasing power requires bigger sample sizes, studying larger effects. Increasing % -priori true hypotheses can done making better predictions – example building reliable findings, relying strong theories. useful recommendations want increase probability performing studies find statistically significant result.Q3: Set “% priori true hypotheses” slider 50%. Leave ‘α level’ slider 5%. Leave ‘% p-hacked studies’ slider 0. title Ioannidis’ paper ‘published research findings false’. One reason might studies often low power. value power PPV 50%. words, level power significant result just likely true, false?80%50%20%5%seems low power alone best explanation published findings might false, unlikely power low enough scientific literature. Ioannidis (2005) discusses scenarios becomes likely published research findings false. assume ‘p-hacked studies’, studies show significant result due bias, enter literature. good reasons believe happens, discussed chapter. ‘presets Ioannidis’ dropdown menu, can select situations. Explore , pay close attention ones PPV smaller 50%.Q4: general, published findings false? Interpret ‘low’ ‘high’ answer options relation values first example chapter 50% probability H1 true, 5% alpha, 80% power,\n0% bias.probability examining true hypothesis low, combined either low power substantial bias (e.g., p-hacking).probability examining true hypothesis high, combined either low power substantial bias (e.g., p-hacking).alpha level high, combined either low power substantial bias (e.g., p-hacking).power low p-hacking high (regardless % true hypotheses one examines).Q5: Set “% priori true hypotheses” slider 0%. Set “% p-hacked studies” slider 0%. Set “α level” slider 5%. Play around power slider. statement true?\nWithout p-hacking, alpha level 5%, 0% hypotheses true, proportion false positives experiments performed 100%.PPV depends power studies.regardless power, PPV equals proportion false positives experiments performed.regardless power, proportion false positives experiments performed 5%, PPV 0% (significant results false positives).","code":""},{"path":"errorcontrol.html","id":"questions-about-optional-stopping","chapter":"2 Error control","heading":"2.7.2 Questions about optional stopping","text":"Q1: Run script plots p-value sample size increases 20 times, count often lowest p-value ends 0.05 (calculate long run probability happening extensive simulations later).Q2: true effect, can observe true positive false negative. Change effect size d <- 0.0 d <- 0.3. relatively small true effect, 200 participants condition, 85% power (85% probability finding significant effect). Run script . one possible example trajectory p-values sample size increases. Run script 20 times. Take good look variation p-value trajectory. Remember N = 200, 85% times p-value ended 0.05. script returns sample size p-value lowest (often, always, maximum sample size, true effect) sample size p-value drops 0.05 first time. statement true?p-value drops 0.05, stays 0.05.p-value randomly moves 0 1, every now end 0.05.p-value often drops 0.05 well 200 participants \ncondition. around 50% simulations, already happens N = 100.p-value typically move 0.05 stay time,\ngiven large enough sample, always move back p > 0.05.Q3: Change effect size d <- 0.8, can regarded large effect. Run script 20 times. Take good look variation p-value trajectory. statement true?p-value randomly moves 0 1, every now end 0.05.p-values drop stay 0.05 much earlier true effect size 0.3.p-values meaningful effect sizes large (e.g., d = 0.8), meaningless effect sizes small (e.g., d = 0.3).examine large effect, whenever p-value drops 0.05, always stay 0.05 sample size increases.Q4: Looking Figure 2.10, statement true?Optional stopping impact Type 1 error rate.Optional stopping inflates Type 1 error rate. can see first five bars (p-values 0.00 0.05), substantially higher horizontal line.Optional stopping inflates Type 1 error rate. can see bars just 0.05, dip substantially uniform distribution present true effect.Q5: script simulate optional stopping provides written output. One summary gives Type 1 error rate individual look. One summary gives Type 1 error rate optional stopping used. running script default values, statement true?look, Type 1 error rate higher alpha level (0.05).\nusing optional stopping (reporting lowest p-value), Type 1 error rate higher 0.05.look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping (reporting lowest p-value), alpha level also approximately equals alpha level (0.05).look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping, Type 1 error rate also higher alpha level (0.05).Q6: Change number looks simulation 2 (change 'looks <- 5' 'looks <- 2'), leave settings . Run simulation . Type 1 error rate using optional stopping 1 interim analysis, rounded 2 digits? (Note due small number simulations, exact alpha level get might differ little bit \nanswer options ).0.050.080.120.18Q7: Wagenmakers (2007) notes: “user NHST always obtain significant result optional stopping (.e., analyzing data accumulate stopping experiment whenever p-value reaches desired significance level)”. correct. ’s true p-value always drop alpha level point time. , need rather large number observations. can calculate maximum Type 1 error rate due optional stopping maximum sample size. example, maximum Type 1 error rate optional stopping used collecting 200 participants condition, looking 200 times (198 times, given can’t perform t-test sample size 1 2 people)? Set number participants 200, number looks 200, number simulations 10000 (simulation take even longer!), alpha 0.05.maximum Type 1 error rate collecting 200 participants \ncondition independent t-test, using optional stopping, rounded 2\ndigits? (Note simulation take , still, due \nrelatively small number simulations, exact alpha level get might\ndiffer little bit answer options – choose answer option\nclosest result).0.050.110.200.41Q8: Wikipedia, look entry Pocock boundary: https://en.wikipedia.org/wiki/Pocock_boundary . ethical reasons look data, data collected. clear medicine, similar arguments can made research areas (see Lakens, 2014). Researchers often want look data multiple times. perfectly fine, long design study number looks advance, control Type 1 error rate.Pocock boundary provides easy way control type 1 error rate sequential analyses. Sequential analysis formal way optional stopping. Researchers use slightly lower alpha level look, make sure overall alpha level (looks) larger 5%.Set number participants 100, number looks 5, \nnumber simulations 50000 (back original script). Wikipedia article Pocock boundary, find corrected alpha level 5 looks data. Change alpha level simulation value. Run simulation. following statements true?Type 1 error rate look approximately 0.03, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.03, overall alpha level approximately 0.15.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.08.Q9: Look graph p-value distribution using Pocock boundary, compare graph got using Pocock boundary. can flip back forth plots generated RStudio using blue arrows plots tab. statement true?Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also \nlikely slightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) less likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also less\nlikely slightly higher p-values (p = 0.04).","code":""},{"path":"likelihoods.html","id":"likelihoods","chapter":"3 Likelihoods","heading":"3 Likelihoods","text":"addition frequentist Bayesian approaches statistical inferences, likelihoods provide third approach statistical inferences (Pawitan, 2001). Unlike Bayesian approaches, likelihoodists incorporate prior inferences. likelihoodists Taper Lele (2011) write:believe Bayes' rule Bayesian mathematics flawed, axiomatic foundational definition probability Bayesianism doomed answer questions irrelevant science. care believe, barely care believe, interested can show.Unlike Neyman-Pearson frequentist approach, likelihoodists interested quantifying relative evidence, unlike Fisherian frequentist approach, likelihoodists specify null alternative model, quantify relative likelihood data models.Likelihood approaches statistical inferences form bridge frequentist approaches Bayesian approaches, independent third approach statistical inferences. time, likelihood functions important part Neyman-Pearson statistics Neyman-Pearson lemma, shows likelihood ratio test powerful test H0 H1, useful determining critical value used reject hypothesis. Bayesian approaches, likelihood combined prior compute posterior probability distribution.can use likelihood functions make inferences unknown quantities. Let’s imagine flip coin 10 times, turns heads 8 times. true probability (indicate Greek letter theta, p) coin landing heads?binomial probability observing x successes n studies :\\[\nPr\\left( k;n, p \\right) = \\frac{n!}{k!\\left( n - k \\right)!}p^{k}{(1 - p)}^{n - k}\n\\]p probability success, k observed number successes, n number trials. first term indicates number possible combinations results (e.g., start eight successes, end eight successes, possible combinations), multiplied probability observing one success trials, multiplied probability observing success remaining trials.Let’s assume expect fair coin. binomial probability observing 8 heads 10 coin flips, p = 0.5? answer :\\[\nPr\\left(0.5;8,10 \\right) = \\frac{10!}{8!\\left( 10 - 8 \\right)!}*0.5^{8}*{(1 - 0.5)}^{10 - 8}\n\\]\nR probability computed :using function:Let’s assume don’t information coin. (might believe coins fair; priors discussed talk Bayesian statistics next chapter). equation Pr(k;n,p) gives probability observing k successes n trials coin’s probability success p. Based data observed, can ask question: value p make observed data likely? answer question, can plug values k n find value p maximizes function. Ronald Fisher called maximum likelihood estimation (considered one important developments 20th century statistics, Fisher published first paper 1912 third year undergraduate 22 (Aldrich, 1997)). Since p can value 0 1, can plot values known likelihood function, can see maximum easily.\nFigure 3.1: Binomial likelihood function 8 successes 10 trials.\nlikelihood Pr(k;n,p) plotted possible values p (0 1) surprising given data observed, likely value true parameter 8 10, p = 0.8, likelihood 0.30 (highest point y-axis). example, p = 0.8 called maximum likelihood estimator. important know likelihood meaning isolation. sense, differs probability. can compare likelihoods function across different values p. can read value θ, see given observed data, low values p (e.g., 0.2) likely.Probabilities likelihoods related, different. Note equation Pr involves information data (k, n) information parameter (p). compute probability, view p fixed (instance, fair coin, plug p = 0.5) estimate probability different outcomes (k, n). resulting function probability mass function. compute likelihood, instead view observed data fixed (e.g., observing 5 heads 10 coin tosses), view Pr function p, estimating value maximizes likelihood particular sample.Likelihoods example statistical inference: observed data, use data draw inference different population parameters. formally, likelihood function (joint) density function evaluated observed data. Likelihood functions can calculated many different models (binomial distributions, normal distributions, see Millar (2011)).likelihood curve rises falls , except extremes, 0 heads heads observed. plot likelihood curves 0 heads 10 coin flips, likelihood curve looks like:\nFigure 3.2: Binomial likelihood function 0 successes 10 trials.\nLikelihoods can easily combined. Imagine two people flipping coin independently. One person observes eight heads 10 flips, observes 4 heads 10 flips. might believe give likelihood curve one person flipping coin 20 times, observing 12 heads, indeed, . plot , likelihood curves standardized dividing curve maximum likelihood curve. curves now maximum 1, can easily compare different likelihood curves.\nFigure 3.3: Combining likelihoods.\ncurve left 4 10 heads, one right 8 10 heads. black dotted curve middle 12 20 heads. grey curve, exactly underneath 12 20 heads curve, calculated multiplying likelihood curves: L(p_combined) = L(p = 0.8) * L(p = 0.4).Figure 3.4 see likelihood curves 10, 100, 1000 coin flips, yield 5, 50, 500 heads, respectively. likelihood curves standardized make easily comparable. sample size increases, curves become narrow (dashed line n = 10, dotted line n = 100, solid line n = 1000). means sample size increases, data become increasingly less likely population parameters removed observed number heads. words, collected increasingly strong evidence p = 0.5, compared possible population parameters.\nFigure 3.4: Likelihood function 5/10, 50/100 500/1000 heads coin flips.\ncan use likelihood function compare possible values θ. example, might believe coin flipped fair, even though flipped eight ten heads. fair coin p = 0.5, observed p = 0.8. likelihood function allows us compute relative likelihood different possible parameters. much likely observed data hypothesis unfair coin average turn heads 80% time, compared alternative theory fair coin turn heads 50% time?can calculate likelihood ratio:\\[\n\\frac{L(p = 0.8)}{L(p = 0.5)}\n\\]0.302/0.044 = 6.87. plot, circles show points \nlikelihood curve L(p = 0.5) L(p = 0.8).\nFigure 3.5: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.8.\ncan subjectively interpret likelihood ratio, tells us unfair coin observed data 6.87 times likely hypothesis coin turn heads 80% time, hypothesis fair coin. convincing ? Let’s round likelihood ratio 7, imagine two bags marbles. One bag contains 7 blue marbles. second contains 7 marbles, one different color rainbow, violet, indigo, blue, green, yellow, orange, red. Someone randomly picks one two bags, draws marble, shows . marble blue: certain marble came bag blue marbles, compared bag rainbow coloured marbles? strong likelihood ratio tells us believe data generated unfair coin turns heads 80% time, relative fair coin, given observed 8 heads 10 tosses.Note likelihood ratios give us relative evidence one specified hypothesis, another specified hypothesis. likelihood ratio can calculated two hypothesized values. example, Figure 3.6 , likelihood ratio calculated compares hypothesis fair coin (p = 0.5) alternative hypothesis coin comes heads 80% time (p = 0.8), observed 4 heads 10 coin flips. see observed data 0.2050/0.0055=37.25 times likely (ignoring rounding differences – try calculate numbers hand using formula provided earlier) hypothesis fair coin hypothesis coin turns heads 80% time.\nFigure 3.6: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.4.\nlikelihood ratio 1 means data equally likely hypotheses. Values away 1 indicate data likely one hypothesis . ratio can expressed favor one hypothesis (example L(p = 0.5)/L(p = 0.8) vice versa (L(p = 0.8)/L(p = 0.5). means likelihood ratio 37.25 H0 relative H1 equivalent likelihood ratio 1/37.25 = 0.02685 H1 relative H0. Likelihood ratios range 0 infinity, closer zero infinity, stronger relative evidence one hypothesis . see chapter Bayesian statistics likelihood ratios sense similar (special case ) Bayes Factor.Likelihoods relative evidence. Just data likely one possible value p another value p doesn’t mean data come either two distributions. values might generate even higher likelihood values. example, consider situation flip coin 100 times, observe 50 heads. compare p = 0.3 versus p = 0.8, find likelihood ratio 803462, implying 803461 times evidence data p = 0.3 p = 0.8. might sound pretty conclusive evidence p = 0.3. relative evidence p = 0.3 compared p = 0.8. look likelihood function, clearly see , surprisingly, p = 0.5 value maximizes likelihood function. Just one hypothesis likely another hypothesis, mean third hypothesis even likely.\nFigure 3.7: Computing likelihood ratio p = 0.3 relative p = 0.8 observing p = 0.5 100 coin flips.\n","code":"\nfactorial(10)/(factorial(8)*(factorial(10-8))) * 0.5^8 * (1 - 0.5)^(10-8)\ndbinom(x = 2, size = 10, prob = 0.5)"},{"path":"likelihoods.html","id":"likelihood-of-mixed-results-in-sets-of-studies","chapter":"3 Likelihoods","heading":"3.1 Likelihood of mixed results in sets of studies","text":"Science cumulative process, evaluate lines research, single studies. One big problem scientific literature nonsignificant results often never published (Fanelli, 2010; Franco et al., 2014). time, statistical power hypothesis tests never 100% (often much lower), mathematical reality unlikely (“good true”) set multiple studies yields exclusively significant results. (Francis, 2014; Schimmack, 2012). can use binomial likelihoods examine likely observe mixed results, understand mixed results nevertheless strong evidence presence effect. following largely based Lakens & Etz (2017).probability observing significant nonsignificant result study depends Type 1 error rate (\\(\\alpha\\)), statistical power test (1-\\(\\beta\\)), probability null hypothesis true (Wacholder et al., 2004). four possible outcomes study: true positive, false positive, true negative, false negative. H0 true, probability observing false positive depends \\(\\alpha\\) level Type 1 error rate (e.g., 5%). H1 true, probability observing true positive depends statistical power performed test (often recommended minimum 80%), turn depends \\(\\alpha\\) level, true effect size, sample size. \\(\\alpha\\) level 5%, H0 true, false positive occur 5% probability (long error rates controlled, e.g., preregistered studies) true negative occur 95% probability. test 80% power, H1 true, true positive probability 80%, false negative probability 20%.perform multiple studies, can calculate binomial probability observe specific number significant nonsignificant findings (J. P. Ioannidis & Trikalinos, 2007). can calculate probability finding exactly two significant results three studies assuming null hypothesis true. H0 true, probability significant results equals \\(\\alpha\\) level, thus \\(\\alpha\\) level carefully controlled (e.g., preregistered studies) p = 0.05. k = 2, n = 3, p = .05, binomial probability function tells us probability finding exactly two significant results three studies 0.007 (0.05 × 0.05 × 0.95 = 0.002375, three orders two three results can observed, 0.002375 × 3 = 0.007).calculate likelihood assuming H1 true, need make assumption power study. Let’s provisionally assume studies powered 80% thus p = .80. probability observing exactly two significant results three studies, assuming power 0.8, 0.384 (0.8 × 0.8 × 0.2 = 0.128, three orders two three results can significant, 0.128 × 3 = 0.384). words, set perform 3 studies, hypothesis correct, test hypothesis 80% power, 38.4% probability observing 2 3 significant results, 9.6% probability observe 1 3 significant results (extremely unlucky individual, 0.8% probability finding significant results three studies, even though true effect). Unless power extremely high, mixed results expected sets studies.likelihoods p = .05 p = .80 highlighted Figure 3.8 circles dotted vertical lines.can use likelihood data assuming H0 H1 true calculate likelihood ratio, 0.384/0.007 = 53.89, tells us observed outcome exactly two significant results three studies 53.89 times likely H1 true studies 80% power, H0 true studies carefully controlled 5% Type 1 error rate. Likelihood ratios 8 32 proposed benchmarks moderately strong strong evidence, respectively (Royall, 1997), implies finding two significant results three studies considered strong evidence H1, assuming 80% power. Shiny app perform calculations available .\nFigure 3.8: Computing likelihood ratio 2 three significant results, assuming alpha 5% 80% power.\nsets studies, likelihood ratio favor H1 versus H0 observing mix significant nonsignificant findings can become surprisingly large. Even though evidence appears mixed, actually strong evidence favor true effect. example, researcher performs six studies 80% power 5% level finds three significant outcomes three nonsignificant outcomes, cumulative likelihood ratio convincingly large 38--1 favor H1 consider set studies strong evidence true effect. Intuitively, researchers\nmight feel convinced set studies three six results statistically significant. math, see set studies can strong evidence favor true effect. better understanding probabilities might important step mitigating negative effects publication bias.Hopefully, researchers become inclined submit nonsignificant findings publication better understanding evidential value lines research mixed results. Publishing performed studies lines research reduce publication bias, increase informational value data scientific literature. Expecting studies lines research statistically significant reasonable, important researchers develop realistic expectations draw meaningful inferences lines research. don’t good feeling real patterns studies look like, continuously exposed scientific literature reflect reality. Almost multiple study papers scientific literature present statistically significant results, even though unlikely given power studies, probability study correct predictions (Scheel, Schijen, et al., 2021). Educating researchers binomial probabilities likelihood ratios straightforward way develop realistic expectations research lines contain evidential value favor H1 actually look like.","code":""},{"path":"likelihoods.html","id":"likettest","chapter":"3 Likelihoods","heading":"3.2 Likelihoods for t-tests","text":"far computed likelihoods binomial probabilities, likelihoods can computed statistical model (Glover & Dixon, 2004; Pawitan, 2001). example, can compute relative likelihood observing t-value null alternative hypothesis (Figure 3.9). course, observed data likely assume observed effect equals true effect, examining likelihood reveals many alternative hypotheses relatively likely null hypothesis. also holds observing nonsignificant results, can likely alternative hypothesis interest, null hypothesis. reason incorrect say effect p > \\(\\alpha\\) (see p-value misconception 1).\nFigure 3.9: Likelihood ratio observed t-value H0 H1.\n","code":""},{"path":"likelihoods.html","id":"test-yourself-2","chapter":"3 Likelihoods","heading":"3.3 Test Yourself","text":"Q1: statement correct perform 3 studies?H1 true, alpha = 0.05, power = 0.80, almost likely observe one non-significant results (48.8%) observe significant results (51.2%).alpha = 0.05 power = 0.80, extremely rare find 3 significant results (0.0125%), regardless whether H0 true H1 true.alpha = 0.05 power = 0.80, 2 3 statistically significant results likely outcome overall (38.4%) H1 true.alpha = 0.05 power = 0.80, probability finding least one false positive (significant result H0 true) three studies 5%.Q2: Sometimes lines three studies, ’ll find significant effect one study, effect two related studies. Assume two related studies exactly every way (e.g., changed manipulation, procedure, questions). two studies work minor differences effect fully understand yet. single significant result Type 1 error, H0 true three studies. statement correct, assuming 5% Type 1 error rate 80% power?else equal, probability Type 1 error one three studies 5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies, 80%, substantially likely.else equal, probability Type 1 error one three studies 13.5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect), 9.6%, slightly, substantially less likely.else equal, probability Type 1 error one three studies 85.7% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect) (thus true effect), 0.8%, substantially less likely.possible know probability observe Type 1 error perform 3 studies.idea studies 80% power slightly optimistic. Examine correct answer previous question across range power values (e.g., 50% power, 30% power).Q3: Several papers suggest reasonable assumption power psychological literature might around 50%. Set number studies 4, number successes also 4, assumed power slider 50%, look table bottom app. likely observe 4 significant results 4 studies, assuming true effect?6.25%12.5%25%37.5%Imagine perform 4 studies, 3 show significant result. Change numbers online app. Leave power 50%. output text tells :observed results equally likely H0 H1, likelihood ratio 1. Benchmarks interpret Likelihood Ratios suggest 1<LR<8 weak evidence, 8<LR<32 moderate evidence, LR>32, strong evidence.data likely alternative hypothesis null hypothesis likelihood ratio 526.32These calculations show , assuming observed three significant results four studies, assuming study 50% power, 526 times likely observed data alternative hypothesis true, null hypothesis true. words, 526 times likely find significant effect three studies 50% power, find three Type 1 errors set four studies.Q4: Maybe don’t think 50% power reasonable assumption. low can power (rounded 2 digits), likelihood remain higher 32 favor H1 observing 3 4 significant results?5% power17% power34% power44% powerThe main take home message calculations understand 1) mixed results supposed happen, 2) mixed results can contain strong evidence true effect, across wide range plausible power values. app also tells much evidence, rough dichotomous way, can expect. useful educational goal. want evaluate results multiple studies, formal way performing meta-analysis.calculations make important assumption: Type 1 error rate controlled 5%. try many different tests study, report result yielded p < 0.05, calculations longer hold.Q5: Go back default settings 2 3 significant results, now set Type 1 error rate 20%, reflect modest amount p-hacking. circumstances, highest likelihood favor H1 can get explore possible values true power?Approximately 1Approximately 4.63Approximately 6.70Approximately 62.37As scenario shows, p-hacking makes studies extremely uninformative.\ninflate error rate, quickly destroy evidence data. can longer determine whether data likely effect, effect. Sometimes researchers complain people worry p-hacking try promote better Type 1 error control missing point, things (better measurement, better theory, etc.) important. fully agree aspects scientific research least important better error control. better measures theories require decades work. Better error control can accomplished today, researchers stop inflating error rates flexibly analyzing data. assignment shows, inflated rates false positives quickly make difficult learn true data collect. relative ease part scientific research can improved, can achieve today (decade) think worth stressing importance error control, publish realistic looking sets studies.Q6: ‘prestigious’ journals (, examined terms scientific quality reproducibility, reporting standards, policies concerning data material sharing, quite low quality despite prestige) publish manuscripts large number studies, statistically significant. assume average power psychology 50%, 3.125% 5 study articles contain exclusively significant results. pick random issue prestigious journal, see 10 articles, reporting 5 studies, manuscripts exclusively significant results, trust reported findings , less, articles reported mixed results? ?Q7: Unless power studies 99.99% rest career (slightly inefficient, great don’t like insecurity), observe mixed results lines research. plan deal mixed results lines research?","code":""},{"path":"bayes.html","id":"bayes","chapter":"4 Bayesian statistics","heading":"4 Bayesian statistics","text":"\"Logic!\" said Professor half . \"teach logic schools? three possibilities. Either sister telling lies, mad, telling truth. know tell lies obvious mad. moment unless evidence turns , must assume telling truth.\"Lion, Witch, Wardrobe. Story Children C. S. Lewis.children's book Lion, Witch, Wardrobe, Lucy Edmund go wardrobe country called Narnia. Lucy tells older brother sister, Peter Susan, Narnia, Edmund wants keep secret, tells Peter Susan just pretending. Peter Susan know believe - Narnia exist, ? ask Professor, lives house wardrobe, advice. Professor asks Susan Peter past experience, Lucy Edward truthful, Peter answers \"till now, said Lucy every time.\" Professor replies quote . three possible options, unlikely Lucy lying, done past, Professor says clear just talking Lucy mad. Therefore, likely option Lucy telling truth. new evidence uncovered, beliefs can updated future. approach knowledge generation, prior probability different hypotheses quantified, possible updated light new data, example Bayesian inference.Although frequentist statistics far dominant approach science, important least rudimentary exposure Bayesian statistics statistics training. Bayesian statistics especially useful inferences made cases data investigation unique, frequentist probability defined limit many trials. example, question might often Lucy lies average, whether Lucy lying specific instance existence Narnia. research, often start prior belief hypothesis true. collecting data, can use data update prior beliefs. Bayesian statistics allows update prior beliefs posterior probabilities logically consistent manner. collected data, prior odds Hypothesis 1 (H1) null-hypothesis (H0) P(H1)/P(H0), collected data, posterior odds P(H1|D)/P(H0|D), can read probability H1, given data, divided probability H0, given data. different approaches Bayesian statistics. first discuss Bayes factors, Bayesian estimation.","code":""},{"path":"bayes.html","id":"bayes-factors","chapter":"4 Bayesian statistics","heading":"4.1 Bayes factors","text":"One approach Bayesian statistics focuses comparison different models might explain data (referred model comparison). Bayesian statistics, probability data specified model (D|P(H0) number expressed sometimes referred absolute evidence, formally referred marginal likelihood. marginal likelihood uses prior probabilities average likelihood across parameter space. example, assume simple model M \nbased single parameter, can take two values, X Y, -prior believe probability values p(X) = 0.4 p(Y) = 0.6. collect data, calculate likelihood parameter values, p(D|X) = 0.02 p(D|Y) = 0.08. marginal likelihood model M P(D|M) = 0.4 × 0.02 + 0.6 × 0.08 = 0.056. often, models continuously varying parameters, marginal likelihood formula based integral, idea remains .comparison two models based relative evidence data provides models comparing. relative evidence calculated dividing marginal likelihood one model marginal likelihood another model, ratio relative evidence based marginal likelihoods called Bayes factor. Bayes factors Bayesian equivalent hypothesis tests (Dienes, 2008). Bayes factor represents much updated beliefs, based observing data. can express Bayes factors indicate much likely H1 given data compared H0 (often indicated B10) much likely H0 become compared H1 (B01), B10 = 1/B01. Similar likelihood ratios 1, Bayes factor 1 change beliefs one model compared model. large Bayes factor H1 H0 increased belief H1, Bayes Factor close H1 H0 0 increased belief H0. prior belief H1 , low (e.g., belief unicorns) even large Bayes factor supports presence unicorn might yet convince unicorns real – updated belief unicorns, now believe least likely (even still think unicorns unlikely exist). contribution Bayes Factor prior calculating posterior odds clear following formula:\\[\n\\frac{P(H1|D)}{P(H0|D)} = \\ \\frac{P(D|H1)}{P(D|H0)}\\  \\times \\ \\frac{P(H1)}{P(H0)}\n\\]\\[\nPosterior\\ Probability = \\ Bayes\\ Factor\\  \\times \\ Prior\\ Probability\n\\]Bayesian analysis data requires specifying prior. , continue example based binomial probability, coin flip. likelihood example, compared two point hypotheses (e.g., p = 0.5 vs. p = 0.8). Bayesian statistics, parameters considered random variables, uncertainty degree belief respect parameters quantified probability distributions.binomial probability lies 0 1. draw probability density want 0 1, turn prior, good reasons (simplicity, mostly) beta-prior often used binomial probabilities. shape beta-prior depends two parameters, \\(\\alpha\\) \\(\\beta\\). Note Greek letters used Type 1 error rate Type 2 error rate, purely coincidental! \\(\\alpha\\) \\(\\beta\\) binomial probabilities unrelated error rates, use letters mainly due \nlack creativity among statisticians limited choice alphabet gives us. also help \\(\\beta\\) one parameters Beta distribution. Try keep different Beta’s apart! probability density function :\\[\n\\int_{}^{}{\\left( x,\\ \\alpha,\\ \\beta \\right) = \\ \\frac{1}{B(\\alpha,\\beta)}}x^{\\alpha - 1}{(1 - x)}^{\\beta - 1}\n\\]B(\\(\\alpha\\), \\(\\beta\\)) beta function. Understanding mathematical basis function beyond scope chapter, can read Wikipedia Kruschke's book Bayesian Data Analysis (J. Kruschke, 2014). beta-prior variety values \\(\\alpha\\) \\(\\beta\\) can seen figure .\nFigure 4.1: Four examples Bayesian priors\nbeta densities reflect different types priors. Let’s assume approached street merchant tries sell special coin heads tails , flipped, almost always turn heads. \\(\\alpha\\) = 1, \\(\\beta\\) = 1 prior newborn baby prior, without idea expect flip coin, thus every value p equally likely. \\(\\alpha\\) = 1, \\(\\beta\\) = 1/2 prior true believer prior. sales merchant tells coin turn heads almost every time, thus, believe turn heads almost every time. \\(\\alpha\\) = 4, \\(\\beta\\) = 4, \\(\\alpha\\) = 100, \\(\\beta\\) = 100 priors slightly extremely skeptical people. \\(\\alpha\\) = 4, \\(\\beta\\) = 4 prior, expect coin fair, willing believe wide range true values possible (curve centered 0.5, curve wide, allowing high low values p). \\(\\alpha\\) = 100, \\(\\beta\\) = 100 prior really convinced coins fair, believe slight bias, (curve centered 0.5, skeptic believes p lie 0.4 0.6 – much narrower range compared slightly skeptic individual).Let’s assume newborn baby, true believer, slightly skeptic extreme skeptic buy coin, flip n = 20 times, observe x = 10 heads. outcome can plotted binomial distribution 10 heads 20 trials, Beta(11, 11) distribution.newborn baby prior Beta distribution \\(\\alpha\\) = 1 \\(\\beta\\) = 1, equals binomial likelihood distribution 0 heads 0 trials. posterior Beta distribution Beta(\\(\\alpha\\)*, \\(\\beta\\)*), :\\(\\alpha\\)* = \\(\\alpha\\) + x = 1 + 10= 11\\(\\beta\\)* = \\(\\beta\\) + n – x = 1 + 20 – 10 = 11Or calculating values directly \\(\\alpha\\) \\(\\beta\\) prior \nlikelihood:\\(\\alpha\\)* = \\(\\alpha\\)prior + \\(\\alpha\\)likelihood – 1 = 1 + 11 - 1= 11\\(\\beta\\)* = \\(\\beta\\)prior + \\(\\beta\\)likelihood - 1 = 1 + 11 – 1 = 11Thus, posterior distribution newborn Beta(11,11) distribution. equals binomial likelihood function 10 heads 20 trials, Beta(11,11) distribution. words, posterior distribution identical likelihood function uniform prior used.Take look Figure . Given 10 heads 20 coin flips, see prior distribution newborn (horizontal grey line), likelihood (blue dotted line) posterior (black line).\nFigure 4.2: Four examples different priors updated based data posterior.\ntrue believer posterior distribution centered maximum likelihood observed data, just bit direction prior. slightly skeptic strong skeptic end much stronger belief fair coin observing data, mainly already stronger prior coin fair.","code":""},{"path":"bayes.html","id":"updating-our-belief","chapter":"4 Bayesian statistics","heading":"4.2 Updating our belief","text":"Now distribution prior, distribution posterior, can see graphs values p belief increased. Everywhere black line (posterior) higher grey line (prior) belief p increased.\nFigure 4.3: Plot prior, likelihood, posterior.\nBayes Factor used quantify increase relative evidence. Let’s calculate Bayes Factor hypothesis coin fair newborn. Bayes Factor simply value posterior distribution p = 0.5, divided value prior distribution p = 0.5:BF10 = Beta(p = 0.5, 11, 11)/Beta(p = 0.5, 1, 1) = 3.70/1 = 3.70You can check online Bayes Factor calculator Jeff Rouder Richard Morey. successes, fill 10, trials, fill 20. want calculate Bayes Factor point null value p = 0.5, fill 0.5. \\(\\alpha\\) \\(\\beta\\) prior 1, given newborns prior Beta(1,1). Clicking ‘submit query’ give Bayes factor 3.70.\nFigure 1.7: Screenshot online calculator binomially distributed observations\ncan calculate plot Bayes Factor, show prior (grey), likelihood (dashed blue) posterior (black). example 20 flips, 10 heads, newborn prior, plot looks like :\nFigure 4.4: Plot prior, likelihood, posterior.\nsee newborn, p = 0.5 become probable, p = 0.4. Now let’s assume strong skeptic, believes coin fair prior Beta(100, 100), buys coin flips 100 times. Surprisingly, coin comes heads 90 100 flips. plot prior, likelihood, posterior now looks much extreme, informed prior, extremely different data. see grey prior distribution, dashed blue likelihood based data, posterior distribution black. Bayes Factor 0 represents substantial drop belief coin fair – indeed, now seems untenable hypothesis, even strong skeptic. shows data can update belief. newborn now completely believe true p coin somewhere around 0.9, strong skeptic reason believe p around 0.65, due strong prior conviction coin fair. Given enough data, even strong skeptic become convinced coin return heads time well.\nFigure 4.5: Plot prior, likelihood, posterior.\ncan now also see difference likelihood inference approach, Bayesian inference approach. likelihood inference, can compare different values p likelihood curve (e.g., p = 0.5 vs p = 0.8) calculate likelihood ratio. Bayesian inference, can compare difference prior posterior value p, calculate Bayes Factor.never seen Bayes Factors , might find difficult interpret numbers. guideline (e.g., interpreting effect sizes small, medium, large) criticism use benchmarks. hand, start somewhere getting feel Bayes Factors mean. Bayes factor 1 3 considered ‘worth bare mention’, larger 3 (smaller 1/3) considered ‘substantial’, larger 10 (smaller 1/10) considered ‘strong’. labels refer increase much believe specific hypothesis, posterior belief hypothesis. think extra-sensory perception extremely implausible, single study BF = 14 increase belief, now think extra-sensory perception pretty much extremely implausible.","code":""},{"path":"bayes.html","id":"bayesest","chapter":"4 Bayesian statistics","heading":"4.3 Bayesian Estimation","text":"posterior distribution summarizes belief expected number heads flipping coin seeing data, averaging prior beliefs data (likelihood). mean Beta distribution can calculated \\(\\alpha\\)/(\\(\\alpha\\)+\\(\\beta\\)). can thus easily calculate mean posterior distribution, expected value based prior beliefs data.can also calculate credible interval around mean, Bayesian version confidence interval slightly different interpretation. Instead Frequentist interpretation parameter one (unknown) true value, Bayesian approach considers data fixed, allow parameter vary. Bayesian approaches, probability distributions represent degree belief. calculating credible interval, one saying ‘believe 95% probable (given prior data) true parameter falls within credible interval’. 95% credible interval simply area posterior distribution 0.025 0.975 quantiles.credible interval confidence interval , uniform prior (e.g., Beta(1,1)) used. case, credible interval numerically identical confidence interval. interpretation differs. Whenever informed prior used, credible interval confidence interval differ. chosen prior representative truth, credible interval representative truth, always correct formalization beliefs. single confidence interval, probability contains true population parameter either 0 1. long run 95% confidence intervals contain true population parameter. important differences Bayesian credible intervals Frequentist confidence intervals keep mind.can plot mean posterior 10 heads 20 coin flips observed, given uniform prior.\nFigure 4.6: Plot mean posterior 10 20 heads observed given uniform prior.\ncan also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood (Albers et al., 2018). chapter shows essence Bayesian inference, decide upon prior distribution, collect data calculate marginal likelihood, use calculate posterior distribution. posterior distribution, can estimate mean 95% credible interval. specific hypothesis, can calculate relative evidence posterior model, compared prior model, Bayes Factor. many different flavors Bayesian statistics, disagreements Bayesians best approach statistical inferences , least great disagreements frequentists Bayesians, many Bayesians dislike Bayes factors (McElreath, 2016). example, Bayesians dislike subjective priors used subjective Bayesian analysis, instead prefer known objective Bayesian analysis (Berger & Bayarri, 2004). research, likely need calculations binomial example used , lot Bayesian tests now available free open source software package JASP. math priors become complex, basic idea remains . can use Bayesian statistics quantify relative evidence, can inform much believe, update beliefs, theories.","code":"\nlibrary(binom)\n\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05"},{"path":"bayes.html","id":"test-yourself-3","chapter":"4 Bayesian statistics","heading":"4.4 Test Yourself","text":"Q1: true believer prior Beta(1,0.5). observing 10 heads 20 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(10, 10)Beta(11, 10.5)Beta(10, 20)Beta(11, 20.5)Q2: strong skeptic prior Beta(100,100). observing 50 heads 100 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(50, 50)Beta(51, 51)Beta(150, 150)Beta(151, 151)Copy R script R. script requires 5 input parameters (identical Bayes Factor calculator website used ). hypothesis want examine (e.g., evaluating whether coin fair, p = 0.5), total number trials (e.g., 20 flips), number successes (e.g., 10 heads), \\(\\alpha\\) \\(\\beta\\) values Beta distribution prior (e.g., \\(\\alpha\\) = 1 \\(\\beta\\) = 1 uniform prior). Run script. calculate Bayes Factor, plot prior (grey), likelihood (dashed blue) posterior (black).see newborn, p = 0.5 become probable, p = 0.4.Q3: Change hypothesis first line 0.5 0.675, run script. testing idea coin returns 67.5% heads, statement true?belief hypothesis, given data, decreased.belief hypothesis, given data, stayed .belief hypothesis, given data, increased.Q4: Change hypothesis first line back 0.5. Let’s look increase belief hypothesis p = 0.5 strong skeptic 10 heads 20 coin flips. Change \\(\\alpha\\) prior line 4 100 \\(\\beta\\) prior line 5 100. Run script. Compare Figure R increase belief newborn (plot previous page). statement true?belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, exactly much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, decreased strong skeptic.Copy R script run . script plot mean posterior 10 heads 20 coin flips observed, given uniform prior (4.6) . script also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood.Q5: Assume outcome 20 coin flips 18 heads. Change x 18 line 2 run script. Remember mean prior Beta(1,1) distribution α/(α+β), 1/(1+1) = 0.5. Frequentist mean simply x/n, 18/20=0.9. statement true?frequentist mean higher mean posterior, mean posterior closer mean prior distribution.frequentist mean lower mean posterior, mean posterior closer mean prior distribution.frequentist mean higher mean posterior, mean posterior mean prior distribution.frequentist mean lower mean posterior, mean posterior mean prior distribution.Q6: , today, best estimate probability sun rises every day? Assume born uniform Beta(1,1) prior. sun can either rise, . Assume seen sun every day since born, means continuous string successes every day alive. ok estimate days alive just multiplying age 365 days. best estimate probability sun rise?Q7: best estimate Frequentist perspective?Q8: think goal science ? Rozeboom (1960) criticized Neyman-Pearson hypothesis testing stating:primary aim scientific experiment precipitate decisions, make appropriate adjustment degree one accepts, believes, hypothesis hypotheses tested\".Frick (1996) argued Rozeboom, stating:Rozeboom (1960) suggested scientists making decisions claims, calculating updating probability claims. However, seem practical. handful potential claims given area psychology, feasible assign probabilities, constantly updating probabilities, expect experimenters keep track ever-changing probabilities. fact, just number claims psychology overwhelming. probably impossible human beings keep track probability claim, especially probabilities constantly changing. case, scientists assign probabilities claims. Instead, scientists act like goal science collect corpus claims considered established (Giere, 1972).comes philosophy science, right wrong answers. Reflect 250 words thoughts two goals science outlines Rozeboom Frick, relate philosophy science.","code":"\nH0 <- 0.5 # Set the point null hypothesis you want to calculate the Bayes Factor for\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) #create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior)\nlikelihood <- dbeta(theta, alikelihood, blikelihood)\nposterior <- dbeta(theta, aposterior, bposterior)\n\n# Create plot\nplot(theta, posterior, ylim = c(0, 15), type = \"l\", lwd = 3, xlab = \"p\", ylab = \"Density\", las = 1)\nlines(theta, prior, col = \"grey\", lwd = 3)\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\")\nBF10 <- dbeta(H0, aposterior, bposterior) / dbeta(H0, aprior, bprior)\npoints(H0, dbeta(H0, aposterior, bposterior), pch = 19)\npoints(H0, dbeta(H0, aprior, bprior), pch = 19, col = \"grey\")\nsegments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty = 2)\ntitle(paste(\"Bayes Factor:\", round(BF10, digits = 2)))\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nymax <- 10 # set max y-axis\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) # create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior) # deterine prior distribution\nlikelihood <- dbeta(theta, alikelihood, blikelihood) # determine likelihood distribution\nposterior <- dbeta(theta, aposterior, bposterior) # determine posterior distribution\nplot(theta, posterior, ylim = c(0, ymax), type = \"l\", lwd = 3, xlab = bquote(theta), ylab = \"Density\", las = 1) # draw posterior distribution\nlines(theta, prior, col = \"grey\", lwd = 3) # draw prior distribution\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\") # draw likelihood distribution\nLL <- qbeta(.025, aposterior, bposterior) # calculate lower limit credible interval\nUL <- qbeta(.975, aposterior, bposterior) # calculate upper limit credible interval\nabline(v = aposterior / (aposterior + bposterior)) # draw line mean\nabline(v = LL, col = \"grey\", lty = 3) # draw line lower limit\nabline(v = UL, col = \"grey\", lty = 3) # draw line upper limit\npolygon(c(theta[theta < LL], rev(theta[theta < LL])), c(posterior[theta < LL], rep(0, sum(theta < LL))), col = \"lightgrey\", border = NA)\npolygon(c(theta[theta > UL], rev(theta[theta > UL])), c(posterior[theta > UL], rep(0, sum(theta > UL))), col = \"lightgrey\", border = NA)\ntitle(paste(\"Mean posterior:\", round((aposterior / (aposterior + bposterior)), digits = 5), \", 95% Credible Interval:\", round(LL, digits = 2), \";\", round(UL, digits = 2)))\nif (!require(binom)) {\n  install.packages(\"binom\")\n}\nlibrary(binom)\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)##   method  x  n shape1 shape2 mean     lower     upper  sig\n## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05"},{"path":"questions.html","id":"questions","chapter":"5 Asking Statistical Questions","heading":"5 Asking Statistical Questions","text":"core design new study evaluation information quality: potential particular dataset achieving given analysis goal employing data analysis methods considering given utility\n(Kenett et al., 2016). goal data collection gain information empirical research observations collected analyzed, often statistical models. Three approaches statistical modelling can distinguished Shmueli (2010): Description, explanation, prediction, discussed . utility often depends effects deemed interesting. thorough evaluation informatin quality study therefore depends clearly specifying goal data collection, statistical modelling approach chosen, usefulness data draw conclusions effects interest chosen analysis method. study low information quality might worth performing, data collected low potential achieve analysis goal.","code":""},{"path":"questions.html","id":"description","chapter":"5 Asking Statistical Questions","heading":"5.1 Description","text":"Description aims answer questions features empirical manifestation phenomenon. Description can involve unique events (e.g., case studies single patients), classes events (e.g., patients certain disease). Examples features interest duration (long), quantity (many), location (), etc.example descriptive question research Kinsey, studied sexual behavior experiences Americans time little scientific research available topic. used interviews provided statistical basis draw conclusions sexuality United States, , time, challenged conventional beliefs sexuality.Descriptive research questions answered estimation statistics. informational value estimation study determined amount observations (observations, higher precision estimates) sampling plan (representative sample, lower sample selection bias, increases ability generalize sample population), reliability measure.Descriptive research questions sometimes seen less exciting explanation prediction questions (Gerring, 2012), essential building block theory formation (Scheel, Tiokhin, et al., 2021). Although estimation question often focus mean score measure, accurate estimates variance measure extremely valuable well. variance measure essential information well-informed sample size justification, planning accuracy, performing -priori power analysis.","code":""},{"path":"questions.html","id":"prediction","chapter":"5 Asking Statistical Questions","heading":"5.2 Prediction","text":"goal predictive modeling apply algorithm statistical model predict future observations (Shmueli, 2010). example, COVID-9 pandemic large number models created combined variables estimate risk people infected COVID, people infected experience negative effects health (Wynants et al., 2020). Ideally, goal develop prediction model accurately captures regularities training data, generalizes well unseen data. bias-variance tradeoff two goals, researchers need decide much bias reduced increases variance, vice-versa (Yarkoni & Westfall, 2017). goal prediction minimize prediction error. Common methods evaluate prediction errors cross-validation, example examined whether model developed training dataset generalizes holdout dataset. development prediction models becoming increasingly popular rise machine learning approaches.","code":""},{"path":"questions.html","id":"explanation","chapter":"5 Asking Statistical Questions","heading":"5.3 Explanation","text":"use statistical models concerns tests explanatory theories. case, statistical models used test causal assumptions, explanations derive theories.\nMeehl (1990) reminds us important distinction substantive theory, statistical hypothesis, observations. Statistical inference involved drawing conclusions statistical hypothesis. Observations can lead conclusion statistical hypothesis confirmed (), conclusion directly translate corroboration theory.never test theory isolation, always include auxiliary hypotheses measures instruments used study, conditions realized experiment, ceteris paribus clause assumes things equal. Therefore, never clear failure corroborate theoretical prediction blamed theory auxiliary hypotheses. generate reliable explanatory theories, researchers therefore perform lines research auxiliary hypotheses systematically tested (Tunç & Tunç, 2020).\nFigure 5.1: Distinction theoretical hypothesis, statistical hypothesis, observations. Figure based Meehl, 1990.\n","code":""},{"path":"questions.html","id":"loosening-and-tightening","chapter":"5 Asking Statistical Questions","heading":"5.4 Loosening and Tightening","text":"three questions , can ask questions description, prediction, explanation loosening phase research, tightening phase (Fiedler, 2004). distinction relative. loosening stage, focus creating variation provides source new ideas. tightening stage, selection takes place goal distinguish useful variants less useful variants. descriptive research, unstructured interview aligned loosening phase, structured interview aligned tightening phase. prediction, building prediction model based training set loosening phase, evaluation prediction error holdout dataset tightening phase. explanation, exploratory experimentation functions generate hypotheses, hypothesis tests function distinguish theories make predictions corroborated theories predictions corroborated.important realize whether goal generate new ideas, test new ideas. Researchers often explicit stage research , runs risk trying test hypotheses prematurely (Scheel, Tiokhin, et al., 2021). Clinical trials research explicit different phases research, distinguishes Phase 1, Phase 2, Phase 3, Phase 4 trials. Phase 1 trial researchers evaluate safety new drug intervention small group non-randomized (often healthy) volunteers, examining much drug safe give, monitoring range possible side effects. phase 2 trial often performed patients participants, can focus detail finding definite dose. goal systematically explore range parameters (e.g., intensity stimulus) identify boundary conditions (Dubin, 1969). phase 3 trial large randomized controlled trial goal test effectiveness new intervention practice. Phase 3 trials require prespecified statistical analyses plan strictly controls error rates. Finally, Phase 4 trial examines long term safety generalizability. Compared Phase 3 trial, loosening, researchers explore possibility interactions drugs, moderating effects certain subgroups population. clinical trials, Phase 3 trial requires huge amount preparation, undertaken lightly.\nFigure 5.2: Four phases clinical research. Source.\n","code":""},{"path":"questions.html","id":"three-statistical-philosophies","chapter":"5 Asking Statistical Questions","heading":"5.5 Three statistical philosophies","text":"Royall (1997) distinguishes three questions one can ask:believe, now observation?, now observation?observation tell versus B? (interpret observation evidence regarding versus B?)One useful metaphor think differences look Hinduism, three ways reach enlightenment: Bhakti yoga, Path Devotion, Karma yoga, Path Action, Jnana yoga, Path Knowledge. three corresponding statistical paths Bayesian statistics, focuses updating beliefs, Neyman-Pearson statistics, focuses making decisions act, likelihood approaches, focus quantifying evidence knowledge gained data. Just like Hinduism different paths mutually exclusive, emphasis three yoga's differs individuals, scientists differ emphasis preferred approach statistics.three approaches statistical modelling (description, prediction, explanation) can examined three statistical philosophies (e.g., frequentist estimation, maximum likelihood estimation, Bayesian estimation, Neyman-Pearson hypothesis tests, likelihood ratio tests, Bayes factors). Bayesian approaches start specified prior belief, use data update belief. Frequentist procedures focus methodological procedures allow researchers make inferences control probability error long run. Likelihood approaches focus quantifying evidential value observed data. used knowledgeably, approaches often yield similar inferences (Dongen et al., 2019; Lakens et al., 2020; Tendeiro & Kiers, 2019). Jeffreys (1939), developed Bayesian hypothesis test, noted following comparing Bayesian hypothesis test frequentist methods proposed Fisher:fact struck repeatedly work, led general principles solution problem, find Fisher already grasped essentials brilliant piece common sense, results either identical mine differ cases doubtful. matter fact applied significance tests numerous applications also worked Fisher’s, yet found disagreement actual decisions reached.time, approach based different principles, allows specific inferences. example, Neyman-Pearson approach quantify evidence, Bayesian approach can lead conclusions relative support one another hypothesis, given specified priors, ignoring rate conclusion misleading. Understanding basic principles useful, criticisms statistical practices (e.g., computing p-values) always boil disagreement principles different statistical philosophies built . However, survey literature, rarely see viewpoint approaches statistical inferences, including p values, provide answers specific questions researcher might want ask. Instead, statisticians often engage call statistician’s\nfallacy — declaration believe researchers really “want know” without limiting usefulness preferred statistical question specific context (Lakens, 2021). well-known example statistician’s fallacy provided Cohen (1994) discussing null-hypothesis significance testing:’s wrong NHST? Well, among many things, tell us want know, much want know want know , desperation, nevertheless believe ! want know ‘Given data, probability H0 true?’Different statisticians argue actually \"want know\" posterior probability hypothesis, false-positive risk, effect size confidence interval, likelihood, Bayes factor, severity hypothesis tested. However, choose statistical strategy matches question want ask (Hand, 1994).","code":""},{"path":"questions.html","id":"do-you-really-want-to-test-a-hypothesis","chapter":"5 Asking Statistical Questions","heading":"5.6 Do You Really Want to Test a Hypothesis?","text":"hypothesis test specific answer specific question. can use dart game metaphor question hypothesis test aims answer. essence, dart game hypothesis test methodological procedure make directional prediction: better worse B?, dart game often compare two players, question whether act player best, player B best. hypothesis test, compare two hypotheses, question whether act null hypothesis true, whether alternative hypothesis true.Historically, researchers often interested testing hypotheses examine whether predictions derived scientific theory hold scrutiny. philosophies science () value theories able make predictions. darter wants convince good player, can make prediction (‘next arrow hit bulls-eye’), throw dart, impress hitting bulls-eye. researcher uses theory make prediction, collects data, observes can claim based predefined methodological procedure results confirm prediction, idea impressed predictive validity theory (de Groot, 1969). test supports idea theory useful starting point generate predictions reality. Philosophers science Popper call ‘verisimilitude’– theory way related truth, ‘truth-likeness’.order impressed prediction confirmed, prediction must able wrong. words, theoretical prediction needs falsifiable. predictions concerned presence absence clearly observable entities (e.g., existence black swan) relatively straightforward divide possible states world set predicted theory (e.g., swans white), set predicted theory (e.g., swans can colors white). However, many scientific questions concern probabilistic events single observations contain noise due random variation – rats certain probability develop tumor, people certain probability buy product, particles certain probability appear collision. want forbid certain outcomes test measuring probabilistic events, can divide states world based probability result observed.\nFigure 5.3: fields make black white predictions presence absence obervables, many sciences, predictions probabilistic, shades grey.\nJust hypothesis test can performed, mean interesting. hypothesis test useful 1) data generating models decided plausibility, 2) possible apply informative methodological procedure.First, two competing models good players. Just dart game little interest played Michael van Gerwen (world champion time writing) decide better dart player . Since play darts well, game two us interesting watch. Similarly, sometimes completely uninteresting compare two data generating models, one representing state world effect, another representing state world effect, cases absence effect extremely implausible.Second, hypothesis test interesting need designed informative study. designing study, need able make sure methodological rule provides severe test, likely corroborate prediction correct, time fail corroborate prediction wrong (Mayo, 2018). world champion darts stand 20 inches away dart board can just push dart location want end , possible show lack skill. blindfolded throwing darts 100 feet, possible world champion display skill. hypothesis test, statistical severity test determined error rates. Therefore, researcher needs able adequately control error rates perform test hypothesis high informational value.now hopefully clear hypothesis tests specific tool, answer specific question: applying methodological rule observed data, decision make want make incorrect decisions often? desire use methodological procedure decide competing theories, real reason report results hypothesis test. Even though might feel like test hypothesis research, carefully thinking statistical question want ask might reveal alternative statistical approaches, describing data observed, quantifying personal beliefs hypotheses, reporting relative likelihood data different hypotheses might approach answers question really want know.","code":""},{"path":"effectsize.html","id":"effectsize","chapter":"6 Effect Sizes","heading":"6 Effect Sizes","text":"Effect sizes important statistical outcome empirical studies. Researchers want know whether intervention experimental manipulation effect greater zero, (obvious effect exists) big effect . Researchers often reminded report effect sizes, useful three reasons. First, allow researchers present magnitude reported effects, allow researchers reflect practical significance effects report, addition statistical significance. Second, effect sizes allow researchers draw meta-analytic conclusions comparing standardized effect sizes across studies. Third, effect sizes previous studies can used planning new study -priori power analysis.measure effect size quantitative description strength phenomenon. expressed number scale. unstandardized effect sizes, effect size expressed scale measure collected . useful whenever people able intuitively interpret differences measurement scale. example, children grow average 6 centimeters year age 2 puberty. can interpret 6 centimeters year effect size, many people world intuitive understanding large 6 cm . p-value used make claim whether efect, whether might just looking random variation data, effect size used answer question large effect . makes effect size estimate important complement p-values studies. p-value tells use can claim children grow age; effect sizes tell us size clothes can expect children wear certain age, long take new clothes small.people parts world use metric system, might difficult understand difference 6 cm . facilitate comparison effect sizes across situations different measurement scales used, researchers can report standardized effect sizes. standardized effect size, Cohen's d, computed dividing difference raw scale standard deviation, thus scaled terms variability sample taken. effect d = 0.5 means difference size half standard deviation measure. means effect sizes determined size effect, size standard deviation, difference standardized effect size can caused difference size unstanderdized effect, difference standard deviation.Standardized effect sizes common variables measured scale people familiar , measured different scales within research area. ask people happy , answer ‘5’ mean something different asked people answer scale 1 5 asked answer scale 1 9. Standardized effect sizes can understood compared regardless scale used measure dependent variable. Despite ease use standardized effect size measures, good arguments report interpret unstandardized effect sizes wherever possible (Baguley, 2009).Standardized effect sizes can grouped two families (Rosenthal, 1994): d family (consisting standardized mean differences) r family (measures strength association). Conceptually, d family effect sizes based difference observations, divided standard deviation observations. r family effect sizes describe proportion variance explained group membership [e.g., correlation (r) 0.5 indicates 25% (r2) variance explained difference groups]. effect sizes calculated sum squares (difference individual observations mean group, squared, summed) effect divided sums squares factors design.","code":""},{"path":"effectsize.html","id":"effect-sizes","chapter":"6 Effect Sizes","heading":"6.1 Effect sizes","text":"important outcome empirical study? might tempted say ’s p-value statistical test, given practically always reported articles, determines whether call something ‘significant’ . However, Cohen Cohen (1990) writes 'Things ’ve learned (far)':learned taught primary product research inquiry one measures effect size, p-values.Although want learn data different every study, rarely single thing always want know, effect sizes important part information gain data collection.measure effect size quantitative description strength phenomenon. expressed number scale, scale used depends effect size measure used. unstandardized effect sizes, can use scale people familiar . example, children grow average 6 centimeters year age 2 puberty. can interpret 6 centimeters year effect size. obvious effect size many benefits p-value. p-value gives indication unlikely children stay size become older – effect sizes tell us size clothes can expect children wear certain age, long take new clothes small.One reason report effect sizes facilitate future research. possible perform meta-analysis power analysis based unstandardized effect sizes standard deviation, easier work standardized effect sizes, especially variation measures researchers use. main goal reporting effect sizes reflect question whether observed effect size meaningful. example, might able reliably measure average 19 years olds grow 1 centimeter next year. difference might statistically significant large enough sample, go shopping clothes 19 years old, something need care . look two examples studies looking effect size, addition statistical significance, improved statistical inferences.","code":""},{"path":"effectsize.html","id":"the-facebook-experiment","chapter":"6 Effect Sizes","heading":"6.2 The Facebook experiment","text":"summer 2014 concerns experiment Facebook performed users examine ‘emotional mood contagion’, idea people’s moods can influenced mood people around . can read article . starters, substantial concern ethical aspects study, primarily researchers performed study asked informed consent participants study (), ask permission institutional review board (ethics committee) university.One criticisms study dangerous influence people’s mood. Nancy J. Smyth, dean University Buffalo’s School Social Work wrote Social Work blog: “might even increased self-harm episodes, control anger, dare say , suicide attempts suicides resulted experimental manipulation. experiment create harm? problem , never know, protections human subjects never put place”.Facebook experiment strong effect people’s mood made people commit suicide otherwise committed suicide, obviously problematic. let us look effects manipulation Facebook used people bit closely.article, let’s see researchers manipulated:Two parallel experiments conducted positive negative emotion: One exposure friends’ positive emotional content News Feed reduced, one exposure negative emotional content News Feed reduced. conditions, person loaded News Feed, posts contained emotional content relevant emotional valence, emotional post 10% 90% chance (based User ID) omitted News Feed specific viewing.measured:experiment, two dependent variables examined pertaining emotionality expressed people’s status updates: percentage words produced given person either positive negative experimental period. total, 3 million posts analyzed, containing 122 million words, 4 million positive (3.6%) 1.8 million negative (1.6%).found:positive posts reduced News Feed, percentage positive words people’s status updates decreased B = −0.1% compared control [t(310,044) = −5.63, P < 0.001, Cohen’s d = 0.02], whereas percentage words negative increased B = 0.04% (t = 2.71, P = 0.007, d = 0.001). Conversely, negative posts reduced, percent words negative decreased B = −0.07% [t(310,541) = −5.51, P < 0.001, d = 0.02] percentage words positive, conversely, increased B = 0.06% (t = 2.19, P < 0.003, d = 0.008)., focus negative effects Facebook study (specifically, increase negative words people used) get idea whether risk increase suicide rates. Even though apparently negative effect, easy get understanding size effect numbers mentioned text. Moreover, number posts researchers analyzed really large. large sample, becomes important check size effect finding substantially interesting, large sample sizes even\nminute differences turn statistically significant (look detail ). , need better understanding “effect sizes”.","code":""},{"path":"effectsize.html","id":"the-hungry-judges-study","chapter":"6 Effect Sizes","heading":"6.3 The Hungry Judges study","text":"\nFigure 6.1: Proportion rulings favor prisoners ordinal position. Circled points indicate first decision three decision sessions; tick marks x axis denote every third case; dotted line denotes food break. Danziger, S., Levav, J., & Avnaim-Pesso, L. (2011). Extraneous factors judicial decisions. Proceedings National Academy Sciences, 108(17), 6889–6892. https://doi.org/10.1073/PNAS.1018033108\nsee graphical representation proportion favorable parole decisions real-life judges making function number cases process across day Figure \\(\\ref{fig:hungryjudges}\\)). study mentioned many popular science books example finding shows people always make rational decisions, \"judicial rulings can swayed extraneous variables bearing legal decisions\" (Danziger et al., 2011). see early day, judges start giving 65% people parole, basically means, \"right, can go back society.\" quickly, number favorable decisions decreases basically zero. quick break , authors say, \"may replenish mental resources providing rest, improving mood, increasing glucose levels body\" parole decisions back 65%, quickly drop basically zero. take another break, percentage positive decisions back 65%, drop course day.calculate effect size drop break, next break (Glöckner, 2016), effect represents Cohen's d approximately two, incredibly large. hardly effects psychology large, let along effects mood rest decision making. surprisingly large effect occurs just , three times course day. mental depletion actually huge real-life impact, society basically fall complete chaos just lunch break every day. least, society organized around incredibly strong effect mental depletion. Just like manufacturers take size differences men women account producing items golf clubs watches, stop teaching time lunch, doctors schedule surgery, driving lunch illegal. psychological effect big, don’t need discover publish scientific journal - already know exists.can look meta-meta-analysis (paper meta-analyzes large number meta-analyses literature) Richard, Bond, & Stokes-Zoota (2003) see effect sizes law psychology close Cohen’s d 2. report two meta-analyzed effects slightly smaller. first effect jury’s final verdict likely verdict majority initially favored, 13 studies show effect size r = 0.63, d = 1.62. second jury initially split verdict, final verdict likely lenient, 13 studies show effect size r = .63 well. entire database, effect sizes come close d = 2 finding personality traits stable time (r = 0.66, d = 1.76), people deviate group rejected group (r = .6, d = 1.5), leaders charisma (r = .62, d = 1.58). might notice almost tautological nature effects. , supposedly, effect size passing time (subsequently eating lunch) parole hearing sentencings.see examining size effect can lead us identify findings can caused proposed mechanisms. effect reported hungry judges study must thefore due confound. Indeed, confounds identified, turns ordering cases random, likely cases deserve parole handled first, cases deserve parole handled later (Chatziathanasiou, 2022; Weinshall-Margel & Shapard, 2011). additional use effect sizes identify effect sizes large plausible. Hilgard (2021) proposes build 'maximum positive controls', experimental conditions show largest possible effect provides upper limit plausible effect size measures.","code":""},{"path":"effectsize.html","id":"cohend","chapter":"6 Effect Sizes","heading":"6.4 Standardised Mean Differences","text":"Effect sizes can grouped two families (Rosenthal et al., 2000): d family (based standardized mean differences) r family (based measures strength association). Conceptually, d family effect sizes based comparison difference observations, divided standard deviation observations. means Cohen’s d = 1 means standardized difference two groups equals one standard deviation. size effect Facebook study quantified Cohen’s d. Cohen’s d (d italicized) used describe standardized mean difference effect. value can used compare effects across studies, even dependent variables measured different scales, example one study uses 7-point scales measure dependent variables, study uses 9-point scales. can even compare effect sizes across completely different measures construct, one study uses self-report measure, another study uses physiological measure. Although can compare effect sizes across different measurements, mean comparable, discuss detail section heterogeneity chapter meta-analysis.Cohen’s d ranges 0 infinity. Cohen (1988) uses subscripts distinguish different versions Cohen’s d, practice follow prevents confusion (without specification, Cohen’s d denotes entire family effect sizes). Cohen refers standardized mean difference two groups independent observations sample \\(d_s\\). get statistical details, let’s first visualize Cohen’s d 0.001 (found Facebook study) means. use vizualization http://rpsychologist.com/d3/cohend/, website made Kristoffer Magnusson, allows visualize differences two measurements (increase negative words used Facebook user number positive words timeline reduced). vizualization actually shows two distributions, one dark blue one light blue, overlap much tiny difference distributions visible (click settings button change slider settings, set step size 0.001 reproduce figure online app).\nFigure 6.2: vizualization 2 groups (although difference hardly visible) representing d = 0.001.\nfour numbers distribution express effect size different ways facilitate interpretation. example, probability superiority expresses probability randomly picked observation one group larger score randomly picked observation group. effect small, probability 50.03% - means people experimental write almost number positive negative words people control condition. number needed treat index illustrates Facebook study person needs type 3570 words observe one additional negative word, compared control condition. know often type many words Facebook, think can agree effect noticeable individual level.understand Cohen’s d two independent groups calculated, let’s first look formula t-statistic:\\[\nt = \\frac{{\\overline{M}}_{1}{- \\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}} \\times \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\n\\]\\({\\overline{M}}_{1}{- \\overline{M}}_{2}\\) difference means, \\(\\text{SD}_{\\text{pooled}}\\) pooled standard deviation (Lakens, 2013), n1 n2 sample sizes two groups compared. t-value used determine whether difference two groups t-test statistically significant (eexplained chapter p-values. formula Cohen’s d_ similar:\\[d_s = \\frac{{\\overline{M}}_{1}{-\\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}}}\\]can see, sample size group (n1 n2) part formula t-value, part formula Cohen’s d (pooled standard deviation computed weighing standard deviation group sample size, cancels groups equal size). distinction useful know, practice means t-value (consequently, p-value) function sample size, Cohen's d independent sample size. true effect (e.g., non-zero effect size population) t-value null-hypothesis test effect zero average become larger (p-value become smaller) sample size increases. effect size, however, increase decrease, become accurate, standard error decreases sample size increases. also reason p-values can used make statement whether effect practically significant, effect size estimates often important complement p-values making statistical inferences.can calculate Cohen’s d independent groups independent samples t-value (can often convenient result section paper reading report effect sizes) :\\[d_s = t ⨯ \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}\\]d = 001 extremely tiny effect, explore effect size bit representative read literature. meta-meta-analysis mentioned earlier, median effect size published studies included meta-analyses psychological literature d = 0.43 (Richard et al., 2003). get feeling effect size, use online app set effect size d = 0.43.\nFigure 6.3: vizualization 2 groups representing d = 0.43.\nOne example meta-analytic effect size meta-meta-analysis exactly \\(d_s\\) = 0.43 finding people group work less hard achieve goal people work individually, called social loafing. effect large enough notice daily life. Yet, look overlap two distributions, see amount effort people put overlaps considerably two conditions (case social loafing, working individually versus working group). see Figure 6.3 probability superiority, probability randomly draw one person group condition one person individual condition, person working group puts less effort, 61.9%. interpretation differences groups also called common language effect size (McGraw & Wong, 1992).\nFigure 6.4: vizualization 2 groups representing d = 2.\nBased data, difference height 21-year old men women Netherlands approximately 13 centimeters (unstandardized effect size), standardized effect size \\(d_s\\) = 2. pick random man random woman walking street hometown Rotterdam, likely man taller woman? see quite likely, probability superiority 92.1%. even huge effect, still considerable overlap two distributions. conclude length people one group larger length people another group, mean everyone one group larger everyone group.Sometimes try explain scientific findings birthday party, skeptical aunt uncle might remark 'well believe true never experience '. probabilistic observations, distribution observed effects. example social loafing, average people put less effort achieve goal working group working . individual population, effect might larger, smaller, absent, even opposite direction. skeptical aunt uncle never experience finding, contradict effect real average population. Indeed, even expected effect people population, least time. Although might exceptions (e.g., almost every individual experience Stroop effect, slower naming ink color color words color word incongruent ink written , congruent ink written ), many effects smaller, sufficient variation, effect present everyone population.Conceptually, calculating Cohen’s d within-subjects comparisons based idea independent groups, differences two observations divided standard deviation within groups observations. However, case correlated samples common standardizer standard deviation difference scores. Testing whether two correlated means significantly different paired samples t-test testing whether difference scores correlated means signicantly different 0 one-sample t-test. Similarly, calculating effect size difference two correlated means similar effect size calculated one sample t-test. standardized mean difference effect size within-subjects designs referred Cohen’s \\(d_z\\), z alludes fact unit analysis longer x y, difference, z, can calculated :\\[d_z = \\frac{M_{dif}}{\\sqrt{\\frac{\\sum{({X_{dif}-M_{dif})}}^2}{N-1}}}\\]\neffect size estimate Cohen’s dz can also calculated directly t-value number participants using formula:\\[d_z = \\frac{t}{\\sqrt{n}}\\]Given direct relationship t-value paired-samples t-test Cohen’s \\(d_z\\), surprising software performs power analyses within-subjects designs (e.g., G*Power) relies Cohen’s \\(d_z\\) input.Maxwell & Delaney (2004) remark: ‘major goal developing effect size measures provide standard metric meta-analysts others can interpret across studies vary dependent variables well types designs.’ Cohen's \\(d_z\\) takes correlation dependent measures account, can directly compared Cohen's \\(d_s\\). researchers prefer use average standard deviation groups observations standardizer (ignores correlation observations), allows direct comparison Cohen’s \\(d_s\\). effect size referred Cohen’s \\(d_av\\) (Cumming, 2013), simply:\\[d_{av} = \\frac{M_{dif}}{\\frac{SD_1+SD_2}{2}}\\]","code":""},{"path":"effectsize.html","id":"interpreting-effect-sizes","chapter":"6 Effect Sizes","heading":"6.5 Interpreting effect sizes","text":"commonly used interpretation Cohen’s d refer effect sizes small (d = 0.2), medium (d = 0.5), large (d = 0.8) based benchmarks suggested Cohen (1988). However, values arbitrary used. practice, see used form circular reasoning: effect small, d = 0.2, d = 0.2 small. see using benchmarks adds nothing, beyond covering fact actually interpret size effect. Furthermore, benchmarks 'medium' 'large' effect even correspond Cohen's d r (explained McGrath & Meyer (2006),bsee 'Test ' Q12). verbal classification based benchmarks ignores fact effect can practically meaningful, intervention leads reliable reduction suicide rates effect size d = 0.1. cases, effect size d = 0.1 might consequence , example effect smaller just noticeable difference, therefore small noticed individuals real world.","code":""},{"path":"effectsize.html","id":"correlations-and-variance-explained","chapter":"6 Effect Sizes","heading":"6.6 Correlations and Variance Explained","text":"r family effect sizes based proportion variance explained group membership (e.g., correlation r = 0.5 indicates 25% (r2) variance explained difference groups). might remember r used refer correlation. correlation two continuous variables can range 0 (completely unrelated) 1 (perfect positive relationship) -1 (perfect negative relationship). get better feel correlations, play game guess correlation see scatterplot, guess correlation variables (see figure 6.5.\nFigure 6.5: Screenshot Guess Correlation game (correct answer r = 0.24).\nr family effect sizes calculated sum squares (difference individual observations mean group, squared, summed) effect divided sums squares factors design. Earlier, mentioned median effect size psychology \\(d_s\\) = 0.43. However, authors actually report results correlation, r = 0.21. can convert Cohen's d r (take care applies \\(d_s\\), \\(d_z\\)):\\[r = \\frac{d_s}{\\sqrt{{d_s^{2}}^{+}\\frac{N^{2} - 2N}{n_{1} \\times n_{2}}}}\\]N total sample size groups, whereas n1 n2 sample sizes individual groups comparing (common use capital N total sample size, lowercase n sample sizes per group). can go http://rpsychologist.com/d3/correlation/ look good visualization proportion variance explained group membership, relationship r r2. amount variance explained often quite small number, see Figure 6.6 correlation 0.21 (median meta-meta-analysis Richard colleagues) see proportion variance explained 4.4%. Funder Ozer (2019) warn misinterpreting small values variance explained indication effect meaningful (even consider practice squaring correlation \"actively misleading\").\nFigure 6.6: Screenshot correation effect size vizualization Kristoffer Magnusson r - 0.21.\nseen , can useful interpret effect sizes identify effects practically insignificant, effects implausibly large. Let’s take look study examines number suicides function amount country music played radio. can find paper won Ig Nobel prize studies first make laugh, think, although case, study make think country music, importance interpreting effect sizes.authors predicted following:contend themes found country music-foster suicidal mood among people already risk suicide thereby associated high suicide rate.collected data:sample comprised 49 large metropolitan areas data music available. Exposure country music measured proportion radio airtime devoted country music. Suicide data extracted annual Mortality Tapes, obtained Inter-University Consortium Political Social Research (ICPSR) University Michigan. dependent variable number suicides per 100,000 population.concluded:significant zero-order correlation found white suicide rates country music (r = .54, p < .05). greater airtime given country music, greater white suicide rate.can compare size effects known effects psychology. database Richard colleagues, effects large, examples leaders effective charisma (r = 0.54), good leader–subordinate relations promote subordinate satisfaction (r = 0.53), people can recognize emotions across cultures (r = 0.53). effects large obvious, raise doubts whether relationship listening country music suicides can size. country music really bad? search literature, find researchers able reproduce analysis original authors. likely results spurious, Type 1 error.Eta squared \\(\\eta^2\\) (part r family effect sizes, extension r can used two sets observations) measures proportion variation Y associated membership different groups deﬁned X, sum squares effect divided total sum squares:\\[\\eta^{2}$ = $\\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\\]\\(\\eta^2\\) .13 means 13% total variance can accounted group membership. Although \\(\\eta^2\\) efficient way compare sizes effects within study (given every effect interpreted relation total variance, \\(\\eta^2\\) single study sum 100%), eta squared easily compared studies, total variability study (\\(SS_total\\)) depends design study, increases additional variables manipulated (e.g., independent variables added). Keppel (1991) recommended partial eta squared (\\(\\eta_{p}^{2}\\)) improve comparability\neffect sizes studies, expresses sum squares effect relation sum squares effect sum squares error associated effect. Partial eta squared calculated :\\[\\eta_{p}^{2} = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{effect}} + \\text{SS}_{\\text{error}}}\\]designs fixed factors (manipulated factors, factors exhaust levels independent variable, alive vs. dead), designs measured factors covariates, partial eta squared can computed F-value degrees freedom (Cohen, 1988):\\[\\eta_{p}^{2} = \\frac{F \\times \\text{df}_{\\text{effect}}}{{F \\times \\text{df}}_{\\text{effect}} + \\text{df}_{\\text{error}}}\\]example, F(1, 38) = 7.21, \\(\\eta_{p}^{2}\\) = 7.21 ⨯ 1/(7.21 ⨯ 1 +\n38) = 0.16.Eta squared can transformed Cohen’s d:d = 2\\(\\times f\\) \\(f^{2} = \\eta^{2}/(1 - \\eta^{2})\\)","code":""},{"path":"effectsize.html","id":"correcting-for-bias","chapter":"6 Effect Sizes","heading":"6.7 Correcting for Bias","text":"Population effect sizes almost always estimated basis samples, measure population effect size estimate based sample averages, Cohen’s d slighlty overestimates true population effect. Cohen’s d refers population, Greek letter δ typically used. Therefore, corrections bias used (even though corrections always lead completely unbiased effect size estimate). d family effect sizes, correction bias population effect size estimate Cohen’s δ known Hedges’ g (although different people use different names – d_unbiased also used). correction bias noticeable small sample sizes, since often use software calculate effect sizes anyway, makes sense always report Hedge’s g instead Cohen’s d (Thompson, 2007).Cohen’s d, \\(\\eta^2\\) biased estimate true effect size population. Two less biased effect size estimates proposed, epsilon squared \\(\\varepsilon^{2}\\) omega squared \\(\\omega^{2}\\). practical purposes, two effect sizes correct bias equally well (Albers & Lakens, 2018; Okada, 2013), preferred \\(\\eta^2\\). Partial epsilon squared (\\(\\varepsilon_{p}^{2}\\)) partial omega squared (\\(\\omega_{p}^{2}\\)) can calculated based F-value degrees freedom.\\[\n\\omega_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}} + 1}{\\text{df}_{\\text{effect}}}}\n\\]\\[\n\\varepsilon_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}}}{\\text{df}_{\\text{effect}}}}\n\\]\nPartial effect sizes \\(\\eta_{p}^{2}\\), \\(\\varepsilon_{p}^{2}\\)) \\(\\omega_{p}^{2}\\) can generalized across different designs. reason, generalized eta-squared (\\(\\eta_{G}^{2}\\)) generalized omega-squared (\\(\\omega_{G}^{2}\\)) proposed (Olejnik & Algina, 2003), although popular. part, might summarizing effect size ANOVA design single index limitations, perhaps makes sense describe pattern results, see section .","code":""},{"path":"effectsize.html","id":"effect-sizes-for-interactions","chapter":"6 Effect Sizes","heading":"6.8 Effect Sizes for Interactions","text":"effect size used power analyses ANOVA designs Cohen's f. two independent groups, Cohen's \\(f\\) = 0.5 * Cohen's d. two groups, Cohen's f can converted eta-squared back \\(f = \\frac{\\eta^2}{(1 - \\eta^2)}\\) \\(\\eta^2 = \\frac{f^2}{(1 + f^2)}\\). predicting interaction effects ANOVA designs, planning study based expected effect size \\(\\eta_{p}^{2}\\) Cohen's f might intuitive approach.start effect size simple two group comparison, ex effect size interaction effect ANOVA. assume predict mean difference 1, know standard deviation measure 2. means standardized effect size d = 0.5. independent t-test mathematically identical F-test two groups. F-test, effect size used power analyses Cohen's f, calculated based standard deviation population means divided population standard deviation (know measure 2), :\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma}\n\\end{equation}\\]\nequal sample sizes\n\\[\\begin{equation}\n\\sigma _{ m } = \\sqrt { \\frac { \\sum_ { = 1 } ^ { k } ( m _ { } - m ) ^ { 2 } } { k } }.\n\\end{equation}\\]formula m grand mean, k number means, m_i mean group. formula might look bit daunting, calculating Cohen's f difficult two groups.take expected means 0 1, standard deviation 2, grand mean (m formula ) (0 + 1)/2 = 0.5. formula says subtract grand mean mean group, square value, sum . (0-0.5)^2 (1-0.5)^2, 0.25. sum values (0.25 + 0.25 = 0.5), divide number groups (0.5/2 = 0.25) take square root, find \\(\\sigma_{ m }\\) = 0.5. can now calculate Cohen's f (using \\(\\sigma\\) = 2 measure):\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma} = \\frac{0.5}{2} = 0.25\n\\end{equation}\\]confirm two groups Cohen's f half large Cohen's d.Now basis look interaction effects. Different patterns means ANOVA can Cohen's f. two types interactions, visualized Figure 6.7. ordinal interaction, mean one group (\"B1\") always higher mean group (\"B2\"). Disordinal interactions also known 'cross-' interactions, occur group larger mean switches . difference important, since disordinal interaction Figure 6.7 larger effect size ordinal interaction.\nFigure 6.7: Schematic illustration disordinal (cross-) ordinal interaction\nMathematically interaction effect computed cell mean minus sum grand mean, marginal mean condition one factor minus grand mean, marginal mean condition factor minus grand mean (Maxwell & Delaney, 2004).consider two cases, one perfect disordinal interaction (means 0 1 flip around condition, 1 0) ordinal interaction (effect present one condition, means 0 1, disappears condition, means 0 0, see Figure 6.8).\nFigure 6.8: Disordinal (cross-) ordinal interaction means 0 1, n = 50 per group, sd 2.\ncan calculate interaction effect follows (go steps detail). First, look disordinal interaction. grand mean (1 + 0 + 0 + 1) / 4 = 0.5.can compute marginal means A1, A2, B1, B2, simply averaging per row column, gets us A1 row (1+0)/2=0.5. perfect disordinal interaction, marginal means 0.5. means main effects. main effect factor (marginal means A1 A2 exactly 0.5), main effect B.can also calculate interaction effect. cell take value cell (e.g., a1b1 1) compute difference cell mean additive effect two factors :1 - (grand mean 0.5 + (marginal mean a1 minus grand mean, 0.5 - 0.5 = 0) + (marginal mean b1 minus grand mean, 0.5 - 0.5 = 0)). Thus, cell get:a1b1: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5a1b2: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b1: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b2: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.5^2 +-0.5^2 + -0.5^2 + 0.5^2 } { 4 } }}{ 2 } = 0.25\\)ordinal interaction grand mean (1 + 0 + 0 + 0) / 4, 0.25. marginal means a1: 0.5, a2: 0, b1: 0.5, b2: 0.Completing calculation four cells ordinal interaction gives:a1b1: 1 - (0.25 + (0.5 -0.25) + (0.5 -0.25)) = 0.25a1b2: 0 - (0.25 + (0.5 -0.25) + (0.0 -0.25)) = -0.25a2b1: 0 - (0.25 + (0.0 -0.25) + (0.5 -0.25)) = -0.25a2b2: 0 - (0.25 + (0.0 -0.25) + (0.0 -0.25)) = 0.25Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.25^2 +-0.25^2 + -0.25^2 + 0.25^2 } { 4 } }}{ 2 } = 0.125\\).see effect size cross-interaction (f = 0.25) twice large effect size ordinal interaction (f = 0.125). make sense think interaction test contrasts. disordinal interaction comparing cells a1b1 a2b2 a1b2 a2b1, (1+1)/2 vs. (0+0)/2. Thus, see t-test contrast, see mean difference 1. ordinal interaction, (1+0)/2 vs. (0+0)/2, mean difference halved, namely 0.5. obviously matters statistical power examine interaction effects experiments.Just stating expect 'medium' Cohen's f effect size interaction effect power analysis best approach. Instead, start thinking pattern means standard deviations (within factors, correlation dependent variables) compute effect size data pattern. prefer hand, can use Superpower (Lakens & Caldwell, 2021). also holds complex designs, multilevel models. cases, often case power analyses easier based simulation-based approaches, based plugging single effect size power analysis software (DeBruine & Barr, 2021).","code":""},{"path":"effectsize.html","id":"test-yourself-4","chapter":"6 Effect Sizes","heading":"6.9 Test Yourself","text":"Q1: One largest effect sizes meta-meta analysis Richard colleagues 2003 people likely perform action feel positively action believe common. effect (respect researchers contributed research meta-analysis) somewhat trivial. Even , correlation r = .66, equals Cohen’s d 1.76. according online app https://rpsychologist.com/cohend/ probability superiority effect size?70.45%\n88.12%\n89.33%\n92.14%Q2: Cohen’s d ______ eta-squared ________r; epsilon-squared\nHedges’ g; omega-squared\nCohen’s d_s; generalized eta-squaredQ3: correlation r = 1.2 :Impossible\nImplausibly large effect size social sciences\nline median effect size psychologyQ4: Let’s assume difference two means observe 1, pooled standard deviation also 1. , average, happens t-value Cohen’s d, simulate studies, function sample size simulations?Given mean difference standard deviation, sample size becomes bigger, t-value become larger, Cohen’s d becomes larger.Given mean difference standard deviation, sample size becomes bigger, t-value gets closer true value, Cohen’s d becomes larger.Given mean difference standard deviation, sample size becomes bigger, t-value become larger, Cohen’s d gets closer true value.Given mean difference standard deviation, sample size becomes bigger, t-value gets closer true value, Cohen’s d gets closer true value.Q5: Go http://rpsychologist.com/d3/correlation/ look good visualization proportion variance explained group membership, relationship r r2. Look scatterplot shared variance effect size r = .21 (Richard, Bond, & Stookes-Zoota, 2003).Given r = 0.21 average effect size psychological research, much variance data average explain?2.1%21%4.4%44%Q6: default, sample size online correlation visualization linked 50. Click cogwheel access settings, change 500. happens?proportion explained variance 5 times large.proportion explained variance 5 times small.proportion explained variance 52 times large.proportion explained variance stays .Q7: old paper find statistical result reported t(36) = 2.14 p < 0.05 independent t-test without reported effect size. Using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Independent t -t Mean Differences dropdown menu) MOTE R function d.ind.t.t, effect size Cohen’s d effect, given 38 participants (e.g., 18 group, leading N – 2 = 36 degrees freedom) alpha level 0.05?d = 0.38d = 0.41d = 0.71d = 0.75Q8: old paper find statistical result 2x3 subjects ANOVA reported F(2, 122) = 4.13, p < 0.05, without reported effect size. Using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Eta – F Variance Overlap dropdown menu) MOTE R function eta.F, effect size partial eta-squared?\\(\\eta_p^2\\) = 0.063\\(\\eta_p^2\\) = 0.996\\(\\eta_p^2\\) = 0.032\\(\\eta_p^2\\) = 0.049Q9: realize computing omega-squared corrects bias eta-squared. old paper F(2, 122) = 4.13, p < 0.05, using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Omega – F Variance Overlap dropdown menu) MOTE R function omega.F, effect size partial omega-squared? HINT: total sample size df error + k, k number groups (6 2x3 ANOVA).\\(\\eta_p^2\\) = 0.047\\(\\eta_p^2\\) = 0.749\\(\\eta_p^2\\) = 0.032\\(\\eta_p^2\\) = 0.024Q10: Several times chapter effect size Cohen’s d converted r, vice versa. can use effectsize R package (can also used compute effect sizes analyze data R) convert median r = 0.21 observed Richard colleagues’ meta-meta-analysis d: effectsize::r_to_d(0.21) yields d = 0.43 (conversion assumes equal sample sizes group). Cohen’s d corresponds r = 0.1?d = 0.05d = 0.10d = 0.2d = 0.3Q11: can useful convert effect sizes r performing meta-analysis effect sizes included based mean differences. Using d_to_r function effectsize package, d = 0.8 correspond ?r = 0.30r = 0.37r = 0.50r = 0.57Q12: questions 10 11 might noticed something peculiar. benchmarks typically used ‘small’, ‘medium’, ‘large’ effects Cohen’s d d = 0.2, d = 0.5, d = 0.8, correlation r = 0.1, r = 0.3, r = 0.5. Using d_to_r function effectsize package, check see whether benchmark ‘large’ effect size correspond d r.McGrath & Meyer (2006) write: “Many users Cohen’s (1988) benchmarks seem unaware correlation coefficient d strictly equivalent, Cohen’s generally cited benchmarks correlation intended infrequently used biserial correlation rather point biserial.”Download paper McGrath Meyer, 2006 (can find links pdf ), page 390, right column, read solution authors prefer.Just stop using silly benchmarks., honestly, just really stop using silly benchmarks.benchmarks d need changed 0.20, 0.67, 1.15The benchmarks correlations need changed .10, .24, .37","code":""},{"path":"confint.html","id":"confint","chapter":"7 Confidence Intervals","heading":"7 Confidence Intervals","text":"report point estimates, acknowledge quantify uncertainty estimates. Confidence intervals provide way quantify precision estimate. reporting estimate confidence interval, results reported within range value contain true value parameter desired percentage. example, report effect size estimate 95% confidence interval, expectation interval wide enough 95% time range values around estimate contains true parameter value.","code":""},{"path":"confint.html","id":"population-vs.-sample","chapter":"7 Confidence Intervals","heading":"7.1 Population vs. Sample","text":"statistics, differentiate population sample. population everyone interested , people world, elderly depressed, people buy innovative products. sample everyone able measure population interested . similarly distinguish parameter statistic. parameter characteristic population, statistic characteristic sample. Sometimes, data entire population. example, measured height people ever walked moon. can calculate average height twelve individuals, know true parameter. need inferential statistics. However, know average height people ever walked earth. Therefore, need estimate parameter, using statistic based sample. Although rare study includes entire population, impossible, illustrated Figure 7.1.\nFigure 7.1: Example registry-based study entire population included study. https://doi.org/10.1093/ije/dyab066\nentire population measured need perform hypothesis test. , population generalize (although possible argue still making inference, even entire population observed, observed metaphorical population one many possible worlds, see Spiegelhalter (2019)). data entire population collected population effect size known, confidence interval compute. total population size known, measured completely, confidence interval width shrink zero closer study gets measuring entire population. known finite population correction factor variance estimator (Kish, 1965). variance sample mean \\(\\sigma^2/n\\), finite populations multiplied finite population correction factor standard error:\n\\[FPC = \\sqrt{\\frac{(N - n)}{(N-1)}}\\]\nN size population, n size sample. N much larger n, correction factor close 1 (therefore correction typically ignored populations large, even populations finite), noticeable effect variance. total population measured correction factor 0, variance becomes 0 well. example, total population consists 100 top athletes, data collected sample 35 athletes, finite population correction \\(\\sqrt{(100 - 35)/(100-1)}\\) = 0.81. superb R package can compute population corrected confidence intervals (Cousineau & Chiasson, 2019).","code":""},{"path":"confint.html","id":"what-is-a-confidence-interval","chapter":"7 Confidence Intervals","heading":"7.2 What is a Confidence Interval?","text":"Confidence intervals statement percentage confidence intervals contain true parameter value. behavior confidence intervals nicely visualized website Kristoffer Magnusson: http://rpsychologist.com/d3/CI/. Figure 7.2 see blue dots represent means sample, fall around red vertical line, represents true value parameter population. Due variation sample, estimates fall red line. horizontal lines around blue dots confidence intervals. default, visualization shows 95% confidence intervals. lines black (means confidence interval overlaps red line indication true population value), red (indicating capture true population value). long run, 95% horizontal bars black, 5% red.\nFigure 7.2: Series simulated point estimates confidence intervals\ncan now see meant sentence “Confidence intervals statement percentage confidence intervals contain true parameter value“. long run, 95% samples, red line (population parameter) contained within 95% confidence interval around sample mean, 5% confidence intervals true. see turn formula confidence intervals, width confidence interval depends sample size standard deviation. larger sample size, smaller confidence intervals.","code":""},{"path":"confint.html","id":"relatCIp","chapter":"7 Confidence Intervals","heading":"7.3 The relation between confidence intervals and p-values","text":"direct relationship CI around effect size statistical significance null-hypothesis significance test. example, effect statistically significant (p < 0.05) two-sided independent t-test alpha .05, 95% CI mean difference two groups include zero. Confidence intervals sometimes said informative p-values, provide information whether effect statistically significant (.e., confidence interval overlap value representing null hypothesis), also communicate precision effect size estimate. true, mentioned chapter p-values still recommended add exact p-values, facilitates re-use results secondary analyses (Appelbaum et al., 2018), allows researchers compare p-value alpha level preferred use (Lehmann & Romano, 2005).order maintain direct relationship confidence interval p-value necessary adjust confidence interval level whenever alpha level adjusted. example, alpha level 5% corrected three comparisons 0.05/3 - 0.0167, corresponding confidence interval 1 - 0.0167 = 0.9833 confidence interval. Similarly, p-value computed one-sided t-test, upper lower limit interval, end interval ranges −∞ ∞.maintain direct relationship F-test confidence interval, 90% CI effect sizes F-test provided. reason explained Karl Wuensch. Cohen’s d can take positive negative values, r² η² squared, can therefore take positive values. related fact F-tests (commonly used ANOVA) one-sided. calculate 95% CI, can get situations confidence interval includes 0, test reveals statistical difference p < .05 (mathematical explanation, see Steiger (2004)). means 95% CI around Cohen's d independent ttest equals 90% CI around η² exactly test performed ANOVA. final detail, eta-squared smaller zero, lower bound confidence interval can smaller 0. means confidence interval effect statistically different 0 start 0. report CI 90% CI [.00; .XX] XX upper limit CI.Confidence intervals often used forest plots communicate results meta-analysis. plot , see 4 rows. row shows effect size estimate one study (Hedges’ g). example, study 1 yielded effect size estimate 0.44, confidence interval around effect size 0.08 0.8. horizontal black line, similarly visualization played around , width confidence interval. touch effect size 0 (indicated black vertical line) effect statistically significant.\nFigure 7.3: Meta-analysis 4 studies\ncan see, based fact confidence intervals overlap 0, studies 1 3 statistically significant. diamond shape next FE model (Fixed Effect model) meta-analytic effect size. Instead using black horizontal line, upper limit lower limit confidence interval indicated left right points diamond, center diamond meta-analytic effect size estimate. meta-analysis calculates effect size combining weighing studies. confidence interval meta-analytic effect size estimate always narrower single study, combined sample size studies included meta-analysis.preceding section, focused examining whether confidence interval overlapped 0. confidence interval approach null-hypothesis significance test. Even though computing p-value, can directly see confidence interval whether p < \\(\\alpha\\). confidence interval approach hypothesis testing makes quite intuitive think performing tests non-zero null hypotheses (Bauer & Kieser, 1996). example, test whether can reject effect 0.5 examining 95% confidence interval overlap 0.5. can test whether effect smaller 0.5 examining 95% confidence interval falls completely 0.5. see leads logical extension null-hypothesis testing , instead testing reject effect 0, can test whether can reject effects interest range predictions equivalence tests.","code":""},{"path":"confint.html","id":"the-standard-error-and-95-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.4 The Standard Error and 95% Confidence Intervals","text":"calculate confidence interval, need standard error. standard error (SE) estimates variability sample means obtained taking several measurements population. easy confuse standard deviation, degree individuals within sample differ sample mean. Formally, statisticians distinguish σ \\(\\widehat{\\sigma}\\), hat means value estimated sample, lack hat means population value – ’ll leave hat, even ’ll mostly talk estimated values based sample formulas . Mathematically (σ standard\ndeviation),\\[\nStandard \\ Error \\ (SE) = \\sigma/\\sqrt n\n\\]standard error sample tend zero increasing sample size, estimate population mean become accurate. standard deviation sample become similar population standard deviation sample size increases, become smaller. standard deviation statistic descriptive sample, standard error describes bounds random sampling process.Standard Error used construct confidence intervals (CI) around sample estimates, mean, differences means, whatever statistics might interested . calculate confidence interval around mean (indicated Greek letter mu: μ), use t distribution corresponding degrees freedom (df : one-sample t-test, degrees freedom n-1):\\[\n\\mu \\pm t_{df, 1-(\\alpha/2)} SE\n\\]95% confidence interval, α = 0.05, thus critical t-value degrees freedom 1- α /2, 0.975th quantile calculated. Remember t-distribution slightly thicker tails Z-distribution. 0.975th quantile Z-distribution 1.96, value t-distribution example df = 19 2.093. value multiplied standard error, added (upper limit confidence interval) subtracted (lower limit confidence interval) mean.","code":""},{"path":"confint.html","id":"overlapping-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.5 Overlapping Confidence Intervals","text":"Confidence intervals often used plots. Figure 7.4 , three estimates vizluaized (dots), surrounded three lines (95% confidence intervals). left two dots (X Y) represent means independent groups X Y scale 0 8 (see axis 0-8 left side plot). dotted lines two confidence intervals visualize overlap confidence intervals around means. two confidence intervals around means columns X Y commonly shown figure scientific article. third dot, slightly larger, mean difference X Y, slightly thicker line visualizes confidence interval mean difference. difference score expressed using axis right (-3 5). plot , mean group X 3, mean group Y 5.6, difference 2.6. plot based 50 observations per group, confidence interval around mean difference ranges 0.49 4.68, quite wide.\nFigure 7.4: Means 95% confidence intervals two independent groups mean difference 95% confidence interval.\nmentioned earlier, 95% confidence interval contain 0, effect statistically different 0. Figure 7.4 mean difference 95% confidence interval around indicaed 'difference' label. 95% confidence interval contain 0, t-test significant alpha 0.05. p-value indicated plot 0.016. Even though two means differ statistically , confidence interval around mean overlap. One might intuitively believe effect statistically significant confidence interval around individual means overlap, true. significance test related confidence interval around mean difference.","code":""},{"path":"confint.html","id":"prediction-intervals","chapter":"7 Confidence Intervals","heading":"7.6 Prediction Intervals","text":"Even though 95% future confidence intervals contain true parameter, 95% confidence interval contain 95% future individual observations. Sometimes, researchers want predict interval within single value fall. called prediction interval. always much wider confidence interval. reason individual observations can vary substantially, means future samples (fall within normal confidence interval 95% time) vary much less.Figure 7.5 orange background illustrates 95% confidence interval around mean, lighter yellow background illustrates 95% prediction interval (PI).\nFigure 7.5: comparison 95% confidence interval (gold) 95% prediction interval (yellow).\ncalculate prediction interval, need slightly different formula standard error, used confidence interval, namely:\\[\nStandard \\ Error \\ (SE) = \\sigma/\\sqrt(1+1/n)\n\\]rewrite formula used confidence interval \\(\\sigma/\\sqrt(1/N)\\), see difference confidence interval prediction interval “1+” always leads wider intervals. Prediction intervals wider, constructed contain single future value 95% time, instead mean. fact prediction intervals wide good reminder difficult te predict happen single individual.","code":""},{"path":"confint.html","id":"capture-percentages","chapter":"7 Confidence Intervals","heading":"7.7 Capture Percentages","text":"can difficult understand 95% confidence interval provide us interval 95% future means fall. percentage means falls within single confidence interval called capture percentage. Figure 7.6 see two randomly simulated studies sample size population. true effect size studies 0, see 95% confidence intervals studies contain true population value 0. However, two confidence intervals cover quite different ranges effect sizes, confidence interval Study 1 ranging -0.07 0.48, confidence interval Study 2 ranging -0.50 0.06. can true future, 95% effect sizes expect fall -0.07 0.048, 95% effect sizes expect fall -0.50 0.06.\nFigure 7.6: Meta-analysis 2 simulated studies population.\nsituation 95% confidence interval happens also 95% capture percentage observed effect size sample happens exactly true population parameter. Figure 7.6 means need observe effect exactly 0. However, can’t know whether observed effect size happens exactly population effect size. sample estimate identical true population value (alsmost always case) less 95% future effect sizes fall within CI current sample. observed two studies observed effect sizes bit removed true effect size, find effect size estimates future studies fall outside observed 95% confidence interval quite often. , percentage future means fall within single confidence interval depends upon single confidence interval happened observe. Based simulation studies possible show average, long run, 95% CI 83.4% capture probability (Cumming & Maillardet, 2006).","code":""},{"path":"confint.html","id":"calculating-confidence-intervals-around-standard-deviations.","chapter":"7 Confidence Intervals","heading":"7.8 Calculating Confidence Intervals around Standard Deviations.","text":"calculate standard deviation (SD) sample, value \nestimate true value population. small samples, estimate can quite far . due law large numbers, sample size increases, measuring standard deviation accurately. Since sample standard deviation estimate uncertainty, can calculate 95% confidence interval around .Keeping uncertainty estimate standard deviation mind can important. researchers perform -priori power analysis based effect size interest expressed raw scale, need accurate estimates standard deviation performing power analysis. Sometimes researchers use pilot data get estimate standard deviation. Since estimate population standard deviation based pilot study uncertainty, sample size -priori power analysis contains uncertainty (see 'Test ' questions ). Use validated existing measures accurate estimates standard deviation population interest available. keep mind estimates sample uncertainty.","code":""},{"path":"confint.html","id":"computing-confidence-intervals-around-effect-sizes","chapter":"7 Confidence Intervals","heading":"7.9 Computing Confidence Intervals around Effect Sizes","text":"Cohen (1994) reflected reason confidence intervals rarely reporting 1994: \"suspect main reason reported embarrassingly large!\" might , another reason might statistical software rarely provided confidence intervals around effect sizes time Cohen wrote article. become increasingly easy report confidence intervals popularity free software packages R, even though packages might provide solutions statistical tests yet. Journal Article Reporting Standards recommend report \"effect-size estimates confidence intervals estimates correspond inferential test conducted, possible\".One easy solution calculating effect sizes confidence intervals MOTE made Dr. Erin Buchanan lab. website comes full collections tutorials, comparisons software packages, demonstration videos giving accessible overviews compute effect sizes confidence intervals wide range tests based summary statistics. means whichever software use perform statistical tests, can enter sample sizes means, standard deviations, test statistics compute effect sizes confidence intervals. example, video gives overview compute confidence interval around Cohen's d independent t-test.MOTE also available R package (Buchanan et al., 2017). Although many solutions exists compute Cohen's d, MOTE sets apart allowing researchers compute effect sizes confidence intervals many additional effect sizes, (partial) omega squared subjects ANOVA (\\(\\omega^{2}\\) \\(\\omega^{2}_p\\)), generalized omega squared ANOVA (\\(\\omega^{2}_G\\)), epsilon squared ANOVA (\\(\\varepsilon^{2}\\)) (partial) generalized eta squared ANOVA (\\(\\eta^{2}_G\\)).MBESS another R package range options compute effect sizes confidence intervals (Kelley, 2007). code reproduces example MOTE .feel comfortable analyzing data R, effectsize package offers complete set convenient solutions compute effect sizes confidence intervals (Ben-Shachar et al., 2020).personally impressed way effectsize package incorporates state art (although might bit biased). example, recommendation default use Welch's t-test instead students t-test (Delacre et al., 2017), based recent simulation study recommended report Hedges’ \\(g_s^*\\) effect size Welch's t-test (Delacre et al., 2021), effectsize package first incorporate .Free statistical software jamovi JAPS strong alternatives SPSS (unlike SPSS) allows users compute Cohen's d confidence interval independent dependent ttests.jamovi, ESCI module allows users compute effect sizes confidence intervals, accompanies educational material focusses estimation less testing (Cumming & Calin-Jageman, 2016).\nFigure 7.7: Output ESCI module jamovi.\nJASP offers wide range frequentist Bayesian analyses, addition Cohen's d also allows users compute omega squared \\(\\omega^{2}\\), less biased version \\(\\eta^{2}\\) (Albers & Lakens, 2018; Okada, 2013).\nFigure 7.8: JASP menu option allows select Cohen's d CI around .\n\nFigure 7.9: JASP output returns Cohen's d confidence interval around .\n","code":"\nMOTE::d.ind.t(m1 = 1.7, m2 = 2.1, sd1 = 1.01, sd2 = 0.96, n1 = 77, n2 = 78, a = .05)$estimate## [1] \"$d_s$ = -0.41, 95\\\\% CI [-0.72, -0.09]\"\nMBESS::smd(Mean.1 = 1.7, Mean.2 = 2.1, s.1 = 1.01, s.2 = 0.96, n.1 = 77, n.2 = 78)## [1] -0.406028\nset.seed(33)\nx <- rnorm(n = 20, mean = 0, sd = 2.5) #create sample from normal distribution\ny <- rnorm(n = 200, mean = 1.5, sd = 3.5) #create sample from normal distribution\n\neffectsize::cohens_d(x,y)## Cohen's d |        95% CI\n## -------------------------\n## -0.44     | [-0.91, 0.02]\n## \n## - Estimated using pooled SD.\neffectsize::cohens_d(x,y, pooled_sd = FALSE)## Cohen's d |         95% CI\n## --------------------------\n## -0.53     | [-0.90, -0.16]\n## \n## - Estimated using un-pooled SD."},{"path":"confint.html","id":"test-yourself-5","chapter":"7 Confidence Intervals","heading":"7.10 Test Yourself","text":"Q1: Go online app Kristoffer Magnusson:\nhttp://rpsychologist.com/d3/CI/. might want confidence intervals contain true population parameter 95%. Drag ‘Slide ’ button far right, see simulation 99% confidence intervals. statement true?confidence intervals larger, sample means fall closer true mean.confidence intervals smaller, sample means fall closer true mean.confidence intervals larger, sample means fall close true mean 95% confidence interval.confidence intervals smaller, sample means fall close true mean 95% confidence interval.Q2: see formulas confidence intervals, sample means confidence intervals depend sample size. can change sample size simulation. default, sample size set 5. Change sample size 50 (can type ). statement true?larger sample size, larger confidence intervals. sample size influence sample means vary around true population mean.larger sample size, smaller confidence intervals. sample size influence sample means vary around true population mean.larger sample size, larger confidence intervals, closer sample means true population mean.larger sample size, smaller confidence intervals, closer sample means true population mean.Q3: studies 1 4 forest plot statistically significant?\nFigure 7.10: Meta-analysis 4 studies\nStudies 1, 2, 3, 4Only study 3None four studiesStudies 1, 2 4Q4: light black diamond bottom row fixed effects meta-analytic effect size estimate. Instead using black horizontal line, upper limit lower limit confidence interval indicated left right points diamond. center\ndiamond meta-analytic effect size estimate. meta-analysis calculates effect size combining weighing studies. statement true?confidence interval meta-analytic effect size estimate always wider single study, additional variation studies.confidence interval meta-analytic effect size estimate always narrow single study, combined sample size studies included meta-analysis.confidence interval meta-analytic effect size estimate become wider narrow compared confidence interval single study, just becomes closer true population parameter.Q5: Let’s assume researcher calculates mean 7.5, standard deviation 6.3, sample 20 people. critical value t-distribution df = 19 2.093. Calculate upper limit confidence interval around mean using formula . :\n\\[\n\\mu \\pm t_{df, 1-(\\alpha/2)} SE\n\\]\n) 1.40\nB) 2.95\nC) 8.91\nD) 10.45Copy code R run code. generate plots like one Figure 7.4. Run entire script often want (notice variability p-values due relatively low power test!), answer following question. p-value plot tell difference statistically significant, p-value .Q6: much two 95% confidence intervals around individual means independent groups overlap effect just statistically significant (p ≈ 0.05) alpha 0.05?95% confidence interval around one mean contain mean group, groups differ significantly .95% confidence interval around one mean overlap 95% confidence interval mean group, groups differ significantly .overlap two confidence intervals approximately half one side confidence interval, groups differ significantly .relationship overlap 95% confidence intervals around two independent means, p-value difference groups.Note visual overlap rule can used comparison made independent groups, dependent groups! 95% confidence interval around effect sizes therefore typically easily interpretable relation significance test.Let’s experience simulation. simulation R script generates large number additional samples, initial one plotted. simulation returns number CI contains mean (95% long run). simulation also returns % means future studies fall within 95% original study, capture percentage. differs (often lower, sometimes higher, ) confidence interval.Q8: Run simulations multiple times. Look output get R console. example: “95.077 % 95% confidence intervals contained true mean” “capture percentage plotted study, % values within observed confidence interval 88.17208 103.1506 : 82.377 %”. running simulations multiple times, look confidence interval around sample mean, relate capture percentage. statement true?farther sample mean true population mean, lower capture percentage.farther sample mean true population mean, higher capture percentage.Q9: Simulations R randomly generated, can make specific simulation reproducible setting seed random generation process. Copy-paste “set.seed(1000)” first line R script, run simulation. sample mean 94. capture percentage? (Don’t forget remove set.seed command want generate random simulations!).95%42.1%84.3%89.2%Capture percentages rarely directly used make statistical inferences. main reason discuss really prevent common misunderstanding 95% future means fall within single confidence interval: Capture percentages clearly show true. Prediction intervals also rarely used psychology, common data science.Q10: run lines first lines code , see alpha level 0.05, 100 observations, true standard deviation 1, 95% CI [0.88; 1.16]. Change assumed population standard deviation 1 2 (st_dev <- 2). Keep settings . 95% CI around standard deviation 2 100 observations?95% CI [1.38; 3.65]95% CI [1.76; 2.32]95% CI [1.82; 2.22]95% CI [1.84; 2.20]Q11: Change assumed population standard deviation back 2 1. Lower sample size 100 20 (n <- 20). inform us width confidence interval standard deviation run pilot study 20 observations. Keep settings . 95% CI around standard deviation 1 20 observations?95% CI [0.91; 1.11]95% CI [0.82; 1.28]95% CI [0.76; 1.46]95% CI [1.52; 2.92]Q12: want 95% CI around standard deviation 1 \n0.05 away assumed population standard deviation, large \nnumber observations ? Note means want 95% CI fall\nwithin 0.95 1.05. notice calculations \ndistribution sample standard deviations symmetrical. Standard\ndeviations can’t smaller 0 (square rooted\nvariance). practice question : smallest number \nobservations upper 95% CI smaller 1.05? Replace n values answer options.n = 489n = 498n = 849n = 948Let’s explore consequences inaccurate estimate population\nstandard deviation -priori power analyses. Let’s imagine want \nperform -priori power analysis smallest effect size interest \nhalf scale point (scale 1-5) measure (unknown) true\npopulation standard deviation 1.2.Q13: Change number observations 50. Change assumed population standard deviation 1.2. Keep effect 0.5. 95% confidence interval standard deviation based sample 50 observation ranges 1.002 1.495. perform -priori power analysis need calculate Cohen’s d,\ndifference divided standard deviation. example, \nwant least observe difference 0.5. Cohen’s d (effect/SD) \nlower bound 95% confidence interval (SD = 1.002) upper\nbound (SD = 1.495)?d = 0.33 d = 0.50d = 0.40 d = 0.60d = 0.43 d = 0.57d = 0.29 d = 0.55If draw sample 50 observations can happen observe value ,\ndue random variation, much smaller much larger true population\nvalue. can examine effect number observations \nthink required perform -priori power analysis.Q14: -priori power analysis performed uses estimate \nCohen’s d based lower 95% CI standard deviation. statement\ntrue?lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size smaller sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d smaller, -priori power analysis yield \nsample size larger sample size really need.lower bound 95% CI smaller true population\nSD, Cohen’s d larger, -priori power analysis yield sample\nsize smaller sample size really need.Q15: Let’s check answer previous question correct. still\nalpha level 0.05, n = 50, standard deviation 1.2, effect interest Run power analyses using pwr package. first power analysis uses Cohen’s d based lower bound 95% confidence interval. second power analysis uses upper bound 95% confidence interval. (also third power analysis based (real-life situations unknown) true standard deviation, just comparison). statement true?sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 86 using \nupper bound 95% CI around standard deviation.sample size per group 68 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 123 using \nupper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based \nlower bound 95% CI around standard deviation, 189 using \nupper bound 95% CI around standard deviation.","code":"\nx <- rnorm(n = 50, mean = 3, sd = 5) # get sample group 1\ny <- rnorm(n = 50, mean = 5, sd = 5) # get sample group 2\n\nd <- data.frame(\n  labels = c(\"X\", \"Y\", \"Difference\"),\n  mean = c(mean(x), mean(y), mean(y) - mean(x)),\n  lower = c(t.test(x)[[4]][1], t.test(y)[[4]][1], t.test(y, x)[[4]][1]),\n  upper = c(t.test(x)[[4]][2], t.test(y)[[4]][2], t.test(y, x)[[4]][2])\n)\n\nplot(NA, xlim = c(.5, 3.5), ylim = c(0, max(d$upper[1:2] + 1)), bty = \"l\", \n     xaxt = \"n\", xlab = \"\", ylab = \"Mean\")\npoints(d$mean[1:2], pch = 19)\nsegments(1, d$mean[1], 5, d$mean[1], lty = 2)\nsegments(2, d$mean[2], 5, d$mean[2], lty = 2)\naxis(1, 1:3, d$labels)\nsegments(1:2, d$lower[1:2], 1:2, d$upper[1:2])\naxis(4, seq((d$mean[1] - 3), (d$mean[1] + 5), by = 1), seq(-3, 5, by = 1))\npoints(3, d$mean[1] + d$mean[3], pch = 19, cex = 1.5)\nsegments(3, d$mean[1] + d$lower[3], 3, d$mean[1] + d$upper[3], lwd = 2)\nmtext(\"Difference\", side = 4, at = d$mean[1], line = 3)\nsegments(1:1, d$upper[1:1], 1:2, d$upper[1:1], lty = 3)\nsegments(1:1, d$lower[1:2], 1:2, d$lower[1:2], lty = 3)\ntext(3, 1, paste(\"P-value\", round(t.test(x, y)$p.value, digits = 3)))\nlibrary(ggplot2)\n\nn <- 20 # set sample size\nnsims <- 100000 # set number of simulations\n\nx <- rnorm(n = n, mean = 100, sd = 15) # create sample from normal distribution\n\n# 95% Confidence Interval\nciu <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\ncil <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n\n# 95% Prediction Interval\npiu <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 + 1 / n)\npil <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 + 1 / n)\n\nggplot(as.data.frame(x), aes(x)) + # plot data\n  geom_rect(aes(xmin = pil, xmax = piu, ymin = 0, ymax = Inf),\n            fill = \"gold\") + # draw yellow PI area\n  geom_rect(aes(xmin = cil, xmax = ciu, ymin = 0, ymax = Inf),\n            fill = \"#E69F00\") + # draw orange CI area\n  geom_histogram(colour = \"black\", fill = \"grey\", aes(y = ..density..), bins = 20) +\n  xlab(\"Score\") +\n  ylab(\"frequency\") +\n  theme_bw(base_size = 20) +\n  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(),\n        panel.grid.minor.x = element_blank()) +\n  geom_vline(xintercept = mean(x), linetype = \"dashed\", size = 1) +\n  coord_cartesian(xlim = c(50, 150)) +\n  scale_x_continuous(breaks = c(seq(50, 150, 10))) +\n  annotate(\"text\", x = mean(x), y = 0.02, label = paste(\n    \"Mean = \", round(mean(x)), \"\\n\",\n    \"SD = \", round(sd(x)), sep = \"\"), size = 6.5)\n# Simulate Confidence Intervals\nciu_sim <- numeric(nsims)\ncil_sim <- numeric(nsims)\nmean_sim <- numeric(nsims)\n\nfor (i in 1:nsims) { # for each simulated experiment\n  x <- rnorm(n = n, mean = 100, sd = 15) # create sample from normal distribution\n  ciu_sim[i] <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n  cil_sim[i] <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n  mean_sim[i] <- mean(x) # store means of each sample\n}\n\n# Save only those simulations where the true value was inside the 95% CI\nciu_sim <- ciu_sim[ciu_sim < 100]\ncil_sim <- cil_sim[cil_sim > 100]\n\n# Calculate how many times the observed mean fell within the 95% CI of the original study\nmean_sim <- mean_sim[mean_sim > cil & mean_sim < ciu]\n\ncat((100 * (1 - (length(ciu_sim) / nsims + length(cil_sim) / nsims))),\n    \"% of the 95% confidence intervals contained the true mean\")## 95.047 % of the 95% confidence intervals contained the true mean\ncat(\"The capture percentage for the plotted study, or the % of values within\n    the observed confidence interval from\", cil, \"to\", ciu,\n    \"is:\", 100 * length(mean_sim) / nsims, \"%\")## The capture percentage for the plotted study, or the % of values within\n##     the observed confidence interval from 97.82077 to 113.6991 is: 74.155 %\nalpha_level <- 0.05 #set alpha level\nn <- 100 #set number of observations\nst_dev <- 1 #set true standard deviation\neffect <- 0.5 #set effect size (raw mean difference)\n\n# calculate lower and upper critical values c_l and c_u\nc_l <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = FALSE))\nc_u <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = TRUE))\n\n# calculate lower and upper confidence interval for sd\nst_dev * c_l\nst_dev * c_u\n\n# d based on lower bound of the 95CI around the SD\neffect/(st_dev * c_l)\n# d based on upper bound of the 95CI around the SD\neffect/(st_dev * c_u)\n\npwr::pwr.t.test(d = effect/(st_dev * c_l), power = 0.9, sig.level = 0.05)\npwr::pwr.t.test(d = effect/(st_dev * c_u), power = 0.9, sig.level = 0.05)\n\n# Power analysis for true standard deviation for comparison\npwr::pwr.t.test(d = effect/st_dev, power = 0.9, sig.level = 0.05)"},{"path":"power.html","id":"power","chapter":"8 Sample size justification","heading":"8 Sample size justification","text":"Scientists perform empirical studies collect data helps answer research question. data collected, informative study respect inferential goals. sample size justification consider informative data given inferential goal, estimating effect size, testing hypothesis. Even though sample size justification sometimes requested manuscript submission guidelines, submitting grant funder, submitting proposal ethical review board, number observations often simply stated, justified. makes difficult evaluate informative study . prevent concerns emerging late (e.g., non-significant hypothesis test observed), researchers carefully justify sample size data collected.\nTable 8.1: Overview possible justifications sample size study.\n","code":""},{"path":"power.html","id":"six-approaches-to-justify-sample-sizes","chapter":"8 Sample size justification","heading":"8.1 Six Approaches to Justify Sample Sizes","text":"Researchers often find difficult justify sample size (.e., number participants, observations, combination thereof). review article six possible approaches discussed can used justify sample size quantitative study (see Table 8.1). exhaustive overview, includes common applicable approaches single studies.1 first justification data (almost) entire population collected. second justification centers resource constraints, almost always present, rarely explicitly evaluated. third fourth justifications based desired statistical power desired accuracy. fifth justification relies heuristics, finally, researchers can choose sample size without justification. justifications can stronger weaker depending conclusions researchers want draw data plan collect.approaches justification sample sizes, even 'justification' approach, give others insight reasons led decision sample size study. surprising 'heuristics' 'justification' approaches often unlikely impress peers. However, important note value information collected depends extent final sample size allows researcher achieve inferential goals, sample size justification chosen.extent approaches make researchers judge data collected informative depends details question researcher aimed answer parameters chose determining sample size study. example, badly performed -priori power analysis can quickly lead study low informational value. six justifications mutually exclusive, multiple approaches can considered designing study.","code":""},{"path":"power.html","id":"six-ways-to-evaluate-which-effect-sizes-are-interesting","chapter":"8 Sample size justification","heading":"8.2 Six Ways to Evaluate Which Effect Sizes are Interesting","text":"informativeness data collected depends inferential goals researcher , cases, inferential goals scientific peers . shared feature different inferential goals considered review article question effect sizes researcher considers meaningful distinguish. implies researchers need evaluate effect sizes consider interesting. evaluations rely combination statistical properties domain knowledge. Table 8.2 six possibly useful considerations provided. intended exhaustive overview, presents common useful approaches can applied practice. evaluations equally relevant types sample size justifications. online Shiny app accompanying manuscript provides researchers interactive form guides researchers considerations sample size justification. considerations often rely information (e.g., effect sizes, number observations, standard deviation, etc.) six considerations seen set complementary approaches can used evaluate effect sizes interest.start, researchers consider smallest effect size interest . Second, although relevant performing hypothesis test, researchers consider effect sizes statistically significant given choice alpha level sample size. Third, important consider (range ) effect sizes expected. requires careful consideration source expectation presence possible biases expectations. Fourth, useful consider width confidence interval around possible values effect size population, whether can expect confidence interval reject effects considered -priori plausible. Fifth, worth evaluating power test across wide range possible effect sizes sensitivity power analysis. Sixth, researcher can consider effect size distribution related studies literature.\nTable 8.2: Overview possible ways evaluate effect sizes interesting.\n","code":""},{"path":"power.html","id":"the-value-of-information","chapter":"8 Sample size justification","heading":"8.3 The Value of Information","text":"Since scientists faced resource limitations, need balance cost collecting additional datapoint increase information datapoint provides. referred value information (Eckermann et al., 2010). Calculating value information notoriously difficult (Detsky, 1990). Researchers need specify cost collecting data, weigh costs data collection increase utility access data provides. value information perspective every data point can collected equally valuable (J. Halpern et al., 2001; Wilson, 2015). Whenever additional observations change inferences meaningful way, costs data collection can outweigh benefits.value additional information cases non-monotonic function, especially depends multiple inferential goals. researcher might interested comparing effect previously observed large effect literature, theoretically predicted medium effect, smallest effect practically relevant. situation expected value sampling information lead different optimal sample sizes inferential goal. valuable collect informative data large effect, additional data less (even negative) marginal utility, point data becomes increasingly informative medium effect size, value sampling additional information decreasing study becomes increasingly informative presence absence smallest effect interest.difficulty quantifying value information, scientists typically use less formal approaches justify amount data set collect study. Even though cost-benefit analysis always made explicit reported sample size justifications, value information perspective almost always implicitly underlying framework sample size justifications based . Throughout subsequent discussion sample size justifications, importance considering value information given inferential goals repeatedly highlighted.","code":""},{"path":"power.html","id":"measuring-almost-the-entire-population","chapter":"8 Sample size justification","heading":"8.4 Measuring (Almost) the Entire Population","text":"instances might possible collect data (almost) entire population investigation. example, researchers might use census data, able collect data employees firm study small population top athletes. Whenever possible measure entire population, sample size justification becomes straightforward: researcher used data available.","code":""},{"path":"power.html","id":"resource-constraints","chapter":"8 Sample size justification","heading":"8.5 Resource Constraints","text":"common reason number observations study resource constraints limit amount data can collected reasonable cost (Lenth, 2001). practice, sample sizes always limited resources available. Researchers practically always resource limitations, therefore even resource constraints primary justification sample size study, always secondary justification.Despite omnipresence resource limitations, topic often receives little attention texts experimental design (example exception, see Bulus & Dong (2021)). might make feel like acknowledging resource constraints appropriate, opposite true: resource limitations always play role, responsible scientist carefully evaluates resource constraints designing study. Resource constraint justifications based trade-costs data collection, value access information data provides. Even researchers explicitly quantify trade-, revealed actions. example, researchers rarely spend resources single study. Given resource constraints, researchers confronted optimization problem spend resources across multiple research questions.Time money two resource limitations scientists face. PhD student certain time complete PhD thesis, typically expected complete multiple research lines time. addition time limitations, researchers limited financial resources often directly influence much data can collected. third limitation research lines might simply small number individuals data can collected, studying patients rare disease. resource constraint justification puts limited resources center justification sample size collected, starts resources scientist available. resources translated expected number observations (N) researcher expects able collect amount money given time. challenge evaluate whether collecting N observations worthwhile. decide study informative, conclude data collection worthwhile?evaluating whether resource constraints make data collection uninformative, researchers need explicitly consider inferential goals collecting data (Parker & Berman, 2003). data always provides knowledge research question data, absolute sense, data collected value. However, possible benefits collecting data outweighed costs data collection.straightforward evaluate whether data collection value know certain someone make decision, without data. situations additional data reduce error rates well-calibrated decision process, even ever slightly. example, without data perform better coin flip guess two conditions higher true mean score measure. data, can perform better coin flip picking condition highest mean. small amount data still likely make mistake, error rate smaller without data. cases, value information might positive, long reduction error rates beneficial cost data collection.Another way small dataset can valuable existence eventually makes possible perform meta-analysis (Maxwell & Kelley, 2011). argument favor collecting small dataset requires 1) researchers share data way future meta-analyst can find , 2) decent probability someone perform high-quality meta-analysis include data future (S. D. Halpern et al., 2002). uncertainty whether ever meta-analysis weighed costs data collection.One way increase probability future meta-analysis researchers commit performing meta-analysis , combining several studies performed small-scale meta-analysis (Cumming, 2014). example, researcher might plan repeat study next 12 years class teach, expectation 12 years meta-analysis 12 studies sufficient draw informative inferences (see ter Schure & Grünwald (2019)). plausible researcher collect required data , can attempt set collaboration fellow researchers field commit collecting similar data identical measures. likely sufficient data emerge time reach inferential goals, might value collecting data.Even researcher believes worth collecting data future meta-analysis performed, likely perform statistical test data. make sure expectations results test well-calibrated, important consider effect sizes interest, perform sensitivity power analysis evaluate probability Type II error effects interest. six ways evaluate effect sizes interesting discussed second part review, useful consider smallest effect size can statistically significant, expected width confidence interval around effect size, effects can expected specific research area, evaluate power effect sizes sensitivity power analysis. decision claim made, compromise power analysis worthwhile consider deciding upon error rates planning study. reporting resource constraints sample size justification recommended address five considerations Table 8.3. Addressing points explicitly facilitates evaluating data worthwhile collect. make easier address relevant points explicitly, interactive form implement recommendations manuscript can found https://shiny.ieis.tue.nl/sample_size_justification/.\nTable 8.3: Overview recommendations reporting sample size justification based resource constraints.\n","code":""},{"path":"power.html","id":"a-priori-power-analysis","chapter":"8 Sample size justification","heading":"8.6 A-priori Power Analysis","text":"designing study goal test whether statistically significant effect present, researchers often want make sure sample size large enough prevent erroneous conclusions range effect sizes care . approach justifying sample size, value information collect observations point probability erroneous inference , long run, larger desired value. researcher performs hypothesis test, four possible outcomes:false positive (Type error), determined \\(\\alpha\\) level. test yields significant result, even though null hypothesis true.false negative (Type II error), determined \\(\\beta\\), 1 - power. test yields non-significant result, even though alternative hypothesis true.true negative, determined 1-\\(\\alpha\\). test yields non-significant result null hypothesis true.true positive, determined 1-\\(\\beta\\). test yields significant result alternative hypothesis true.Given specified effect size, alpha level, power, -priori power analysis can used calculate number observations required achieve desired error rates, given effect size.2 Figure 8.1 illustrates statistical power increases number observations (per group) increases independent t test two-sided alpha level 0.05. interested detecting effect d = 0.5, sample size 90 per condition give us 90% power. Statistical power can computed determine number participants, number items (Westfall et al., 2014) can also performed single case studies (Ferron & Onghena, 1996; McIntosh & Rittmo, 2020)Although common set Type error rate 5% aim 80% power, error rates justified (Lakens, Adolfi, et al., 2018). explained section compromise power analysis, default recommendation aim 80% power lacks solid justification. general, lower error rates (thus higher power), informative study , resources required. Researchers carefully weigh costs increasing sample size benefits lower error rates, probably make studies designed achieve power 90% 95% common articles reporting single study. additional consideration whether researcher plans publish article consisting set replication extension studies, case probability observing multiple Type errors low, probability observing mixed results even true effect increases (Lakens & Etz, 2017), also reason aim studies low Type II error rates, perhaps even slightly increasing alpha level individual study.\nFigure 8.1: Power curve independent t test effect d = 0.5 \\(\\alpha\\) = 0.05 function sample size.\nFigure 8.2 visualizes two distributions. left distribution (dashed line) centered 0. model null hypothesis. null hypothesis true statistically significant result observed effect size extreme enough (two-sided test either positive negative direction), significant result Type error (dark grey areas curve). true effect, formally statistical power null hypothesis significance test undefined. significant effects observed null hypothesis true Type errors, false positives, occur chosen alpha level. right distribution (solid line) centered effect d = 0.5. specified model alternative hypothesis study, illustrating expectation effect d = 0.5 alternative hypothesis true. Even though true effect, studies always find statistically significant result. happens , due random variation, observed effect size close 0 statistically significant. results false negatives (light grey area curve right). increase power, can collect larger sample size. sample size increases, distributions become narrow, reducing probability Type II error.3\nFigure 8.2: Null (d = 0, grey dashed line) alternative (d = 0.5, solid black line) hypothesis, \\(\\alpha\\) = 0.05 n = 80 per group.\nimportant highlight goal -priori power analysis achieve sufficient power true effect size. true effect size unknown. goal -priori power analysis achieve sufficient power, given specific assumption effect size researcher wants detect. Just like Type error rate maximum probability making Type error conditional assumption null hypothesis true, -priori power analysis computed assumption specific effect size. unknown assumption correct. researcher can make sure assumptions well justified. Statistical inferences based test Type II error controlled conditional assumption specific effect size. allow inference , assuming true effect size least large used -priori power analysis, maximum Type II error rate study larger desired value.point perhaps best illustrated consider study -priori power analysis performed test presence effect, test absence effect. designing study, essential consider possibility effect (e.g., mean difference zero). -priori power analysis can performed null hypothesis significance test, test absence meaningful effect, equivalence test can statistically provide support null hypothesis rejecting presence effects large enough matter (Lakens, 2017; Meyners, 2012; Rogers et al., 1993). multiple primary tests performed based sample, analysis requires dedicated sample size justification. possible, sample size collected guarantees tests informative, means collected sample size based largest sample size returned -priori power analyses.example, goal study detect reject effect size d = 0.4 90% power, alpha level set 0.05 two-sided independent t test, researcher need collect 133 participants condition informative null hypothesis test, 136 participants condition informative equivalence test. Therefore, researcher aim collect 272 participants total informative result tests planned. guarantee study sufficient power true effect size (can never known), guarantees study sufficient power given assumption effect researcher interested detecting rejecting. Therefore, -priori power analysis useful, long researcher can justify effect sizes interested .researchers correct alpha level testing multiple hypotheses, -priori power analysis based corrected alpha level. example, four tests performed, overall Type error rate 5% desired, Bonferroni correction used, -priori power analysis based corrected alpha level .0125.-priori power analysis can performed analytically, performing computer simulations. Analytic solutions faster less flexible. common challenge researchers face attempting perform power analyses complex uncommon tests available software offer analytic solutions. cases simulations can provide flexible solution perform power analyses test (Morris et al., 2019). following code example power analysis R based 10000 simulations one-sample t test zero sample size 20, assuming true effect d = 0.5. simulations consist first randomly generating data based assumptions data generating mechanism (e.g., normal distribution mean 0.5 standard deviation 1), followed test performed data. computing percentage significant results, power can computed design.wide range tools available perform power analyses. Whichever tool researcher decides use, take time learn use software correctly perform meaningful -priori power analysis. Resources educate psychologists power analysis consist book-length treatments (Aberson, 2019; Cohen, 1988; Julious, 2004; Murphy et al., 2014), general introductions (Baguley, 2004; Brysbaert, 2019; Faul et al., 2007; Maxwell et al., 2008; Perugini et al., 2018), increasing number applied tutorials specific tests (Brysbaert & Stevens, 2018; DeBruine & Barr, 2021; P. Green & MacLeod, 2016; J. K. Kruschke, 2013; Lakens & Caldwell, 2021; Schoemann et al., 2017; Westfall et al., 2014). important trained basics power analysis, can extremely beneficial learn perform simulation-based power analyses. time, often recommended enlist help expert, especially researcher lacks experience power analysis specific test.reporting -priori power analysis, make sure power analysis completely reproducible. power analyses performed R possible share analysis script information version package. many software packages possible export power analysis performed PDF file. example, G*Power analyses can exported 'protocol power analysis' tab. software package provides way export analysis, add screenshot power analysis supplementary files.\nFigure 8.3: details power analysis performed can exported G*Power.\nreproducible report needs accompanied justifications choices made respect values used power analysis. effect size used power analysis based previous research factors presented Table 8.5 (effect size based meta-analysis) Table 8.6 (effect size based single study) discussed. effect size estimate based existing literature, provide full citation, preferably direct quote article effect size estimate reported. effect size based smallest effect size interest, value just stated, justified (e.g., based theoretical predictions practical implications, see Lakens, Scheel, et al. (2018)). overview aspects reported describing -priori power analysis, see Table 8.4.\nTable 8.4: Overview recommendations reporting -priori power analysis.\n","code":"\nN <- 50\nd <- 0.5\np_upper <- 0.05\nncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d\ncrit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)\nlow_x <- min(-1 - crit_d)\nhigh_x <- max(d + 1 + crit_d)\n# calc d-distribution\nx <- seq(low_x, high_x, length = 10000) # create x values\n# Set max Y for graph\nd_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2) # calculate distribution of d based on t-distribution\ny_max <- max(d_dist) + 1\n# create plot\npar(bg = \"white\")\nd <- round(d, 2)\nplot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = substitute(paste(\"Cohen's d\")), ylab = \"Density\", main = \"\", cex.lab = 1.3, cex.axis = 1.2)\n# abline(v = seq(low_x,high_x,0.1), h = seq(0,0.5,0.1), col = \"lightgrey\", lty = 1)\n#    axis(side = 1, at = seq(low_x,high_x,0.1), labels = FALSE)\n# Add Type I error rate right\ncrit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)\ny <- seq(crit_d, 10, length = 10000)\nz <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon\npolygon(c(crit_d, y, 10), c(0, z, 0), col = rgb(0.3, 0.3, 0.3), border = rgb(0.3, 0.3, 0.3))\n# Add Type I error rate left\ncrit_d <- -abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)\ny <- seq(-10, crit_d, length = 10000)\nz <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon\npolygon(c(y, crit_d, crit_d), c(z, 0, 0), col = rgb(0.3, 0.3, 0.3), border = rgb(0.3, 0.3, 0.3))\n# Add Type II error rate\ncrit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)\ny <- seq(-10, crit_d, length = 10000)\nz <- (dt(y * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2)) # determine upperbounds polygon\npolygon(c(y, crit_d, crit_d), c(0, z, 0), col = rgb(0.7, 0.7, 0.7), border = rgb(0.7, 0.7, 0.7))\n# add d = 0 line\nd_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = 0) * sqrt(N / 2)\nlines(x, d_dist, col = \"grey40\", type = \"l\", lwd = 3, lty = 5)\n# add effect line\nd_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2) # calculate distribution of d based on t-distribution\nlines(x, d_dist, col = \"black\", type = \"l\", lwd = 3)\npow_eq <- TOSTER::powerTOSTtwo(\n  alpha = 0.05, \n  statistical_power = 0.9, \n  low_eqbound_d = -0.4, \n  high_eqbound_d = 0.4)\np <- numeric(10000)   # to store p-values\nfor (i in 1:10000) {  #simulate 10k tests\n  x <- rnorm(n = 20, mean = 0.5, sd = 1)\n  p[i] <- t.test(x)$p.value # store p-value\n}\nsum(p < 0.05) / 10000 # Compute power"},{"path":"power.html","id":"planning-for-precision","chapter":"8 Sample size justification","heading":"8.7 Planning for Precision","text":"researchers suggested justify sample sizes based desired level precision estimate (Cumming & Calin-Jageman, 2016; J. K. Kruschke, 2018; Maxwell et al., 2008). goal justifying sample size based precision collect data achieve desired width confidence interval around parameter estimate. width confidence interval around parameter estimate depends standard deviation number observations. aspect researcher needs justify sample size justification based accuracy desired width confidence interval respect inferential goal, assumption population standard deviation measure.researcher determined desired accuracy, good estimate true standard deviation measure, straightforward calculate sample size needed desired level accuracy. example, measuring IQ group individuals researcher might desire estimate IQ score within error range 2 IQ points 95% observed means, long run. required sample size achieve desired level accuracy (assuming normally distributed data) can computed :\\[N = \\left(\\frac{z \\cdot sd}{error}\\right)^2\\]N number observations, z critical value related desired confidence interval, sd standard deviation IQ scores population, error width confidence interval within mean fall, desired error rate. example, (1.96 × 15 / 2)^2 = 216.1 observations. researcher desires 95% means fall within 2 IQ point range around true population mean, 217 observations collected. desired accuracy non-zero mean difference computed, accuracy based non-central t-distribution. calculations expected effect size estimate needs provided, relatively little influence required sample size (Maxwell et al., 2008). also possible incorporate uncertainty observed effect size sample size calculation, known assurance (Kelley & Rausch, 2006). MBESS package R provides functions compute sample sizes wide range tests (Kelley, 2007).less straightforward justify desired level accuracy related inferential goals. literature helps researchers choose desired width confidence interval. Morey (2020) convincingly argues practical use-cases planning precision involve inferential goal distinguishing observed effect effect sizes (Bayesian perspective, see J. K. Kruschke (2018)). example, researcher might expect effect size r = 0.4 treat observed correlations differ 0.2 (.e., 0.2 < r < 0.6) differently, effects r = 0.6 larger considered large caused assumed underlying mechanism (Hilgard, 2021), effects smaller r = 0.2 considered small support theoretical prediction. goal indeed get effect size estimate precise enough two effects can differentiated high probability, inferential goal actually hypothesis test, requires designing study sufficient power reject effects (e.g., testing range prediction correlations 0.2 0.6).researchers want test hypothesis, example prefer estimation approach testing approach, absence clear guidelines help researchers justify desired level precision, one solution might rely generally accepted norm precision aim . norm based ideas certain resolution measurements research area longer lead noticeably different inferences. Just researchers normatively use alpha level 0.05, plan studies achieve desired confidence interval width around observed effect determined norm. Future work needed help researchers choose confidence interval width planning accuracy.","code":""},{"path":"power.html","id":"heuristics","chapter":"8 Sample size justification","heading":"8.8 Heuristics","text":"researcher uses heuristic, able justify sample size , trust sample size recommended authority. started PhD student 2005 common collect 15 participants subject condition. asked common practice, one really sure, people trusted justification somewhere literature. Now, realize justification heuristics used. Berkeley (1735) already observed: \"Men learn elements science others: every learner hath deference less authority, especially young learners, kind caring dwell long upon principles, inclining rather take upon trust: things early admitted repetition become familiar: familiarity length passeth evidence.\"papers provide researchers simple rules thumb sample size collected. papers clearly fill need, cited lot, even advice articles flawed. example, Wilson VanVoorhis & Morgan (2007) translate absolute minimum 50+8 observations regression analyses suggested rule thumb examined S. B. Green (1991) recommendation collect ~50 observations. Green actually concludes article \"summary, specific minimum number subjects minimum ratio subjects--predictors supported\". discuss general rule thumb N = 50 + 8 provided accurate minimum number observations 'typical' study social sciences 'medium' effect size, Green claims citing Cohen (1988). Cohen actually claim typical study social sciences 'medium' effect size, instead said (1988, p. 13): \"Many effects sought personality, social, clinical-psychological research likely small effects defined\". see string mis-citations eventually leads misleading rule thumb.Rules thumb seem primarily emerge due mis-citations /overly simplistic recommendations. Simonsohn, Nelson, Simmons (2011) recommended \"Authors must collect least 20 observations per cell\". later recommendation authors presented conference suggested use n > 50, unless study large effects (Simmons et al., 2013). Regrettably, advice now often mis-cited justification collect 50 observations per condition without considering expected effect size. authors justify specific sample size (e.g., n = 50) based general recommendation another paper, either mis-citing paper, paper citing flawed.Another common heuristic collect number observations collected previous study. strategy recommended scientific disciplines widespread publication bias, /novel surprising findings largely exploratory single studies published. Using sample size previous study valid approach sample size justification previous study also applies current study. Instead stating intend collect sample size earlier study, repeat sample size justification, update light new information (effect size earlier study, see Table 8.6).Peer reviewers editors carefully scrutinize rules thumb sample size justifications, can make seem like study high informational value inferential goal even study yield uninformative results. Whenever one encounters sample size justification based heuristic, ask : 'heuristic used?' important know logic behind heuristic determine whether heuristic valid specific situation. cases, heuristics based weak logic, widely applicable. might possible fields develop valid heuristics sample size justifications. example, possible research area reaches widespread agreement effects smaller d = 0.3 small interest, studies field use sequential designs (see ) 90% power detect d = 0.3. Alternatively, possible field agrees data collected desired level accuracy, irrespective true effect size. cases, valid heuristics exist based generally agreed goals data collection. example, Simonsohn (2015) suggests design replication studies 2.5 times large sample sizes original study, provides 80% power equivalence test equivalence bound set effect original study 33% power detect, assuming true effect size 0. original authors typically specify effect size falsify hypothesis, heuristic underlying 'small telescopes' approach good starting point replication study inferential goal reject presence effect large described earlier publication. responsibility researchers gain knowledge distinguish valid heuristics mindless heuristics, able evaluate whether heuristic yield informative result given inferential goal researchers specific study, .","code":""},{"path":"power.html","id":"no-justification","chapter":"8 Sample size justification","heading":"8.9 No Justification","text":"might sound like contradictio terminis, useful distinguish final category researchers explicitly state justification sample size. Perhaps resources available collect data, used. researcher performed power analysis, planned precision, . cases, instead pretending justification sample size, honesty requires state sample size justification. necessarily bad. still possible discuss smallest effect size interest, minimal statistically detectable effect, width confidence interval around effect size, plot sensitivity power analysis, relation sample size collected. researcher truly specific inferential goals collecting data, evaluation can perhaps performed based reasonable inferential goals peers learn existence collected data.try spin story looks like study highly informative . Instead, transparently evaluate informative study given effect sizes interest, make sure conclusions follow data. lack sample size justification might problematic, might mean study informative effect sizes interest, makes especially difficult interpret non-significant effects, estimates large uncertainty.","code":""},{"path":"power.html","id":"what-is-your-inferential-goal","chapter":"8 Sample size justification","heading":"8.10 What is Your Inferential Goal?","text":"inferential goal data collection often way related size effect. Therefore, design informative study, researchers want think effect sizes interesting. First, useful consider three effect sizes determining sample size. first smallest effect size researcher interested , second smallest effect size can statistically significant (studies significance test performed), third effect size expected. Beyond considering three effect sizes, can useful evaluate ranges effect sizes. can done computing width expected confidence interval around effect size interest (example, effect size zero), examine effects rejected. Similarly, can useful plot sensitivity curve evaluate range effect sizes design decent power detect, well consider range effects design low power. Finally, situations useful consider range effects likely observed specific research area.","code":""},{"path":"power.html","id":"what-is-the-smallest-effect-size-of-interest","chapter":"8 Sample size justification","heading":"8.11 What is the Smallest Effect Size of Interest?","text":"strongest possible sample size justification based explicit statement smallest effect size considered interesting. smallest effect size interest can based theoretical predictions practical considerations. review approaches can used determine smallest effect size interest randomized controlled trials, see Cook et al. (2014) Keefe et al. (2013), reviews different methods determine smallest effect size interest, see King (2011) Copay et al. (2007), discussion focused psychological research, see Lakens, Scheel, et al. (2018).can challenging determine smallest effect size interest whenever theories developed, research question far removed practical applications, still worth thinking effects small matter. first step forward discuss effect sizes considered meaningful specific research line peers. Researchers differ effect sizes consider large enough worthwhile (Murphy et al., 2014). Just every scientist find every research question interesting enough study, every scientist consider effect sizes interesting enough study, different stakeholders differ effect sizes considered meaningful (Kelley & Preacher, 2012).Even though might challenging, important benefits able specify smallest effect size interest. population effect size always uncertain (indeed, estimating typically one goals study), therefore whenever study powered expected effect size, considerable uncertainty whether statistical power high enough detect true effect population. However, smallest effect size interest can specified agreed upon careful deliberation, becomes possible design study sufficient power (given inferential goal detect reject smallest effect size interest certain error rate). smallest effect interest may subjective (one researcher might find effect sizes smaller d = 0.3 meaningless, another researcher might still interested effects larger d = 0.1), might uncertainty parameters required specify smallest effect size interest (e.g., performing cost-benefit analysis), smallest effect size interest determined, study can designed known Type 2 error rate detect reject value. reason -priori power based smallest effect size interest generally preferred, whenever researchers able specify one (Aberson, 2019; Albers & Lakens, 2018; Brown, 1983; Cascio & Zedeck, 1983; Dienes, 2014; Lenth, 2001).","code":""},{"path":"power.html","id":"the-minimal-statistically-detectable-effect","chapter":"8 Sample size justification","heading":"8.12 The Minimal Statistically Detectable Effect","text":"minimal statistically detectable effect, critical effect size, provides information smallest effect size , observed, statistically significant given specified alpha level sample size (Cook et al., 2014). critical t value (e.g., t = 1.96 \\(\\alpha\\) = 0.05, large sample sizes) can compute critical mean difference (Phillips et al., 2001), critical standardized effect size. two-sided independent t test critical mean difference :\\[M_{crit} = t_{crit}{\\sqrt{\\frac{sd_1^2}{n_1} + \\frac{sd_2^2}{n_2}}}\\]critical standardized mean difference :\\[d_{crit} = t_{crit}{\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]Figure 8.4 distribution Cohen’s d plotted 15 participants per group true effect size either d = 0 d = 0.5. figure similar Figure 8.2, addition critical d indicated. see small number observations group observed effects larger d = 0.75 statistically significant. Whether effect sizes interesting, can realistically expected, carefully considered justified.\nFigure 8.4: Critical effect size independent t test n = 15 per group \\(\\alpha\\) = 0.05.\nG*Power provides critical test statistic (critical t value) performing power analysis. example, Figure 8.5 shows correlation based two-sided test, \\(\\alpha\\) = 0.05, N = 30, effects larger r = 0.361 smaller r = -0.361 can statistically significant. reveals sample size relatively small, observed effect needs quite substantial statistically significant.\nFigure 8.5: critical correlation test based total sample size 30 \\(\\alpha\\) = 0.05 calculated G*Power.\nimportant realize due random variation study probability yield effects larger critical effect size, even true effect size small (even true effect size 0, case significant effect Type error). Computing minimal statistically detectable effect useful study -priori power analysis performed, studies published literature report sample size justification (Lakens, Scheel, et al., 2018), researchers rely heuristics sample size justification.can informative ask whether critical effect size study design within range effect sizes can realistically expected. , whenever significant effect observed published study, either effect size surprisingly larger expected, likely, upwardly biased effect size estimate. latter case, given publication bias, published studies lead biased effect size estimates. still possible increase sample size, example ignoring rules thumb instead performing -priori power analysis, . possible increase sample size, example due resource constraints, reflecting minimal statistically detectable effect make clear analysis data focus p values, effect size confidence interval (see Table 8.3).also useful compute minimal statistically detectable effect 'optimistic' power analysis performed. example, believe best case scenario true effect size d = 0.57 use optimistic expectation -priori power analysis, effects smaller d = 0.4 statistically significant collect 50 observations two independent group design. worst case scenario alternative hypothesis true effect size d = 0.35 design allow declare significant effect effect size estimates close worst case scenario observed. Taking account minimal statistically detectable effect size make reflect whether hypothesis test yield informative answer, whether current approach sample size justification (e.g., use rules thumb, letting resource constraints determine sample size) leads informative study, .","code":"\nknitr::include_graphics(\"images/gpowcrit2.png\")"},{"path":"power.html","id":"what-is-the-expected-effect-size","chapter":"8 Sample size justification","heading":"8.13 What is the Expected Effect Size?","text":"Although true population effect size always unknown, situations researchers reasonable expectation effect size study, want use expected effect size -priori power analysis. Even expectations observed effect size largely guess, always useful explicitly consider effect sizes expected. researcher can justify sample size based effect size expect, even study informative respect smallest effect size interest. cases study informative one inferential goal (testing whether expected effect size present absent), highly informative second goal (testing whether smallest effect size interest present absent).typically three sources expectations population effect size: meta-analysis, previous study, theoretical model. tempting researchers overly optimistic expected effect size -priori power analysis, higher effect size estimates yield lower sample sizes, optimistic increases probability observing false negative result. reviewing sample size justification based -priori power analysis, important critically evaluate justification expected effect size used power analyses.","code":""},{"path":"power.html","id":"using-an-estimate-from-a-meta-analysis","chapter":"8 Sample size justification","heading":"8.14 Using an Estimate from a Meta-Analysis","text":"perfect world effect size estimates meta-analysis provide researchers accurate information effect size expect. Due widespread publication bias science, effect size estimates meta-analyses regrettably always accurate. can biased, sometimes substantially . Furthermore, meta-analyses typically considerable heterogeneity, means meta-analytic effect size estimate differs subsets studies make meta-analysis. , although might seem useful use meta-analytic effect size estimate effect studying power analysis, need take great care .researcher wants enter meta-analytic effect size estimate -priori power analysis, need consider three things (see Table 8.5). First, studies included meta-analysis similar enough study performing reasonable expect similar effect size. essence, requires evaluating generalizability effect size estimate new study. important carefully consider differences meta-analyzed studies planned study, respect manipulation, measure, population, relevant variables.Second, researchers check whether effect sizes reported meta-analysis homogeneous. , considerable heterogeneity meta-analysis, means included studies can expected true effect size estimate. meta-analytic estimate used based subset studies closely represent planned study. Note heterogeneity remains possibility (even direct replication studies can show heterogeneity unmeasured variables moderate effect size sample (Kenny & Judd, 2019; Olsson-Collentine et al., 2020)), main goal selecting similar studies use existing data increase probability expectation accurate, without guaranteeing .Third, meta-analytic effect size estimate biased. Check bias detection tests reported meta-analysis state---art, perform multiple bias detection tests (Carter et al., 2019), consider bias corrected effect size estimates (even though estimates might still biased, necessarily reflect true population effect size).\nTable 8.5: Overview recommendations justifying use meta-analytic effect size estimate power analysis.\n","code":""},{"path":"power.html","id":"using-an-estimate-from-a-previous-study","chapter":"8 Sample size justification","heading":"8.15 Using an Estimate from a Previous Study","text":"meta-analysis available, researchers often rely effect size previous study -priori power analysis. first issue requires careful attention whether two studies sufficiently similar. Just using effect size estimate meta-analysis, researchers consider differences studies terms population, design, manipulations, measures, factors lead one expect different effect size. example, intra-individual reaction time variability increases age, therefore study performed older participants expect smaller standardized effect size study performed younger participants. earlier study used strong manipulation, plan use subtle manipulation, smaller effect size expected. Finally, effect sizes generalize studies different designs. example, effect size comparison two groups often similar effect size interaction follow-study second factor added original design (Lakens & Caldwell, 2021).Even study sufficiently similar, statisticians warned using effect size estimates small pilot studies power analyses. Leon, Davis, Kraemer (2011) write:Contrary tradition, pilot study provide meaningful effect size estimate planning subsequent studies due imprecision inherent data small samples.two main reasons researchers careful using effect sizes studies published literature power analyses effect size estimates studies can differ true population effect size due random variation, publication bias inflates effect sizes. Figure 8.6 shows distribution \\(\\eta_p^2\\) study three conditions 25 participants condition null hypothesis true 'medium' true effect \\(\\eta_p^2\\) = 0.0588 (Richardson, 2011). Figure 8.4 critical effect size indicated, shows observed effects smaller \\(\\eta_p^2\\) = 0.08 significant given sample size. null hypothesis true effects larger \\(\\eta_p^2\\) = 0.08 Type error (dark grey area), alternative hypothesis true effects smaller \\(\\eta_p^2\\) = 0.08 Type II error (light grey area). clear significant effects larger true effect size (\\(\\eta_p^2\\) = 0.0588), power analyses based significant finding (e.g., significant results published literature) based overestimate true effect size, introducing bias.even access effect sizes (e.g., pilot studies performed ) due random variation observed effect size sometimes quite small. Figure 8.6 shows quite likely observe effect \\(\\eta_p^2\\) = 0.01 small pilot study, even true effect size 0.0588. Entering effect size estimate \\(\\eta_p^2\\) = 0.01 -priori power analysis suggest total sample size 957 observations achieve 80% power follow-study. researchers follow pilot studies observe effect size pilot study , entered power analysis, yields sample size feasible collect follow-study, effect size estimates upwardly biased, power follow-study systematically lower desired (Albers & Lakens, 2018).\nFigure 8.6: Distribution partial eta squared null hypothesis (dotted grey curve) medium true effect 0.0588 (solid black curve) 3 groups 25 observations.\nessence, problem using small studies estimate effect size entered -priori power analysis due publication bias follow-bias effect sizes researchers end using power analysis come full F distribution, known truncated F distribution (Taylor & Muller, 1996). example, imagine extreme publication bias situation illustrated Figure 8.6. studies accessible researchers come part distribution \\(\\eta_p^2\\) > 0.08, test result statistically significant. possible compute effect size estimate , based certain assumptions, corrects bias. example, imagine observe result literature One-Way ANOVA 3 conditions, reported F(2, 42) = 0.017, \\(\\eta_p^2\\) = 0.176. take effect size face value enter effect size estimate -priori power analysis, suggested sample size achieve suggest need collect 17 observations condition.However, assume bias present, can use BUCSS R package (S. F. Anderson et al., 2017) perform power analysis attempts correct bias. power analysis takes bias account (specific model publication bias, based truncated F distribution significant results published) suggests collecting 73 participants condition. possible bias corrected estimate non-centrality parameter used compute power zero, case possible correct bias using method. alternative formally modeling correction publication bias whenever researchers assume effect size estimate biased, researchers can simply use conservative effect size estimate, example computing power based lower limit 60% two-sided confidence interval around effect size estimate, Perugini et al. (2014) refer safeguard power. approaches lead conservative power analysis, necessarily accurate power analysis. simply possible perform accurate power analysis basis effect size estimate study might biased /small sample size (Teare et al., 2014). possible specify smallest effect size interest, great uncertainty effect size expect, might efficient perform study sequential design (discussed ).summarize, effect size previous study -priori power analysis can used three conditions met (see Table 8.6). First, previous study sufficiently similar planned study. Second, low risk bias (e.g., effect size estimate comes Registered Report, analysis results impacted likelihood publication). Third, sample size large enough yield relatively accurate effect size estimate, based width 95% CI around observed effect size estimate. always uncertainty around effect size estimate, entering upper lower limit 95% CI around effect size estimate might informative consequences uncertainty effect size estimate -priori power analysis.\nTable 8.6: Overview recommendations justifying use effect size estimate single study.\n","code":"\n# convert eta of 0.176 to f of 0.4621604\npwr_res <- pwr.anova.test(k = 3,  \n               f = 0.4621604, \n               sig.level = 0.05, \n               power = 0.8)$n\n\n\n\nBUCSS_res <- ss.power.ba(\n  F.observed = 4.5,\n  N = 45,\n  levels.A = 3,\n  effect = c(\"factor.A\"),\n  alpha.prior = 0.05,\n  alpha.planned = 0.05,\n  assurance = 0.5,\n  power = 0.8,\n  step = 0.001\n)"},{"path":"power.html","id":"using-an-estimate-from-a-theoretical-model","chapter":"8 Sample size justification","heading":"8.16 Using an Estimate from a Theoretical Model","text":"theoretical model sufficiently specific can build computational model, knowledge key parameters model relevant data plan collect, possible estimate effect size based effect size estimate derived computational model. example, one strong ideas weights feature stimuli share differ , possible compute predicted similarity judgments pairs stimuli based Tversky's contrast model (Tversky, 1977), estimate predicted effect size differences experimental conditions. Although computational models make point predictions relatively rare, whenever available, provide strong justification effect size researcher expects.","code":""},{"path":"power.html","id":"compute-the-width-of-the-confidence-interval-around-the-effect-size","chapter":"8 Sample size justification","heading":"8.17 Compute the Width of the Confidence Interval around the Effect Size","text":"researcher can estimate standard deviation observations collected, possible compute -priori estimate width 95% confidence interval around effect size (Kelley, 2007). Confidence intervals represent range around estimate wide enough long run true population parameter fall inside confidence intervals 100 - \\(\\alpha\\) percent time. single study true population effect either falls confidence interval, , long run one can act confidence interval includes true population effect size (keeping error rate mind). Cumming (2013) calls difference observed effect size upper 95% confidence interval (lower 95% confidence interval) margin error.compute 95% CI effect size d = 0 based t statistic sample size (Smithson, 2003), see 15 observations condition independent t test 95% CI ranges d = -0.7156777 d = 0.71567774. margin error half width 95% CI, 0.7156777. Bayesian estimator uses uninformative prior compute credible interval (similar) upper lower bound (Albers et al., 2018; J. K. Kruschke, 2011), might conclude collecting data left range plausible values population effect large informative. Regardless statistical philosophy plan rely analyzing data, evaluation can conclude based width interval tells us 15 observation per group learn lot.One useful way interpreting width confidence interval based effects able reject true effect size 0. words, effect, effects able reject given collected data, effect sizes rejected, effect? Effect sizes range d = 0.7 findings \"People become aggressive provoked\", \"People prefer group groups\", \"Romantic partners resemble one another physical attractiveness\" (Richard et al., 2003). width confidence interval tells can reject presence effects large, existed, probably already noticed . true effects study realistically much smaller d = 0.7, good possibility learn anything already know performing study n = 15. Even without data, research lines consider certain large effects plausible (although effect sizes plausible differ fields, discussed ). hand, large samples researchers can example reject presence effects larger d = 0.2, null hypothesis true, analysis width confidence interval suggest peers many research lines likely consider study informative.see margin error almost, exactly, minimal statistically detectable effect (d = 0.7479725). small variation due fact 95% confidence interval calculated based t distribution. true effect size zero, confidence interval calculated based non-central t distribution, 95% CI asymmetric. Figure 8.7 visualizes three t distributions, one symmetric 0, two asymmetric distributions noncentrality parameter (normalized difference means) 2 3. asymmetry clearly visible small samples (distributions plot 5 degrees freedom) remains noticeable larger samples calculating confidence intervals statistical power. example, true effect size d = 0.5 observed 15 observations per group yield \\(d_s\\) = 0.50, 95% CI [-0.23, 1.22]. compute 95% CI around critical effect size, get \\(d_s\\) = 0.75, 95% CI [0.00, 1.48]. see 95% CI ranges exactly -2.9894746^{-8} 1.4835407, line relation confidence interval p value, 95% CI excludes zero test statistically significant. noted , different approaches recommended evaluate informative study often based information.\nFigure 8.7: Central (black) 2 non-central (darkgrey lightgrey) t distributions.\n","code":"\npower_tost <- TOSTER::powerTOSTtwo(alpha = 0.05, N = 50, low_eqbound_d = -0.6, high_eqbound_d = 0.6)"},{"path":"power.html","id":"plot-a-sensitivity-power-analysis","chapter":"8 Sample size justification","heading":"8.18 Plot a Sensitivity Power Analysis","text":"sensitivity power analysis fixes sample size, desired power, alpha level, answers question effect size study detect desired power. sensitivity power analysis therefore performed sample size already known. Sometimes data already collected answer different research question, data retrieved existing database, want perform sensitivity power analysis new statistical analysis. times, might carefully considered sample size initially collected data, want reflect statistical power study (ranges ) effect sizes interest analyzing results. Finally, possible sample size collected future, know due resource constraints maximum sample size can collect limited, want reflect whether study sufficient power effects consider plausible interesting (smallest effect size interest, effect size expected).Assume researcher plans perform study 30 observations collected total, 15 participant condition. Figure 8.8 shows perform sensitivity power analysis G*Power study decided use alpha level 5%, desire 90% power. sensitivity power analysis reveals designed study 90% power detect effects least d = 1.23. Perhaps researcher believes desired power 90% quite high, opinion still interesting perform study statistical power lower. can useful plot sensitivity curve across range smaller effect sizes.\nFigure 8.8: Sensitivity power analysis G*Power software.\ntwo dimensions interest sensitivity power analysis effect sizes, power observe significant effect assuming specific effect size. two dimensions can plotted create sensitivity curve. example, sensitivity curve can plotted G*Power clicking 'X-Y plot range values' button, illustrated Figure 8.9. Researchers can examine power -priori plausible range effect sizes, can examine effect sizes provide reasonable levels power. simulation-based approaches power analysis, sensitivity curves can created performing power analysis range possible effect sizes. Even 50% power deemed acceptable (case deciding act null hypothesis true non-significant result relatively noisy decision procedure), Figure 8.9 shows study design power extremely low large range effect sizes reasonable expect fields. Thus, sensitivity power analysis provides additional approach evaluate informative planned study , can inform researchers specific design unlikely yield significant effect range effects one might realistically expect.\nFigure 8.9: Plot effect size desired power n = 15 per group alpha = 0.05.\nnumber observations per group larger, evaluation might positive. might specific effect size mind, collected 150 observations per group, sensitivity analysis shown power sufficient range effects believe interesting examine, still approximately 50% power quite small effects. sensitivity analysis meaningful, sensitivity curve compared smallest effect size interest, range effect sizes expected. sensitivity power analysis clear cut-offs examine (Bacchetti, 2010). Instead, idea make holistic trade-different effect sizes one might observe care , associated statistical power.","code":"\npow_es <- pwr::pwr.t.test(\n  sig.level = 0.05,\n  n = 15,\n  power = 0.9,\n  type = \"two.sample\",\n  alternative = \"two.sided\")$d\nknitr::include_graphics(\"images/gpow_sensitivity_1.png\")"},{"path":"power.html","id":"the-distribution-of-effect-sizes-in-a-research-area","chapter":"8 Sample size justification","heading":"8.19 The Distribution of Effect Sizes in a Research Area","text":"personal experience commonly entered effect size estimate -priori power analysis independent t test Cohen's benchmark 'medium' effect size, known default effect. open G*Power, 'medium' effect default option -priori power analysis. Cohen's benchmarks small, medium, large effects used -priori power analysis (Cook et al., 2014; Correll et al., 2020), Cohen regretted proposed benchmarks (Funder & Ozer, 2019). large variety research topics means 'default' 'heuristic' used compute statistical power just unlikely correspond actual situation, also likely lead sample size substantially misaligned question trying answer collected data.researchers wondered better default , researchers basis decide upon effect size -priori power analysis. Brysbaert (2019) recommends d = 0.4 default psychology, average observed replication projects several meta-analyses. impossible know average effect size realistic, clear huge heterogeneity across fields research questions. average effect size often deviate substantially effect size expected planned study. researchers suggested change Cohen's benchmarks based distribution effect sizes specific field (Bosco et al., 2015; Funder & Ozer, 2019; Hill et al., 2008; Kraft, 2020; Lovakov & Agadullina, 2017). always, effect size estimates based published literature, one needs evaluate possibility effect size estimates inflated due publication bias. Due large variation effect sizes within specific research area, little use choosing large, medium, small effect size benchmark based empirical distribution effect sizes field perform power analysis.knowledge distribution effect sizes literature can useful interpreting confidence interval around effect size. specific research area almost effects larger value reject equivalence test (e.g., observed effect size 0, design reject effects larger example d = 0.7), -priori unlikely collecting data tell something already know.difficult defend use specific effect size derived empirical distribution effect sizes justification effect size used -priori power analysis. One might argue use effect size benchmark based distribution effects literature outperform wild guess, strong enough argument form basis sample size justification. point researchers need admit ready perform -priori power analysis due lack clear expectations (Scheel et al., 2020). Alternative sample size justifications, justification sample size based resource constraints, perhaps combination sequential study design, might line actual inferential goals study.","code":""},{"path":"power.html","id":"additional-considerations-when-designing-an-informative-study","chapter":"8 Sample size justification","heading":"8.20 Additional Considerations When Designing an Informative Study","text":"far, focus justifying sample size quantitative studies. number related topics can useful design informative study. First, addition -priori prospective power analysis sensitivity power analysis, important discuss compromise power analysis (useful) post-hoc retrospective power analysis (useful, e.g., Zumbo & Hubley (1998), Lenth (2007)). sample sizes justified based -priori power analysis can efficient collect data sequential designs data collection continued terminated based interim analyses data. Furthermore, worthwhile consider ways increase power test without increasing sample size. additional point attention good understanding dependent variable, especially standard deviation. Finally, sample size justification just important qualitative studies, although much less work sample size justification domain, proposals exist researchers can use design informative study. topics discussed turn.","code":""},{"path":"power.html","id":"compromise-power-analysis","chapter":"8 Sample size justification","heading":"8.21 Compromise Power Analysis","text":"compromise power analysis sample size effect fixed, error rates test calculated, based desired ratio Type Type II error rate. compromise power analysis useful large number observations collected, small number observations can collected.first situation researcher might fortunate enough able collect many observations statistical power test high effect sizes deemed interesting. example, imagine researcher access 2000 employees required answer questions yearly evaluation company testing intervention reduce subjectively reported stress levels. quite confident effect smaller d = 0.2 large enough subjectively noticeable individuals (Jaeschke et al., 1989). alpha level 0.05 researcher statistical power 0.994, Type II error rate 0.006. means smallest effect size interest d = 0.2 researcher 8.3 times likely make Type error Type II error.Although original idea designing studies control Type Type II error error rates researchers need justify error rates (Neyman & Pearson, 1933), common heuristic set Type error rate 0.05 Type II error rate 0.20, meaning Type error 4 times unlikely Type II error. default use 80% power (20% Type II \\(\\beta\\) error) based personal preference Cohen (1988), writes:proposed convention , investigator basis setting desired power value, value .80 used. means \\(\\beta\\) set .20. arbitrary reasonable value offered several reasons (Cohen, 1965, pp. 98-99). chief among takes consideration implicit convention \\(\\alpha\\) .05. \\(\\beta\\) .20 chosen idea general relative seriousness two kinds errors order .20/.05, .e., Type errors order four times serious Type II errors. .80 desired power convention offered hope ignored whenever investigator can find basis substantive concerns specific research investigation choose value ad hoc.see conventions built conventions: norm aim 80% power built norm set alpha level 5%. take away Cohen aim 80% power, justify error rates based relative seriousness error. compromise power analysis comes . share Cohen's belief Type error 4 times serious Type II error, building earlier study 2000 employees, makes sense adjust Type error rate Type II error rate low effect sizes interest (Cascio & Zedeck, 1983). Indeed, Erdfelder et al. (1996) created G*Power software part give researchers tool perform compromise power analysis.\nFigure 8.10: Compromise power analysis G*Power.\nFigure 8.10 illustrates compromise power analysis performed G*Power Type error deemed equally costly Type II error, study 1000 observations per condition lead Type error Type II error 0.0179. Faul, Erdfelder, Lang, Buchner (2007) write:course, compromise power analyses can easily result unconventional significance levels greater \\(\\alpha\\) = .05 (case small samples effect sizes) less \\(\\alpha\\) = .001 (case large samples effect sizes). However, believe benefit balanced Type Type II error risks often offsets costs violating significance level conventions.brings us second situation compromise power analysis can useful, know statistical power study low. Although highly undesirable make decisions error rates high, one finds oneself situation decision must made based little information, Winer (1962) writes:frequent use .05 .01 levels significance matter convention little scientific logical basis. power tests likely low levels significance, Type Type II errors approximately equal importance, .30 .20 levels significance may appropriate .05 .01 levels.example, plan perform two-sided t test, can feasibly collect 50 observations independent group, expect population effect size 0.5, 70% power set alpha level 0.05. can choose weigh types error equally, set alpha level 0.149, end statistical power effect d = 0.5 0.851 (given 0.149 Type II error rate). choice \\(\\alpha\\) \\(\\beta\\) compromise power analysis can extended take prior probabilities null alternative hypothesis account (Maier & Lakens, 2022; Miller & Ulrich, 2019; Murphy et al., 2014).compromise power analysis requires researcher specify sample size. sample size requires justification, compromise power analysis typically performed together resource constraint justification sample size. especially important perform compromise power analysis resource constraint justification strongly based need make decision, case researcher think carefully Type Type II error rates stakeholders willing accept. However, compromise power analysis also makes sense sample size large, researcher freedom set sample size. might happen , example, data collection part larger international study sample size based research questions. designs Type II error rates small (power high) statisticians also recommended lower alpha level prevent Lindley's paradox, situation significant effect (p < \\(\\alpha\\)) evidence null hypothesis (Good, 1992; Jeffreys, 1939). Lowering alpha level function statistical power test can prevent paradox, providing another argument compromise power analysis sample sizes large (Maier & Lakens, 2022). Finally, compromise power analysis needs justification effect size, either based smallest effect size interest effect size expected. Table 8.7 lists three aspects discussed alongside reported compromise power analysis.\nTable 8.7: Overview recommendations justifying error rates based compromise power analysis.\n","code":""},{"path":"power.html","id":"what-to-do-if-your-editor-asks-for-post-hoc-power","chapter":"8 Sample size justification","heading":"8.22 What to do if Your Editor Asks for Post-hoc Power?","text":"Post-hoc, retrospective, observed power used describe statistical power test computed assuming effect size estimated collected data true effect size (Lenth, 2007; Zumbo & Hubley, 1998). Post-hoc power therefore performed looking data, based effect sizes deemed interesting, -priori power analysis, unlike sensitivity power analysis range interesting effect sizes evaluated. post-hoc retrospective power analysis based effect size observed data collected, add information beyond reported p value, presents information different way. Despite fact, editors reviewers often ask authors perform post-hoc power analysis interpret non-significant results. sensible request, whenever made, comply . Instead, perform sensitivity power analysis, discuss power smallest effect size interest realistic range expected effect sizes.Post-hoc power directly related p value statistical test (Hoenig & Heisey, 2001). z test p value exactly 0.05, post-hoc power always 50%. reason relationship p value observed equals alpha level test (e.g., 0.05), observed z score test exactly equal critical value test (e.g., z = 1.96 two-sided test 5% alpha level). Whenever alternative hypothesis centered critical value half values expect observe alternative hypothesis true fall critical value, half fall critical value. Therefore, test observed p value identical alpha level exactly 50% power post-hoc power analysis, analysis assumes observed effect size true.statistical tests, alternative distribution symmetric (t test, alternative hypothesis follows non-central t distribution, see Figure 8.7), p = 0.05 directly translate observed power 50%, plotting post-hoc power observed p value see two statistics always directly related. Figure 8.11 shows, p value non-significant (.e., larger 0.05) observed power less approximately 50% t test. Lenth (2007) explains observed power also completely determined observed p value F tests, although statement non-significant p value implies power less 50% longer holds.\nFigure 8.11: Relationship p values power independent t test \\(\\alpha\\) = 0.05 n = 10.\neditors reviewers ask researchers report post-hoc power analyses like able distinguish true negatives (concluding effect, effect) false negatives (Type II error, concluding effect, actually effect). Since reporting post-hoc power just different way reporting p value, reporting post-hoc power provide answer question editors asking (Hoenig & Heisey, 2001; Lenth, 2007; Schulz & Grimes, 2005; Yuan & Maxwell, 2005). able draw conclusions absence meaningful effect, one perform equivalence test, design study high power reject smallest effect size interest (Lakens, Scheel, et al., 2018). Alternatively, smallest effect size interest specified designing study, researchers can report sensitivity power analysis.","code":""},{"path":"power.html","id":"sequential-analyses","chapter":"8 Sample size justification","heading":"8.23 Sequential Analyses","text":"Whenever sample size justified based -priori power analysis can efficient collect data sequential design. Sequential designs control error rates across multiple looks data (e.g., 50, 100, 150 observations collected) can reduce average expected sample size collected compared fixed design data analyzed maximum sample size collected (Proschan et al., 2006; Wassmer & Brannath, 2016). Sequential designs long history (Dodge & Romig, 1929), exist many variations, Sequential Probability Ratio Test (Wald, 1945), combining independent statistical tests (Westberg, 1985), group sequential designs (Jennison & Turnbull, 2000), sequential Bayes factors (Schönbrodt et al., 2017), safe testing (Grünwald et al., 2019). approaches, Sequential Probability Ratio Test efficient data can analyzed every observation (Schnuerch & Erdfelder, 2020). Group sequential designs, data collected batches, provide flexibility data collection, error control, corrections effect size estimates (Wassmer & Brannath, 2016). Safe tests provide optimal flexibility dependencies observations (ter Schure & Grünwald, 2019).Sequential designs especially useful considerable uncertainty effect size, plausible true effect size larger smallest effect size interest study designed detect (Lakens, 2014). situations data collection possibility terminate early effect size larger smallest effect size interest, data collection can continue maximum sample size needed. Sequential designs can prevent waste testing hypotheses, stopping early null hypothesis can rejected, stopping early presence smallest effect size interest can rejected (.e., stopping futility). Group sequential designs currently widely used approach sequential analyses, can planned analyzed using rpact (Wassmer & Pahlke, 2019) gsDesign (K. M. Anderson, 2014).5","code":""},{"path":"power.html","id":"increasing-power-without-increasing-the-sample-size","chapter":"8 Sample size justification","heading":"8.24 Increasing Power Without Increasing the Sample Size","text":"straightforward approach increase informational value studies increase sample size. resources often limited, also worthwhile explore different approaches increasing power test without increasing sample size. first option use directional tests relevant. Researchers often make directional predictions, ‘predict X larger Y’. statistical test logically follows prediction directional (one-sided) t test. directional test moves Type error rate one side tail distribution, lowers critical value, therefore requires less observations achieve statistical power.Although discussion directional tests appropriate, perfectly defensible Neyman-Pearson perspective hypothesis testing (Cho & Abe, 2013), makes (preregistered) directional test straightforward approach increase power test, riskiness prediction. However, might situations want ask directional question. Sometimes, especially research applied consequences, might important examine null effect can rejected, even effect opposite direction predicted. example, evaluating recently introduced educational intervention, predict intervention increase performance students, might want explore possibility students perform worse, able recommend abandoning new intervention. cases also possible distribute error rate 'lop-sided' manner, example assigning stricter error rate effects negative positive direction (Rice & Gaines, 1994).Another approach increase power without increasing sample size, increase alpha level test, explained section compromise power analysis. Obviously, comes increased probability making Type error. risk making either type error carefully weighed, typically requires taking account prior probability null-hypothesis true (Cascio & Zedeck, 1983; Miller & Ulrich, 2019; Mudge et al., 2012; Murphy et al., 2014). make decision, want make claim, data can feasibly collect limited, increasing alpha level justified, either based compromise power analysis, based cost-benefit analysis (Baguley, 2004; Field et al., 2004).Another widely recommended approach increase power study use within participant design possible. almost cases researcher interested detecting difference groups, within participant design require collecting less participants participant design. reason decrease sample size explained equation Maxwell et al. (2017). number participants needed two group within-participants design (NW) relative number participants needed two group -participants design (NB), assuming normal distributions, :\\[NW = \\frac{NB (1-\\rho)}{2}\\]required number participants divided two within-participants design two conditions every participant provides two data points. extent reduces sample size compared -participants design also depends correlation dependent variables (e.g., correlation measure collected control task experimental task), indicated (1-\\(\\rho\\)) part equation. correlation 0, within-participants design simply needs half many participants participant design (e.g., 64 instead 128 participants). higher correlation, larger relative benefit within-participants designs, whenever correlation negative (-1) relative benefit disappears. Especially dependent variables within-participants designs positively correlated, within-participants designs greatly increase power can achieve given sample size available. Use within-participants designs possible, weigh benefits higher power downsides order effects carryover effects might problematic within-participants design (Maxwell et al., 2017).6 designs multiple factors multiple levels can difficult specify full correlation matrix specifies expected population correlation pair measurements (Lakens & Caldwell, 2021). cases sequential analyses might provide solution.general, smaller variation, larger standardized effect size (dividing raw effect smaller standard deviation) thus higher power given number observations. additional recommendations provided literature (Allison et al., 1997; Bausell & Li, 2002; Hallahan & Rosenthal, 1996), :Use better ways screen participants studies participants need screened participation.Assign participants unequally conditions (data control condition much cheaper collect data experimental condition, example).Use reliable measures low error variance (Williams et al., 1995).Smart use preregistered covariates (Meyvis & Van Osselaer, 2018).important consider ways reduce variation data come large cost external validity. example, intention--treat analysis randomized controlled trials participants comply protocol maintained analysis effect size study accurately represents effect implementing intervention population, effect intervention people perfectly follow protocol (Gupta, 2011). Similar trade-offs reducing variance external validity exist research areas.","code":""},{"path":"power.html","id":"know-your-measure","chapter":"8 Sample size justification","heading":"8.25 Know Your Measure","text":"Although convenient talk standardized effect sizes, generally preferable researchers can interpret effects raw (unstandardized) scores, knowledge standard deviation measures (Baguley, 2009; Lenth, 2001). make possible research community realistic expectations standard deviation measures collect, beneficial researchers within research area use validated measures. provides reliable knowledge base makes easier plan desired accuracy, use smallest effect size interest unstandardized scale -priori power analysis.addition knowledge standard deviation important knowledge correlations dependent variables (example Cohen's dz dependent t test relies correlation means). complex model, aspects data-generating process need known make predictions. example, hierarchical models researchers need knowledge variance components able perform power analysis (DeBruine & Barr, 2021; Westfall et al., 2014). Finally, important know reliability measure (Parsons et al., 2019), especially relying effect size published study used measure different reliability, measure used different populations, case possible measurement reliability differs populations. increasing availability open data, hopefully become easier estimate parameters using data earlier studies.calculate standard deviation sample, value estimate true value population. small samples, estimate can quite far , due law large numbers, sample size increases, measuring standard deviation accurately. Since sample standard deviation estimate uncertainty, can calculate confidence interval around estimate (Smithson, 2003), design pilot studies yield sufficiently reliable estimate standard deviation. confidence interval variance \\(\\sigma^2\\) provided following formula, confidence standard deviation square root limits:\\[(N - 1)s^2/\\chi^2_{N-1:\\alpha/2},(N - 1)s^2/\\chi^2_{N-1:1-\\alpha/2}\\]Whenever uncertainty parameters, researchers can use sequential designs perform internal pilot study (Wittes & Brittain, 1990). idea behind internal pilot study researchers specify tentative sample size study, perform interim analysis, use data internal pilot study update parameters variance measure, finally update final sample size collected. long interim looks data blinded (e.g., information conditions taken account) sample size can adjusted based updated estimate variance without practical consequences Type error rate (Friede & Kieser, 2006; Proschan, 2005). Therefore, researchers interested designing informative study Type Type II error rates controlled, lack information standard deviation, internal pilot study might attractive approach consider (Chang, 2016).","code":""},{"path":"power.html","id":"conventions-as-meta-heuristics","chapter":"8 Sample size justification","heading":"8.26 Conventions as meta-heuristics","text":"Even researcher might use heuristic directly determine sample size study, indirect way heuristics play role sample size justifications. Sample size justifications based inferential goals power analysis, accuracy, decision require researchers choose values desired Type Type II error rate, desired accuracy, smallest effect size interest. Although sometimes possible justify values described (e.g., based cost-benefit analysis), solid justification values might require dedicated research lines. Performing research lines always possible, studies might worth costs (e.g., might require less resources perform study alpha level peers consider conservatively low, collect data required determine alpha level based cost-benefit analysis). situations, researchers might use values based convention.comes desired width confidence interval, desired power, input values required perform sample size computation, important transparently report use heuristic convention (example using accompanying online Shiny app). convention use 5% Type 1 error rate 80% power practically functions lower threshold minimum informational value peers expected accept without justification (whereas justification, higher error rates can also deemed acceptable peers). important realize none values set stone. Journals free specify desire higher informational value author guidelines (e.g., Nature Human Behavior requires registered reports designed achieve 95% statistical power, department required staff submit ERB proposals , whenever possible, study designed achieve 90% power). Researchers choose design studies higher informational value conventional minimum receive credit .past fields changed conventions, 5 sigma threshold now used physics declare discovery instead 5% Type error rate. fields attempts unsuccessful (e.g., Johnson (2013)). Improved conventions context dependent, seems sensible establish consensus meetings (Mullan & Jacoby, 1985). Consensus meetings common medical research, used decide upon smallest effect size interest (example, see Fried et al. (1993)). many research areas current conventions can improved. example, seems peculiar default alpha level 5% single studies meta-analyses, one imagine future default alpha level meta-analyses much lower 5%. Hopefully, making lack adequate justification certain input values specific situations transparent motivate fields start discussion improve current conventions. online Shiny app links good examples justifications possible, continue updated better justifications developed future.","code":""},{"path":"power.html","id":"sample-size-justification-in-qualitative-research","chapter":"8 Sample size justification","heading":"8.27 Sample Size Justification in Qualitative Research","text":"value information perspective sample size justification also applies qualitative research. sample size justification qualitative research based consideration cost collecting data additional participants yield new information valuable enough given inferential goals. One widely used application idea known saturation indicated observation new data replicates earlier observations, without adding new information (Morse, 1995). example, imagine ask people pet. Interviews might reveal reasons grouped categories, interviewing 20 people, new categories emerge, point saturation reached. Alternative philosophies qualitative research exist, value planning saturation. Regrettably, principled approaches justify sample sizes developed alternative philosophies (Marshall et al., 2013).sampling, goal often pick representative sample, sample contains sufficiently diverse number subjects saturation reached efficiently. Fugard Potts (2015) show move towards informed justification sample size qualitative research based 1) number codes exist population (e.g., number reasons people pets), 2) probability code can observed single information source (e.g., probability someone interview mention possible reason pet), 3) number times want observe code. provide R formula based binomial probabilities compute required sample size reach desired probability observing codes.advanced approach used Rijnsoever (2017), also explores importance different sampling strategies. general, purposefully sampling information sources expect yield novel information much efficient random sampling, also requires good overview expected codes, sub-populations code can observed. Sometimes, possible identify information sources , interviewed, least yield one new code (e.g., based informal communication interview). good sample size justification qualitative research based 1) identification populations, including sub-populations, 2) estimate number codes (sub-)population, 3) probability code encountered information source, 4) sampling strategy used.","code":""},{"path":"power.html","id":"discussion","chapter":"8 Sample size justification","heading":"8.28 Discussion","text":"Providing coherent sample size justification essential step designing informative study. multiple approaches justifying sample size study, depending goal data collection, resources available, statistical approach used analyze data. overarching principle approaches researchers consider value information collect relation inferential goals.process justifying sample size designing study sometimes lead conclusion worthwhile collect data, study sufficient informational value justify costs. cases unlikely ever enough data perform meta-analysis (example lack general interest topic), information used make decision claim, statistical tests allow test hypothesis reasonable error rates estimate effect size sufficient accuracy. good justification collect maximum number observations one can feasibly collect, performing study anyway waste time /money (Brown, 1983; Button et al., 2013; S. D. Halpern et al., 2002).awareness sample sizes past studies often small meet realistic inferential goals growing among psychologists (Button et al., 2013; Fraley & Vazire, 2014; Lindsay, 2015; Sedlmeier & Gigerenzer, 1989). increasing number journals start require sample size justifications, researchers realize need collect larger samples used . means researchers need request money participant payment grant proposals, researchers need increasingly collaborate (Moshontz et al., 2018). believe research question important enough answered, able answer question current resources, one approach consider organize research collaboration peers, pursue answer question collectively.sample size justification seen hurdle researchers need pass can submit grant, ethical review board proposal, manuscript publication. sample size simply stated, instead carefully justified, can difficult evaluate whether value information researcher aims collect outweighs costs data collection. able report solid sample size justification means researchers knows want learn study, makes possible design study can provide informative answer scientific question.","code":""},{"path":"power.html","id":"designing-efficient-studies","chapter":"8 Sample size justification","heading":"8.29 Designing efficient studies","text":"far, mainly focused perform power analysis based assumptions true effect size . Although true effect size unknown, true effect size population, impact statistical power. therefore useful consider can design studies optimally efficient answer research question. ways can increase power test worth exploring.","code":""},{"path":"power.html","id":"use-directional-tests-where-relevant.","chapter":"8 Sample size justification","heading":"8.29.1 Use directional tests where relevant.","text":"Researchers often make directional predictions, ‘predict X larger Y’. statistical test logically follows prediction directional (one-sided) t-test. directional test moves Type 1 error rate one side tail, lowers critical value, requires less observations achieve statistical power. Although discussion directional tests appropriate, perfectly defensible Neyman-Pearson perspective hypothesis testing. However, might situations want ask directional test. Sometimes might important effect opposite prediction also reach statistical significance. example, evaluating recently introduced educational intervention, predict intervention increase performence students, might want remain open possibility students perform worse, able recommend abandoning new intervention. also possible distribute error rate 'lop-sided' manner, example assigning stricter error rate effects negative positive direction (Rice & Gaines, 1994).","code":""},{"path":"power.html","id":"use-sequential-analysis-whenever-possible","chapter":"8 Sample size justification","heading":"8.29.2 Use sequential analysis whenever possible","text":"Sequential analyses greatly increase efficiency tests (Lakens, 2014). ways determined sample size study discussed , sequential design, error rates controlled repeatedly analyze data comes , average decrease sample size collect.","code":""},{"path":"power.html","id":"increase-your-alpha-level","chapter":"8 Sample size justification","heading":"8.29.3 Increase your alpha level","text":"explained section compromise power analysis balancing error rates increasing alpha level also increases statistical power (reduces Type 2 error rate). Obviously comes increased probability fooling . risk carefully weighed, typically requires taking account prior probability null-hypothesis true (Miller & Ulrich, 2019). make decision, data can feasibly collect limited, compromise power analysis justified, increasing alpha level.","code":""},{"path":"power.html","id":"use-within-designs-where-possible","chapter":"8 Sample size justification","heading":"8.29.4 Use within designs where possible","text":"One widely recommended approach increase power using within subject design. Indeed, need fewer observations detect mean difference two conditions within-subjects design (dependent t-test) -subjects design (independent t-test). reason straightforward, always explained, even less often expressed easy equation . sample size needed within-designs (NW) relative sample needed -designs (NB), assuming normal distributions, (Maxwell & Delaney, 2004, p. 561, formula 45):\\[NW = \\frac{NB (1-\\rho)}{2}\\]divide two due fact two-condition within design every participant provides two data-points. extent reduces sample size compared -subject design depends correlation two dependent variables, indicated (1-\\(\\rho\\)) part equation. correlation 0, within-subject design simply needs half many observations -subject design (e.g., 64 instead 128 observations). higher correlation, larger relative benefit within designs, whenever correlation negative (-1) relative benefit disappears. Note correlation -1, need 128 observations within-design 128 observations -design, within-design need collect two measurements participant, making within design work -design. However, negative correlations dependent variables psychology rare, perfectly negative correlations probably never occur.correlation increases power within designs, reduces number observations need? Let’s see effect correlation power simulating plotting correlated data. R script , ’m simulating two measurements IQ scores specific sample size (.e., 10000), mean (.e., 100 vs 106), standard deviation (.e., 15), correlation two measurements. script generates three plots.start simulation correlation measurements 0. First, see two normally distributed IQ measurements left, means 100 106, standard deviations 15 (due large sample size, numbers equal input simulation, although small variation might still occur). scatter plot right, can see correlation measurements indeed 0. look distribution mean differences. mean difference -6 (line simulation settings), standard deviation 21. also expected. standard deviation difference scores \\(\\sqrt{2}\\) times large standard deviation measurement, indeed, 15×\\(\\sqrt{2}\\) = 21.21, rounded 21. situation correlation measurements zero equals situation independent t-test, correlation measurements taken account.\nFigure 8.12: Distributions two dependent groups means 100 106 standard deviation 15, distribution differences, correlation 0.\nNow let’s increase correlation dependent variables 0.7. Nothing changed plot means. correlation measurements now strongly positive, see plot right. important difference lies standard deviation difference scores. SD difference scores 11 instead 21 uncorrelated example. standardized effect size difference divided standard deviation, effect size (Cohen’s dz within designs) larger test test .\nFigure 8.13: Distributions two independent groups means 100 106 standard deviation 15, distribution differences, correlation 0.7.\nset correlation negative value, standard deviation difference scores increase. like think dependent variables within-designs dance partners. well-coordinated (highly correlated), one person steps left, person steps left distance. coordination (correlation), one dance partner steps left, dance partner just likely move wrong direction right direction. dance couple take lot space dance floor.see correlation dependent variables important aspect within designs. recommend explicitly reporting correlation dependent variables within designs (e.g., participants responded significantly slower (M = 390, SD = 44) used feet used hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p < 0.001, Hedges' g = 0.43, M_diff = 19, 95% CI [12; 26]).Since dependent variables within designs psychology positively correlated, within designs greatly increase power can achieve given sample size available. Use within-designs possible, weigh benefits higher power downsides order effects carryover effects might problematic within-subject design (Maxwell et al., 2017).can use Shiny app play around different means, sd's, correlations, see effect distribution difference scores.","code":""},{"path":"power.html","id":"remove-statistical-variation-where-possible","chapter":"8 Sample size justification","heading":"8.29.5 Remove statistical variation where possible","text":"smaller variation, larger standardized effect size (dividing raw effect smaller denominator) thus higher power given number observations. overview different approaches reduce variance, see Allison et al. (1997). discuss:Better ways screen participants studies participants need screened participation.Assigning participants unequally conditions (control condition much cheaper experimental condition, example).Using multiple measurements increase measurement reliability (use well-validated measures, may add).Smart use (preregistered, ’d recommend) covariates.Another approach mention , possible, collect multiple observations participant. can also increase power, especially variation individual level, analyze data hierarchical models.","code":""},{"path":"power.html","id":"use-bayesian-statistics-with-informed-priors","chapter":"8 Sample size justification","heading":"8.29.6 Use Bayesian statistics with informed priors","text":"Regrettably, almost approaches statistical inferences become limited number observations small. confident predictions (peers agree), incorporating prior information give benefit. discussion benefits risks approach, see van de Schoot et al. (2015).","code":""},{"path":"power.html","id":"what-if-best-practices-are-not-enough","chapter":"8 Sample size justification","heading":"8.30 What if best practices are not enough?","text":"end, getting informative answer requires designing informative experiment, sometimes matter efficient design experiment, need collect data. Sometimes. secret tricks magical solutions. ? solutions , regrettably, lot work making small change design study. time start take seriously.","code":""},{"path":"power.html","id":"ask-for-more-money-in-your-grant-proposals","chapter":"8 Sample size justification","heading":"8.30.1 Ask for more money in your grant proposals","text":"grant organizations distribute funds awarded function much money requested. need money collect informative data, ask . Obviously grants incredibly difficult get, ask money, include budget acknowledges data collection cheap hoped years ago. experience, psychologists often asking much less money collect data scientists. Increasing requested funds participant payment factor 10 often reasonable, given requirements journals provide solid sample size justification, realistic effect size estimates emerging preregistered studies.","code":""},{"path":"power.html","id":"improve-management","chapter":"8 Sample size justification","heading":"8.30.2 Improve management","text":"implicit explicit goals meet still now 10 years ago, receive miraculous increase money time research, update evaluation criteria long overdue. sincerely hope manager capable , ‘upward management’ might needed. coda Lakens & Evers (2014) wrote:else equal, researcher running properly powered studies clearly contribute cumulative science researcher running underpowered studies, researchers take science seriously, former rewarded tenure systems reward procedures, latter.","code":""},{"path":"power.html","id":"change-what-is-expected-from-phd-students","chapter":"8 Sample size justification","heading":"8.30.3 Change what is expected from PhD students","text":"PhD, assumption performed enough research 4 years employed full-time researcher write thesis 3 5 empirical chapters (chapters multiple studies). studies ideally published, least publishable. consider important PhD students produce multiple publishable scientific articles PhD’s, greatly limit types research can . Instead evaluating PhD students based publications, can see PhD time researchers learn skills become independent researcher, evaluate based publishable units, terms clearly identifiable skills. personally doubt data collection particularly educational 20th participant, probably prefer hire post-doc well-developed skills programming, statistics, broadly read literature, someone used time collect participant 21 200. make easier PhD students demonstrate skills level can evaluate learned sensible manner counting number publications wrote. Currently, difference resources PhD students disposal huge confound try judge skill based resume. Researchers rich universities obviously resources – difficult develop tools allow us judge skills people resources much less confound.","code":""},{"path":"power.html","id":"get-answers-collectively","chapter":"8 Sample size justification","heading":"8.30.4 Get answers collectively","text":"society serious issues psychologists can help address. questions incredibly complex. long lost faith idea bottom-organized scientific discipline rewards individual scientists manage generate reliable useful knowledge can help solve societal issues. questions need well-coordinated research lines hundreds scholars work together, pool resources skills, collectively pursuit answers important questions. going limit research questions can answer small labs, big societal challenges going solved. Call pessimist.reason resort forming unions organizations goal collectively coordinate . greatly dislike team science, don’t worry – always options make scientific contributions . now, almost ways scientists want pursue huge challenges large well-organized collectives hundreds thousands scholars (recent exception proves rule remaining unfunded: see Psychological Science Accelerator). honestly believe research question important enough answered, get together everyone also thinks , pursue answers collectively.","code":""},{"path":"references.html","id":"references","chapter":"9 References","heading":"9 References","text":"","code":""}]
