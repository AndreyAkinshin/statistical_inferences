[
["index.html", "Global Health Research: Designs and Methods Welcome", " Global Health Research: Designs and Methods Eric P. Green 2020-07-11 Welcome This book is in the process of being updated for 2019. New chapters will be released on a rolling basis. This work is shared under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. A big thanks to the creators of the bookdown and msmbstyle packages for R used to compile the web version of this book. The source code for this book is available on Github. ISBN 978-0-578-44376-8 "],
["contents.html", "Contents", " Contents Introduction Module 1: Getting Started With Global Health Research Chapter 1: Global Health Research Chapter 2: Developing Research Ideas Chapter 3: Systematic Reviews and Meta-Analyses Chapter 4: Critical Appraisal Module 2: Define Your Study Aims Chapter 5: Research Questions and Aims Chapter 6: The Role of Theory in Global Health Chapter 7: Outcomes and Indicators Module 3: Understand Inference Chapter 8: Statistical Inference Chapter 9: Causal Inference Module 4: Select a Research Design Chapter 10: Experimental Chapter 11: Quasi-Experimental Chapter 12: Observational Module 5: Specify Your Methods Chapter 13: Sampling and Power Chapter 14: Quantitative Data Collection Chapter 15: Qualitative Data Collection Module 6: Practice Good Science and Make an Impact Chapter 16: Collaborations Chapter 17: Research Ethics Chapter 18: Open Science Chapter 19: Sharing Your Work Chapter 20: Impacting Policy and Practice "],
["preface.html", "Preface", " Preface This book will introduce you to research designs and methods in global health. I wrote this text for undergraduate and graduate students taking my introductory course at Duke University. Therefore, it shares the two central aims of my course: to make you a better consumer of research and to help you design your first study. Module 1 begins with an introduction to global health research and teaches you how to identify research problems, search the literature, and practice critical appraisal. In Module 2, you’ll learn how to ask evidence-based research questions, create study aims, integrate theory, and specify important constructs, outcomes, and indicators. We’ll turn to research designs in Module 3. In global health, we are often interested in knowing what treatments, programs, interventions, and policies “work” and why. To answer questions of impact, researchers sometimes design randomized controlled trials. Randomization is not always possible or advisable, however, and researchers must build a causal argument using non-experimental designs. We will discuss the core principles of causal inference and consider the strengths and limitations of research designs most commonly used in the behavioral and social sciences, public health, and medicine. Module 4 will help you fill in the remaining details for a Method section. In particular, you’ll learn about sampling and data collection procedures. Modules 5 and 6 conclude with a discussion of how to practice good science and make an impact with your work. One limitation of this book is it does not teach statistics. Statistical concepts are discussed throughout but not in great detail. Because statistical analysis is an intrinsic part of the study design stage, I recommend downloading a copy of OpenIntro Statistics and reading it alongside this book. Visit themethodsection.com for additional materials. "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements If you find something you like in this book, I can probably trace its origin to one of the many people who helped me pull it all together. I want to thank them for this good work and absolve them from responsibility for any errors. I’ll start with my students. A big thank you to my graduate student teaching assistants who reviewed early drafts, including Kaitlin Saxton, Kathleen Perry, Olivia Fletcher, and Jenae Logan, as well as students in my courses who provided feedback—anonymously or not (Kelsey Sumner, Karly Gregory, Qian Yudong, Christina Schmidt). Next I’d like to thank my colleagues at Duke who provided a lot of support. Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. Biostatistican Liz Turner for fielding lots of technical questions. Gavin Yamey for helping me understand what we do and don’t know about funding for global health research. On the institutional side, I’m grateful to the Learning Innovations team for coming on this journey with me, including Andrea Novicki, Heather Hans, William Williamson, Ben Richardson, Michael Blair, and Quentin Ruiz-Esparza. Thanks as well to Mary Story and Sarah Martin for supporting me from within the Duke Global Health Institute, and to Duke OIT staff, including Zach Hill, Jeremy Hopkins, and Richard Biever, who helped me to get domains and servers working, despite my efforts to thwart their progress. I used R and numerous tools from RStudio to create this book and course materials. I’m very thankful for their support to educators like me. The same goes for members of the open source community who create and maintain awesome software, including Yihui Xie (bookdown), Mike Smith (msmbstyle), Jonathan Weisberg (css help), and many others. Several scholars were very generous with their time and agreed to let me interview them. Thanks to Daniel Halperin, Salim Abdulla, Paul Garner, Wendy O’Meara, and Vikram Patel (so far). Other colleagues shared comments on drafts, including Daniel Lakens and Solomon Kurz. Finally, I appreciate my wife Eve for listening to boring stories about why I deleted certain paragraphs (and for suggesting I delete many others), my son Tucker for asking, “Really, dad, will you ever finish this book?”, my daughter Annie for napping occasionally during paternity leave, and my parents for always encouraging me and, more practically, for watching Annie from time to time so I might not disappoint my son. Jef Brown, an Artist &amp; Illustrator living in Seattle, WA, created the cover illustration. Creating the cover for this book, I wanted to create imagery that reflected the “action” of research in Global Health. Rather than depict a portrait of impoverished villages, a nebulous globe, or still life of medicine- I created a representation of the data that drives the change. I was inspired by the bubble charts of Hans Rosling and the aesthetic philosophy of Wassily Kandinsky. For both, “shape” was an important tool for expression. In creating the constellation of circles I was able to convey a dynamic representation for a larger living idea. "],
["introduction.html", "Introduction The HIV/AIDS Pandemic: A Microcosm of Global Health Research The Long Road from Evidence to Practice The Takeaway", " Introduction Figure 0.1: The Morbidity and Mortality Weekly Report is a weekly series published by the CDC that disseminates critical public health information and recommendations. On June 5, 1981, the US Centers for Disease Control and Prevention published a case report that would mark the start of a global pandemic that no one saw coming. This 1-page bulletin described five men in Los Angeles who had developed a rare lung infection called Pneumocystis carinii pneumonia. In the period October 1980-May 1981, 5 young men, all active homosexuals, were treated for biopsy-confirmed Pneumocystis carinii pneumonia at 3 different hospitals in Los Angeles, California. Two of the patients died. All 5 patients had laboratory-confirmed previous or current cytomegalovirus (CMV) infection and candidal mucosal infection. Pneumocystis pneumonia in the United States is almost exclusively limited to severely immunosuppressed patients. The occurrence of pneumocystosis in these 5 previously healthy individuals without a clinically apparent underlying immunodeficiency is unusual. The fact that these patients were all homosexuals suggests an association between some aspect of a homosexual lifestyle or disease acquired through sexual contact and Pneumocystis pneumonia in this population. Three thousand miles away, doctors in New York observed a cluster of similar cases. Hymes et al. (1981) described these patients in an article published in the September 19 issue of The Lancet, the first article on the disease to appear in the scientific literature. News outlets including the *Associated Press*, the *Los Angeles Times*, and the *San Francisco Chronicle* covered the release of the June 5 MMWR. The [*New York Times*](https://www.nytimes.com/1981/07/03/us/rare-cancer-seen-in-41-homosexuals.html) published its first article on the epidemic, &quot;Rare Cancer Seen in 41 Homosexuals&quot;, about a month later. AIDS would not make the [front page of the *New York Times*](https://www.nytimes.com/times-insider/2014/10/30/1983-having-claimed-558-lives-aids-finally-made-it-to-the-front-page/) until May 25, 1983. As clinicians and researchers began to connect the dots between the likely presence of an infectious agent and opportunistic infections among gay men, the public came to know this crisis as the “gay cancer” and eventually “Gay-Related Immune Deficiency” or GRID. On June 17, 1982, the National Broadcasting Company, an American English-language television network, broadcast a report on the evening news about a new study showing how the “lifestyle” of some homosexual men had created an “epidemic” of a “rare form of cancer”. By the time this report aired, there were 413 reported cases in the US, a third of whom had already died. The CDC estimated that thousands more might be affected. It would be a few more months until the CDC used the term AIDS (Acquired Immune Deficiency Syndrome) for the first time in September 1982, and nearly another 2 years until researchers identified the retrovirus that causes AIDS, HTLV-III, now known as HIV (human immunodeficiency virus). No one in the early 1980s could fathom that more than 70 million people worldwide would contract HIV over the next few decades, or that more than 35 million people would die (WHO 2019). The HIV/AIDS Pandemic: A Microcosm of Global Health Research The scientific story of the HIV/AIDS pandemic can teach us a lot about the full spectrum of global health research, and in particular highlights the typical sequence of work that follows the outbreak of a new disease. First came the case reports that alerted the medical and research communities to the new disease and described its onset and clinical features. These reports provided the motivation for future studies and generated hypotheses about the causative agents. Figure 0.2: Results from Rogers et al. (1983) showing that cases have deficiency in T-Helper cells compared to controls. Figure reproduced by Yours Truly based on reported results. Then came a case-control study that pointed to the role of cellular immune deficiency in the development of opportunistic infections (Rogers et al. 1983; Jaffe et al. 1983). As we’ll discuss in a later chapter, case-control studies are often the first designs that public health researchers employ when studying a new topic because they are relatively easy to conduct. In the first HIV case-control study, researchers recruited homosexual men with biopsy-confirmed Kaposi’s sarcoma or Pneumocystis carinii pneumonia as cases and identified homosexuals without either condition from nearby clinics and friend networks as controls. Researchers took biological samples and asked participants about their past sexual behavior, looking for associations with the presence of disease. Typically a cohort study like this would not be possible to complete so quickly, but @jaffe1985 were &#39;fortunate&#39; in that they had access to a group of nearly 7,000 homosexual and bisexual men who enrolled in Hepatitis B studies (and provided blood samples) between 1970 and 1980—*before we knew anything about HIV*. That meant that the researchers were able to establish the percentage of men in the sample who tested positive for HIV as far back as 1978 and followed them forward in time to see who developed AIDS and when. A potentially more informative observational design is the cohort study, but this design requires researchers to follow a group of people over time and observe who develops the condition. This ‘wait and see’ approach takes time, money, and a good bit of coordination. Jaffe et al. (1985) published the first AIDS cohort study, finding that there was a long incubation time from infection to development of AIDS. The authors reported that for every person in the sample with AIDS, there were 30 with HIV. This suggested to them that even if transmission of HIV stopped right then, there would be several hundred thousand infected Americans who were likely to develop AIDS eventually, representing a major public health concern. From this point on, the scholarly literature on HIV and AIDS exploded. Lakeh and Ghaffarzadegan (2017) estimated that the annual publication rate was more than 14,000 papers per year by 2012. Observational studies like these conducted in the early 1980s were followed over the next few decades by bench science on the virus and on drug candidates, diagnostic validity studies of rapid HIV tests, the establishment of demographic surveillance sites, drug and prevention trials, and many, many other types. The literature today is a microcosm of global health research. Experts from just about every discipline have contributed to our knowledge. Biology, economics, psychology, biostatistics, medicine, law, and so on. Figure 0.3: Global trend in HIV/AIDS publications (Lakeh and Ghaffarzadegan 2017). Figure reproduced by Yours Truly based on WebPlotDigitizer extraction of published image. Throughout this book, I’ll return to the HIV/AIDS pandemic as well as introduce many other global health challenges, such as Zika, malaria, and depression, to teach you about how research problems and questions are identified, how studies are designed to provide answers, and how scientists make a difference with their work. The Long Road from Evidence to Practice This last idea—making your work have an impact—is one that we will revisit throughout this book. Most researchers will, I assume, tell you that they did not pursue careers in science to publish papers as the end goal. They got into research to make a difference. To change the way we think about a topic, or to change policy or practice. To improve lives, or possibly even to save lives. But science is often incremental, with one study building on another, and publishing scientific papers is one important piece of the puzzle. While there are examples of individual studies pushing a field to change, overnight it might seem in some cases, these studies are frequently the culmination of years or decades of prior work. The road to impact can be long, hard, and often fraught with skepticism and self-doubt. Figure 0.4: Tinderbox, by Daniel Halperin and Craig Timberg (2012). Learn more by listening to an episode of NPR’s Fresh Air or reading reviews in the Washington Post and New York Times. Few people know this better than Dr. Daniel Halperin, an epidemiologist, medical anthropologist, and author of the 2012 book, Tinderbox: How the West Sparked the AIDS Epidemic and How the World Can Finally Overcome It. Halperin’s co-author, journalist Craig Timberg, describes him as follows: Halperin is tall, dark haired, and lean, and his smile is disarming. This is good, because he has made a career of telling people that most of what they think they know about HIV is wrong. When I met him while working on a story in 2005, he immediately departed from the script of most experts I had met. While their conversations had focused mainly on condoms, HIV testing, or the distant hope of a breakthrough vaccine, Halperin veered quickly into the realm of the impolite. He insisted that the two most important factors in understanding the spread of AIDS through African societies were sexual behavior and male circumcision. Halperin was not the first to posit a link between male circumcision and HIV transmission, but he was one of the more vocal advocates of this idea during a time when few wanted to consider it fully. Halperin and other advocates of male circumcision as a prevention strategy would eventually be proven correct, but it would take nearly two decades (Figure 0.5). Figure 0.5: Scientific timeline of male circumcision and HIV/AIDS. THE FIRST HINTS THAT MALE CIRCUMCISION MIGHT BE PROTECTIVE AGAINST HIV Expert Opinion, 1986 Letters to the Editor, often published under the heading &quot;Correspondence&quot;, are short pieces usually under 400 words about an article published in the journal or topic of interest to the author. The story started in 1986 with two letters published in medical journals in which the authors raised the possibility that uncircumcised men could be at higher risk of transmission because of the delicate nature of the foreskin of the penis (Alcena 1986; Fink 1986). Timberg and Halperin explain the biological mechanism: The reason is simple—if rather graphic—anatomy. The skin on the shaft of a man’s penis is, like that on most of his body, relatively thick and tough, allowing it to serve as a natural barrier against infection. But the foreskin of an uncircumcised man is unusually vulnerable, because it is soft, thin, and a bit moist, making it easier for pathogens to penetrate. HIV targets certain types of immune cells that are close to the surface in foreskin tissue. During erection the foreskin is stretched back down to cover the upper part of the penis shaft, turning this most vulnerable skin outward, where it can come into contact with fluids that may contain the virus. The penis of a circumcised man, by contrast, presents a daunting challenge to HIV. There is no inner foreskin to turn outward and fewer easily accessible immune cells that the virus could infect. The man is much safer, and so are his future sex partners. With fewer infections, the overall community is safer too. Case-Control, 1988 When these letters made their way into print, a study that would provide the first (and unexpected) empirical evidence of a potential link between male circumcision and HIV transmission was already underway in the Pumwani slum of Kenya’s capital Nairobi. At this time in 1986, researchers knew that HIV causes AIDS and that HIV was not limited to homosexuals in the United States, but there was a puzzle. In the US and Europe, HIV mainly affected homosexual men. But in Africa, HIV transmission was most common among heterosexual partners. So Allan Ronald, Francis Plummer, and a team that included Peter Piot, who would eventually become the Executive Director of UNAIDS, began recruiting men who frequented a group of Pumwani prostitutes who had an HIV rate of 85%(!) to study transmission dynamics. Figure 0.6: Abbreviated results from Simonsen et al. (1988) showing that uncircumcised men were more likely to have HIV. In the first article they published, a case-control study of 340 men, 11% of whom had HIV, the team found that the odds of being uncircumcised were 2.7 times higher among HIV-positive men (cases) compared to the HIV-negative controls (Simonsen et al. 1988). The authors were surprised: The association between HIV infection and lack of circumcision is interesting an unexpected…However, since 85 percent of American white men are circumcised, the foreskin may have been overlooked as a risk factor… Prospective Cohort, 1989 The team followed these men over two years, ultimately recruiting and following a cohort of 293 men who were seronegative (HIV-negative) at enrollment (Cameron et al. 1989). 24 (8.2%) of these men seroconverted to HIV-I during the follow-up period, which means they went from being HIV-negative to HIV-positive. When the team looked at the circumcision numbers, they were astonished. As told in Tinderbox, Plummer, the lead author of the study, was running the analysis from the University of Manitoba one night and called his colleague Allan Ronald, exclaiming, “Allan, you won’t believe what we found!” Plummer was amazed because he found that 18 (23%) of the 79 uncircumcised men got HIV, compared to only 6 (3%) of the 214 circumcised men. The odds of being uncircumcised were 10.2 times higher among men who got HIV! This study was important because it confirmed the unexpected results of the case-control (cross-sectional) study with a stronger, prospective (‘wait-and-see’) design, and it suggested the link between circumcision and HIV might be even stronger than expected. Figure 0.7: Estimated HIV seroprevalence in capital city and percentage of males who are circumcised for 37 African countries (Bongaarts et al. 1989). Correlational, 1989 A few months before the results of the Pumwani cohort study would go to press, the esteemed demographer John Bongaarts and several co-authors combined ethnographic data on circumcision practices from the 1950s and 1960s with the best estimates of seroprevalence in African capital cities to look for a pattern. The relationship was unmistakable. When circumcision was high, seroprevalence was low, and vice versa. The data underlying the analysis were not perfect, but a compelling story was coming together on the basis of these observational studies. “DON’T TALK ABOUT CIRCUMCISION ANYMORE” Figure 0.8: Overlap between regions where men are typically uncircumcised and the ‘AIDS Belt’ (red outline), where HIV rates are highest (Caldwell and Caldwell 1996). As Halperin recounts in Tinderbox, he first learned about the possible role of male circumcision in 1996 when his mother visited the dentist and picked up the March issue of Scientific American. This issue featured an article by John and Pat Caldwell (1996) that reviewed evidence for explanations for why half of all cases of HIV worldwide were concentrated in a chain of countries in eastern and southern Africa known as the AIDS Belt. In the years since the first observational studies suggesting a link were published in the late 1980s, the global health community largely ignored male circumcision as a possible explanation for varying seroprevalence rates across Africa. But as the Caldwells saw it, the evidence pointed convincingly to the combination of risky sexual behavior and a lack of male circumcision. ## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family ## not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : ## font family not found in Windows font database Figure 0.9: Data on male circumcision and seroprevalence from Halperin and Bailey (1999), visualized by Yours Truly. Halperin said that this article “altered the trajectory of his professional life”. Three years later—a full decade since Cameron et al. (1989) published the results of the Pumwani cohort study—Halperin teamed up with anthropologist and epidemiologist Robert Bailey to write a commentary for The Lancet called “Male circumcision and HIV infection: 10 years and counting” (Halperin and Bailey 1999). Their article was a not-so-gentle call to action. By avoiding [male circumcision] altogether, medical professionals and public-health authorities may inadvertently be harming the very individuals whom they are trying to help…The hour has passed for the international health community to recognise the compelling evidence that show a significant association between lack of male circumcision and HIV infection. A couple years after this article came out, Halperin took a job in Washington, DC with the US Agency for International Development, or USAID, as an “HIV Prevention Advisor”. As he describes in Tinderbox, his ideas did not sit well with most of his new colleagues. Superiors scolded him, “Don’t talk about circumcision anymore”, and “We do condoms!” IN SEARCH OF A DEFINITIVE ANSWER Figure 0.10: Forest plot showing the results of 15 (of 27) studies that estimated the association between male circumcision and HIV transmission and adjusted for potential confounding (Weiss, Quigley, and Hayes 2000). We’ll discuss systematic reviews, meta-analysis, and forest plots like this one in a later chapter. When Halperin worked at USAID, it was condoms or bust. But things were starting to change, in part due to the publication of an influential systematic review and meta-analysis of studies on male circumcision and the risk of HIV infection in sub-Saharan Africa (Weiss, Quigley, and Hayes 2000). The authors included 27 observational studies in the review and found that 21 reported a lower risk of HIV among circumcised men. Among the 15 studies that adjusted for potential confounding, all 15 reported that circumcision was effective (adjusted relative risk = 0.42, CI 0.34±0.54). Weiss, Quigley, and Hayes (2000) concluded: The data from observational studies provide compelling evidence of a substantial protective effect of male circumcision against HIV infection in sub-Saharan Africa, especially in populations at high risk of HIV/ STD. The continuing rapid spread of HIV infection, especially in eastern and southern Africa, suggests that the potential public health benefit of introducing safe services for male circumcision on a wider scale should be explored. But in the eyes of many, “compelling” is not the same as “definitive”. Before male circumcision could become part of the HIV prevention toolbox, people wanted experimental evidence. And they were about to get it. Three large-scale trials of male circumcision got underway in Africa in 2002. Different teams of researchers independently enrolled 3,274 uncircumcised, HIV-negative men in South Africa, 4,996 in Uganda, and 2,784 in Kenya in separate studies. Half of the participants in each study were randomized to be circumcised, with their consent of course. The rest formed the uncircumcised control group. As we will explore together in later chapters, this design, the randomized controlled trial, is often held up as the best source of evidence. There is nuance to this idea that we must unpack, but it’s safe to say that these trials were created to provide a definitive answer about the link between male circumcision and HIV. The results were astonishing. While these studies closed one chapter, they did not close the book on male circumcision. One big question for **implementation science** was how to incorporate male circumcision into the broader public health response. Other researchers began to focus on the potential unintended consequences of male circumcision. In trials like these, it’s common to plan interim analyses where a board of advisors takes a scheduled peek at the data at certain points before the trial is over to make sure that the treatment group is not getting too much worse—or too much better. The latter is exactly what happened in the South Africa trial when the team took their peek in 2004. Circumcised men had a relative risk of HIV acquisition of 0.4, suggesting that circumcision offered a protective effect of 60%. The data and safety monitoring board decided to stop the trial early and offer the treatment to every man in the control group. The other two trials experienced the same fate because interim analyses suggested a similar effect. Figure 0.11: Randomized controlled trials of male circumcision. In his book with Stephen Inrig, *The AIDS Pandemic: Searching for a Global Response* [-@merson2018, pp. 378], former Director of the World Health Organization&#39;s Global Programme on AIDS (GPA), Michael Merson, offers a reflection on this period and concludes in retrospect that GPA should have funded more research on male circumcision when it had the chance. And like that, the question of efficacy was put to rest. After nearly two decades—and many preventable infections and deaths—it became clear to everyone that male circumcision helps to prevent HIV transmission. The Takeaway The road from research question to policy impact can be long. Decades, in fact, as demonstrated by the scientific history of male circumcision and HIV prevention. But for researchers and advocates who are willing doggedly follow the data wherever it leads, real change is possible. As of 2017, nearly 15 million men living in the AIDS Belt have been circumcised, potentially averting more than 500,000 new HIV infections through 2030 (WHO 2017). Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["module-1-getting-started-with-global-health-research.html", "MODULE 1 Getting Started With Global Health Research", " MODULE 1 Getting Started With Global Health Research By the end of this module, you should be able to: describe the landscape of global health research begin developing research ideas effectively search the literature and make use of systematic reviews and meta-analyses critically appraise scientific work "],
["ghr.html", "1 Global Health Research 1.1 What is Global Health? 1.2 What Makes Research Scientific? 1.3 What Constitutes Global Health Research? 1.4 Who Funds Global Health Research? 1.5 Who Produces Global Health Research? 1.6 Where is Global Health Research Published? 1.7 The Takeaway", " 1 Global Health Research 1.1 What is Global Health? New York County Courthouse, circa 2009 Judge presiding over jury selection: And what do you do, Mr. Green? Me: I do research in global health, global mental health. Judge: Me: Access to mental health services, things like that. Judge: Health policy. Me: No, studies on interventions to expand access. Judge: All around the globe. Me: Not exactly. I work in a few countries, but on issues that face lots of low-income communities. Judge: What is global health? Me: Well, you see…rambles… The judge also bristled at my statement that I lived in Alphabet City, clarifying for me that it was called the Lower East Side. Whatever. It has a [Wikipedia entry](https://en.wikipedia.org/wiki/Alphabet_City,_Manhattan). Judge: Thank you. Mr. Green, you are dismissed. In fairness to the Honorable Judge, it’s not clear what I should have expected her to imagine when I said “global health”. Everyone understands medicine, and most folks probably have a good sense of what we mean by public health. But global health? Label Description Medicine Clinical care for individuals Public health Prevention and health promotion of populations (community, sub-national units, nation) Tropical medicine Health issues facing tropical and subtropical regions International health Health issues abroad (from the perspective of the researcher) Global health Clinical care and prevention (including inequality in access and outcomes) related to health issues that transcend national borders Planetary health Interdependence between human health, animal health, and health of the environment Maybe I would have made the cut had had I said, “global health is the study and practice of improving health and achieving equity in health for all people worldwide”. This is how Koplan et al. (2009) defined global health in a Lancet article published a few months after my court appearance. They also offered this longer version: Global health emphasises transnational health issues, determinants, and solutions; involves many disciplines within and beyond the health sciences and promotes interdisciplinary collaboration; and is a synthesis of population-based prevention with individual-level clinical care. &quot;Determinants&quot; of health/illness is one of those terms that you&#39;ll come across a lot in global health. It refers to the things that contribute to our health and wellbeing. [Pretty much everything](http://tinyurl.com/zvdxezv). Ironically, despite sounding so *deterministic*, the study of determinants is mostly *probabilistic*. A social determinant like poverty is generally associated with worse health outcomes, but not everyone living in poverty has the same experience. This definition reflects the reality that the determinants of ill health and inequality are complex, so the search for solutions must span multiple disciplines. In the study of malaria, for example, you can read about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), and the adoption and use of bed nets (psychology), just to name a few areas of inquiry. Therefore, we say that global health research is multidisciplinary and interdisciplinary. It is multidisciplinary in the sense that no one field can solve the great global health challenges of our time, and interdisciplinary because collaboration is core to progress (Merson, Black, and Mills 2011). 1.2 What Makes Research Scientific? We conduct research all the time. Your internet browser history probably has many good examples. Mine does. When was the term “AIDS” first used? How widespread is male circumcision today? Do bees poop? (I have a curious 4 year old.) But my research into the answers to these questions is not an example of scientific research. According to the Common Rule, a US federal policy that governs research with human subjects, research is defined as: a systematic investigation, including research development, testing, and evaluation, designed to develop or contribute to generalizable knowledge (Title 45, Subtitle A, Subchapter A, Part 46.102) Global health largely focuses on applied research questions. What are risk factors for a certain disorder? Does this new treatment improve health? Unlike research questions in the natural sciences, the answers to most global health research questions are probabilistic. Uncertain. What goes up might come down, for some people. There are three main characteristics of scientific research that apply to most of global health (King, Keohane, and Verba 1994): The goal is inference. The procedures are public. The conclusions are uncertain. 1.2.1 ALL ABOUT INFERENCE By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. Inference refers to the process of making conclusions about some unobserved or unmeasured phenomenon based on direct observations of the world. What is known is used to infer something about things that are not known. This process can be deductive or inductive. In deductive reasoning, we start from general theories and make hypotheses. Then we collect data and make conclusions based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true. The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not or cannot observe directly. This is sometimes referred to as empiricism, and our systematic observations are empirical evidence. Empiricism is at the heart of scientific research. 1.2.2 RESEARCH AS A PUBLIC ACT Figure 1.1: A poorly documented Method section. Source: http://tinyurl.com/yxtabza3 Scientific research uses public methods that can be examined and replicated. A Method section in a scientific paper is like a recipe. If you have ever tried to follow a confusing recipe, you can appreciate the importance of good documentation. Your study’s recipe must be clear (well written), thorough (no “dash” of this or that), and shared publicly (not a secret passed down to lab members). Replication is a core principle of scientific research. No one study rules the day. If the results of a study are robust, another research group should be able to follow your recipe and replicate the findings. When such findings are replicated, we all have more confidence in the results. In later chapters we’ll learn strategies for writing a good Method section (and appendices!) and discuss why replications are actually rare in practice—and why they can be so controversial. 1.2.3 LIVING WITH UNCERTAINTY Every method has limitations, every measurement has error, and every model is wrong to some extent. Take the estimation of maternal mortality rates as an example. Hogan et al. (2010) published maternal mortality estimates for 181 countries. Some countries, such as the United States, have vast amounts of data in vital registries that attempt to track all births and deaths. It’s not perfect, so we still estimate the maternal mortality rate using a statistical model (we’ll discuss models more in a later chapter). As you can see in the left panel of Figure 1.2, the United States has a very low level of maternal mortality, between about 10-20 maternal deaths for every 100,000 live child births. Compared to some countries, the US has a lot of data points for estimating the level and trend in maternal deaths, so the “uncertainty band” is narrow. Figure 1.2: Estimates of the maternal mortality rate in the United States and Afghanistan, (Hogan et al. 2010). Visualized by Yours Truly based on values extracted from published plots. Now take a look at Afghanistan on the right. Note that the y-axis scale is much larger in the 1000s, reflecting the fact that many more Afghan women die of causes related to pregnancy or childbirth. Next, pay attention to the width of the uncertainty band. It spans a range of more than 3000 deaths. Compare this to a range of fewer than 5 deaths in the US! This is because there are very few data points available to estimate the “true” value in Afghanistan, and these individual data points can differ by more than 1000 deaths. We don’t know much about maternal mortality in Afghanistan, but the blue line is our best guess. Your [spidy sense](https://en.wiktionary.org/wiki/Spidey-sense) should tingle a bit when you read an article that ignores or hides uncertainty. The takeaway message is that there is uncertainty in everything. No single estimate can be considered “The Truth.” Instead, we must focus on the origin of estimates and recognize the limitations of what we know or what is being reported. 1.3 What Constitutes Global Health Research? Global health research brings together scholars and practitioners from many different disciplines to tackle big challenges. Therefore, the methods of these disciplines are the methods of global health research. We can organize the landscape as shown in Figure 1.3. Figure 1.3: A research taxonomy Research is divided into two main categories: basic and applied. Overlapping with applied research is an area of work called “monitoring and evaluation”, or M&amp;E. Let’s examine what constitutes research before turning to M&amp;E. 1.3.1 BASIC RESEARCH Basic research—also known as “pure” or “blue skies” research—is the pursuit of fundamental knowledge of phenomena. For example, scientists conduct laboratory experiments to understand the parasitic life cycle and how parasites interact with humans at different stages. Another example is the scientific investigation of the properties of cancer cells to better understand how they grow and spread. The information generated by basic science becomes the basis for applied science. It’s the “bench” in the “bench to bedside” cascade of research needed to take an idea from the lab (the bench) to a new medical treatment (delivered to the patient’s bedside). Harvard Medical School’s Rachel Wilson explains this beautifully: The new therapies of today were the prototypes of yesterday. And the prototypes of yesterday were previously just findings in laboratories, and before that they were just an idea. Unless we have new ideas, we’re not going to have useful therapies. Great new therapies don’t just fall like apples from a tree. 1.3.2 APPLIED RESEARCH Applied research focuses on specific problems or real-world applications. Whereas the basic scientist might toil away in the lab to understand how cellular enzymes function without a specific clinical application in mind—discovering this function is the end goal for this scientist—the applied researcher will take this knowledge of enzymes to develop new therapies that could treat cancer. In the world of malaria research, an applied research question might be, “How can we increase the coverage and use of bed nets that prevent malaria transmission?” The basic science, such as the behavioral habits of the mosquito and the transmission conditions for malaria, have already been characterized by entomologists and epidemiologists. Clinical Research Read about Aunt Debbie&#39;s journey: [https://tinyurl.com/y5ae459m](https://tinyurl.com/y5ae459m). Applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, outcomes research, and health services research. Basic research provides the foundation for all clinical research. Figure 1.4: Pipeline from basic research to FDA approval. Source: http://www.phrma.org/advocacy/research-development/clinical-trials. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/dsfPOpE-GEs&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;What are clinical trial phases? National Cancer Institute (2016). [https://tinyurl.com/y2rur4nx](https://tinyurl.com/y2rur4nx) One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies, such as the Federal Food and Drug Administration (FDA), will approve their use with humans: Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same. Phase Enrollment Goal Preclinical research Are there signs that the drug candidate will have an effect in the lab? Phase 0 &lt;10 What happens in the body (pharmacokinetics) when a very low dose is administered to human subjects? (optional phase) Phase I 10s Is the drug safe? What is the best dose that balances possible effects with toxicity? Phase II 100s When using this optimal dose, is there any effect of the drug on clinical markers or health outcomes? Phase III 1000s What is the effect of the drug on clinical markers or health outcomes when compared to an existing treatment or placebo in a randomized evaluation? Success at this stage is required for regulatory approval in some countries. Phase IV Are there long-term adverse effects of the drug once it is available on the market? Case study: Developing a malaria vaccine The development of a vaccine for malaria provides a good example of the life cycle of a clinical trial. In 2015, after 30 years of research, a vaccine candidate called RTS,S, or Mosquirix™, made the news for having gotten one step closer to becoming a licensed vaccine after a successful Phase III trial. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/Ww9jWFwfmHE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;Malaria vaccine one step closer. CNN (2015). [https://tinyurl.com/y4sfn4ep](https://tinyurl.com/y4sfn4ep) Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GlaxoSmithKline (GSK) and the Walter Reed Army Institute of Research. In 1987, a promising vaccine candidate entered preclinical research. During the preclinical phase, researchers performed tests on nonhuman subjects to collect data on how well the vaccine worked (efficacy), how much damage it could do to an organism (toxicity), and how the body affected the vaccine (pharmacokinetics). Figure 1.5: RTS,S timeline. Source: http://tinyurl.com/y3o2godv. If you compare the sample sizes of the Mosquirix™ trials, you&#39;ll see that they do not follow the rules of thumb noted in the table above. These types of deviations are common in research. The goals of these particular trials demanded larger sample sizes than is typical. Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997. This small sample size is typical of Phase I trials, where the main objectives are usually to determine a safe dosing range and to evaluate side effects. These researchers reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over the following decade (Phase IIa and Phase IIb) demonstrated the efficacy of the vaccine against several end points (or outcomes) (Moorthy and Ballou 2009). A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4 years (Alonso et al. 2004). Each child was randomly assigned to receive 3 doses of RTS,S or a control vaccine. After 6 months, the prevalence of malaria was 37% lower in the treatment group than in the control group. A follow-up study with 214 infants also showed partial protection from malaria (Aponte et al. 2007). This Phase II trial was an important proof-of-concept study. The final results of a large Phase III trial with more than 15,000 infants and young children in 7 African countries were published in The Lancet in 2015 (RTS,S Clinical Trials Partnership 2015). Children who participated in the study were randomly assigned to 1 of 3 arms: (a) 3 doses of RTS,S and a booster dose at month 20, (b) 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20, or (c) 4 doses of a comparator vaccine. RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3–4-year period. This Phase III trial achieved its goal—to show that the treatment was efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion” to help inform the decision of the WHO and African national regulatory authorities regarding their recommendation of the vaccine. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. Implementation Research and Translational Research Implementation *research* is a subset of implementation *science*. Whereas implementation research can focus on scaling a particular intervention, implementation science is the broader study of the methods to scale and promote the adoption of interventions. The research on RTS,S, will not end there, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to produce and deliver at a scale of millions. Studies that assess how best to get efficacious treatments to the people who need them most fall under the domain of implementation research. Translational research was originally focused on moving from &quot;bench to bedside&quot;, or from basic research in the lab to clinical research with humans. Today we recognize that other bottlenecks exist that prevent a good idea from making it out of the lab and ultimately impacting population health and policy. Making the jump from studies showing clinical efficacy to wide-spread adoption and population-level impact is one example of research translation. Practitioners of translational research point to 4 key bottlenecks: T1: Translation from basic science to clinical research T2: Translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: Translation from efficacy trials (i.e., Phase III trials) to real-world effectiveness through implementation research T4: Translation from evidence about delivery at scale to the adoption of new policies Figure 1.6: Bottlenecks in translating ideas into applications. Source: Medical University of South Carolina, http://bit.ly/2iq2Blv Summary Term Definition Example Basic research Fundamental research in the lab to characterize and understand some entity What are the properties of this enzyme? Pre-clinical research (applied) Test tube/culture (in vitro) or animal (in vivo) research to prepare for clinical research with human subjects What happens to this enzyme when a particular compound is introduced? Clinical research (applied) Research with human subjects to determine safety and efficacy with respect to clinical markers and health outcomes When formulated into a vaccine, is this vaccine candidate safe for humans? Does it have an effect on human health? Implementation research (applied) Research on how interventions/treatments work under real-world conditions What is the best way to distribute the vaccine and promote wide-scale adoption to achieve the same effects as shown in controlled trials? Translational research is cross-cutting, linked to several key bottlenecks where good ideas can stall and ultimately die. 1.3.3 MONITORING AND EVALUATION Figure 1.7: A research taxonomy, revisited. Another arena of applied work in global health is monitoring and evaluation, or M&amp;E. Evaluation In the United States, program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi, Lipsey, and Freeman 2003). But is program evaluation really research? Campbell had an outsized impact on the field. It is no surprise that an organization dedicated to synthesizing the best available evidence on social interventions, the [Campbell Collaboration](http://www.campbellcollaboration.org/), bears his name. Methods giant Donald Campbell thought so: The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available (Campbell 1969). But not everyone agrees. Some have argued that program evaluation is really designed for program implementers and funders, and that the messy nature of program implementation requires a loosening of research standards (Cronbach 1982). We should simply evaluate the evidence, or “learn what we can.” In their introductory text on evaluation, Rossi et al. (2003) strike a balance in views on this question of whether program evaluation is research. Their answer is perhaps a bit unsatisfying but is arguably true nevertheless: It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit—and no less. Surely a lower bound in terms of quality or what is worthwhile exists, but the line is so context dependent that a simple rule is best: “Don’t go beyond the data.” Every organization wants to claim “impact,” but not every evaluation can be based on the design and implementation. Impact evaluation Revisiting the research taxonomy presented earlier, we see that there is overlap between evaluation (orange) and research (blue), a nod to Campbell’s view that program evaluation can be considered research in many cases. Furthermore, we see that something called “impact evaluation” is a subset of evaluation. A clinical trialist who works on Phase III studies is unlikely to refer to the trial as an &quot;impact evaluation&quot;, even though the randomized study design is a type of impact evaluation. This language is more commonly used by economists and others who study the impact of social sector programs and interventions. For instance, see the [International Initiative for Impact Evaluation](http://www.3ieimpact.org/) (3ie) and the World Bank&#39;s [Development Impact Evaluation](http://www.worldbank.org/en/research/dime) (DIME) group. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/HEJlT8t5ezU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;What is impact evaluation? World Bank (2016). [https://tinyurl.com/yyxbpxvd](https://tinyurl.com/yyxbpxvd) In an impact evaluation, the goal is to infer that some program—or some treatment, intervention, or policy—had an effect on some outcome. As we’ll discuss in later chapters, there are several research designs that will generate evidence of “impact”. One design in particular, the randomized controlled trial common to Phase II and III clinical trials, is an important type of impact evaluation. Not all impact evaluations are RCTs, but all RCTs are impact evaluations. Monitoring Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (or with fidelity)? How many people participate, and does the program reach the intended targets? These are all program-monitoring questions. Bookmark these two terms. They will become very important when we talk about how to think about study results. Accurate monitoring is essential for reporting data to funders, but it is also essential for all good evaluations. The reason is simple: If a program fails—that is, has no impact—the next question is why? Did the program fail because the idea or theory behind the program was wrong (theory failure)? Or was the implementation of the program so troubled that there was never a chance for success (i.e., implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation to assess the impact of the program on a continual basis. 1.4 Who Funds Global Health Research? The precise amount spent on global health research and development (R&amp;D) is not clear [@schaferhoff:2015]. Billions of dollars are spent on global health research every year. This funding comes from public institutions such as the National Institutes of Health (NIH) in the United States and private philanthropy organizations such as the Bill and Melinda Gates Foundation. The [World RePort database](https://worldreport.nih.gov/app/) from the NIH—the [largest funder of health research in the world](http://www.healthresearchfunders.org/health-research-funding-organizations/)—lists health research projects totaling nearly $45 billion in 2015. It&#39;s possible to filter on the country of the recipient organization, but this does not count money disbursed to US and European organizations working on global health issues. In 2015, the international community disbursed $36.4 billion in development assistance for health, down from a peak of $38 billion in 2013 (Institute for Health Metrics and Evaluation 2016). Although Figure 1.8 does not present total research dollars contributed, per se, it does show the sources of global health financing, the channels through which they flow, and the areas on which they focus (see here for an interactive version). Figure 1.8: Flows of global health financing. Abbreviations: BMGF, Bill &amp; Melinda Gates Foundation; SWAps &amp; HSS, sector-wide approaches and health-sector support; Gavi, the Vaccine Alliance. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/fgh/ The United States Government contributed one-third of all development assistance for health in 2015 ($13 billion). More than half of USG funding was directed to HIV/AIDS, malaria, tuberculosis, and other infectious diseases. Noncommunicable diseases received &lt;1% of resources, despite accounting for 7 of the 10 leading causes of death globally (Figure 1.9). Figure 1.9: Global deaths per 100,000. Abbreviations: COPD, chronic obstructive pulmonary disease. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/gbd-compare/ 1.5 Who Produces Global Health Research? Academic centers around the world, like the Duke Global Health Institute, are major contributors to global health research. Public and private donors like the USG, the Bill and Melinda Gates Foundation, and the World Bank make grants to university-affiliated faculty members who partner with colleagues inside and outside governments to plan and conduct research. Donors also channel research support to nonprofit research organizations such as FHI 360 and RTI International, who work in a similar fashion. Interesting hybrid models include the Abdul Latif Jameel Poverty Action Lab (JPAL) and Innovations for Poverty Action (IPA). JPAL is a global network of university-affiliated professors from more than 50 universities that use randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website contains excellent resources regarding the methods used in randomized evaluations, as well as links to published studies and policy briefs. IPA is a sister organization that is also a leader in the use of randomized evaluations to study important policy questions about global poverty. 1.6 Where is Global Health Research Published? &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/HN6zpQGvkSA&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;What is global health? The Lancet (2016). [https://tinyurl.com/y6gmgabw](https://tinyurl.com/y6gmgabw) Global health research is published in medical journals (e.g., The Lancet and JAMA), general science journals (e.g., Science and PLOS ONE), discipline-specific journals (e.g., The Journal of Immunology and Epidemiology), and disease-specific journals (e.g., AIDS and Malaria Journal). Journals specializing in global health include The Lancet Global Health, BMJ Global Health, Global Public Health, and Global Health: Science and Practice. 1.7 The Takeaway Research is defined as a systematic investigation whose goal is to develop or contribute to generalizable knowledge. As global health is the study and practice of improving health and achieving equity in health, global health research includes all systematic efforts to create generalizable knowledge of how we can improve health and achieve equity in health for everyone, everywhere. Usually when people speak of &quot;the literature,&quot; they mean [scholarly](https://en.wikipedia.org/wiki/Academic_publishing) or [peer-reviewed](https://en.wikipedia.org/wiki/Peer_review) journal articles. In addition, a body of work called &quot;[grey literature](https://en.wikipedia.org/wiki/Grey_literature)&quot; is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples include technical reports or [white papers](https://en.wikipedia.org/wiki/White_paper) published on the web. Given the complexity of our global health challenges, global health research is multidisciplinary and interdisciplinary. This means that, as a budding global health researcher, you will likely collaborate on teams with colleagues from many different fields. Each discipline has its own methodological preferences and core approaches, but we all operate in the same research landscape that is divided into basic and applied research. Much of the literature in global health focuses on our applied challenges, so you will read a lot about clinical research and trials, translational research, implementation research, and M&amp;E (monitoring and evaluation). So no matter what discipline you call home, or what stage of research you find most interesting, you can make great contributions to our collective efforts to improve health and achieve equity in health. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["ideas.html", "2 Developing Research Ideas 2.1 Finding a Research Problem 2.2 Searching the Literature 2.3 The Takeaway", " 2 Developing Research Ideas &lt;iframe src=&quot;https://giphy.com/embed/p9bj7nrUPAypq&quot; width=&quot;250&quot; height=&quot;250&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;Actual footage of my first research meeting in grad school, &lt;a href=&quot;https://giphy.com/gifs/muppets-p9bj7nrUPAypq&quot;&gt;via GIPHY.&lt;/a&gt;&lt;/p&gt; In my experience as a former student and as a mentor, one of the most anxiety-producing moments in student life is when a new mentor says, “tell me about your ideas”. Why is this innocent prompt so terrifying? It might be because we have the wrong understanding of innovation and the origin of good ideas. Figure 2.1: Where Good Ideas Come From, by Steven Johnson. Also see his book, Ghost Map, a retelling of the story of John Snow and the 1854 outbreak of cholera in London. In his book Where Do Good Ideas Come From?, Steven Johnson argues that good ideas are usually not eureka moments as we tend to imagine. Instead, good ideas are often the product of fertile environments where ideas are allowed to “connect, fuse, and recombine” over time. Have a listen. I think two of Johnson’s insights are particularly relevant for us: the notion of the slow hunch and the adjacent possible. These concepts help to explain why generating research ideas can be so hard for students. First, the student timeline is often hostile to innovation. Learning and assignments are compressed into semesters, but many ideas need more time to take shape. As Johnson writes: Most great ideas first take shape in a partial, incomplete form. They have the seeds of something profound, but they lack a key element that can turn the hunch into something truly powerful…They start with a vague, hard-to-describe sense that there’s an interesting solution to a problem that hasn’t yet been proposed, and they linger in the shadows of the mind, sometimes for decades, assembling new connections and gaining strength. Ideas take time to develop. This is the slow hunch concept. “But I have a research meeting with my mentor next Tuesday and she wants a list of ideas!!,” you say. Don’t panic. When you recognize that good ideas come from the accumulation of slow hunches, you will learn to enjoy the process. Your first idea will be the start of your journey, not the end. Your best ideas might not conform to this semester’s schedule, and that is OK. Johnson borrows the metaphor of the **adjacent possible** from theoretical biologist [Stuart Kauffman](https://amzn.to/2ZoTEye) to illustrate the importance of connecting ideas. The second insight is that we are always sitting on the edge of a breakthrough, but we need access to certain raw materials to unlock this discovery. This is the adjacent possible concept, which Johnson describes as follows: &lt;iframe src=&quot;https://giphy.com/embed/Tb2i75AlI926I&quot; width=&quot;300&quot; height=&quot;261&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;Searching for the adjacent possible the night before your paper is due, &lt;a href=&quot;https://giphy.com/gifs/cheezburger-cartoons-doors-Tb2i75AlI926I&quot;&gt;via GIPHY&lt;/a&gt;&lt;/p&gt; Think of [the adjacent possible] as a house that magically expands with each door you open. You begin in a room with four doors, each leading to a new room that you haven’t visited yet. Those four rooms are the adjacent possible. But once you open one of those doors and stroll into that room, three new doors appear, each leading to a brand-new room that you couldn’t have reached from your original starting point. Keep opening new doors and eventually you’ll have built a palace. The first room is a metaphor for your initial understanding of a new topic. It is pretty empty because you are new, but your room has many potential doors to open—many strands of research to explore. When you open one door by searching the literature—and by connecting with other colleagues at talks, conferences, meet-ups—more doors appear. Figuring out which doors to walk through (and which ones to close) is a skill that comes with practice. Many literature searches can lead to apparent dead ends, but with each door you open, new hunches form. And when you have opened enough doors: …one day [your hunches] are transformed into something more substantial: sometimes jolted out by some newly discovered trove of information, or by another hunch lingering in another mind, or by an internal association that finally completes the thought. One suggestion is to adopt the &quot;commonplacing&quot; strategy for promoting discovery. It was all the rage in the Enlightenment Era. Read more about commonplace books in *Good Ideas* or in [this 2010 lecture](https://tinyurl.com/y5yrstz5). Johnson offers several suggestions for probing the adjacent possible and nurturing our slow hunches, pointing us to the importance of seeking out environments that will increase the likelihood that our ideas will collide with others. In this chapter, we will focus on how to open doors by searching the scientific literature. 2.1 Finding a Research Problem Every study begins with a motivating research problem. A research problem is your study’s ikigai, its “reason for being”, to borrow a Japanese concept. A research problem should convey a clear reason for being, a clear sense of purpose. Typically, a research problem gets framed as the gap in our knowledge—a gap in the literature. What&#39;s a bad example of a research problem? How about studying what happens to mosquitos when they die? Maybe they go to mosquito heaven, but we cannot use systematic, public methods to gather data. Therefore, this is not a research problem we can solve. This will just have to remain a gap in the literature. A defining characteristic of all research problems is that they are solvable (Leary 2012). To qualify as a research problem, we must be able to use systematic, public methods to gather and analyze data on the problem. For instance, research has shown that insecticide treated bed nets can prevent malaria infections in kids, but in many places kids are not sleeping underneath bed nets every night. Therefore, a research problem to solve is that we do not know how to encourage families to adopt and use this effective method of prevention. So how do you find research problems worth studying? &lt;iframe src=&quot;https://giphy.com/embed/d3mlE7uhX8KFgEmY&quot; width=&quot;300&quot; height=&quot;165&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;&lt;a href=&quot;https://giphy.com/gifs/culture--think-hmm-d3mlE7uhX8KFgEmY&quot;&gt;via GIPHY&lt;/a&gt;&lt;/p&gt; Once you have studied a field for a few years, the answer is easy: You have cultivated a list of journals, conferences, funders, and colleagues whom you follow to keep up with the latest developments and priorities. You probably wake up most mornings with new ideas to explore. Let’s assume this does not describe you. So where do YOU start? Do you have any personal or work experience that can shape your thinking? Dr. Salim Abdulla, the distinguished malaria researcher profiled in Chapter 2, grew up in Tanzania and witnessed the damaging effects of the disease up close. This influenced his decision to make vector control his life’s work. Or take the example of XXX… No experience? No problem. You are in good company. Most students I have mentored came to me with a general awareness about a health challenge, not a specific problem or research idea. “I’m interested in mental health”. When you are just starting out, the key is to begin exposing yourself to ideas. Start reading the health and science sections of major publications like the New York Times and set news alerts for topics you find interesting Subscribe to newsletters from relevant professional associations or ‘learned societies’ Attend research talks in-person or via online webinars hosted by universities, global health research organizations, or professional associations Look for experts active on social media Above all else, start reading the scientific literature. A useful strategy for finding reading material is to visit the websites of the most popular journals in your discipline and browse recent issues. Not sure what journals to search? Check out Google Scholar’s “Top Publications” feature. Figure 2.2: Google Scholar. Once you find a few articles of interest, try the following strategy for expanding your search and identifying gaps in the literature worth studying: Find the keywords: An article’s keywords often make great search terms. Look near the abstract. Read the Introduction: A good Introduction will frame gaps in our knowledge of a topic, so pull out a highlighter and get to work. Review the Discussion: The Discussion section may also hold new leads. Authors typically use the Discussion to link their study results to the existing literature to demonstrate how the results add to what is already known. A good Discussion section will also include limitations of the current study and might offer ideas for future research. Take note of the cited authors and journals: The Reference section may hold clues to your next great find. Search for more work by these authors and see what else these journals are publishing. 2.2 Searching the Literature As you increase your exposure to research ideas and become familiar with journals that publish interesting work, you will be ready to begin searching the literature. 2.2.1 SELECTING A DATABASE Most databases offer the option to create an account and save search strategies and results. This makes it easy to retrace and improve your search over time. Your developing research interests will dictate where you should search. For biomedical and clinical research, your first stop should be PubMed. PubMed is a fantastic free resource from the US National Library of Medicine that searches the MEDLINE database. For interdisciplinary science and social science research, the Web of Science and Scopus databases are good choices, but both require paid subscriptions; check your local library for access. A free alternative is Google Scholar. Research librarians are an excellent resource to determine whether other databases are suitable for your topic (see here for a comprehensive list of databases). 2.2.2 GENERATING SEARCH TERMS Once the proper database(s) is identified, specific search terms are needed. These usually coincide with the keywords published in related articles. One of the reasons PubMed/MEDLINE is a great (free!) resource is that all articles indexed in the database are tagged by a team of indexers with at least a bachelor&#39;s degree in a biomedical science. So unlike Google Scholar that only searches the text of articles, PubMed/MEDLINE also searches human-tagged meta-data. When searching PubMed/MEDLINE, it is often helpful to look up the correct MeSH terms for your topic. MeSH, which stands for “Medical Subject Headings,” is a controlled vocabulary thesaurus that is used by a team of specialists at the National Library of Medicine to index articles in PubMed/MEDLINE. This thesaurus is helpful because there are many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “breast neoplasm.” A search for “breast neoplasm” in PubMed actually searches more than 30 entry terms, such as “Tumors, Breast”, “Mammary Neoplasms, Human”, and “Carcinoma, Human Mammary”. A simple search for “breast neoplasm” will return a few hundred thousand results, so as you refine your research interests, you can add search terms and refine your strategy. The mnemonic PICO might help, especially if you are interested in clinical research. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome. PICO stands for Population/Problem, Intervention, Comparison, and Outcome. Let’s use PICO to develop a focused, searchable research question on preventing malaria during pregnancy. The problem is malaria infections, and our population of interest is pregnant women living in malaria-endemic areas. Not every clinical question involves testing of a treatment or intervention (“I”), but we will focus on these types of questions in this book. Let’s say we are interested in malaria chemoprevention, the use of an antimalarial medicine to prevent malaria infection. The “C” in PICO, comparison, answers the question, “compared to what?” Chemoprevention compared to what? Maybe we are interested in comparing the preventive effects of an antimalarial medicine compared to a placebo. The outcome (“O”) is our target. In this example, we are interested in whether chemoprevention prevents malaria infection. We can define this more specifically as parasitemia, the presence of malaria parasites in the blood. Combining all of this information yields a research question like this one: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitemia? 2.2.3 DEVELOPING A SEARCH STRATEGY Once you have identified initial search terms, it is time to build a query. Query construction is an iterative process, full of trial and error. Figure 2.3: Boolean operators: AND, OR, NOT Some basic Boolean operators are needed to conduct effective searches: AND, OR, NOT. For instance, consider the search PubMed runs when the terms “malaria OR pregnancy” are entered: (\"malaria\"[MeSH Terms] OR \"malaria\"[All Fields]) OR (\"pregnancy\"[MeSH Terms] OR \"pregnancy\"[All Fields]) These four terms are combined with OR, meaning we keep results that match any of these terms. PubMed returns more than 1 million results. Of course, it would make more sense to search for “malaria AND pregnancy,” instead of “malaria OR pregnancy”, as we are interested in malaria among pregnant women: (\"malaria\"[MeSH Terms] OR \"malaria\"[All Fields]) AND (\"pregnancy\"[MeSH Terms] OR \"pregnancy\"[All Fields]) To further limit the results to humans, we could add `AND &quot;humans&quot;[MeSH Terms]` to the end. &lt;br&gt; &lt;br&gt; `(&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) AND &quot;humans&quot;[MeSH Terms]` The first two terms and last two terms are combined separately with OR. These combinations are then combined with AND (notice the use of parentheses to segment the operations), shrinking the pool of results by 99%. The AND operator will always maintain or decrease the number of results. Combining the components of the PICO questions and Boolean operators can be very useful. Consider our research question once more: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/6wWeeCBBlk4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;5 Tips for Searching PubMed. Duke University Medical Center Library &amp; Archives (2012). [https://tinyurl.com/y66y7uvx](https://tinyurl.com/y66y7uvx) Here is a reasonable search strategy in plain English: P: (pregnancy OR pregnant women) AND malaria endemic I: AND chemoprevention (to search for specific drugs, string them together with ORs) C: AND placebo O: AND parasitaemia 2.2.4 ACCESSING JOURNAL ARTICLES This is often much harder than you would imagine, in part because scientific publishing is a big business. And boy do publishers have it good. Consider this lifecycle of an academic article: Private and public donors fund research Researchers conduct studies, write up the results, and send their work to a journal for free, often signing over the copyright Other scientists accept invitations from the journal to review the work pro bono Libraries pay the journal large sums of money to give members access the article Everyone else without library access is left to pay out of pocket [Plan S](https://www.coalition-s.org/) (the &quot;S&quot; stands for &quot;shock&quot;) might change the game. Developed in 2018, Plan S is an initiative to make all research funded with public funds to be published in open repositories or journals by 2020. No wonder publisher profit margins have exceeded some of our most innovative companies, such as Apple, Facebook, and Amazon! Thankfully there has been a move toward open access publishing in recent years that has increased the number of freely available articles, but too much knowledge still remains trapped behind paywalls. Figure 2.4: The number and proportion of open access articles split between Gold, Green, Hybrid, Bronze and closed access (from 1950 - 2016; Piwowar et al. 2018). To learn about the open access classification system, see here. If you are affiliated with a university and struggle to find free access to an article, ask a librarian for help. Sometimes when you are browsing for articles off-campus from a non-university IP address, it will appear as if you do not have access. Ask for help connecting. If authentication is not the issue and you still cannot access an article, your institution might not subscribe to the journal. In this case, see if your library offers an [inter-library loan program](https://en.wikipedia.org/wiki/Interlibrary_loan). If you are affiliated with a university, you (probably) have access to a wide range of journals. Here is what you can do if find yourself on the other side of a paywall looking in: Search for the article in Google or Google Scholar and include the search term “pdf”. This might help you locate a copy outside the paywall. Search free cloud services like ResearchGate to see if the authors have posted a pre-publication copy as many journals permit. Try a browser extension like unpaywall to see if there is an open access version of the paper you seek. Try emailing the author directly or contacting them through a service like ResearchGate. You’ll find that most authors are willing to share their work with you. Usually searching for the author’s name and university will bring up a profile with contact information. You can also try your luck with the hashtag “icanhazpdf” on Twitter and see if a friend on the internet will help you out. Don’t pay for access until you have exhausted every other option. 2.2.5 WATCH OUT FOR JUNK SCIENCE Figure 2.5: Bohannon (2013) submitted a fake paper to 304 journals on a naughty list of probable predatory journals or a nice list of open access publishers. 82% of the naughty list journals that responded published the bogus paper, mostly without any process that had a whiff of peer review. Surprisingly, 45% of the supposed real journals also published the fake results. One takeaway is that it can be hard to know if you are reading or submitting your work to a legitimate journal or a scam. If you search for studies on the open web (vs a database like PubMed), you are bound to come across articles published by a predatory publisher. These “journals” are happy to take your money and publish your work quickly with no peer review. Once you publish your first paper, you will become acquainted with them through the spam they send to your inbox daily. Here is a possible example. The Global Journal of Intellectual &amp; Developmental Disabilities (sounds real, right?) will publish anything I send AND let me join their editorial board despite having no expertise or publishing record on the topic. As of this writing, the GJIDD does not appear to be indexed with PubMed (you can check the NCBI databases). 2.2.6 USE A REFERENCE MANAGER The importance of using a software program for managing search results and references cannot be overstated. The manual collation and assembly of a bibliography is, simply put, a colossal waste of time. You can choose from several reference managers. Zotero is free and open source, which makes it a good choice for collaborations. Most programs share these core features: Easily imports references from databases like PubMed; moves from the search results to the reference manager instantly Automatically retrieves full-text PDFs Syncs PDFs to tablets and phones Connects to word processing software; inserting references in papers is easy Automatically creates bibliographies based on works cited Instantly reformats in-text citations and references to different styles, such as APA, AMA, or Harvard Shares collections by automatically sync-ing via the Cloud to facilitate collaboration. Easily exports references to other reference managers 2.3 The Takeaway Research ideas take time to develop. Be kind to yourself on days where it feels like your slow hunches are a bit too slow, and every door you open seems like a dead end rather than a portal to the adjacent possible. You will get there. You can jumpstart the process by learning about interesting work. Attend talks, watch webinars, meet with mentors, talk with fellow students. Above all else, read the literature. When you are just starting out, try skimming widely read journals to get a sense of current developments in global health disciplines. Follow interesting leads by noting article keywords, inspecting reference lists, and seeking out clues authors leave you in the introduction and discussion. Once you narrow your focus a bit, try searching scientific databases. If you wade into the open web, watch out for predatory journals masquerading as legitimate science (and hit delete on emails inviting you to publish your work, esteemed ladies and gentlemen). And please, please use a reference manager to keep track of all the cool science you are reading. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["filtered.html", "3 Systematic Reviews and Meta-Analyses 3.1 Systematic Reviews 3.2 Meta-Analysis 3.3 The Takeaway", " 3 Systematic Reviews and Meta-Analyses You will query the scientific literature hundreds or thousands of times in your career, so learning how to run effective searches is essential. But sometimes, it pays to let someone else do the work for you. I’m talking about systematic reviews and meta-analyses. 3.1 Systematic Reviews If you are interested in the impact (efficacy) of interventions—and I use the term broadly to mean clinical treatments, prevention interventions, social programs, and policies—then there might be a fast track to your learning called a systematic review. Figure 3.1: Literature reviews, systematic reviews, and meta-analyses A systematic review is a type of literature review in which the goal is to summarize all of the relevant evidence about the impact of an intervention on some outcome. Scholars who conduct systematic reviews register their research protocols in advance to clearly state the literature search strategy, rules for including and excluding studies, and a plan for the analysis. Some systematic reviews include a meta-analysis, which is quantitative technique for combining the results of multiple studies to estimate a pooled effect size that takes variations in study size and quality into consideration. It is likely you’ve written literature reviews for class or maybe your own academic manuscripts, so some aspects of systematic reviews will be familiar. Let’s consider systematic and literature reviews side-by-side to understand how systematic reviews are unique. Table 3.1: Comparing systematic reviews and literature reviews. Systematic Reviews Literature Reviews The goal of a systematic review is to be comprehensive and to include every relevant article. Literature reviews, on the other hand, do not follow such rigid or explicit methods. They are not expected to be exhaustive. For this reason, most systematic reviews are conducted by teams, given the large scope of the data initially collected for most research topics. Literature reviews can usually be conducted by a single person rather than a team Like any other aspect of research, however, systematic reviews must define and follow a method that can be replicated. Literature reviews don’t have to follow such rigid methods or make the methods explicit. Most systematic reviews preregister the research plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Preregistration gives other researchers confidence that the team is not selectively choosing advantageous results at the end to make an interesting paper. This registration informs other researchers that a group is working on a certain area of study, which can discourage duplicate research efforts that may, therefore, fail to be published. Not the case for literature reviews. These preregistration plans include a specific search strategy using specific search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement. Importantly, both inclusion criteria and exclusion criteria must be clearly outlined when a systematic review is undertaken. One inclusion criteria might be that assignment to study arms had to be random; an exclusion criteria might be all studies without a control arm that used a placebo. Most systematic searches specify several, if not many, criteria regarding which studies to include or exclude. Team members screen the search results and sort them according to these criteria, beginning with titles and abstract reviews and moving to full-text reviews later. Screening for a literature review is typically less intensive. In systematic reviews, specific details are extracted from every study included, such as numbers of participants, methods, analysis techniques, and key outcomes. An annotated bibliography might suffice for a literature review. In addition, the research team formally assesses the quality of each study, including the potential for bias, and these assessments are considered when the results are synthesized. This process is more ad hoc for literature reviews. 3.1.1 WHERE TO FIND SYSTEMATIC REVIEWS Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. Many of the reviews in these databases can be accessed by searching within PubMed using the Clinical Queries feature. 3.1.2 HOW TO READ SYSTEMATIC REVIEWS Figure 3.2: Go to PubMed and download ‘Drugs for preventing malaria in pregnant women in endemic areas: any drug regimen versus placebo or no treatment’ by Radeva-Petrova et al. (2014). Throughout this chapter, we’ll use use a systematic review of the effects of chemoprevention on malaria by Radeva-Petrova et al. (2014) as a learning example. Please take a moment and use the link to the right to download and skim through the article. It’s open access. Radeva-Petrova et al. (2014) set out to answer this basic question: Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes associated with malaria? One indicator of malaria infection is parasitemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, less parasitemia should be observed among women exposed to the medication (i.e., treatment). Few interventions are 100% effective, so scientists often talk about reductions in the risk of bad outcomes like malaria. So what did they find when they looked across the universe of published studies? Abstract and plain language summary Cochrane reviews follow a standard format that can look overwhelming, but this format makes them easy to read and understand. As with most journal articles, Cochrane reviews begin with an Abstract and a Plain language summary, which can be helpful for newcomers to a the topic. For example, Radeva-Petrova et al. (2014) include the following passage in their plain language summary: For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don’t know if it prevents maternal deaths, as this would require very large studies to detect an effect. This paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points to some gaps in the literature). Google does not filter the evidence in this manner. Starting with a systematic review pays off almost every time one is available. Summary tables Next come the Summary tables, such as the one presented below from Radeva-Petrova et al. (2014). These tables provide enough information to make an initial judgment about the intervention. Figure 3.3: Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Let’s use the summary table from Radeva-Petrova et al. (2014) as an example. Start by focusing on the red box and examine the comparative risk column that shows the assumed risk among the control group. The risk of antenatal parasitemia is 286 events per every 1,000 people. This is the median control group risk across 8 trials of 3,663 women. The relative risk is 0.39, which means that malaria chemoprevention is associated with a 61% decrease in parasitemia. This is the pooled, or “meta,” effect size from an included meta-analysis (we’ll come to meta-analysis shortly). The corresponding risk among the intervention group is 286*0.39=111 per 1,000 people. As shown in the final column, the quality of this evidence is rated “high.” This is a reference to GRADE criteria, a systematic approach to evaluating the quality of empirical evidence: High—Further research is very unlikely to change our confidence in the estimate of effect. Moderate—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate. Low—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate. Very Low—We are very uncertain about the estimate. Background The Background section introduces the intervention and explains which knowledge gaps the review is intended to fill. Radeva-Petrova et al. (2014) summarized the problem of malaria during pregnancy, explained the hypothesized mechanism by which chemoprevention prevents malaria, framed why this review is important, and listed the objectives of the study. Figure 3.4: Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Methods See the [PRISMA Statement](http://www.equator-network.org/reporting-guidelines/prisma/) for a checklist of details to report in a systematic review. The Methods section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to replicate the review. These are the main components: A description of the population and intervention The key outcomes of interest The search strategy and databases Inclusion and exclusion criteria for studies reviewed Procedures for extracting information from each study Procedures for assessing bias and conducting a meta-analysis (if one is included) Results Figure 3.5: Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj The Results section typically begins with details about how many primary articles were identified, screened, and excluded. This information is often presented graphically in a flow diagram like this one from Radeva-Petrova et al. (2014). The Results section will also summarize what we learn when looking across all of the included studies. Not every study is designed or implemented equally, however, so review authors must also evaluate the potential for bias in each study to help the reader make sense of the findings. Bias is a topic we will come to in a later chapter, but for now you can think of bias as anything that systematically takes us away from the “truth”. A small number of high quality studies with a low risk of bias is preferable to lots of poorly reported or conducted studies. Figure 3.6: Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj This heatmap from Radeva-Petrova et al. (2014) shows how authors summarize the risk of bias in Cochrane reviews. The rows represent each study included in the review, and the columns indicate the review authors’ determination about potential bias in each study. Each study is labeled as having a low risk of bias (green +), high risk of bias (red -), or not enough information to decide (yellow ?). As we move forward in the book, we’ll consider why a study like Villegas et al. (2007) was judged to have a low risk of bias across all dimensions, where as a study like Cot et al. (1995) was classified as having a high risk of bias and flagged for not providing enough information. Discussion and conclusions The Discussion section provides a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. In the Conclusion, the authors frame the overall results in terms of their implications for practice and research. Radeva-Petrova et al. (2014) conclude: Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anemia and parasitaemia in the mother, and on birth-weight in infants. Or, “chemoprevention works.” **&quot;Help! My mentor wants me to conduct a systematic review.&quot;** &lt;br&gt; Stay calm. Your first move should be to check the [Prospero database](https://www.crd.york.ac.uk/prospero/) to see if anyone has registered your research question. If so, you might consider proposing a collaboration or modifying your aims. Your second move should be to consult a clinical librarian or someone with experience conducting a systematic review for tips on setting up your project. Remember, working on a systematic reviews is a team sport. Appendices Often, systematic reviews will include appendices that print table after table of data on included and excluded studies and forest plots if the review includes a meta-analysis. A good appendix is a treasure trove of information. Radeva-Petrova et al. (2014) wrap up on page 120! 3.2 Meta-Analysis @lewis:2001 discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the tree-line optics typical of most forest plots. Many systematic reviews feature a meta-analysis that combines the results from multiple studies to estimate an overall effect size. The results of a meta-analysis are typically summarized in a forest plot like the one shown in Figure 3.7. Let’s take a look at this helpful guide from Ried (2006) that breaks it all down. Figure 3.7: Source: Ried (2006), http://bit.ly/2j9pfSz A forest plot summarizes the results of several studies that measured the effect of the same intervention on the same outcome. One study result is described and plotted per row, and the overall effect (i.e., the “pooled” or “meta” effect) of all the studies is displayed at the bottom. In each row, the study sample is divided into an intervention arm and a control arm, presented in the n/N format where n represents the number of participants who experienced a certain outcome and N is the total number of participants in the study arm. In this example, 141 were people assigned to the intervention group in Study A. Of these 141 people, 1 person experienced the adverse outcome that the forest plot summarizes. Next, a plot of the effect size and the confidence interval is created. An effect size is a measure of the strength or magnitude of a relationship, such as the relationship between taking a medicine and experiencing a bad outcome. This example shows a specific type of effect size: relative risk. Each study’s point estimate of the relative risk is plotted around a line of “no effect.” A risk of 1 means that there is no difference between the intervention and control groups. When the outcome is something bad, like death, the intervention should be designed to reduce the risk, which is represented by a risk ratio less than 1. The size of the effect estimate is based on how much the study contributed to the meta-analysis. Studies are not created equal, and the weight parameter lets researchers account for these differences in the analysis. Each point estimate is surrounded by a confidence interval (typically 95%) that is summarized numerically in the final column. Basically, if a study is repeated 100 times, the effect size is expected to be within this interval 95% of the time. When this interval crosses the line of no effect, the effect could be null or could even run in the opposite direction. In this case, the result is considered not “statistically significant.” Finally, the test for heterogeneity is presented toward the bottom. Heterogeneity means diversity (and is the opposite of homogeneity). Heterogeneity in a forest plot refers to the diversity in effect size estimates across studies. Heterogeneity complicates the interpretation of a meta-analysis; it signals that we might be comparing apples and oranges. For instance, the intervention may work differently in different contexts, and the included studies were gathered from all over the world. In such a case, it might not make sense to attempt to determine one overall meta effect size from a comparison of the studies. The first way to assess heterogeneity is to consider the plots. Do the confidence intervals from each study form a vertical column, even if the point estimates shift between them? If so, heterogeneity is probably low. Heterogeneity can also be summarized numerically. Two estimates of heterogeneity are often presented: chi-square (χ2) and I^2, which is generally preferred. Values greater than 75% may indicate that a change in the meta-analysis method (random vs fixed effects) is needed. If heterogeneity is reported with a high I^2 value, authors should address this in the methods or limitations section of the study. 3.2.1 CHEMOPREVENTION EXAMPLE Now let’s consider the meta-analysis by Radeva-Petrova et al. (2014). The forest plot shown in Figure 3.8 displays the results of 10 studies (8 trials) of cases of parasitemia among 3,663 pregnant women who were randomized to an intervention group (n=2,053) that received a preventive antimalarial drug or to a control group (n=1,610) that received a placebo (no drug). These details should look familiar from the summary of results table. Figure 3.8: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Details about each study are reported in separate rows in this figure. The study by Shulman et al. (1999) in row 6 found that 30 of the 567 women in the intervention group tested positive for parasitemia (i.e., malaria). Comparing this number to 199 of the 564 woman in the control group yields a risk ratio of 0.15, i.e., (30/567)/(199/564) = 0.15. In other words, the chemoprevention reduced the risk of parasitemia by 85%. This is a huge effect size! The effect size for each study is presented in the far-right column and is depicted graphically in the size of the point estimate square. All point estimates fall to the left of the line of no effect (&lt;1), thus indicating a favorable effect of the chemoprevention intervention, i.e., reduced risk of parasitemia. A risk ratio of 1 would indicate no difference in risk, and a ratio &gt;1 would mean the risk was higher among the intervention group, thus favoring the control group (with no treatment). The overall (pooled) effect size is 0.39, or a 61% reduction in the risk of parasitemia. Calculating this pooled effect size is not as simple as averaging the effects of the 10 studies because the studies were not given equal weight, as shown in the “weight” column. For instance, Greenwood et al. (1989) had a sample size of only 34 children (i.e., 21+13=34). As a result, the effect size estimate is very noisy. The 95% confidence interval is very large and crosses 1. Consequently, the weight of this study is only 6.7%, which is lower than the others. Simply put, studies with weaker research designs, such as this one, have less weight in the pooled analysis. A single forest plot provides a summary of the best available evidence and an estimate of the overall effect size, along with uncertainty intervals. A Google search cannot begin to offer that! 3.3 The Takeaway Systematic reviews and meta-analyses are great resources for discovering the best scientific evidence about the impact of an intervention on specific clinical, behavioral, or social outcomes. Finding a good review can save you hours of searching and will give you a ready-made search strategy to update or modify. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["critical.html", "4 Critical Appraisal 4.1 Getting started with critical appraisal 4.2 The anatomy of a scientific paper 4.3 Communicating your critical appraisal 4.4 The Takeaway", " 4 Critical Appraisal &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/Z_yiUf3f92s&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;EBM Explained. Sketchy EBM (2015). [https://tinyurl.com/yyrrlqq9](https://tinyurl.com/yyrrlqq9) As health professionals and researchers, we have a duty to make recommendations and decisions based on evidence, not beliefs. The term “evidence-based” first came into use in the 1990s, nearly two decades after Archie Cochrane, the namesake of the Cochrane Reviews, wrote the book “Effectiveness and Efficiency” and pointed to the need for the medical field to learn from the highest quality studies. This struck a chord with the profession, and medical schools began teaching their students evidence-based medicine (EBM), defined by Sackett et al. (1996) as: The conscientious, explicit and judicious use of current best evidence in making decisions about the care of the individual patient. It means integrating individual clinical expertise with the best available external clinical evidence from systematic research. Since then, evidence-based medicine has expanded to evidence-based practice (or EBP) more generally, as well as to population-level approaches such as evidence-based public health (Brownson, Fielding, and Maylahn 2009) and evidence-based global health policy (Yamey and Feachem 2011). The name &quot;evidence-based medicine&quot; is credited to Gordon Guyatt after the name &quot;scientific medicine&quot; was rejected [@smith2014]. Visit [ebm.jamanetwork.com/](https://ebm.jamanetwork.com/) to watch an oral history of evidence-based medicine. This focus on evidence has saved countless lives and improved health around the globe. But how does data become evidence? Each year a few million new articles enter the scientific literature (Ware and Mabe 2015). Who determines what should be published and which studies should be designated as “high quality” evidence? Well, we do. As scientists and researchers, we review manuscripts from our colleagues prior to publication, comment on articles once they appear in print, prepare systematic reviews and meta-analyses of published work, and sometimes attempt to replicate published findings in new studies. Broadly speaking, this process is known as critical appraisal, and it’s the focus of this chapter. 4.1 Getting started with critical appraisal It feels daunting to critically appraise someone else’s work when you’re starting out in research. I find that students default to providing the type of feedback that feels most comfortable: spelling and grammar. Your colleague might appreciate this type of feedback, but copyediting is not critical appraisal, nor is it the core function of peer review. Yes, we need to help each other become better communicators of our ideas, but not at the expense of providing a critical review of the science. So how do you approach the task of critical appraisal when you’re still building your foundation in research? Checklists! Figure 4.1: STARD guidelines for diagnostic studies, www.equator-network.org One type of checklist is a set of reporting guidelines specific to the research design used in the study. You can find every reporting guideline on the website of the Equator Network, which stands for Enhancing the QUAlity and Transparency Of health Research. Reviewing a randomized controlled trial? Check out the CONSORT guidelines. Diagnostic validity study? Try the STARD guidelines. Pro tip: Include a completed checklist as an appendix when you submit manuscripts for publication. It helps reviewers do their job and sends a good signal about your attention to detail. These guidelines are important because they represent the consensus of the scientific community regarding the essential details that a reader needs to evaluate a manuscript, organize a replication attempt, and include the study in a systematic review or meta-analysis. When you’re writing a manuscript, use the guidelines to make an outline that includes each piece of information. When you’re reviewing a manuscript, use the guidelines to confirm that the authors provided a full accounting of their study. Systematic reviews are a great source for examples of critical appraisal because a systematic review is a critical appraisal. If you find that the manuscript is lacking key details recommended by the reporting guidelines, you can enumerate these omissions for the authors and suggest that they follow [INSERT GUIDELINE NAME] to give the reader enough information to evaluate the work. This approach is useful even if you’re reading a published article and want to decide whether the evidence presented is of high quality. 4.2 The anatomy of a scientific paper &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/eSEP2T-xz8g&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;How I read a paper! Sketchy EBM (2015). [https://tinyurl.com/y3n7vc7m](https://tinyurl.com/y3n7vc7m) A scientific paper consists of four parts: An introduction that frames the research question A set of methodologies and a description of data A set of results A set of claims Let’s consider what to think about when you read each section. INTRODUCTION SECTION A good Introduction explains the aim of the paper and puts the research question in context. In public health and medicine, this section is typically very short compared to the introductions in other disciplines like economics. Does the Introduction identify a gap in the literature that this paper will fill? Do the authors cite relevant literature? Does it appear that the authors have accounted for recent developments in the field? Is the research question clearly stated? Do the authors specify the aims of the paper? We’ll consider research questions and aims in Chapter 5. 4.2.1 METHOD SECTION A good Method section provides enough information to enable a reader to replicate the findings in a new study. Journal space constraints make this challenging, so authors often post supplemental materials online that provide additional details. Review recent issues of the journal and consult the appropriate reporting guidelines to create an outline with subheadings that can guide your writing and reviewing. The organization of the Method section varies by discipline and journal, but generally it includes some information about the research design, intervention (if appropriate), sample, materials or measures, data sources and procedures, and analysis strategy. Can the study design answer the research question? There are many different designs that can potentially answer most research questions, but not all designs are created equal. A graphic like Figure 4.2 is commonly used in the evidence-based medicine literature to convey this point. Meta-analyses and systematic reviews are ‘studies of studies’, and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence. No one study is the final word on a research question, so it makes sense that a meta-analysis that pools results and accounts for variable study quality could potentially provide a better answer than any one study alone. Figure 4.2: Levels of evidence However, the Cochrane Handbook for Systematic Reviews (2011) cautions researchers to pay attention to design features (e.g., how participants were selected) rather than labels (e.g., cohort study) because labels are broad categories. Therefore, this hierarchy is not absolute; these rankings reflect ideals. For example, RCTs can be poorly designed or poorly implemented, and the evidence from such a flawed study is not necessarily better than the evidence from a nonrandomized study just because it carries the label “randomized.” I’ll describe the logic and assumptions of common designs in Chapters 9 through 11. How were participants selected and recruited? I focus on &#39;[human subjects research](https://grants.nih.gov/policy/humansubjects/research.htm)&#39; in this book, but sampling is also an important topic in research not involving people. &lt;br&gt; &lt;br&gt; Sidebar: How should you refer to the humans who participate in your studies? Is it ok to call these humans &quot;people&quot; or &quot;participants&quot;? Sure. They are people. I use &quot;participants&quot; most of the time. Others prefer &quot;volunteers&quot;. Few insist on &quot;subjects&quot;, with the notable exception of US federal regulations governing research. We can’t collect data from every person in our population of interest (unless we define our population very narrowly, e.g., “authors of textbooks named Eric Green”), so we have to sample a subset of people from the population. For instance, let’s say that we want to know what eligible voters in the US think of a certain presidential candidate. There were roughly 160 million people registered to vote in the 2018 US presidential election. We can’t contact all 160 million, so imagine we survey 1000 registered voters, or 0.000006% of the population. The details around who these people are and how they got into our study matter for our inferences. Can these 1,000 people represent all US voters? To answer this question for the reader, we have to describe sample selection and recruitment in the Method section. What made someone eligible or ineligible to participate? Who was excluded, intentionally or not? How were participants selected and invited to participate? Was this selection process random, or did the researchers invite participants based on availability? How many were invited, and how many accepted the invitation? Who refused the invitation to participate, and how are they different from the people who accepted? We’ll revisit sampling and sample size in Chapters 12 and 13. What materials and/or measures were used? Almost every study uses some type of materials or measures. Diagnostic studies, for instance, evaluate a diagnostic test or a piece of hardware that analyzes the test samples. Environmental studies often use sophisticated instruments to take atmospheric measurements. Studies like these always provide specific details in the Method section about the materials and equipment used. Study variables also need to be precisely defined in the Method section. For instance, hyperparasitemia describes a condition of many malaria parasites in the blood. But what constitutes “many”? The World Health Organization (WHO) defines it as “a parasite density &gt; 4% (~200,000/µL)” (WHO 2015b). A manuscript should be precise with respect to how measurement is operationalized. This holds for studies measuring social or psychological constructs. For instance, in a study of anxiety, a definition of the concept of “anxiety” should be provided. Is an anxiety disorder diagnosed by a psychiatrist? If so, what is the basis for this diagnosis? Or is anxiety inferred from a participant’s self-reported symptoms on a checklist or screening instrument? If so, what are the questions and how is the instrument scored? We’ll tackle issues of measurement, including study outcomes and indicators, in Chapter 7. How was the study conducted and how were the data collected? The Method section should also describe what happened after participants were recruited and enrolled. What happened first, second, third? Who collected the data, and how were they trained? For intervention studies, the data collection procedures should describe how participants were randomized to study arms and what happened (or did not happen) in each arm. Were the participants, data collectors, and/or patients blind to the treatment assignment? We’ll discuss data collection methods in Chapters 14 and 15. How were the data analyzed? In this section authors typically describe the approach and logic of the core analysis. In an economics paper, this might be called the empirical strategy. If analyzing qualitative data, what is the analysis method? Common methods include content analysis, narrative analysis, discourse analysis, and grounded theory. If analyzing quantitative data, is the statistical/econometric model described clearly? What are the assumptions of the analysis? Was the study approved by an ethics board? The US Federal Policy for the Protection of Human Subjects (i.e., the “Common Rule”) defines research as “a systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge…” If the research involves human subjects, it must be reviewed and approved by an institutional review board (IRB) before any subjects can be enrolled. Most studies fall under IRB oversight, but some, such as retrospective studies or quality control interventions, may qualify as exempt. Increasingly, researchers are taking the additional step of registering their study protocol prior to the study launch in a study clearinghouse like https://clinicaltrials.gov/. This registration is a requirement for drug investigations regulated by the FDA, and it’s expected by many journals. Preregistration does not ensure trustworthy results, but the practice fosters a welcome increase in research transparency. If the analysis described in an article deviates from the planned analysis, the authors are expected to provide a compelling justification. We will return to pre-registration and open science more generally in Chapter 18. 4.2.2 RESULTS SECTION Can each finding be linked to data and procedures presented in the Method section? Every finding in the Results section should be linked to a methodology and source of data documented in the Method section. Articles in medical journals are some of the shortest, so supplemental materials posted online may be needed to obtain a clearer sense of what the authors did and found. Is the analysis correct? This is a hard question to answer during critical appraisal. Most of the time (at least in public health and medicine) you do not have access to the data and analysis code, so you cannot verify that the analysis is correct. You have to base your assessment on the authors’ written description of the data and analysis. Even if you did have access to materials to reproduce the analysis, some analyses are so complex that only people with extensive training feel qualified to question the accuracy of the results. When reviewing a study with complex analyses, it may be necessary to consult with colleagues. 4.2.3 DISCUSSION SECTION Is each claim linked to a finding presented in the Results? Each claim should be supported by results that are reported in the paper. If there is no link between a claim in the Discussion section and a finding in the Results section, the authors may be “going beyond the data.” For example, if a manuscript presents data on the efficacy of a new treatment for malaria but does not include any data on cost, then it would be inappropriate to claim that the treatment is cost-effective. Although it’s legitimate to speculate a bit in the Discussion section based on documented findings, authors should be careful to label all speculation as such—and these hypothetical forays should never be included the article’s Abstract. Is each claim justified? Consider each claim in relation to the results presented to evaluate whether the authors’ arrived at the correct interpretation of the data presented. Did the authors come to a reasonable conclusion, or did they make conclusions that are not supported by the analysis? For instance, if the analysis provided only weak or mixed evidence that a new program is efficacious, it would be inappropriate to recommend scaling-up the program. Are the claims generalizable? One approach to promoting generalizability is to randomly sample participants from the population of interest. For example, Wanzira et al. [-@wanzira:2016] analyzed data from the 2014 Uganda Malaria Indicator Survey, a large national survey, and found that women who knew that sulfadoxine/pyrimethamine is a medication used to prevent malaria during pregnancy had greater odds of taking at least two doses than women who did not have this knowledge. Because the UMIS is nationally representative, the results could apply to Ugandan women who did not participate in the study. Would the results be generalizable to women in Tanzania? An argument could be made that they would. Would the results be generalizable to women in France? No, probably not; among other things, malaria is not an issue there. Just about every study is conducted on a narrowly constructed sample. Have you read a psychology paper recently? Chances are the sample was WEIRD: White, Educated, Industrialized, Rich, and Democratic. Most likely the participants came from the “undergraduate pool”—psychology majors willing to take part in the study for extra credit. In global health, it’s common to read qualitative studies involving 20 patients who receive services from one rural primary healthcare facility—a handful of people from one small town in one small corner of a country of millions. When we read these studies, we are not interested in the hyper-local story, per se (Shadish, Cook, and Campbell 2003). Instead, we want to know if the results can generalize to the broader target population. When a study is so highly localized that the results are unlikely to generalize to new people and places, we say that the study has low external validity. I’ll tell you more about generalizability and external validity in Chapter 12. Are the claims put in context? A good Discussion section puts the study findings in context by telling the reader how the study adds to the existing literature. Do the results replicate or support other work? Or do the findings run contrary to other published studies? What are some ideas about why this might be? How does the study advance our knowledge or fill gaps in the literature? What are the limitations? No study is perfect, so it’s customary to include a paragraph or two outlining the shortcomings. Such limitations span all aspects of the study design and methods, from sample size to generalizability of results, to data validity and approaches to statistical analysis. Communicating shortcomings can provide a valuable resource for future researchers in terms of caveats and research directions. 4.3 Communicating your critical appraisal The following steps can apply to critical appraisals that you do not intend to share with the authors, such as an [annotated bibliography](https://guides.library.cornell.edu/annotatedbibliography) to guide your own work. When you’re asked to review a manuscript for a journal or a grant proposal for a funder, there is an expectation that you will communicate your feedback in written form. We will dive into peer review and the publishing process in Chapter 19, but I want to use the remaining part of this chapter to discuss tips for communicating your critical appraisal. STEP 0: DON’T BE A JERK It’s easy to tear down someone else’s work. It’s much harder to give constructive feedback that will help make this work better. This is not to say that you should give a pass to poorly conducted research or confusing or incomplete write-ups. The point of critical appraisal is to share critical feedback. But feedback that focuses on someone’s perceived failings as a person or scientist are out of bounds. If you receive such feedback, try to remember that you’re not your work and this type of feedback is lazy and unprofessional. If you’re learning from a mentor who is lazy and unprofessional, find new opportunities for mentorship. Don’t be a jerk. “I don’t see how your approach has potential to shed light on a question that anyone might have.” pic.twitter.com/jgWrKaKlGG — ShitMyReviewersSay ((???)) March 11, 2019 It’s not hard to avoid insulting language. Here are a few examples from PLOS: Table 4.1: What to write when you are not feeling generous. Source: PLOS. When you want to say Say this instead The authors appear to have no idea what they are talking about. I don’t think they have read any of the literature on this topic. The study fails to address how the findings relate to previous research in this area. The authors should rewrite their Introduction and Discussion to reference the related literature, especially recently published work such as Darwin et al. The writing is so bad, it is practically unreadable. I could barely bring myself to finish it. While the study appears to be sound, the language is unclear, making it difficult to follow. I advise the authors work with a writing coach or copyeditor to improve the flow and readability of the text. It’s obvious that this type of experiment should have been included. I have no idea why the authors didn’t use it. This is a big mistake. The authors are off to a good start, however, this study requires additional experiments, particularly [type of experiment]. The manuscript is fatally flawed. The study does not appear to be sound. The writing is terrible. The authors should revise the language to improve readability. STEP 1: READ TO UNDERSTAND THE AIMS Stiller-Reeve (2018) published tips for peer review and a handy workflow. His suggestion is to read the paper several times with a different objective each time. On your first pass, read to get an overview of the article and to understand what the authors set out to do. In your own words, write a paragraph that expresses your understanding of the study’s aims and objectives. What do you understand the paper to be about, and what is your perception of the potential contribution? If the peer review system is working, however, this should be rare. Journal editors and grant funders conduct initial screenings and can reject such papers without sending them out for review. So before you throw in the towel and claim to have found fatal flaw, take the time to understand the paper&#39;s aims and approach. Sometimes you will read a manuscript with one or more fatal flaws that no amount of constructive criticism will help. To be clear, fatal flaws are about the science, not the writing. Unclear writing can be fixed. Bad science cannot. For instance, if the aim of the study is to estimate the national prevalence of disease in Uganda and the study is limited to a convenience sample of patients who sought care at a particular health clinic, this would be a fatal flaw. This study design cannot answer the question the authors set out to answer. When this happens, it’s OK to describe this fatal flaw and recommend that the paper be rejected or the grant not be funded. You do not have to continue spending time on a manuscript that is dead on arrival. STEP 2: DIG INTO THE DETAILS If there are no fatal flaws, read the paper again to understand the details of the scientific approach: sampling, measurement, procedures, intervention, analysis. Find the appropriate reporting guidelines from the Equator Network and use the questions above to go through each section of the paper to determine whether the authors describe the study in sufficient detail. STEP 3: CONSIDER THE CONCLUSIONS On your third pass, use your understanding of the details presented in the Method and Results sections to think critically about the authors’ conclusions. Do you have the same interpretations of the data? STEP 4: WRITE YOUR REPORT Summary paragraph Your report should begin with the paragraph you wrote in Step 1 that conveys your understanding of the aims and potential contribution of the paper. This paragraph should end with your overall recommendation. Should the paper or proposal be published or funded? Revised? Rejected? In this case the reviewer went on to answer his or her question in the negative. Nevertheless, it was a constructive review, even if we were disappointed in the outcome. We later published a revised version in a different journal [@blattman2016]. Here is an example of a summary paragraph from a review of a paper I co-authored that was not accepted at the first journal. The authors conduct a field experiment in rural northern Uganda to estimate the effect of cash grants, training, supervision of grant use, and incentives to form social groups on the entry into self employment, and labor market earnings. Residents of 120 villages and an NGO identify 10 to 15 of the poorest individuals to participate in the program. The majority of the participants (86 percent) are female. At baseline, participants have cash earnings of only about $2 per week. The experiment is a ‘randomized wait-list’ design, with 60 villages selected for immediate treatment. The remaining 60 receive treatment about 20 months later. Treatment is a cash grant of $150, plus five days of business training leading to a simple plan for a new business. Most recipients also receive five follow-up visits intended to encourage them to invest the grant in a business and to provide advice on the business. The authors find that the grants, training and follow-up treatment has a large and significant effect on running a non farm business, doubling the baseline rate of around 40 percent for women. Cash earnings of women (men) increased by 92 (74) percent. The experiment is well designed and executed, and the paper is well written. The paper clearly should be published in a good journal. The question is whether it rises to the level of [this journal]. Major concerns and minor comments The rest of the report should support your overall recommendation. It’s customary to divide your comments into two sections: (1) Major concerns and (2) Minor comments. If you’re recommending that the manuscript or proposal be revised, major concerns are issues that the authors must explain or remedy to get your endorsement for publication or funding. Maybe you’re unclear about the sampling strategy as described and, therefore, you cannot make an overall determination about the validity of the results. Or maybe you feel that the analysis method is flawed, suboptimal, or incomplete. Major concerns get at the veracity of the science and conclusions. Minor concerns, on the other hand, include issues that you believe detract from the paper’s potential, but do not represent core matters of science. You do not need to copyedit the draft, but you might include suggestions for improving the readability of the paper under minor concerns. Likewise, you might offer additional references the authors could consider adding to provide more context for the results. When you’re writing minor concerns, you’re offering ideas for how to make the paper better. 4.4 The Takeaway Critical appraisal is an important part of the scientific process. It’s what we do when asked to review a manuscript prior to publication, evaluate a grant proposal, read a new paper published in our field, and prepare a systematic review or meta-analysis. You might feel unprepared to provide critical feedback on someone else’s work until you know more about research methods, designs, and analysis, but it’s easy to get started if you use a framework like the reporting guidelines published by the Equator Network or some of the tools listed below in additional resources. When it’s time to communicate your feedback, be nice. Be critical, be honest about the flaws you perceive, but always be nice and assume that the authors did their best. If their best is not publishable and cannot be fixed with revision, then recommend that the paper be rejected or the grant not funded. It happens. Even in rejecting someone’s work you can offer ideas for improvement. It’s common to structure your feedback according to the major concerns you see, followed by minor comments that could improve the accuracy or clarity of the paper. Don’t be this reviewer: ‘This reads like a pretty good MA level seminar paper but comes nowhere near the intellectual status required for publication in journal X’ pic.twitter.com/FCZmsaB6kb — ShitMyReviewersSay ((???)) February 11, 2019 Additional Resources Critical appraisal worksheets from the Centre for Evidence-Based Medicine BMJ Series on “How to Read a Paper” Critical appraisal resources from Duke Medicine Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["module-2-define-your-study-aims.html", "MODULE 2 Define Your Study Aims", " MODULE 2 Define Your Study Aims By the end of this module, you should be able to: Ask a good research question and develop study aims and hypotheses Develop a theory of change and logic model Identify indicators to measure throughout the causal chain "],
["aims.html", "5 Research Questions and Aims 5.1 Types of research questions 5.2 Specifying your research question 5.3 Outlining research aims 5.4 Developing hypotheses 5.5 The lifecycle of a research question 5.6 The Takeaway", " 5 Research Questions and Aims &lt;iframe src=&quot;https://giphy.com/embed/82okbIOv8krUhwgHCz&quot; width=&quot;300&quot; height=&quot;169&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;Give yourself a pat on the back. Go ahead. No one is watching. &lt;a href=&quot;https://giphy.com/gifs/CMNHospitals-childrens-miracle-network-hospitals-kid-pediatric-patients-82okbIOv8krUhwgHCz&quot;&gt;via GIPHY&lt;/a&gt;&lt;/p&gt; If you’re following the plan I’ve outlined so far, you’ve been seeking out research ideas by attending presentations, talking with fellow students and mentors, skimming interesting journals, searching research databases like PubMed for keywords you’ve identified, and finding relevant systematic reviews and meta-analyses. You’ve also been developing your critical appraisal skills and, in the process, have been taking note of gaps in our knowledge. All of this work is leading you to identify potential research problems worth solving. The next step is to take a broad research problem and narrow your focus to a more specific research question and develop study aims and (potentially) hypotheses. I’ll walk you through the process and showcase the lifecycle of a good research question. 5.1 Types of research questions There are three basic types of research questions we can ask (Hernán, Hsu, and Healy 2019): Descriptive Predictive/Relational Causal (counterfactual prediction) This framework is an attempt to simplify the world to help you learn, but you will soon see that the lines between these three categories can blur. For one, a study that aims to assess the evidence for a claim that X causes Y can include elements of prediction and description. Second, answering questions of all three types can involve statistical inference, as we often want to quantify the uncertainty in our estimates. So there is a possibility of conflating our aims (e.g., to estimate the causal effect of X on Y) and methods (e.g., the use of a statistical test to examine the association—a relationship—between X and Y) (Hernán 2018). Nevertheless, it is helpful to erect some boundaries to introduce these concepts and let you decide if they are useful as you gain more expertise. 5.1.1 DESCRIPTIVE Every study uses an element of description. Let’s say you recruit a sample of 100 people who suffer from the same disorder and conduct a trial to estimate the effect of a new drug on some clinical outcome. When you summarize what you know about these 100 people at the time they were recruited, for instance the average age of the group, you’re describing the sample. Descriptive summaries appear in nearly every research article. But we can distinguish between the use of descriptive statistics—e.g., what is the mean age of these 100 people, the sample—and descriptive research questions. One common descriptive research question in global health follows this format: What percentage of women of reproductive age in Nepal use a modern method of contraception? As we will discuss later in the book, you could answer this question by conducting a survey of contraceptive behavior with a representative sample of women in Nepal. That’s what the DHS Program did in 2010 (Ministry of Health and Population, New ERA, and ICF International Inc. 2011). &quot;[Modern methods](https://www.who.int/news-room/fact-sheets/detail/family-planning-contraception)&quot; like condoms, implants, pills, etc, are distinguished from (and are more effective than) &quot;traditional methods&quot; such as withdrawal and the rhythm method. Researchers surveyed a random sample of 10,826 households across the country and interviewed 12,674 women between the ages of 15 and 49 about their health behaviors and preferences. They estimated that 43.2% of married women reported using some modern method of contraception. Figure 5.1: Current use of contraception by age in Nepal. Source: DHS Nepal 2011, https://tinyurl.com/y4u5wfkv. Of course this is what they learned from the sample, but the research question required inference to the all women in Nepal in this demographic (the target population). As you’ll learn in Chapter 13, there is some error involved in speaking with some but not all women in Nepal, and the researchers estimated that the true percentage probably ranged from 41.0% to 45.3%.1 This is an example of descriptive inference to answer a descriptive research question. 5.1.2 PREDICTIVE/RELATIONAL Of course not everyone needs to be using modern methods of contraception. If you&#39;re not sexually active, you&#39;re not at risk for pregnancy. Or if you&#39;re trying to get pregant, modern methods will make that challenging. Therefore, public health officials wanting to promote modern method use would take this indicator and combine it with several others in the dateset to estimate the [&quot;unmet need&quot; for family planning](https://dhsprogram.com/topics/Unmet-Need.cfm): women who say that they want to prevent or delay pregnancy, but are not using contraception. Description is essential to science and decision-making related to needs and resources. The result from Nepal suggests that more than half of married women of reproductive age were not using a modern method of contraception in 2010. This is a very useful thing to know if you work for the Ministry of Health and are concerned about promoting reproductive health. But you probably also want to go the next step and ask, “What predicts modern method use?” Stated differently, what factors are associated with/correlated with/related to modern method use? Who is most likely to use modern methods? What are the barriers to modern method use? These are questions about the strength and direction of the relationship between two or more variables and represent our second category of research questions. Figure 5.2: Predicted probabilities of use of modern method of contraception. Source: Yours truly using data from the DHS Nepal 2011 survey, https://tinyurl.com/y4u5wfkv. If you inspect the above figure from Nepal once again, you will see that the cross-tabulation of any modern method by age shows that modern method use is more common among older women compared to younger women. Modern method use appears to increase with age, according to this simple descriptive summary. So we might want to ask, “To what extent does age predict self-reported modern method usage among currently married women of reproductive age in Nepal?” We can answer this question with a statistical model called logistic regression. The figure shows the predicted probabilities of modern method use by age among currently married women. As we expected, modern method use is more common among older women. 5.1.3 CAUSAL Figure 5.3: Unfortunately, no one can be told what the Matrix is. You have to see it for yourself. This is your last chance. After this there is no turning back. You take the blue pill, the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes… Questions about the relationship between X and Y make up the bulk of literature in global health. However, sometimes we want to go beyond asking to what degree X and Y are related and ask, “Does X cause Y?”, or “What is the causal effect of X on Y?” These are causal questions. They reflect our desire to know whether our treatments, policies, and programs promote behavior change and make people healthier. I’ll introduce you to this topic of causal inference in more detail in Chapter 8, but let’s consider for a moment the nature of a causal question. To answer a causal question, we must think “what if”. If you want to determine whether a new drug helps people recover faster from a disease, a fundamental problem you’ll face is that you can’t give this new drug and the old drug to the same person simultaneously. Once this person goes down the new drug path, they can’t go down the old drug path (at the same time). So we have to create a situation where we can ask, “What would have happened if this person had gone down the old drug path instead?” This alternate path is known as the counterfactual, and we’ll discuss it much more depth as we go. For now, I’ll simply describe it as the path not taken. Research in low-income countries has estimated that nearly all women want to prevent or space their next pregnancy after they have just given birth, but [6 in 10 women do not start a method of family planning after delivering a baby](https://www.familyplanning2020.org/ppfp). Precisely when women become fertile after childbirth varies and is influenced by factors like breastfeeding, but in most cases ovulation returns before family planning is started. This puts women at risk of getting pregnant again before they want to. One option is to have an IUD inserted immediately after delivery of the placenta or within the first month postpartum. Consider another example from Nepal. Pradhan et al. (2019) looked at the path not taken for counseling women on the benefits of having an intra-uterine device (IUD) inserted following childbirth to prevent a new pregnancy. The authors used a stepped-wedge design that you’ll meet in Chapter 9 to estimate the causal effect of offering postpartum family planning counseling on the proportion of new moms who opt to have an IUD inserted. By this design, the counseling intervention was ‘turned on’ in six hospitals in a stepwise fashion. The three hospitals in Group 1 received the intervention first in a staggered start. Approximately six months later, the remaining hospitals began offering the counseling intervention, one after the other. This figure demonstrates that rates of uptake appeared to jump in each group after the intervention was introduced. Pradhan et al. (2019) estimated that the intervention increased IUD uptake by 4.4  percentage points [95%CI: 2.8–6.4 pp]. Figure 5.4: Trends in PPIUD uptake. Source: Pradhan et al. (2019). On the surface, questions of causal inference (or counterfactual prediction if you prefer) resemble questions of prediction/relation.2 For X to cause Y, it must be true that X is related to or associated with Y. What distinguishes these questions is often the research design we use to find answers, so that is where we will spend our time. And it will be time well-spent. Causal inference is one of the most important topics for applied global health researchers. When it comes to making decisions about prevention and treatment and how to allocate resources, we want to know what works, why, and for whom. 5.2 Specifying your research question &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/_0HxMpJsm0I&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; Need to develop a good qualitative research question? See this short video by Yale University (2015). Fundamentals of Qualitative Research Methods: Developing a Qualitative Research Question (Module 2). Source: [https://tinyurl.com/y5sexqwg](https://tinyurl.com/y5sexqwg). A good research question is a specific research question. I’ll share two mneumonics to help you get specific: FINER and PICO. Imagine that we wanted to study the uptake or use of bed nets. We could ask a descriptive research question like, “How many children sleep under bed nets?” It’s a good start, but this question is too general. Children of what age? Living where? We also need to define what we mean by sleeping under a bed net. In this line of research, it is common to ask about the previous night, as in the night before the survey. A better way to phrase the question is, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?” An example of a predictive research question on the same topic is, “What are the predictors of the use of insecticide treated net among children under 5 years of age in Kenya?” Both of these examples are FINER than the first one: Feasible, Interesting, Novel, Ethical, and Relevant (Hulley, Newman, and Cummings 2007). Table 5.1: FINER research questions. Feasible Some resarch questions will take a long time to answer, cost too much, require too many participants, require skills or equipment that you do not have, or will be too complex to implement. Interesting Research requires funding and effort. If you do not ask a sufficiently interesting question, you will not get funding. If you manage to get funding but lose interest in the question, you might not finish. Unlike other domains, global health research tends to have long timelines, and it’s important to work on things you will find interesting over the long term. Novel Replication is an important part of science, but the majority of funding goes to research that asks new and interesting questions. Ethical It would be very interesting to create a prison simulation to determine whether charactristics of the people or situation cause abusive behavior, but this would not be ethical because it could lead to the harmful treatment of research subjects. Right? Relevant In addition to being interesting, a research question should also be relevant. The answer should move the field forward in some way. Making this determination requires a thorough review of the literature and conversations with senior colleagues. The second pneumonic is PICO, which I introduced in Chapter 3. In clinical research, good clinical questions always include PICO: Patient/Population/Problem, Intervention/Prognostic factor/Exposure, Comparison, and Outcome. Table 5.2: PICO research questions. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome Let’s use PICO to develop a research question about the efficacy of mosquito bed nets in preventing malaria. The problem is malaria infections. The population is children under 5 years of age. Because intervention studies tend to be smaller in reach than nationally representative surveys, we might add “living around the Lake Victoria basin in Kenya”. The intervention is the application of an insecticide-treated net. The comparison group might be children living in families who are provided an untreated bed net.3 One outcome measure could be the rate of parasitaemia after the intervention. We combine all of these elements into a single research question: Among children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide-treated mosquito nets more effective than untreated nets at preventing parasitaemia? 5.3 Outlining research aims Your research question should drive your overall study objectives, known in some circles as specific aims. Aims are the work products you will complete within a given project period. “Prevent malaria” is not an aim. “Cure cancer” is not an aim. These might be the goals that motivate you to come to work every day, but they do not describe the work you propose to complete as part of a specific project. For some formative work (especially studies that use qualitative methods), you might propose to *describe* some phenomenon. This will often be seen as a weak aim for larger grant proposals. Reviewers might expect you to have already completed this formative work and discuss it under preliminary evidence. When formulating aims, the typical advice is to use strong verbs, such as identify, define, quantify, establish, estimate. Write your aims as headlines and follow-up with a description of how you will achieve the aim. Using the example research question above, you might develop the following aim: Aim 1: Estimate the effect of insecticide-treated mosquito nets for preventing parasitaemia. You would then go on describe the experiment you plan to conduct as part of this aim, as well as specify a hypothesis. It’s common to propose multiple aims in a project proposal. Small grant proposals could reasonably have 1 or 2 aims, which might correspond to 1 to 2 studies. Larger proposals might outline 2 to 4 aims. More than this and your project probably is not feasible (or at least will be perceived by reviewers as not feasible). If you do propose multiple aims, make sure that failure in one aim does not prevent you from achieving other aims. For instance, if your first aim is to develop a new process for analyzing biological samples and you fail to achieve this aim, any other aims that depend on this new process are impossible to complete. This makes your entire proposal ride on a reviewer’s assessment of your chances of succeeding on the first aim. 5.3.1 WRITING A CONCEPT NOTE OR SPECIFIC AIMS PAGE Ideas don’t sell themselves. Before you can change the world, you have to convince collaborators to join your team and funders to believe in the work. One of the best ways to strengthen your pitch is to develop a short concept note. If you are applying for NIH funds, concept notes are known as Specific Aims documents and are always limited to 1 page. Figure 5.5: Anatomy of a specific aims page. Source: Inspired by Sneck (2015). To view a full resolution version of this figure, visit https://tinyurl.com/y5s35jo5. Most people will tell you that it pays to follow a known format when writing your Specific Aims page. Grant writing is not creative writing. Your 1-page Specific Aims page is all that some reviewers will read, often while flying at 30,000 feet, cramped in economy with no space for their laptop. Or after a full day’s work and kid bedtimes. You get the picture. Make your reviewer’s job easy by drafting a tight 1-pager following some conventions. Example specific aims page Here is an example specific aims page from my work, broken down in a common 4-paragraph format. It’s not a perfect example, but along with others (see here) it might help to guide your work. In this example, my opening paragraph tries to hook you as the reader, tells you what we know, the gaps in our knowledge, and why filling this gap is a critical need. The next paragraph gives an overview of what I propose to do in the context of my long-term goals. Then come two specific aims with supporting details, followed by a short payoff paragraph that tells people why it makes sense to fund this proposal. Figure 5.6: Specific aims example from a study of caregiver readiness to disclose a child’s HIV status to the child (7R21HD076695-020). 5.4 Developing hypotheses Figure 5.7: Can you see yourself in little Jimmy? Chances are you’ve presented a poster like this at a science fair at some point in your life. It depicts the scientific method in action: Define a question Develop a hypothesis Collect data related to this hypothesis Analyze the data Make conclusions based on the data Replicate the study We call this the hypothetico-deductive model because we develop a hypothesis (a conjecture) and deduce the consequences that should follow from this hypothesis. Stating what we expect based on a hypothesis makes it falsifiable. We can collect data and determine whether the data are consistent with or opposed to this prediction. If the data conflict with the hypothesis, we reject the hypothesis. If the data are consistent with the hypothesis, we say that we have ‘corroborated’ our hypothesis. In this model of hypothesis testing, data never ‘prove’ a hypothesis, and we never ‘accept’ our hypothesis because it’s possible that new data could falsify it. [Daniël Lakens](https://twitter.com/lakens), the creator of the excellent Coursera course, &quot;[Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/info)&quot;, turned me on to a great book called &quot;[Understanding Psychology as a Science](https://amzn.to/2RthGEH)&quot; by @dienes2008. I highly recommend this book (and Lakens&#39; course) to build a strong foundation in your own philosophy of science. This characteristic of falsifiability is what the famous philosopher of science Karl Popper said distinguishes science from non-science. Science advances as theories are subjected to tests in which we make specific, falsifiable predictions based on these theories. As we will see, not all tests are ‘severe’ and it’s possible to proceed with a falsificiation-lite approach that has the veneer of science, but is empty at its core. 5.4.1 WHERE DO HYPOTHESES COME FROM? In the classical view of science, we develop hypotheses out of existing theory, an explanation of some aspect of our world that has been confirmed through repeated, falsifiable tests. We’ll discuss the role of theory in global health in the following chapter, so for now let it suffice to say that theories can be a good source of hypotheses. Pro tip: If you&#39;re looking to explore or describe some phenomenon and are struggling to identify a testable hypothesis, you can probably stop trying. You might be operating in a hypothesis-*generating* framework, rather than a hypothesis-*testing* framework. &lt;br&gt; &lt;br&gt; It&#39;s also worth noting that a study can include elements of both qualitative and quantitive approaches. We call this **mixed methods research**, and it&#39;s a topic we&#39;ll cover in **[Chapter 15](qualitative)**. We can also begin with our observations of the world and generate hypotheses to test. This can be an informal process, or it can take the form of formal qualitative research. Whereas quantitative studies are often deductive—state a hypothesis, deduce the consequences that would be consistent with the hypothesis, and collect data to test this hypothesis—qualitative studies are more often inductive or exploratory. In qualitative research, we make observations about the world and use these observations to describe why or how the world appears to operate the way it does. From these observations we can create testable hypotheses for future studies that, if not rejected, can generate theory. 5.4.2 WHEN ARE HYPOTHESES DEVELOPED? Figure 5.8: The hypothetico-deductive model of the scientific method is short-circuited by a range of questionable research practices (red). HARKing, or hypothesizing after results are known, involves generating a hypothesis from the data and then presenting it as a priori. Source: https://cos.io/rr/ This probably seems like a redundant question because I already listed the order of the scientific method, and I acknowledged that you probably learned this order first hand in primary school. But here’s the thing: scientists sometimes Hypothesize After the Results are Known. This is known as HARKing, and it’s a no no (Kerr 1998). To be clear: it’s 100% OK to collect data, analyze the results, and generate new ideas (hypotheses) to test in future studies. That’s called exploratory analysis, and if you label it as such everyone is happy. But it’s not OK—in fact it’s dishonest and a form of fraud—to look at your data, develop hypotheses that conform to the data, and pass your new ideas off as a priori hypotheses that you tested. That’s lying. And it’s not science. You can’t falsify a hypothesis that you create after looking at the data. I’m going to refrain from hitting you with some depressive statistics about the state of science today. It’s only Chapter 5. I’m still working on building your enthusiasm for research. We’ll revisit research sins like HARKing in Chapter 18, along with strategies for preventing such bad behavior. 5.5 The lifecycle of a research question Research questions have a way of sticking with us. When you read an article published in your favorite journal, chances are that the authors have been living and breathing the motiving research question for years. To make this point and demonstrate the lifecycle of a research question, I will walk you through an example from Dr. Wendy O’Meara’s research group on subsidies for malaria testing and treatment. Figure 5.9: Example timeline. 5.5.1 GRANT PROPOSAL Figure 5.10: Specific aims page from the O’Meara lab. For a full resolution version, visit https://tinyurl.com/y2k6jsa4. Once you have your core team in place, it’s time to get funding to make your idea a reality. In the O’Meara lab’s example, they developed a proposal in response to this Funding Opportunity Announcement (FOA) from the NIH: The purpose of this dissemination and implementation research funding opportunity announcement (FOA) is to support innovative approaches to identifying, understanding, and overcoming barriers to the adoption, adaptation, integration, scale-up and sustainability of evidence-based interventions, tools, policies, and guidelines. Conversely, there may be a benefit in understanding circumstances that create a need to “de-implement” or reduce the use of strategies and procedures that are not evidence-based, have been prematurely widely adopted, or are harmful or wasteful. [The NIH offers different sized research grants](https://www.niaid.nih.gov/grants-contracts/research-project-grants). The R03 is the smallest grant with $100,000 in direct costs over 2 years. This mechanism is good for pilot or feasibility studies. The R21 comes with up to $275,000 in direct costs over 2 years. Researchers apply for an R21 when they want to test a new idea with some preliminary evidence. The R01 is the largest grant with an unlimited budget for work to be carried out over 5 years. They directed their proposal to the National Institute of Allergy and Infectious Diseases (NIAID), one of the many NIH institutes participating in this funding opportunity. The team proposed two aims as part of an R01 grant: Aim 1: Estimate the effect of antimalarial subsidy level on ACT purchase and adherence to RDT results. In the first phase of the project, we will test the effect of a range of subsidy levels on the demand for ACTs, and also test how the response to subsidy varies with or without information from diagnostic testing. Aim 2: Evaluate the public health impact of targeted antimalarials subsidies through scale-up. In the second phase, we will determine the community-wide effects of targeting the antimalarial subsidy in both a high and a low malaria transmission region of Kenya through a partnership between CHWs and the private retail sector. In 2014 when this proposal was funded, the payline for new R01s at NIAID was the 13th percentile. That means the O’Meara lab’s proposal scored within the top 13% of all proposals submitted. Think of how crushed you would be to have a proposal sitting at the 14th percentile! 5.5.2 FORMATIVE WORK, AIM 1 The O’Meara lab conducted two studies as part of this project. The first was a formative study to determine the optimal subsidy level for malaria treatment. Let’s use the paper they published on this Aim 1 work to examine how research problems, questions, and hypotheses come together in the Introduction of a scientific manuscript. Take a moment and download this article in BMJ Global Health by O’Meara et al. (2016). I’ve highlighted the authors’ descriptions of the research problem, what is known about the problem, the research question, and the hypothesis. Figure 5.11: Example Introduction section published in BMJ Global Health. Source: O’Meara et al. (2016) The research problem is that we don’t know how to combine diagnostic testing for malaria with drug subsidy programs in order to expand access to first-line treatments while ensuring that the medication is not overused by individuals without malaria. As noted in the Abstract: There is an urgent need to understand how to improve targeting of artemisinin combination therapy (ACT) to patients with confirmed malaria infection, including subsidised ACTs sold over-the-counter. The authors framed the objective of the study as follows: We sought to study the relationship between a subsidised diagnostic test and a conditionally subsidised treatment intervention in order to understand how ACTs can be targeted to malaria cases in the retail sector. We designed an individually randomised experiment to determine the effect of a conditional subsidy for ACT on the decision to be tested for malaria when the test is subsidised or not, and the subsequent effect of the test on drug purchasing decisions. While most scientific papers follow the pattern of describing the research problem and outlining the objectives of the paper, you will find that it&#39;s relatively uncommon, at least in the health sciences, to explicitly state the research question as a question. If you look at recent issues of a journal like [*The Lancet Global Health*](https://www.thelancet.com/journals/langlo/home), you&#39;ll also find that many articles do not even state explicit hypotheses. We can use PICO to construct the implied research question: Table 5.3: Reframing the research question in O’Meara et al. (2016). Population individuals (&gt;1 year) in Bungoma County, Kenya with untreated symptoms of malaria Intervention conditional subsidy for ACT Comparison no subsidy Outcome uptake of malaria testing and rational use of ACTs Among individuals (&gt;1 year) in Bungoma County, Kenya, does offering a subsidy for ACT conditional on testing positive for malaria increase the uptake of malaria testing and the rational use of ACTs compared to no subsidy? If you’re coming from a discipline like economics, you might wonder if I forgot to paste the full Introduction of O’Meara et al. (2016). I did not. This is it. It’s common in the health sciences to publish very brief Introduction sections. For comparison, here’s an example Introduction from Blattman, Jamison, and Sheridan (2017) published in American Economic Review. The first thing you notice is the difference in length. The complete typeset paper is 42 pages, compared to only 10 pages for the above example published in BMJ Global Health. The AER article is gated, but if you download the pre-print and read the Introduction, you’ll also get a sense for how economists structure scientific papers differently. Figure 5.12: Example Introduction section published in American Economic Review. Source: Blattman, Jamison, and Sheridan (2017) 5.5.3 PUBLIC HEALTH IMPACT, AIM 2 With the formative work completed, the O’Meara lab moved on to the second aim to evaluate the public health impact of targeted antimalarials subsidies through scale-up. They designed and conducted a stratified cluster-randomised controlled trial in Western Kenya with 32 communities assigned to get usual care or free malaria testing and a partially subsidized voucher to purchase ACTs if testing positive. Study protocol As we’ll discuss in Chapter 18, it is becoming more common to publish a study protocol prior to starting data collection (or at least prior to viewing the data) that outlines your research question, hypothesis, study design, data collection methods, and analysis plan. Download the O’Meara lab’s protocol (Laktabai et al. 2017) from BMJ Open and notice how a study protocol, like a grant, uses future tense to describe what will be done. Figure 5.13: Aim 2 study protocol. Source: Laktabai et al. (2017) Trial registration Regulatory bodies require that drug trials be prospectively registered with a database like ClinicalTrials.gov. Registration of non-drug trials is not mandatory in the same way, but most journals require it as a condition of publication of clinical trials.4 There are many registries to choose from, but ClinicalTrials.gov is the oldest and most widely used. This is where the O’Meara lab registered their study for Aim 2 (NCT02461628). Figure 5.14: Aim 2 trial registration. Source: ClinicalTrials.gov, https://tinyurl.com/yxundeds. Published paper Figure 5.15: Adjusted modeled RRs and 95% CIs for the primary outcome of uptake of testing and 3 composite outcomes. Source: O’Meara et al. (2018). Approximately 5 years after seeking funding to estimate the impact of targeted subsidies for antimalarials in the retail sector, O’Meara et al. (2018) published the results of their Aim 2 study. They reported that 18 months after introducing the program to the intervention communities, the proportion of people opting for testing prior to treatment increased by 25%, and the proportion of antimalarials dispensed to true cases of malaria improved by 40% relative to the control communities. The authors conclude: In summary, we demonstrate that it is possible to target ACT subsidies to diagnostically confirmed malaria cases. Allocation of subsidy dollars between testing and treatment for test- positive individuals may present a better use of programmatic resources than unconditional private sector subsidies. 5.6 The Takeaway The first step in asking a good research question is knowing what type of question you want to ask. Some questions are descriptive (e.g., What is the prevalence of condom use among adolescent males in Nigeria?). Other questions are predictive or relational (e.g., To what degree are wealth and smartphone ownership correlated?). But a lot of questions that drive policy are causal in nature (e.g., What is the impact of text message reminders on medication adherence?). Once you know what type of question you want to ask, a mnemonic like PICO or FINER can guide how you construct the question. The next step is to turn your research question into a study proposal anchored in a few specific aims that represent the work products you will complete within a given project period. Crafting a tight concept note or Specific Aims page early on can help clarify your thinking, recruit colleagues, and secure funding. Depending on your aims, you might outline specific hypotheses to test. By registering these hypotheses along with your aims and procedures, you will establish a public record and demonstrate your commitment to practicing good science. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["theory.html", "6 The Role of Theory in Global Health 6.1 Theory, what is it good for? (supposedly somethin’) 6.2 An example of theory in action 6.3 Get to know a conceptual framework/model 6.4 Theory of change 6.5 Logic model 6.6 The Takeaway", " 6 The Role of Theory in Global Health Global health, many would agree, is more a bunch of problems than a discipline. As such it lacks theories that can generalise findings—through an iterative process of knowledge construction, empirical testing, critique, new generalisation, and so on—into durable intellectual frameworks that can be applied not only to distinctive health problems, but to different contexts and future scenarios. That’s psychiatrist and medical anthropologist Arthur Kleinman writing in The Lancet about the atheoretical approach that characterizes much of global health research (Kleinman 2010). Rather than test and build theory though the hypothetico-deductive model—that is, making predictions based on theory and collecting data to test these predictions—global health scholars and practitioners mostly solve problems. Yes, you’ll find a veneer of the hypothetico-deductive model in our work, a hypothesis here and there, but across a large swath of the research landscape in global health, we test empty hypotheses that do not put theory to the test. We ask questions like, “Does MY PROGRAM improve IMPORTANT HEALTH OUTCOME?” We then hypothesize that, yes, MY PROGRAM will improve health compared to NOT MY PROGRAM, and use a statistical model to test whether the improvement is equal to zero.5 Skeptical? Let’s put this claim to the test. Based on the Kleinman theory of the atheoretical practice of global health6, I hypothesize that a search of recent research articles published in a leading global health journal will turn up no evidence that the studies sought to contribute to theory. To test this hypothesis, I found the latest issue of The Lancet Global Health (July 2019, Vol 7, Num 7) and searched the 13 original research articles for the words theor*, conceptual, hypoth*, and mechanis*. Here’s what I found: nothing. None of the articles had anything to say about theory.7 Should we have expected something different? Several studies were descriptive in nature, so we might not expect them to have much of a theoretical basis or any hypothesis testing to falsify a theory. For instance, Blencowe et al. (2019) estimated prevalence of low birth weight for 195 countries from 2000 to 2015. Hsia et al. (2019) described the use of antibiotics to treat pediatric patients across 56 countries. And Ahmed et al. (2019) estimated modern contraceptive prevalence rates across eight sub-Saharan African countries. To use Kleinman’s phrase, these articles reported on a bunch of problems.8 I don&#39;t intend to pick on these studies. Each one makes an important contribution to the literature. @wagg2019 find that a simple group-exercise intervention helps manage urinary incontinence in rural, older Bangladeshi women who might not be able to access pharmaceutical or surgical interventions. @sullender2019 report that vaccinating children in India against influenza had short-term, indirect protective effects for unvaccinated household members. And @gureje2019 show that a stepped care intervention for depression delivered by community health workers did not result in a clinically significant benefit compared to enhanced usual care, but might be more cost-effective to implement for the same outcome. The collection of articles did, however, include several intervention trials (Wagg et al. 2019; Sullender et al. 2019; Gureje et al. 2019) that had the potential to contribute to theory, but did not (at least not directly). Instead, these trials sought only to estimate the effect of each intervention on the primary outcome and make an evidence-based claim about whether the intervention “worked”, but not why or why not. Some would call these trials examples of “black box” evaluations. A black box evaluation is one that looks to see if X leads to Y—does the intervention improve some health outcome—but does not explore the mechanisms of this change (or the failure to change). Figure 6.1: Black box evaluations. Scholars have been lamenting this black box trend for some time, and not just in global health. Chen and Rossi (1989) wrote about the need for theory-driven evaluation and criticized theoretically-empty program evaluations as being “at best social accounting studies that enumerate clients, describe programs and sometimes count outcomes.”9 Writing about evaluation in international development, White (2009) called for theory-based impact evaluation to open the black box approach. More recently, Wilke and Humphreys (2019) observed the same trend in political science and economics with the rise of black box field experiments. Is theory dead? Does it even matter? 6.1 Theory, what is it good for? (supposedly somethin’) Wilke and Humphreys (2019) consider this question and make a strong, affirmative case for theory. Citing economist, Nobel laureate, and black box evaluation critic Sir Angus Deaton (2010), the authors remind us that many scholars believe the whole point of research is to learn about theories—to understand how the world works. Let’s consider a few roles for theory before turning to an example of theory in action.10 1. Use theory to improve intervention design You’d be surprised how often organizations get funding to implement a Program X that promises to make Outcome Y better without very much thought about how doing X will bring about change in Y. In the second half of this chapter, my aim will be to convince you that it’s easy and worthwhile to think critically about this issue and develop what we call a theory of change. 2. Use theory to improve research design For reasons that are a bit too technical for this point in our journey, theory can help us design better experiments. For instance, theory can inform the choices we make about where to conduct a study, which treatments to examine, how to recruit participants and randomize them to different study arms, and what variables to measure alongside the outcome of interest. We’ll focus on this last point in the next chapter. 3. Use theory to improve inferences Imagine that you are a policymaker in Pakistan interested in geriatric health and you come across the paper by Wagg et al. (2019) in The Lancet Global Health (discussed above). You want to know whether the group-exercise intervention they tested in rural Bangladesh is a good use of public funds in Pakistan. Wagg et al. (2019) reported good effects in the Bangladesh trial and, despite listing several limitations, concluded (without evidence or justification) that “these results would likely be generalisable to other communities with few resources in which physiotherapists are working with groups of older people in the community”. But you might wonder if the results will really travel to your setting and hold despite some necessary contextual changes to the program design you’ll need to make. Wilke and Humphreys (2019) suggest that drawing your assumptions in a causal model could help to extend the inferences you can make from this one study in Bangladesh. 4. Use empirical studies to BUILD theory In the same way that we can use qualitative research to observe the world and, through induction, generate ideas we can test in future studies, we can also use quantitative evaluations to generate theory. On this point you might find authors of black box evaluations quick to say, “But we do this!” In some cases this is probably true, at least in their own minds. I know that I accumulate ideas generated from my work that I do not formally document as theoretical ideas in a Discussion section of any particular article.11 My reading of Wilke and Humphreys (2019) is a reminder that empirical work is important for theory generation, even if not explicitly linked to published study results. 5. Use empirical studies to TEST theory This brings us back to the hypothetico-deductive model of science: use theory to generate falsifiable predictions and collect data to test these hypotheses. This is the classic deductive approach to science that some would argue is missing from global health studies. 6.2 An example of theory in action Figure 6.2: The putative role of PfATP4 in Na+ homeostasis in the malaria parasite, P. falciparum. Source: Spillman and Kirk (2015). While much of global health might be described as atheoretical, you can find good examples of theory-informed research if you know where to look. One area to explore is basic science and early phase clinical trials where scientists develop theories about the mechanism of action of drug candidates. For instance, Spillman and Kirk (2015) reviewed studies that have tested hypotheses derived from a theory that the protein ‘PfATP4’ plays a role in helping malaria parasites to maintain a low cytosolic Na+ concentration and receive energy. Another area is development economics. Let’s consider an experiment from Dr. Pascaline Dupas on HIV education in Kenya (Dupas 2011). I consider it to be an interesting example of theory building and testing. Download your copy here. 6.2.1 EVALUATING WHAT WORKS IN HIV EDUCATION In 2014, an estimated 1.4 million people in Kenya were living with HIV, a prevalence rate of 5.3% among adults aged 15 to 49. Without a cure for AIDS, prevention remains critical to ending the epidemic. Starting in 2001, Kenya integrated HIV/AIDS education into the primary school curriculum as a new prevention strategy (JPAL 2007). At the time, the focus of this program—and many other programs across sub-Saharan Africa—was complete risk avoidance, otherwise known as abstinence. Information on risk reduction was limited. Specifically, students were not learning about the differential prevalence of HIV infection by age and gender. Girls were not learning that the older “sugar daddies” who provide nice things like phones and airtime in return for sex are more likely than the girls’ goofy age mates to be infected. An organization called International Child Support (ICS) Africa aimed to change this by rolling out a “Relative Risk Information Campaign” in Kenya. The intervention was brilliant in its simplicity. A program staffer would talk with students for 40 minutes. During this time, the staffer showed the class a 10-minute video on sugar daddies and led a discussion about cross-generational sex. During the session, the staffer reviewed results of recent studies and wrote facts about the distribution of HIV prevalence on the chalk board. Researchers from the Abdul Latif Jameel Poverty Action Lab (JPAL) tested ICS Africa’s risk reduction strategy in a randomized experiment in Western Kenya. In the first phase (2003), 328 schools were randomized to teacher training on the national HIV prevention curriculum (Duflo et al. 2006). In the second phase (2004), 71 of these schools were stratified and randomized to receive the sugar daddy intervention (Dupas 2011). In total, there were 4 study arms: (1) teacher training only, (2) sugar daddy only, (3) teacher training and sugar daddy, and (4) nothing. The results were shocking. Teacher training was a bust. Although the training led to a change in teaching practices—notably that trained teachers mentioned HIV in class more often than nontrained teachers—it had little effect on HIV knowledge or childbearing rates. Increasing knowledge about HIV makes intuitive sense as an outcome for a study about HIV prevention. But why childbearing? \\*thinks\\*. Because it is harder to lie about having a baby than it is to lie about private sexual behavior, childbearing is considered a more objective measure of unprotected sex. Unprotected sex is also a main driver of HIV transmission, so childbearing serves as a proxy for HIV risk from unprotected sex. In contrast, the 40-minute sugar daddy discussion and video reduced childbearing with men at least 5 years older by 65%—and not because girls started having babies with males their own age. The overall incidence of childbearing fell by 28%. With a cost of $28.20 USD per school and $0.80 per student, the cost per childbirth averted was $91 (JPAL 2007). 6.2.2 THE THEORY BEHIND THE RELATIVE RISK REDUCTION CAMPAIGN Figure 6.3: Written description of the relative risk reduction conceptual framework. Source: Dupas (2011). Unlike The Lancet Global Health articles referenced above, Dupas (2011), which was published in the American Economic Journal: Applied Economics, includes a long Introduction to set up the paper, a thorough background section that discusses HIV education in Kenya, and a section called “Theoretical Motivation”. In this section, Dupas proposes a conceptual framework that I have attempted to present visually in the figure below. This framework puts forward a theory of why teenage girls engage in unprotected sex with older men. Core to the theory is an assumption that girls do not have information about the relative risk of contracting HIV from older men compared to their male peers. As Dupas explains: Teenage girls maximize their expected utility based on their beliefs about the risks of HIV infection and pregnancy occurring, and how those risks vary with condom use and partners’ characteristics. Figure 6.4: A visual representation of the Dupas (2011) conceptual framework for relative risk reduction. Visualization by Yours Truly, not necessarily endorsed by Dupas. Dupas goes on to derive several predictions from this framework (sounds very hypothetico-deductive, right?). Here is one such prediction: If all men have the same reservation price for sex with teenage girls (…), information on the distribution of [HIV] prevalence among men unambiguously leads teenage girls to move toward lower-risk partners (teenage boys) and thus reduce the rate of cross-generational transmission of HIV.12 In other words, all else equal, telling girls that older men are riskier sexual partners should lead them to have less sex with older men. That is a testable hypothesis. So what did she find?13 in response to the RR information, teenage girls substituted away from older partners and toward protected sex with teenage partners, but not more than one-for-one. And there you have it: proof that global health is not completely atheoretical! 6.3 Get to know a conceptual framework/model Your exposure to conceptual models—and your ideas about what makes a good model—will depend in part on the disciplinary home of the literature you read most often. But at the core, every model is a simplified representation of a more complex reality. A plastic replica of the human heart is a model. So is an epidemic model of Ebola transmission. Neither one is perfect, but both are valuable tools for teaching and learning. As the mathematician and statistician George Box famously wrote, “Essentially, all models are wrong, but some are useful” (Box and Draper 1987). Let’s take a discipline-specific tour of conceptual models before landing on one of my favorites, the theory of change. 6.3.1 IN PSYCHOLOGY AND RELATED DISCIPLINES Figure 6.5: Health belief model. Source: http://bit.ly/2i9Lw0Ehbm.jpg Conceptual models are very common in psychology, public health, and related fields. One of the most commonly used conceptual models in the study of health behavior change is the health belief model (Figure 6.5). The health belief model was developed in the 1950s when researchers in the U.S. Public Health Service were trying to understand why people were reluctant to engage in preventive health behaviors (Rosenstock 1974). This model suggests that health behaviors are explained by a person’s (a) perception of the benefits versus risks of action, (b) perceived threat from the health issue, (c) self-efficacy for change, and (d) cues to take action. Figure 6.6: Information-motivation-behavioral skills model Another example is the information-motivation-behavioral skills (IMB) model proposed by Fisher and Fisher (1992) to explain HIV-related risk behaviors (Figure 6.6). Conceptual models like this one can be tested empirically (i.e., with data). For instance, Zhang et al. (2011) explored the fit of the IMB model to data on condom use among female commercial sex workers in China. Figure 6.9 (below) is a path diagram showing the results of a structural equation model that was fit to the data. 6.3.2 IN EPIDEMIOLOGY Directed Acyclic Graphs (Causal diagrams) Figure 6.7: Graphical presentation of confounding in directed acyclic graphs. Identification of a minimal set of factors to resolve confounding. In (a), the backdoor path from chronic kidney disease (CKD) to mortality can be blocked by just conditioning on age, as depicted by the box around age. However if we assume that cancer also causes CKD (b), the backdoor paths can only be closed by conditioning on two factors, either age and cancer (as depicted) or cancer and dementia. Source: Suttorp et al. (2015). A related type of causal diagram is the directed acyclic graph, or DAG (or causal diagram/graph). This idea from mathematics and computer science has been applied to observational research to identify potential confounding variables that need to be addressed to make valid causal inferences (Greenland, Pearl, and Robins 1999). Figure 6.7 shows two DAGs. In this example, Suttorp et al. (2015) shows that, at a minimum, it is necessary to control for age when estimating the relationship between chronic kidney disease and mortality (a). But if we also assume that cancer is directly related to kidney disease (b), it is also important to control for cancer. Drawing out these relationships helps clarify that it may not be necessary to control for dementia because there is no direct relationship between dementia and kidney disease. We’ll review causal diagrams in more detail in Chapter 8. Epidemic model Figure 6.8: Compartmental flow of a mathematical model of the Ebola Epidemic in Liberia and Sierra Leone, 2014. Source: Rivers et al. (2014). Epidemic models are used to explain or predict the spread of an epidemic. Kermack and McKendrick (1927) proposed a deterministic compartmental model called SIR that consists of the number of uninfected people susceptible to the disease (S), the number of infected (I), and the number of people removed (R) through death or immunization. Rivers et al. (2014) used this basic framework to create a compartmental model of the 2014 Ebola outbreak in Liberia and Sierra Leone (Figure 6.8): Susceptible (S) Exposed (E) Infectious (I) Hospitalized (H) Funeral (F; handling bodies) Recovered/Removed (R) 6.3.3 IN STATISTICS Figure 6.9: Information-Motivation-Behavioral Skills structural equation model. Source: Zhang et al. (2011). Statistical models are nondeterministic and thus incorporate stochastic variables. In other words, some of the variables being modeled have probability distributions rather than constant inputs like those in physics or mathematics. Statistical models are typically communicated as a set of equations and are visualized as a set of results. An exception is the path diagram represented visually in Figure 6.9. 6.3.4 IN ECONOMICS Figure 6.10: Effect of increased inequality on population mortality. Source: Wildman and Shen (2014). Economic models can be stochastic or nonstochastic. The field of econometrics shares much in common with statistics, including a focus on stochastic models. Economics more broadly, however, also uses nonstochastic models. For instance, Figure 6.10 shows the hypothesized mathematical relationship between mortality and income (Wildman and Shen 2014). This graph is based on theory and does not plot actual empirical data collected in a particular setting. 6.4 Theory of change &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/wUiKdwgJpD8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; Theory of Change: It&#39;s Easier Than You Think. Chemonics International (2018). Source: [https://tinyurl.com/y5fc8m6l](https://tinyurl.com/y5fc8m6l). Underneath any good claim of causal inference is a theoretical model of how the researcher thinks X actually causes Y. Few studies set out to test a specific mechanism of impact, but many impact evaluations are designed around a theory about how the world works. This is called a theory of change. It’s a type of conceptual model used extensively in the evaluation literature. A theory of change articulates how an intervention—or a policy, program, or treatment—is expected to impact an outcome. It explains how X causes Y, and what is needed for this to happen. This concept may be referred to as a theory of change, a mechanism of change, a logic model, a logical framework, a causal model, a results chain, a pipeline model, a results framework, a program theory, or one of several other combinations of these terms. Let’s review strategies for creating a theory of change diagram and its cousin the logic model as a precursor to thinking about study measurement. 6.4.1 DEVELOPING A THEORY OF CHANGE Theory of change diagrams can be designed in various ways. There is no RIGHT WAY™ to create one, as long as the fundamentals of how X leads to Y are conveyed. If the diagram is easy to understand, it is a good diagram. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/urU-a_FsS5Y&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; Building Adult Capabilities to Improve Child Outcomes: A Theory of Change. Center on the Developing Child at Harvard University (2013). Source: [https://tinyurl.com/y4a3n25a](https://tinyurl.com/y4a3n25a). Most theory of change diagrams include a few common elements. The United Kingdom’s Department for International Development, commonly known as DFID or UK Aid, commissioned a report on the uses of theories of change in international development that identified several common components: Influence of context Discussion of long-term change Process/sequence of change explained Underlying assumptions Presented as a diagram and narrative summary My preferred approach to outlining a theory of change is to follow this template from the W.K. Kellogg Foundation. Here is an editable Google Docs version. Figure 6.11: Theory of change template. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay. Start with the (research) problem statement (Box 1). This box gets at the heart of the reason the intervention exists. What is the problem to solve? Although this seems obvious, too often there is a disconnect between the primary aim—solving a problem or answering a research question—and the intervention strategy. Next, take and inventory of the assets that already exist and the needs that remain (Box 2). There is always something to build upon, so it is important to look for strengths in addition to challenges. This process is best conducted in collaboration with people impacted by the problem so that the proposed solution is grounded in their reality. If available, descriptive data sources like the DHS can help to outline boxes 1 and 2. For a more local perspective, it is often beneficial to conduct a brief needs assessment in partnership with representatives from the local community if resources permit. Box 3 jumps to the desired results. If the program works, what will change? With the results articulated, reconsider the factors that might affect the program’s success positively or negatively (Box 4). The next step is to outline strategies for achieving the desired results, accounting for potential barriers and facilitators (Box 5). At this point, what the program will actually do should be stated clearly. Finally, Box 6 reminds us that every theory of change is built on a set of assumptions. It is important to be thorough and transparent when considering the hidden beliefs that underlie the ideas about how this program will achieve results. 6.4.2 THEORY OF CHANGE EXAMPLE: HIV RISK REDUCTION Let’s return to our relative risk reduction example from Dupas (2011) and use this template to create a theory of change. Take a moment to compare this theory of change diagram to the previous conceptual framework. Figure 6.12: Sugar daddy awareness theory of change. 6.5 Logic model While a theory of change tends to be a high-level depiction of the “why”, a logic model—or logframe—is more detailed and focuses on the “how”. Logical models are useful tools for program planning, monitoring program implementation, and program evaluation and reporting. 6.5.1 DEVELOPING A LOGIC MODEL If you leave school and take a job in global health or international development—especially if you work at USAID, DFID, or one of their grantees/contractors—you&#39;ll become very familiar with theory of change diagrams and logic models. The most common thing that former students write to tell me is that they impressed the boss by knowing how to create a logic model on Day 1. You&#39;re welcome. Logic models are often presented in the “results chain” or “pipeline” format shown in Figure 6.13. Inputs and activities represent the planned work. Outputs, outcomes, and impact are the intended results. Figure 6.13: Logic model. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay. Inputs are the resources needed to implement the program, most often including people, money, program materials, and time. Activities comprise what the program will do, like trainings, events, and distribution of goods. Outputs are counts of what the program did, such as the number of people trained, number of events held, number of goods delivered and number of people who benefitted. Conversely, outcomes are expected indicators of change—the short- and medium-term results of the program. Examples of outcomes include increased knowledge, decreased risky behavior, and improved functioning. Similar to outcomes, impacts are what you expect will happen in the long-run as a result of the program. Things like lower HIV prevalence and reduced mortality. Judging by the eyerolls I observe in class, it must seem redundant to students to further explain the difference between outputs and outcomes. But judging by historical performance on the midterm, a bit of redundancy is warranted. So think of it this way: If you have money, trainers, and a curriculum (the inputs) and use these resources to deliver a program (the activity) to groups of people, outputs are the count of how many times you delivered the program and how many people you reached. Outcomes are the changes that happen as a result of delivering the program.14 So what’s the difference between an outcome and impact? Generally speaking, impacts are longer-term changes to population-level indicators. Most studies are too small and short to actually detect any change in “impacts”. For instance, in the Kenya HIV risk reduction program the ultimate goal was to reduce HIV transmission, but the study “only” estimated the impact of the program on unprotected sex (the outcome). The assumption is that reducing unprotected sex in the short-term will reduce HIV transmission over the longer term if the program were to be scaled-up. 6.5.2 LOGIC MODEL EXAMPLE: HIV RISK REDUCTION Figure 6.14 shows what a logic model might look like for the relative risk reduction program. Figure 6.14: Sugar daddy awareness logic model. 6.6 The Takeaway It’s been said that global health is more a bunch of problems than a discipline. If you pick up a recent article on a global health topic, chances are good that you won’t find much, if any, reference to theory. Instead, black box evaluations are the norm. These studies ask “does my intervention work, and to what degree”, but not why, why not, or how. There are compelling reasons to think that more fully integrating theory into our work would benefit our studies in terms of design and inference, while also advancing the progress of science. As shown in the Kenya relative risk reduction evaluation, a good conceptual framework can clarify the theoretical motivation underlying your intervention. One common form of conceptual model in the evaluation literature is called a theory of change. A theory of change lifts the lid on the black box and states how intervention activities are hypothesized to generate outcomes, and what assumptions must hold for this to happen. Another way to draw these assumptions is called a logic model. Both models serve as an excellent bridge to our next topic: measurement. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["measurement.html", "7 Outcomes and Indicators 7.1 Using Conceptual Models to Plan Study Measurement 7.2 Terminology 7.3 What Makes a Good Indicator? 7.4 Constructing Indicators 7.5 The Takeaway", " 7 Outcomes and Indicators This chapter describes key measurement concepts, such as how to identify, define, and quantify study constructs. We’ll start by reviewing an example from the global mental health literature and use a conceptual model to think through important targets of measurement. Then we’ll consider what makes a good indicator of study constructs and outcomes and discuss common types of indicators you’ll come across in global health. 7.1 Using Conceptual Models to Plan Study Measurement &lt;iframe src=&quot;https://giphy.com/embed/65NO1TrKrJUT6&quot; width=&quot;300&quot; height=&quot;226&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;Drawing a blank on conceptual models? Head back to [**Chapter 6**](#aims) for a quick refresher on logic models. &lt;a href=&quot;https://giphy.com/gifs/65NO1TrKrJUT6&quot;&gt;via GIPHY&lt;/a&gt;&lt;/p&gt; A good conceptual model, such as a theory of change or a logic model, can be a bridge to good measurement. I’ll demonstrate this using a study by Patel et al. (2017) that reports on the results of a randomized controlled trial in India to test the efficacy of a lay counsellor-delivered, brief psychological treatment for severe depression called the Healthy Activity Program, or HAP. Please download the article here and give it a read. Figure 7.1: Abstract from Patel et al. (2017) published in The Lancet. 7.1.1 PROCESS INDICATORS Figure 7.2: Logic model. Process indicators in a logic model capture how well a program is implemented—the “M” (monitoring) in M&amp;E. As researchers, we care about collecting good process and monitoring data to develop a better understanding why programs do or do not work. For example, program costs must be accurately tracked to estimate cost-effectiveness. Or it may be important to determine whether the intervention was delivered according to the plan. Let’s start by using the article to create a logic model for the HAP intervention. All of the details we need are provided by the authors in the Method section. Before you continue with this chapter, see if you can identify the intervention inputs, activities, and outputs, otherwise known as process indicators. Go ahead, I’ll wait for you. via GIPHY Inputs You’ll recall from the previous chapter that inputs are the resources needed to implement a program. The most basic input of all is money. The authors tell us that HAP delivery costs included “patient contact and counsellor training, supervision, and salary”, an average of $66 per person. Aside from funding for these human resources—including “an international expert in behavioural activation”, local specialists, and lay counselors—we’re told that HAP is a “manualised psychological treatment”, which means there is a curriculum.15 Activities The main HAP activities were psychotherapy for patients and supervision of lay counselors. HAP was designed to be delivered in an individual, face-to-face format (telephone when necessary) over 6 to 8 weekly sessions each lasting 30 to 40 minutes. Supervision was to consist of weekly peer-led group supervision and twice monthly individual supervision. In a study like this, it’s important to measure if the intervention was delivered as intended. This is called treatment fidelity, and it’s a measure of how closely the actual implementation of a treatment or program reflects the intended design. Patel et al. (2017) measured fidelity in several ways, including external ratings of a randomly selected 10% of all intervention sessions. An expert not involved in the program listened to recorded sessions and compared session content against the HAP manual. This is a key part of the measurement plan because the intervention trained regular people with no prior experience to deliver a psychological treatment. We learn in the Results section that treatment fidelity was high, but imagine it had been poor. How would you interpret data suggesting that the intervention did not convey any benefit? Low treatment fidelity usually results in an attenuation (i.e., shrinking) of treatment effects, which is a threat to internal validity. If the study shows no effect but treatment fidelity is low, the null result may not be valid. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it is not possible to truly replicate the study. Outputs Treatment compliance is one of those terms like &quot;subjects&quot; that can make you cringe a bit. Research is voluntary, and participants have the right to decline a treatment offer or stop treatment at any point. We call this behavior &quot;non-compliance&quot;, which sounds to some like participants are misbehaving. Non-compliance can make your analysis and interpretation more complicated, but participants are not to blame. Look instead to the root causes in the research design, study procedures, or the intervention itself to find ways to limit non-compliance. Outputs are counts. One important type of count in a trial like this is treatment compliance, a measure of the extent to which people were treated or not treated according to their study assignment. Sometimes people assigned to the treatment group do not take-up the treatment, or they complete only part of the planned intervention. Noncompliance to randomization on the treatment side is called one-sided noncompliance. When members of the control group16 are also noncompliant with randomization, meaning they are treated despite being assigned to the no treatment condition, this is called two-sided noncompliance. Patel et al. (2017) randomly assigned 495 eligible adults to the HAP plus enhanced usual care group (n=247) or the enhanced usual care condition alone group (n=248). No one in the control group (EUC-only) was treated with HAP, but 31% of the HAP group had an unplanned discharge and did not complete the treatment. Analysis strategies for one- and two-sided noncompliance are discussed in a later chapter. 7.1.2 OUTCOMES AND IMPACTS The hypothesized outcome in Patel et al. (2017) was a reduction in severe depression: The two primary outcomes were depression severity assessed by the modified Beck Depression Inventory version II (BDI-II) and remission from depression as defined by a PHQ-9 score of less than 10, both assessed 3 months after enrollment. From the Introduction, we learn the following about the potential, long-term impact of achieving this outcome of depression remission: Depression substantially impairs quality of life, social functioning, and workforce participation among people with the disease, their family members, and their communities, with an annual global cost attributable to depression estimated at US$1·15 trillion. 7.1.3 HAP LOGIC MODEL Putting it all together, we have the following logic model of the HAP intervention. In the next few sections we’ll take a closer look at outcome definitions and measurement. Figure 7.3: Logic model based on Patel et al. (2017). 7.1.4 OTHER CONCEPTUAL MODELS Figure 7.4: A visual representation of the Dupas (2011) conceptual framework from Chapter 6. A logic model might not be the right choice for your study, but the point holds: any conceptual model will help you think more critically about measurement issues. Specifying a model is a great first step toward ensuring that you will collect all of the data you need to achieve your study aims. 7.2 Terminology In the last chapter, I introduced outputs (things you count), outcomes (things you want to change), and impacts (hypothesized longer-term changes). Let’s review a few new key measurement terms defined in Table 7.1. Table 7.1: Common measurement terms, adapted from Glennerster and Takavarasha (2013). Term Definition Example Construct A characteristic, behavior, or phenomenon to be assessed and studied. Often cannot be measured directly. Depression Outcome In an impact evaluation, ‘constructs’ will be referred to as outcomes—the intended results of the program. Also referred to as an endpoint in a trial. Decreased depression Indicator Observable measures of outcomes or other study constructs. Depression severity score on a depression scale Instrument The tools used to measure indicators. Also referred to as a measure. A depression scale, made up of questions/items about symptoms of depression Variable The numeric values of the indicators. Respondent The person (or group) that we measure. 7.2.1 CONSTRUCTS At the top of the list are study constructs. Constructs are what you investigate. In the Patel et al. (2017) example, the key construct of interest was depression. Many constructs are hard to measure directly—think empowerment, corruption, democracy. Even constructs that seem straightforward, like mortality, can be hard to define and measure. For instance, we care a lot about the causes of maternal mortality. But what counts as a maternal death? Clearly dying in childbirth counts, but what about a car accident during pregnancy? No? What about homicide during pregnancy? And what about deaths that occur shortly after delivery? Even 1 month after delivery? Before we can define what we mean by a maternal death, we have to get clear on the construct itself. For maternal mortality, this means thinking about biological mechanisms as well as social constructs like intimate partner violence. 7.2.2 OUTCOMES Some scholars investigate the nature of constructs as their primary aim. For instance, my colleagues and I conducted a study in Kenya where we tried to understand what people meant by the word “depression” and how to best measure this construct (Green et al. 2018). The goal was to understand everything we could about this thing called depression. Outside of the clinical trial/impact evaluation literature, outcomes are often known as **dependent variables** or **response variables**. Additional constructs of interest might be called &quot;covariates&quot;, &quot;independent variables&quot;, or &quot;exposure variables&quot;. But most often, researchers are interested in the relationship between constructs. What predicts depression? What interventions reduce depression? Framed this way, depression is an outcome. In a theory of change or logic model, outcomes take on the language of change: increases and decreases. The hypothesized outcome of implementing the HAP intervention was a reduction in severe depression. But the word “outcome” is also used more generally to represent the study endpoint. In a trial, your outcomes/endpoints are your markers of success. Death. Survival. Time to disease. Blood pressure. Tumor shrinkage. These are all endpoints you might seek to measure after offering some treatment or intervention. One study cannot definitively answer every possible research question. There are tradeoffs in terms of the time, money, and resources, so investigators must prioritize among all possible outcomes. Most studies are designed to provide the best evidence possible about one or two primary outcomes linked directly to the main study objective. Secondary outcomes may be registered, investigated, and reported as well, but these analyses may be more exploratory in nature if the study design is not ideal for measuring these additional outcomes. Patel et al. (2017) included the following secondary outcomes in addition to depression severity and remission from depression: Secondary outcomes were disability on the WHO Disability Assessment Schedule II and total days unable to work in the previous month, behavioural activation on the five-item abbreviated Activation Scale based on the Behavioural Activation for Depression Scale-Short Form, suicidal thoughts or attempts in the past 3 months, intimate partner violence (not a prespecified hypothesis), and resource use and costs of illness estimated from the Client Service Receipt Inventory. 7.2.3 INDICATORS AND INSTRUMENTS The language of qualitative studies is a bit different. These studies emphasize study constructs, but not indicators or measures. Quantification is not typically the goal. Moving from general to specific, the next term is indicator. Every outcome needs to be defined and measured. For example, Patel et al. (2017) hypothesized that the HAP intervention would reduce severe depression (the outcome). But what indicates depression? Patel et al. (2017) specified two indicators: a depression symptom severity score (a continuous measure of severity where higher scores suggest someone is experiencing more severe symptoms of depression); and depression remission (a binary/dichotomous indicator of the absence of depression based on a person’s depression score relative to a reference cutoff score; &lt; 10 on the Patient Health Questionnaire-9). Indicators are measured with instruments. Patel et al. (2017) used two instruments: the Beck Depression Inventory version II (BDI-II, pdf; Beck, Steer, and Brown 1996); and the Patient Health Questionnaire-9 (PHQ-9, pdf; Kroenke and Spitzer 2002). Figure 7.5: Instruments used in Patel et al. (2017). Source: https://datacompass.lshtm.ac.uk/513/. 7.3 What Makes a Good Indicator? When you select and define indicators of outcomes and other key variables, you are said to be operationalizing your constructs. This is a critical part of your planning. When you finish the study and present your findings, one of the first things colleagues will ask is, “How did you define [YOUR AWESOME OUTCOME]?” In other words, did you select a good indicator? Is your indicator DREAMY™: Defined clearly specified Relevant related to the construct Expedient feasible to obtain Accurate valid measure of construct Measurable able to be quantified customarY recognized standard 7.3.1 DEFINED It’s important to clearly specify and define all study variables, especially the indicators of primary outcomes. This is a basic requirement that enables a reader to critically appraise the work, and it serves as a building block for future replication attempts. Patel et al. (2017) preregistered and defined two indicators of depression severity and remission: Mean difference in total score measured at 3 months by the Beck’s Depression Inventory (BDI-II), a 21-item questionnaire assessment of depressive symptoms. Each item is scored on a Likert scale of 0 to 3. It measures depression severity based on symptom scores. Remission, defined as a score of &lt;10 measured at 3 months by the Patient Health Questionnaire (PHQ-9), a nine-item questionnaire for the detection and diagnosis of depression based on DSM-IV criteria. It is scored on a scale of 0 to 3 based on frequency of symptoms. 7.3.2 RELEVANT Indicators should be relevant to the construct of interest. In Patel et al. (2017), scores on the BDI-II and PHQ-9 are clearly measures of depression. An example of a nonrelevant indicator would be scores on the Beck Anxiety Inventory, a separate measure of anxiety. While anxiety and depression are often comorbid, anxiety is a distinct construct. 7.3.3 EXPEDIENT It should be feasible to collect data on the indicator given a specific set of resource constraints. Asking participants to complete a 21-item questionnaire and a 9-item questionnaire (as in Patel et al. (2017)) does not represent a large burden on study staff or participants. However, collecting and analyzing biological samples (e.g., hair, saliva, or blood) might. 7.3.4 ACCURATE Accurate is another word for “valid”. Indicators must be valid measures of study constructs. When deciding on indicators and instruments, Patel et al. (2017) had to ask themselves whether scores on the BDI-II and PHQ-9 measure a concept called depression among their target population. The authors cited their own previous work to support the decision to use these instruments (Patel et al. 2008). 7.3.5 MEASUREABLE Indicators must be quantifiable. Psychological constructs like depression are often measured using scales like the BDI-II and the PHQ-9. Other constructs require more creativity. For instance, Olken (2005) measured corruption in Indonesia by digging core samples of newly built roads to estimate the amount of materials used in construction and then compared cost estimates against reported expenditures to calculate a measure of corruption (i.e., by determining the missing expenditures). 7.3.6 CUSTOMARY Whenever possible, it’s smart to use standard indicators and follow existing definitions and calculation methods. One way to learn about standards and customs is to follow the current literature and locate articles that measure the same constructs. Familiarity with what is being published and the methods being used is a significant advantage in achieving publication of a research study.17 For example, if you’re planning an impact evaluation of a microfinance program on poverty reduction and wish to publish the results in an economics journal, start by reviewing highly cited work by other economists to understand current best practices. How do these scholars define and measure outcomes like income, consumption, and wealth? If studying population health, for instance, a good source of customary indicators is the World Health Organization’s Global Reference List of the 100 core health indicators (WHO 2015a): Figure 7.6: WHO 100 core health indicators. Source: http://bit.ly/1NgGeLh. Figure 7.7: Sustainable Development Goals. Source: http://bit.ly/2cuDpWN. Another good source of customary indicators is the United Nations Sustainable Development Goals (SDG), which include 230 indicators to measure 169 targets for 17 goals. For instance, a search of the SDG indicator metadata repository reveals that Goal 3 is to ensure healthy lives, and Target 3.1 is to reduce the global maternal mortality ratio to less than 70 per 100,000 live births by 2030. The UN plans to measure progress against this target with two indicators: Figure 7.8: SDG Goal 3 targets and indicators. Source: https://unstats.un.org/sdgs/metadata/. 7.4 Constructing Indicators 7.4.1 SIMPLE Some indicators are based on a single measurement and require only a definition. For instance, a hemoglobin level of less than 7.0 g/dl is an indicator of severe anemia. If you were evaluating the impact of a new diet on severe anemia, you would need only to record the result of a blood test (instrument). Returning to the SDG example, Indicator 3.1.2 is the proportion of births attended by skilled health personnel. To calculate this indicator, you need to know two things: the number of women aged 15-49 with a live birth attended by a skilled health personnel (doctors, nurses or midwives) during delivery; and the total number women aged 15-49 with a live birth in the same period. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/YfTXcc13GOI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; ITN Access Indicator Snapshot. The DHS Program (2015). Source: [https://tinyurl.com/y5ydg6qm](https://tinyurl.com/y5ydg6qm). You also only need to know two things from a Malaria Indicator Survey to calculate the indicator “proportion of households with at least one insecticide treated net”: the number of households surveyed with at least one ITN; and the total number of households surveyed. But getting a good answer to (1) takes some work. To qualify as an ITN, the net must have been factory-treated or soaked with insecticide within the past 12 months. To determine whether a household owns an ITN, survey administrators ask the following sequence of questions. Figure 7.9: Survey flow to determine whether a household has at least one insecticide treated net. The end result is a binary indicator (yes/no) of whether the household has a bednet that has been dipped in the past 12 months or is factory-treated. Sure, it’s possible to ask this in one question—“Does your household have any factory-treated mosquito nets or nets that have been dipped in a liquid to kill or repel mosquitoes in the past 12 months?” But this is a long and complicated question, and it’s more effective to break it up into smaller parts. 7.4.2 COMPOSITE Latent (unobservable) constructs like empowerment, quality of life, and depression, and some manifest (observable) constructs like wealth, are often measured with multiple items on a survey or questionnaire that are combined into indexes or scales. The terms index and scale are often used interchangeably, but they are not synonymous. While they share in common the fact that multiple items or observations go into their construction, making them composite measures or composite indicators, the method for and purpose of combining these items or observations are distinct. In an index, indicators “cause” the concept that is being measured. For example, a household’s wealth is determined by the assets it owns (e.g., livestock, floor quality). Conversely, in a scale, the construct “causes” the indicators. If you are depressed, you will exhibit certain symptoms. Figure 7.10: Scale vs index. Indexes Indexes combine items into an overall composite, often without concern for how the individual items relate to each other. For instance, the Dow Jones Industrial Average is a stock-market index that represents a scaled average of stock prices of 30 major U.S. companies such as Walt Disney and McDonald’s. The Dow Jones is a popular indicator of market strength and is constantly monitored during trading hours. Every index has its quirks, and the Dow Jones is no exception. Companies with larger share prices have more influence on the index. &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/U0FxxY1cUvU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; An alternative to the wealth index is a new measure called the [Equity Tool](http://www.equitytool.org/). Metrics for Management (2015). Source: [https://tinyurl.com/y2gqtvqn](https://tinyurl.com/y2gqtvqn). An index popular in the global health field is the DHS wealth index. As a predictor of many health behaviors and outcomes, economic status is a covariate in high demand. Failing to measure economic status in a household survey would be as grave as failing to note a respondent’s gender or age, but measuring economic status is not nearly as easy. In an ideal data world, every survey would include accurate information on household income and consumption as measures of household wealth. Income is volatile, however, and consumption is very hard to measure over short periods. Thus, in the late 1990s, researchers proposed creating an index of household assets as a measure of a household’s economic status (Rutstein and Johnson 2004). Data for the wealth index come from DHS surveys conducted in a particular country. Asset variables include individual and household assets (e.g., phone, television, car), land ownership, and dwelling characteristics, such as water and sanitation facilities, housing materials (i.e., wall, floor, roof), persons sleeping per room, and cooking facilities. Principal component analysis is a data reduction technique in which indicators are standardized (i.e., transformed into *z*-scores) so that they each have a mean of 0 and a variance of 1. A principal component (i.e., an eigenvector) is a linear combination of the original indicators; thus, every indicator (e.g., yes/no response to owning a phone) has a loading factor that represents the correlation between the individual indicator and the principal component. The first principal component always explains the most variance. In constructing the wealth index, the first component measures the concept called &quot;wealth,&quot; so the factor loadings on the first principal component are used to create a score for each household. A key decision in creating indexes like the wealth index is whether to weight the individual components. Should owning a car be given the same weight as owning a phone in the construction of wealth? Most researchers would probably say no, so the next question is how to assign differential weights to the components. Filmer and Pritchett (2001) first proposed assigning weights via principal component analysis, or PCA. Once the weights are determined, a household gets an overall score that is the sum of the weights for having (or not having) each asset. It’s then possible to divide the distribution of household wealth index scores into quintiles. Each household falls into 1 of 5 wealth quintiles reflecting their economic status (relative to the sample). Hereby, the relationship between health outcomes and wealth can be examined. Figure 7.11: Example wealth index construction from 2014 Bangladesh DHS. Scales Composite scales are very common in health and social science research. This is because many constructs like depression, empowerment, and quality of life do not (yet) have reliable biomarkers or objective measures. Constructs like depression need to be defined and measured indirectly. In some domains like psychology and psychiatry, you could, at least in theory, have a clinician diagnose a disorder like depression based on some agreed upon criteria. According to the Diagnostic and Statistical Manual for Mental Disorders, currently the DSM-V, there are five criteria for major depressive disorder. If someone meets criteria A–E, they are diagnosed with major depressive disorder (MDD). Figure 7.12: DSM-V criteria for major depressive disorder. A diagnosis by a trained mental health professional like a psychiatrist is considered the gold standard measure of depression. Gold standards are in short supply in many places, however, and more feasible methods of measuring this latent construct called depression are needed. A reasonable alternative is to develop a set of questions—a scale—that measure symptom severity. Presumably, if a person scores high enough on this scale, he or she would be considered “depressed”. One example of a depression scale is the Beck Depression Inventory used by Patel et al. (2017) to measure depression severity. The BDI-II consists of 21 groups of statements.18 Figure 7.13: Example scale. &lt;iframe src=&quot;https://giphy.com/embed/LVrHbt3VK3OSHtP4c8&quot; width=&quot;300&quot; height=&quot;169&quot; frameBorder=&quot;0&quot; class=&quot;giphy-embed&quot; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;**CAUTION:** Just because you find or create a scale like the BDI-II, it does not mean that the scale actually measures your construct of interest with your population of interest. We&#39;ll return to this important topic in [**Chapter 14**](#quantitative). &lt;a href=&quot;https://giphy.com/gifs/rcrracing-caution-austin-dillon-flag-LVrHbt3VK3OSHtP4c8&quot;&gt;via GIPHY&lt;/a&gt;&lt;/p&gt; Each item is a manifest variable—something measured directly by asking the question. The latent variable depression is measured indirectly by summing the responses to all 21 manifest variables to create the BDI-II scale score. 7.5 The Takeaway When you are planning a new study, you should be able to draw a logical flow from your research question to the measurement of primary study outcomes/constructs. Figure 7.14 demonstrates this idea using Patel et al. (2017) as an example. Figure 7.14: From research question to study instruments So ask yourself: Given my research question, what is the key construct of interest? What other constructs need to be measured at the same time to fully understand the key construct? Drawing a conceptual model can help you identify measurement issues. What are the indicators for these constructs? In other words, how can these constructs be quantified? Which measurement instrument will enumerate these quantities? What type of data will this instrument yield? (we’ll dig into this more in Chapter 14) Additional Resources on Indicators Table 7.2: Indicator resources. Topic Resource Malaria Roll Back Malaria (2013). Household Survey Indicators for Malaria. Measure Evaluation (2016). Monitoring and Evaluation of Malaria Programs. HIV/AIDS WHO (2015). Consolidated Strategic Information Guidelines for HIV in the Health Sector. TB WHO (2015). A Guide to Monitoring and Evaluation for Collaborative TB/HIV Activities: 2015 Revision. Family Planning FP2020 (2015). Measurement Annex. Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["module-3-understand-inference.html", "MODULE 3 Understand Inference", " MODULE 3 Understand Inference By the end of this module, you should be able to: "],
["statisticalinference.html", "8 Statistical Inference 8.1 Two Major Approaches to Statistical Inference 8.2 Frequentist Approach 8.3 Bayesian Approach 8.4 Keep Learning", " 8 Statistical Inference Consider the following figure. It comes from a randomized clinical trial of 2,303 healthy postmenopausal women that set out to answer the question, “Does dietary supplementation with vitamin D3 and calcium reduce the risk of cancer among older women?” (Lappe et al. 2017). Before we go any further, look at the image and decide what you think. Figure 8.1: Invasive and in situ cancer incidence among healthy older women receiving vitamin D and calcium vs placebo. Source: Lappe et al. (2017). If you said &quot;No, look at the p-value&quot;, please be patient and I&#39;ll deal with you in moment. If you said, &quot;Maybe, but there&#39;s no estimate of uncertainty&quot; or &quot;Maybe, but what&#39;s more important is the size of the risk decrease&quot;, then you are my favorite. Please go get a cookie. If you said “Yes”, you’re probably in good company. I think most readers will come to the same conclusion. Without knowing anything else about the specific analysis or statistics in general, you can look at this figure and see that both groups started at 0% of participants with cancer (which makes sense given the design), over time members of both groups developed some type of cancer, and by the end of the study period cancer was more common among the non-supplement (placebo) group. But “Yes” is not what the authors concluded. Here’s what they said: In this RCT…supplementation with vitamin D3 and calcium compared with placebo did not result in a significantly lower risk of all-type cancer at 4 years. [emphasis added] Here&#39;s a sample of news reports about the results: &lt;br&gt; [**New York Times**](https://www.nytimes.com/2017/04/10/health/vitamin-d-deficiency-supplements.html?_r=0): &quot;The supplements &lt;span style=&quot;color=black;font-weight:bold&quot;&gt;did not protect&lt;/span&gt; the women against cancer&quot; [**Medical News Today**](https://www.medicalnewstoday.com/articles/316627): &quot;Clinical trial finds that vitamin D, calcium &lt;span style=&quot;color=black;font-weight:bold&quot;&gt;have no effect&lt;/span&gt; on cancer risk&quot; [**Time**](https://time.com/4714335/vitamin-d-supplement-calcium-cancer/): &quot;There were &lt;span style=&quot;color=black;font-weight:bold&quot;&gt;no differences&lt;/span&gt; in cancer rates between the two groups&quot; [**WebMD**](https://www.webmd.com/cancer/news/20170328/high-doses-of-vitamin-d-fail-to-cut-cancer-risk-study-finds#1): &quot;High Doses of Vitamin D &lt;span style=&quot;color=black;font-weight:bold&quot;&gt;Fail to Cut Cancer Risk&lt;/span&gt;&quot; Technically they are correct. While the supplement group had a 30% lower risk for cancer compared to the placebo group (a hazard ratio of 0.70), the 95% confidence interval around this estimate spanned from 0.47 (a 53% reduction) to 1.02 (a 2% increase), thus crossing the line of “no effect” for ratios at 1.0. The p-value was 0.06 and their a priori significance cutoff was 0.05, so the result was deemed “not significant” and the conclusion was that supplements do not lower cancer risk. But is that the best take? Not everyone thought so. Here’s what Ken Rothman and his colleagues wrote to the journal editors: JAMA published a different letter to the editor that raised similar issues [@jaroudi:2017], and the authors of the original paper responded [@lappe:2017b]: &lt;br&gt; &quot;...the possibility that the results were &lt;span style=&quot;color=black;font-weight:bold&quot;&gt;clinically significant&lt;/span&gt; should be considered. The 30% reduction in the hazard ratio suggests that this difference may be clinically important.&quot; JAMA rejected this letter from my colleagues &amp; me (“low priority”), so we're publishing on twitter, hoping JAMA will take it more seriously. pic.twitter.com/bh7byo4CrR — Ken Rothman ((???)) May 9, 2017 The best answer, at least in my view, is that the trial was inconclusive. The point estimate is that supplements reduced cancer risk by 30%, but the data are also consistent with a relative reduction of 53% and an increase of 2%. In absolute terms, the group difference in cancer prevalence at Year 4 was 1.69 percentage points. It seems like there might be a small effect. Whether a small effect is clinically meaningful is for the clinical experts on your research team to decide. This example highlights some of the challenges with statistical inference. Recall from Chapter 1 that science is all about inference: using limited data to make conclusions about the world. We’re interested in this sample of 2,300 because we think the results can tell us something about cancer risk in older women more generally. But to make this leap, we have to make several inferences. First, we have to decide whether we think the observed group differences in cancer risk in our limited study sample reflect a true difference or not. This is a question about statistical inference. Second, we have to ask whether this difference in observed cancer risk was caused by the supplements. This is a question of causal inference (and internal validity), and we’ll take this on in the next chapter. Finally, if we think the effect is real, meaningful, and caused by the intervention, do we think the results apply to other groups of older women? This is a question of generalizability and external validity, a topic we’ll cover in Chapter 14. But for now, let’s explore statistical inference. There’s a lot to unpack from this example. I’ll start by telling you about the most common approach that involves p-values and null hypotheses, highlight some of the challenges and controversies of this approach, and then present some alternatives. I’ll end with some suggestions about how you can continue to build your statistical inference skills. 8.1 Two Major Approaches to Statistical Inference I&#39;m ignoring a third major school of statistical inference called the Likelihood approach. See @dienes2008 for a nice introduction. There are two main approaches to statistical inference: the Frequentist approach and the Bayesian approach. A key distinction between the two is the assumed meaning of probability. Believe it or not, smart people continue to argue about the definition of probability. If you are a Frequentist, then you believe that probability is an objective, long-run relative frequency. Your goal when it comes to inference is to limit how often you will be wrong in the long run. You can be both and use whichever approach makes the most sense for the task, but I&#39;ll be a bit more black-and-white for now. If you are a Bayesian, however, you favor a subjective view of probability that says you should start with your degree of belief in a hypothesis and update that belief based on the data you collect. I’ll explain what this all means, but before we get too far along, please think about what YOU want to know most: the probability of observing the data you collected if your preferred hypothesis was not true; or the probability of your hypothesis being true based on the data you observed? 8.2 Frequentist Approach The Frequentist approach (and terms like &quot;significant&quot;) originated with Sir Ronald Fisher, but Jerzy Neyman and Egon Pearson worked out the logic of hypothesis testing and inference [@dienes2008]. If you are a standard user of Frequentist methods, you are probably a follower of Neyman-Pearson. Open just about any medical or public health journal and you’ll find loads of tables with p-values and asterisks, and results described as “significant” or “non-significant”. These are artifacts of the Frequentist approach, specifically the Neyman-Pearson approach. 8.2.1 HOW IT WORKS In the Neyman-Pearson approach, you set some ground rules for inference, collect and analyze your data, and compare your result to the benchmarks you set. Inference is essentially mindless automatic once you set the ground rules. To explore how it works, let’s return to the trial of the Healthy Activity Program for depression discussed in Chapter 7. As a reminder, Patel et al. (2017) conducted a randomized controlled trial to answer the following research question: In this randomized controlled trial, the control group was not completely inactive. They received something called &quot;enhanced usual care&quot;. Here is how @patel:2016 described it: &quot;In the enhanced usual care group, usual care and treatment provided by the [primary care] physician was enhanced by provision of screening results to both patient and physician and use of a contextualised version of the mhGAP guidelines (a manual containing the specific mhGAP guidelines for primary care physicians treating depression), including when and where to refer for psychiatric care.&quot; Among adults 18–65 years of age with a probable diagnosis of moderately severe to severe depression from 10 primary health centres in Goa, India, does enhanced usual care plus a manualised psychological treatment based on behavioural activation delivered by lay counselors—the Healthy Activity Program, or HAP—reduce depression severity compared to enhanced usual care alone? [not a direct quote] Step 1: Specify two hypotheses In hypothesis testing, we set up two precise statistical hypotheses: a null hypothesis (H0) and an alternative hypothesis (H1). Most often the null hypothesis is stated as the hypothesis of no difference: The &quot;null&quot; hypothesis doesn&#39;t have to be a hypothesis of no difference [@dienes2008]. The null could be a specific difference that is a value other than 0, e.g., μ~1~ `-` μ~2~ `=` 3, or a band of differences. H0: μ1 = μ2 (or μ1 - μ2 = 0), meaning there is no difference in average depression severity between the group that was invited to receive HAP plus enhanced usual care compared to the group that only received enhanced usual care This point null hypothesis can reject only one value: exactly zero. Is this strong support for a theory? A “two-tailed” alternative hypothesis states that there is a difference, but does not specify which arm is superior: H1: μ1 ≠ μ2 (or μ1 - μ2 ≠ 0), meaning that the average difference between the groups is not zero. It might seem confusing because H1 is the hypothesis we talk about and write about, but it’s actually the null hypothesis (H0) that we test and decide to reject or accept (technically, ‘fail to reject’). In her great book [*Learning Statistics with R*](https://learningstatisticswithr.com/book/hypothesistesting.html), Danielle Navarro frames this as &quot;the trial of the null hypothesis&quot;. The null is where statistical inference happens. The Frequentist rejects or retains the null hypothesis, but does not directly prove the alternative. They simply decide whether there is sufficient evidence to convict (i.e., reject) the null. Step 2: Imagine a world in which H0 is true (i.e., innocent) This is where things get a bit weird. Frequentists subscribe to the long run view of probability. In this framework, you have to establish a collective, a group of events that you can use to calculate the probability of observing any single event (Dienes 2008). Your study is just one event. You can’t determine the probability of obtaining your specific results without first defining the collective of all possible studies. I know what you’re thinking. This seems nuts. In my defense, I said it gets a bit weird. Hang with me though. The good news is that you do not have to repeat your study an infinite number of times to get the collective. You can do it with your imagination and the magic of statistics. So put on your wonder cap and imagine that you conducted thousands of experiments where H0 was true. Yeah, that’s right, I’m asking you to picture running your study over and over again with a new group of people, but the truth is always that the intervention does not work. The aim of this thought exercise is to establish what type of data we’re likely to find when the null hypothesis is TRUE. I used a bit of back-of-the-envelope math to come up with the total accessible population. First, @patel:2016 conducted the study in 10 primary health centers in Goa, India, and these PHCs have a catchment of about 30,000 people each. Second, based on the [2016 census](https://en.wikipedia.org/wiki/Demographics_of_India#cite_note-96), about 55.9% of the population falls in the eligible age range for the trial. Third, @patel:2016 estimate the prevalence of depression to be 4%. Taken together, this is about 6700 depressed people. To kick things off, consider this hypothetical accessible population of 6,705 depressed people who are eligible for the HAP trial (see Figure 8.2). In each of your imagined studies, you’ll recruit a new sample of 332 patients from this population. Let’s assume that the baseline level of depression severity among these patients ranges from a score of 10 to 63 on the depression instrument you’re using, the Beck Depression Inventory-II (BDI-II). I say “assume” because you’ll never get to know more than 332 of these 6,705 patients. We know they exist, but we don’t know the true population size or the true average level of depression among this group. Figure 8.2: Hypothetical population of depressed patients. We get the center and spread of the distribution from the authors’ trial protocol that reports that in a prior study the control group BDI-II mean was 24.5, and the standard deviation was 10.7 (Patel et al. 2014). Patel et al. (2017) did not use the BDI-II to determine eligibility for the HAP trial—just the PHQ-9—but we can assume that no participants in the trial had a BDI-II score of less than 10 at enrollment. Next, imagine that each orange dot in Figures 8.2 and 8.3 represents 1 of the 332 patients you recruited into the actual study you conducted. You do know each one of these people, you do measure their depression level at baseline, and you do calculate the sample mean. This is the only sample you’ll see as the researcher in real life, but let’s pretend that this sample was #7,501 out of an imaginary set of 10,000 possible studies. Figure 8.3: Here’s the sample of 332 patients you recruit into your study. Patel et al. (2017) did not actually administer the BDI-II at baseline, but we can pretend. Why a sample of 332? Patel et al. (2017) tell us they planned to recruit 500 participants but expected to lose track of 15% of them. That’s a planned sample size of 425. But they also tell us that the study design was a bit more complicated, so the planned effective sample size was 425/1.28 = 332. Your trial design is a randomized controlled trial, so you randomize these 332 people to the treatment group (HAP plus enhanced usual care) or the control group (enhanced usual care only). As shown in Figure 8.4, you allocate 1:1, meaning that 50% (166) patients end up in the treatment arm, and 50% in the control arm. Figure 8.4: Distribution of baseline BDI-II scores by study arm. Even with random assignment the group means are not 100% identical at baseline. This is normal. As the sample size gets bigger, randomization produces better balance. Now I’d like you to imagine that your intervention is NOT superior to enhanced usual care (and vice versa). A few months after the treatment arm completes the program, you reassess everyone in the study and find that the average depression score in both groups decreases by 5 points (see Figure 8.5). Since the baseline mean for the HAP group was 24.2, and the enhanced usual care arm mean was 28.0, the endline means shift down by 5 points to 19.2 and 23.0, respectively. The effect size—in this example the average post-intervention difference between groups—is 19.2 - 23.0 = -3.8. The instrument you’re using to measure this outcome, the BDI-II, ranges from a possible score of 0 to 63. So an absolute difference of 3.8 points is small, but it’s not 0. Figure 8.5: Distribution of endline BDI-II scores by study arm. Here’s what it might look like if depression severity reduces by 5 points in both groups. Student in the first row raises hand: If the null hypothesis of no difference is actually true, why isn’t every study result exactly zero? It’s a good question. The reason is this: there’s error in data collection and sampling error that comes from the fact that we only include a small fraction of the population in our study samples. Therefore, we might get a result that is near 0—but not exactly 0—even if the null hypothesis is really true. Hopefully Figure 8.6 will make this point clear. To help you imagine a world in which H0 is true, I drew 10,000 samples of 332 people from the simulated accessible population of ~6700, randomly assigned each person to a study arm, and calculated the treatment effect for the study if everyone’s depression score reduced by exactly 5 points. This figure plots all 10,000 study results. Figure 8.6: 10,000 simulated results when there’s no effect. Unlike the previous figures, this time the dots are study results, not people. Here’s the key thing to observe: I simulated 10,000 studies where everyone always improved by an equal amount—i.e., no treatment effect—but there is NOT just one stack of results piled 10,000 high at exactly zero. Instead, the results form a nice bell shaped curve around 0. &lt;iframe src=&#39;https://gfycat.com/ifr/QuaintTidyCockatiel&#39; frameborder=&#39;0&#39; scrolling=&#39;no&#39; allowfullscreen width=&#39;300&#39; height=&#39;189&#39;&gt;&lt;/iframe&gt;&lt;p&gt; &lt;a href=&quot;https://gfycat.com/quainttidycockatiel&quot;&gt;via Gfycat&lt;/a&gt;&lt;/p&gt; This is the central limit theorem at work (aka, the magic of statistics). When plotted together, the results of our imaginary study replications form a distribution that approximates a normal distribution as the number of imaginary replications increases. This is fortunate because we know useful things about normal curves. For instance, we can find any study result on the curve and know where it falls in the distribution. Is it in the fat part around 50%? Or is it a rather extreme result far in the tails at 1% or 2%? To conclude Step 2, let’s think back to the Frequentist definition of probability that relies on having some collective of events. The plot in Figure 8.6 represents this collective. Frequentists can only talk about a particular result being in the 1st or 50th percentile of results if there is a group of results that make up the collective. Without the collective—the denominator—there can be no probability. Of course in reality, no Frequentist repeats a study over and over 10,000+ times to get the collective. They rely on the central limit theorem to imagine the most plausible set of results that might occur when the null hypothesis is really true. This statistically derived, but imaginary, collective is fundamental to the Frequentist approach. Step 3: Set some goal posts Skeptical student: OK, I get that a study result does not have to be exactly zero for the null to be true. But how different from zero must a result be for Frequentists to reject the null hypothesis that the treatment had no effect? @lakens:2018 argue that &quot;the optimal alpha level will sometimes be lower and sometimes be higher than the current convention of .05&quot;, which they describe as arbitrary. They want you to think before you experiment—and to justify your alpha. In the Frequentist approach, you decide if the data are extreme relative to what is plausible under the null by setting some goal posts before you conduct the study. If the result crosses the threshold, it’s automatically deemed “statistically significant” and the null hypothesis is rejected. If it falls short, the null hypothesis of no difference is retained. This threshold is known as the alpha level. For Frequentists, alpha represents how willing they are to be wrong in the long run when it comes to rejecting the null hypothesis. Traditionally, scientists set alpha to be no greater than 5%. Returning to our simulated results in Figure 8.6, you can see that I drew the goal posts as dotted red lines. They are positioned so that 5% of the distribution of study results falls outside of the lines, 2.5% in each tail. This is a two-tailed test, meaning that we’d look for a result in either direction—treatment group gets better (left, negative difference) OR worse (right, positive difference). @patel:2016 used a two-tailed test, which was appropriate because it was possible the treatment could have made people more depressed. I&#39;m showing a one-tailed test below because it simplifies the learning. You can also draw a single goal post that contains the full alpha level (e.g., 5%). I show this in Figure 8.7 below. This is appropriate when you have a directional alternative hypothesis, such as the treatment mean minus the control mean will be negative. In the HAP example, this would indicate that the treatment group ended the trial with a lower level of depression severity. Step 4: Make a decision about the null With the goal posts set, the decision is automatic. Your actual study result either falls inside or outside the goal posts, and you must either retain or reject the null hypothesis. I’ll say it again: statistical inference happens on the null. Going back to our example, imagine that you conducted study #7,501 of 10,000. When you collected endline data, you found a mean difference between the two arms of -3.8 as shown in Figure 8.7. What’s your decision with respect to the null? Do you retain or reject? Figure 8.7: 10,000 simulated results when there’s no effect. Result of study #7501 falls outside of the goal post, so the null hypothesis is rejected. Reject! A difference of -3.8 falls outside of the alpha level you set at 5%. Therefore, you automatically reject the null hypothesis and label the result “statistically significant”. But there’s something else we know about this result, the raw effect size of -3.8: it falls at the 1st percentile of our simulated collective. It’s an extreme result relative to what we expected if the null is true. We’d say it has a p-value of 1%. The p-value is a conditional probability. It’s the probability of observing a result as big or bigger than our study result (-3.8)—and here’s the conditional part—IF THE NULL HYPOTHESIS IS TRUE. Smart student mutters to self while taking notes: Why does he keep saying “if the null hypothesis is true” like some lawyer who loves fine print? I simulated 10,000 studies where there was no effect so we could visualize plausible results when the null is true, but in practice we just conduct one study and rely on the central limit theorem to tell us the probability of observing different results if the null hypothesis is true. Earlier I asked what you would rather know: &lt;br&gt; (a) the probability of observing the data you collected if your preferred hypothesis was not true; or (b) the probability of your hypothesis being true based on the data you observed? Frequentist hypothesis testing and *p*-values give you (a), not (b). You can get (b), but not with the method of Neyman-Pearson. For that you&#39;ll need the Bayesian approach, which I&#39;ll come to shortly. I heard that! And good thing, because it’s important to say this again: when it comes to inference, we never know the “truth”. We do not know if the null hypothesis is actually true or false. That’s why the p-value is a conditional probability. A p-value is the probability of observing the data if the null hypothesis is true P(D|H), NOT the probability that the null hypothesis is true given the data P(H|D). Let that sink in. The p-value might not mean what you want it to mean. Furthermore, since we can’t know the truth about the null hypothesis, it’s possible that we make the wrong decision when we reject or retain it. If the null hypothesis is really true—and that’s what I simulated—we made a mistake by rejecting the null. We called the result statistically significant, but this is a false positive. Statisticians refer to this mistake as a Type I error, though I think the term false positive is more intuitive since we’re falsely claiming that our treatment had an effect when it did not. The good news is that in the long run we will only make this mistake 5% of the time if we stick to the Frequentist approach. The bad news is that we have no way of knowing if THIS EXPERIMENT is one of the times we got it wrong. The other type of mistake we can make is called a Type II error—a false negative. We make this mistake when the treatment really does have an effect, but we fail to reject the null. We’ll talk more about false negatives—and power—in Chapter 14. Figure 8.8: Possible outcomes of inferential statistics. You are significant, even if your p-value is not Hmmm, only 1 out of 20 studies rejects the null when the null is simulated to be true...that&#39;s, like, 5% of the time. Check out Figure 8.9 for an animated look at the simulation discussed above. It flips through 20 of the 10,000 studies I simulated to show no treatment effect. Watch for your study, #7501. It’s the only one of the set of 20 that rejects the null. Figure 8.9: Here’s an animated gif of 20 draws from a simulation of 10,000 studies where the treatment has no effect. Watch for the one study that rejects the null. 8.2.2 PUBLISHED EXAMPLE: HAP TRIAL RESULTS Figure 8.10: Participant flow diagram, HAP trial. Source: Patel et al. (2017). Patel et al. (2017) enrolled and randomized 495 depressed adults in Goa, India to receive the HAP intervention plus enhanced usual care or enhanced usual care alone. They reported 2 post-randomization exclusions and 5% loss to follow-up, for a final intent-to-treat analysis sample of 493 (245 treatment, 248 control). Figure 8.11 shows the primary and secondary outcomes. We’re interested in the BDI-II score measured 3-months after the treatment group completed the HAP intervention (red outline). After adjusting for the study site (PHC) and participants’ depression scores at baseline (measured via a different instrument, PHQ-9), the authors find that HAP reduced depression severity by an average of 7.57 points (BDI-II ranges from 0 to 63). If you look back to Figure 8.7, you’ll see that a difference of this size is off the chart. So you should not be surprised that Patel et al. (2017) report a p-value of &lt; 0.0001. If the null hypothesis is true—meaning that there really was no treatment effect—you would expect to get a difference at least this big less than 0.01% of the time. Figure 8.11: HAP trial primary and secondary outcomes. Source: Patel et al. (2017). 8.2.3 CAVEATS AND CONSIDERATIONS Keep these in mind when you review manuscripts or write up your own Frequentist analysis. A. Statistical significance does not imply practical or clinical significance In his Coursera course, &quot;[Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/info)&quot;, [Daniël Lakens](https://twitter.com/lakens) frames *p*-values as a reflection of &quot;how surprising the data is, assuming that there is no effect&quot;. To say that a result is “statistically significant” tells the world only that the result was sufficiently surprising to reject the null hypothesis (based on your definition of surprising—alpha—and what data you could expect to see if the null hypothesis is true). Statistical significance does not imply any type of practical or clinical significance. It does not mean that your finding is “significant” in the colloquial sense of “meaningful” or “important”. Was the HAP trial result clinically significant? The authors classified the result as a &quot;moderate effect&quot;, but such qualitative labels are hard to interpret without context. An average reduction in depression severity of 7.57 points on the BDI-II is equivalent to complete remission of 2/21 symptoms or minor improvement on 7/21 symptoms. You can also think of a shift of 7.57 points in relative terms: it is 12% of the overall 63-point scale and a reduction of 28% compared to the control group&#39;s mean. Dividing by the pooled standard deviation gives a standardized effect size is 0.48 sd. By comparison then, HAP is about two-thirds as effective as similar treatment models delivered by **professionals in high-income settings** [@cuijpers:2016]. By the authors&#39; estimate, &quot;the cost per BDI-II point improvement was $6&quot;, making HAP a cost-effective option. If statistical significance is your jam, just (plan to) get more fruit. For reasons that will become clear in Chapter 14, simply increasing the sample size will shrink the p-value. With enough resources, you could conduct a study that finds a new intervention “significantly” reduces average systolic blood pressure (the top number) from 120 to 119, p &lt; 0.05. But who cares? Whether this effect size is clinically or practically significant is completely separate from whether you have enough data to say that the effect is not zero. This is why you should always report effect sizes, not just p-values. More on this in a moment. B. You can peek at your data, but you have to pay in alpha You also have to pay in alpha when you run multiple tests. Taking the long run view of probability can really cramp your style. More data = more precision = lower p-values = statistical significance, but be careful. In the Neyman-Pearson approach that takes the long view on probability, you cannot look at your data, find that p = 0.052, and recruit more participants just to push down the p-value—without paying a statistical price. You might also plan interim analyses to see if it&#39;s necessary to stop the trial because the early data suggest such a clear benefit that denying the experimental treatment to the control group becomes unethical. To be clear, there are lots of reasons why it could make sense to take an interim look at the data before the end of your study. Chief among them is participant safety. If you are testing a new intervention that could cause harm, you would likely create a committee and a plan for looking at your data at various points of the study. But you have to pay the price for peeking. When you take interim peeks at the data you have to move the goal posts outward. Rather than an alpha of 0.05, you might have to raise the bar to 0.01, for instance. There is no free lunch. C. No, your p-value is not “trending toward significance” If you review a manuscript where an author decribes a *p*=0.052 as &quot;trending toward significance&quot;, you are required by law to inform them that, by their logic, it is also trending *away* from significance. In the Neyman-Pearson approach, results are either above the alpha level you set or below it, statistically significant or non-significant. You may not modify the word “significant” with language like “trending” or “marginally” or “approaching”. If you set alpha to 0.05 so your long term error rate is 5%, a p=0.052 for a particular study is non-significant. p-values are not a measure of the strength of your evidence (Dienes 2008). D. A non-significant finding does not equal “no effect” If you want to establish that there is no effect—to be on #teamPreciseNull—then check out equivalence testing. @lakens:2017 has a nice primer. A common mistake is to infer from a p-value of 0.052 that there is “no effect” or “no difference”. As Lakens explains in his Coursera course on statistical inference, all you can take away from a p-value greater than your alpha threshold is that the result is not sufficiently surprising if the null hypothesis is true. It’s possible that the effect is small—too small for you to detect with a small sample size. Scroll to the beginning of this chapter to see an example of this mistake in print. 8.2.4 CRITICISMS &lt;iframe src=&quot;https://fivethirtyeight.abcnews.go.com/video/embed/56150342&quot; width=&quot;300&quot; height=&quot;169&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; allowfullscreen&gt;&lt;/iframe&gt;*p*-values are hard! Source: [https://tinyurl.com/jofqf6s](https://tinyurl.com/jofqf6s). The Frequentist approach dominates the literature, but it’s not without its critics. Lots of critics, in fact. More than 800 scientists recently signed on to a proposal to abandon statistical significance (Amrhein, Greenland, and McShane 2019), and some journals have banned reporting p-values. Other researchers have proposed keeping significance testing, but redefining statistical significance to a higher bar, from an alpha of 0.05 to 0.005 (Benjamin et al. 2018). The misuse and misunderstanding of p-values and statistical significance is so widespread that the American Statistical Association issued a statement reminding scientists of what a p-value does and does not tell us (Wasserstein and Lazar 2016). Frustration with null-hypothesis significance testing, or NHST, is not new, however. People like Paul Meehl have been warning us for decades (Meehl 1967, 1990). So why the recent hubbub? Come closer. No, closer. We’ve realized that science is in a crisis—a replication crisis. And NHST is partly to blame. You might remember from Chapter 1 that replication is the process of repeating a study to see if you get the same results. Most methods books will tell you that replication is core to the practice of science, but until recently replication studies in the health and social sciences were very rare. Strong incentives for novelty keep researchers, funders, and journals looking ahead, not behind. But a 2011 paper concluding that ESP is real—yes, that ESP, extrasensory perception—was the breaking point for some. This paper led to some serious soul searching, followed by real efforts to repent, reform, and replicate. Over the course of 10 years, social psychologist Daryl Bem ran 9 experiments, ~100 participants per experiment, in which he asked people to respond to experimental stimuli before the stimuli were presented to test his ideas about precognition and premonition (Bem 2011). Some of the experiments had college students guess which of two curtains on a computer monitor was hiding an erotic image. After the student responded, the answer was randomly assigned. All but 1 of these 9 experiments produced statistically significant results supporting ESP. Many in the scientific community lost their minds. A 2017 Slate piece by Daniel Engber provides a fascinating look at the scientific reaction to the ESP paper when it was published in Journal of Personality and Social Psychology . …for most observers, at least the mainstream ones, the paper posed a very difficult dilemma. It was both methodologically sound and logically insane. Daryl Bem had seemed to prove that time can flow in two directions—that ESP is real. If you bought into those results, you’d be admitting that much of what you understood about the universe was wrong. If you rejected them, you’d be admitting something almost as momentous: that the standard methods of psychology cannot be trusted, and that much of what gets published in the field—and thus, much of what we think we understand about the mind—could be total bunk. QRP is known in medical circles as **&quot;questionable research practices&quot;**. It is a debilitating condition that might affect as many as 1 in every 2 scientists [@john:2012]. Side effects can include most research findings being false [@ioannidis:2005]. The conclusion for many skeptics of Bem’s work was that this paper followed the conventions of the time, but the conventions were flawed. Bem’s paper was just a symptom of the disease—QRP. Figure 8.12: It’s almost like you need a p-value less than 0.05 to get published. One notorious QRP goes hand in hand with NHST: p-hacking. You see Reader, data analysis is a garden of forking paths. Even simple analyses require the analyst to make lots of decisions. There are many pathways one can take to get from question to answer. p-hacking is going down one path, finding a p-value of 0.052, and turning around to go down another path that leads to 0.049. It’s true that an analyst of any stripe can engage in a multitude of QRPs, but p-hacking is uniquely Frequentist. Of course, it’s not the p-value’s fault that it’s often misunderstood and abused. Even the “abandon statistical significance” camp recognizes its value for some tasks. Their main criticism is that the conventional use of p-values encourages us to think dichotomously—there either is an effect or there is not—and this is bad for science. Non-significant doesn’t mean “no effect”, but that’s often the conclusion when p = 0.052 (just scroll to the top of the chapter for an example). Furthermore, when publication decisions are made on the basis of p &lt; 0.05, we distort the literature and encourage QRPs. And when we encourage QRPs—particularly when our sample sizes are small and we look for small effect sizes—we end up with a crisis. We publish a lot of noise that fails to replicate. We’ll pick up this replication crisis thread in a later chapter on open science. For now, let’s consider some alternatives to p-values and NHST. 8.2.5 “THERE HAS TO BE A BETTER WAY!” &lt;iframe width=&quot;300&quot; height=&quot;169&quot; src=&quot;https://www.youtube.com/embed/iJ4kqk3V8jQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; The New Statistics: Confidence Intervals, NHST, and p Values. Psychological Science (2014). [https://youtu.be/iJ4kqk3V8jQ](https://youtu.be/iJ4kqk3V8jQ) Some argue that the way to avoid dichotomous thinking is to embrace uncertainty and focus on estimating effects—to embrace the “New Statistics” (Calin-Jageman and Cumming 2019). Figure 8.13 compares the “new” and the “old” (NHST) using the HAP trial results. Figure 8.13: From statistical significance to estimation and quantification of uncertainty. 3-month endline data from the HAP trial (Patel et al. 2017). Panel A represents NHST, where the goal is to determine whether or not there is a statistically significant difference (there is!). Panel B compares the same group means but places the “significant” results in the context of individual data points from all study participants, reminding us that a lot of people in the treatment group remained at a high level of depression severity even though the treatment group improved on average relative to the control group. Panel C focuses on estimation. It displays the point estimate of the average treatment effect (treatment mean minus control mean, -7.6) and the 95% confidence interval. The confidence interval gives us the same information as a p-value, plus more. That fact that the interval does not include 0 tells us that the difference is statistically significant; the p-value is less than 0.05. -7.6 is our best estimate of the treatment effect, but the range of the interval tells us that we cannot reject effects between -10.3 and -4.9. The flip side of this is that we can rule out very large effects greater than -10.3 and very small effects less than -4.9. But here’s the catch with confidence intervals: they are still bound by Frequentists ideas of probability. Therefore, a confidence interval is a long-run metric. If you were to repeat your study over and over and estimate confidence intervals each time, 95% of the time the true value would fall inside the interval. But we don’t know if in this study—the one we actually conducted—the true value falls inside this specific interval. 95% of the time the interval will contain the true value, but this might be one of those times it doesn’t. Bottom line: Frequentist confidence intervals are an improvement over p-values, but they still do not tell you the probability that your hypothesis is correct. For that we need the Bayesian approach. 8.3 Bayesian Approach Figure 8.14 shows the basic idea behind Bayesian analysis in three panels. The first panel shows a simulation of results we believe are plausible before collecting data. The second panel displays the data we collect and our best estimate of how well our model fits the data. The third panel updates our prior belief based on the data we observed. Figure 8.14: Bayes’ Theorem in three panels. Source: Tristan Mahr, https://tinyurl.com/ya2tvoaj In Bayesian data analysis, we proclaim our prior belief about the hypothesis P(H), collect some data and determine the likelihood of the data given our hypothesis P(D|H), and combine the likelihood and prior to obtain the posterior probability of the hypothesis given the data we observed P(H|D). This is Bayes’ theorem at work: the posterior is proportional to the likelihood times the prior. What happens when you give the pros the same dataset and ask them to use their preferred approach to statistical inference? Check out @van:2019. Conceptually, this is very different from the Frequentist approach, even though the answers we get might be similar. In the Frequentist approach, we get a p-value that quantifies the probability of the DATA if the null hypothesis were true. But in the Bayesian approach, we get the probability of the HYPOTHESIS given the data we observed. With Bayesian data analysis, there’s no need to imagine an infinite number of trials—and no need to worry about multiple comparisons or peeks at the data. You can look at your data every day if you want. Every data point updates your belief in the hypothesis. Let’s reexamine the HAP trial from a Bayesian perspective to see the real benefits of a Bayesian approach when it comes to interpretation. 8.3.1 BAYESIAN RE-ANALYSIS OF THE HAP TRIAL Figure 8.15 compares the Frequentist 95% confidence interval reported in Patel et al. (2017) to the Bayesian 95% credible interval based a weakly informative prior and the trial data. The results are essentially the same, but the interpretation is very different. Figure 8.15: Bayesian re-analysis of HAP primary outcome of depression severity. Anonymized data provided by the authors. With Frequentist confidence intervals, we can say that if we repeated this trial over and over, 95% of intervals would contain the true value. We don’t know if the interval on the left is one of the times we got it wrong, but if we act like this interval contains the true value we will only be wrong only 5% of the time. Bayesians do not typically frame inference in terms of significance testing. Some data analysts use so called Bayes factors in the spirit of significance testing. I think Bayes factors take us back down the road of NHST-style dichotomous thinking, so I’ll leave it for you to explore on your own. The Bayesian credible interval on the right tells us something different: we are 95% confident that the treatment effect falls in *this specific interval. We can say this because Bayesian analysis gives us a posterior probability distribution that is not conditional on the null hypothesis being true. A particularly handy feature of posterior distributions is that we can also estimate the probability of any particular point. For instance, there is a 87% chance that the effect size is less than -6. And look at that—we get the probability of a hypothesis given the data. Nice, right? 8.4 Keep Learning Statistical inference is hard. Here are a few resources to keep learning: Statistical Inference: Daniël Lakens’s Coursera course, “Improving your statistical inferences” Statistical Inference: Zoltan Dienes’s book Understanding Psychology as a Science Frequentist Approach: Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars, by Deborah Mayo New Statistics: Understanding the New Statistics, by Geoff Cumming Bayesian Approach: Statistical Rethinking by Richard McElreath Share Your Feedback This book is a work in progress, so I’d really appreciate your feedback on this chapter. References "],
["causalinference.html", "9 Causal Inference", " 9 Causal Inference Forthcoming "],
["module-4-select-a-research-design.html", "MODULE 4 Select a Research Design", " MODULE 4 Select a Research Design By the end of this module, you should be able to: Articulate the benefits and limitations of random assignment Explain the logic and limitations of different quasi-experimental and observational designs that can be used when randomization is not possible or ethical "],
["experimental.html", "10 Experimental", " 10 Experimental Forthcoming "],
["quasiexperimental.html", "11 Quasi-Experimental", " 11 Quasi-Experimental Forthcoming "],
["observational.html", "12 Observational", " 12 Observational Forthcoming "],
["module-5-specify-your-methods.html", "MODULE 5 Specify Your Methods", " MODULE 5 Specify Your Methods By the end of this module, you should be able to: Select the best method of data collection given study objectives and resources Devise a sampling strategy to meet study objectives and resources Design a high-powered study "],
["samplingandpower.html", "13 Sampling and Power", " 13 Sampling and Power Forthcoming "],
["quant.html", "14 Quantitative Data Collection Procedures", " 14 Quantitative Data Collection Procedures Forthcoming "],
["qual.html", "15 Qualitative Data Collection Procedures", " 15 Qualitative Data Collection Procedures Forthcoming "],
["references.html", "16 References", " 16 References "]
]
