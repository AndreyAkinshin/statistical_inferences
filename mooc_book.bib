
@book{aberson_applied_2019,
  title = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}: 2nd {{Edition}}},
  shorttitle = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Aberson, Christopher L.},
  year = {2019},
  month = feb,
  edition = {2 edition},
  publisher = {{Routledge}},
  address = {{New York}},
  abstract = {Applied Power Analysis for the Behavioral Sciences is a practical "how-to" guide to conducting statistical power analyses for psychology and related fields. The book provides a guide to conducting analyses that is appropriate for researchers and students, including those with limited quantitative backgrounds. With practical use in mind, the text provides detailed coverage of topics such as how to estimate expected effect sizes and power analyses for complex designs. The topical coverage of the text, an applied approach, in-depth coverage of popular statistical procedures, and a focus on conducting analyses using R make the text a unique contribution to the power literature. To facilitate application and usability, the text includes ready-to-use R code developed for the text. An accompanying R package called pwr2ppl (available at https://github.com/chrisaberson/pwr2ppl) provides tools for conducting power analyses across each topic covered in the text.},
  isbn = {978-1-138-04456-2},
  language = {English}
}

@article{albers_when_2018-1,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies.
Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{allison_power_1997,
  title = {Power and Money: {{Designing}} Statistically Powerful Studies While Minimizing Financial Costs},
  shorttitle = {Power and Money},
  author = {Allison, David B. and Allison, Ronald L. and Faith, Myles S. and Paultre, Furcy and {Pi-Sunyer}, F. Xavier},
  year = {1997},
  volume = {2},
  pages = {20--33},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/1082-989X.2.1.20},
  abstract = {Adequate statistical power is increasingly demanded in research designs. However, obtaining adequate research funding is increasingly difficult. This places researchers in a difficult position. In response, the authors advocate an approach to designing studies that considers statistical power and financial concerns simultaneously. Their purpose is twofold: (a) to introduce the general paradigm of cost optimization in the context of power analysis and (b) to present techniques for such optimization. Techniques are presented in the context of a randomized clinical trial. The authors consider (a) selecting optimal cutpoints for subject screening tests; (b) optimally allocating subjects to different treatment conditions; (c) choosing between obtaining more subjects or taking more replicate measurements; and (d) using prerandomization covariates. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Methods},
  keywords = {Cost Containment,Experimentation,Experimenters,Statistical Power},
  number = {1}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the Codebook {{Package}} to {{Facilitate Data Reuse}}},
  author = {Arslan, Ruben C.},
  year = {2019},
  month = may,
  pages = {2515245919838783},
  issn = {2515-2459},
  doi = {10.1177/2515245919838783},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once\textemdash by their creators\textemdash and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en}
}

@book{babbage_reflections_1830,
  title = {Reflections on the {{Decline}} of {{Science}} in {{England}}: {{And}} on {{Some}} of {{Its Causes}}},
  shorttitle = {Reflections on the {{Decline}} of {{Science}} in {{England}}},
  author = {Babbage, Charles},
  year = {1830},
  publisher = {{B. Fellowes}},
  abstract = {Book digitized by Google and uploaded to the Internet Archive by user tpb.},
  collaborator = {{unknown library}},
  language = {English}
}

@article{bayarri_rejection_2016,
  title = {Rejection Odds and Rejection Ratios: {{A}} Proposal for Statistical Practice in Testing Hypotheses},
  shorttitle = {Rejection Odds and Rejection Ratios},
  author = {Bayarri, M.J. and Benjamin, Daniel J. and Berger, James O. and Sellke, Thomas M.},
  year = {2016},
  month = jun,
  volume = {72},
  pages = {90--103},
  issn = {00222496},
  doi = {10.1016/j.jmp.2015.12.007},
  journal = {Journal of Mathematical Psychology},
  keywords = {Bayes factors,Bayesian,Frequentist,Odds},
  language = {en}
}

@book{berkeley_defence_1735,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  year = {1735},
  volume = {3}
}

@article{bigby_understanding_2014,
  title = {Understanding and Evaluating Systematic Reviews and Meta-Analyses},
  author = {Bigby, Michael},
  year = {2014},
  month = mar,
  volume = {59},
  pages = {134},
  issn = {0019-5154},
  doi = {10.4103/0019-5154.127671},
  abstract = {A systematic review is a summary of existing evidence that answers a specific clinical question, contains a thorough, unbiased search of the relevant literature, explicit criteria for assessing studies and structured presentation of the results. A systematic review that incorporates quantitative pooling of similar studies to produce an overall summary of treatment effects is a meta-analysis. A systematic review should have clear, focused clinical objectives containing four elements expressed through the acronym PICO ( \textbf{P}atient, group of \textbf{p}atients, or \textbf{p}roblem, an \textbf{I}ntervention, a \textbf{C}omparison intervention and specific \textbf{O} utcomes). Explicit and thorough search of the literature is a pre-requisite of any good systematic review. Reviews should have pre-defined explicit criteria for what studies would be included and the analysis should include only those studies that fit the inclusion criteria. The quality (risk of bias) of the primary studies should be critically appraised. Particularly the role of publication and language bias should be acknowledged and addressed by the review, whenever possible. Structured reporting of the results with quantitative pooling of the data must be attempted, whenever appropriate. The review should include interpretation of the data, including implications for clinical practice and further research. Overall, the current quality of reporting of systematic reviews remains highly variable.},
  journal = {Indian Journal of Dermatology},
  language = {en},
  number = {2},
  pmid = {24700930}
}

@incollection{blume_likelihood_2011,
  title = {Likelihood and Its {{Evidential Framework}}},
  booktitle = {Philosophy of {{Statistics}}},
  author = {Blume, Jeffrey D.},
  year = {2011},
  pages = {493--511},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-51862-0.50014-9},
  isbn = {978-0-444-51862-0},
  language = {en}
}

@book{borenstein_introduction_2009,
  title = {Introduction to Meta-Analysis},
  editor = {Borenstein, Michael},
  year = {2009},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester, U.K}},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  isbn = {978-0-470-05724-7},
  keywords = {Meta-analysis,Meta-Analysis as Topic},
  lccn = {R853.M48 I58 2009}
}

@article{brown_grim_2017,
  title = {The {{GRIM Test}}: {{A Simple Technique Detects Numerous Anomalies}} in the {{Reporting}} of {{Results}} in {{Psychology}}},
  shorttitle = {The {{GRIM Test}}},
  author = {Brown, Nicholas J. L. and Heathers, James A. J.},
  year = {2017},
  month = may,
  volume = {8},
  pages = {363--369},
  issn = {1948-5506},
  doi = {10.1177/1948550616673876},
  abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20\% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
  journal = {Social Psychological and Personality Science},
  language = {en},
  number = {4}
}

@misc{buchanan_mote_2017,
  title = {{{MOTE}}: {{Effect Size}} and {{Confidence Interval Calculator}}.},
  author = {Buchanan, Erin M. and Scofield, J and Valentine, K. D.},
  year = {2017}
}

@article{carter_correcting_2019,
  title = {Correcting for {{Bias}} in {{Psychology}}: {{A Comparison}} of {{Meta}}-{{Analytic Methods}}},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Sch{\"o}nbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  year = {2019},
  month = jun,
  volume = {2},
  pages = {115--144},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses\textemdash that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{cohen_earth_1994,
  title = {The Earth Is Round (p {$<$} .05).},
  author = {Cohen, Jacob},
  year = {1994},
  volume = {49},
  pages = {997--1003},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  journal = {American Psychologist},
  language = {en},
  number = {12}
}

@article{cohen_earth_1995,
  title = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05): {{Rejoinder}}},
  shorttitle = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05)},
  author = {Cohen, Jacob},
  year = {1995},
  month = dec,
  volume = {50},
  pages = {1103},
  issn = {0003-066X},
  doi = {http://dx.doi.org/10.1037/0003-066X.50.12.1103},
  copyright = {\textcopyright{} 1995, American Psychological Association},
  journal = {American Psychologist},
  keywords = {Null Hypothesis Testing (major)},
  language = {English},
  number = {12}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  address = {{Hillsdale, N.J}},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  lccn = {HA29 .C66 1988}
}

@article{colquhoun_reproducibility_2017,
  title = {The {{Reproducibility Of Research And The Misinterpretation Of P Values}}},
  author = {Colquhoun, David},
  year = {2017},
  month = aug,
  pages = {144337},
  doi = {10.1101/144337},
  abstract = {We wish to answer this question If you observe a "significant" P value after doing a single unbiased experiment, what is the probability that your result is a false positive?. The weak evidence provided by P values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe P = 0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3:1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the P value. And if you want to limit the false positive risk to 5 \%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe P = 0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100:1 odds on there being a real effect. That would usually be regarded as conclusive, But the false positive risk would still be 8\% if the prior probability of a real effect was only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe P = 0.00045. It is recommended that the terms "significant" and "non-significant" should never be used. Rather, P values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed P value. Despite decades of warnings, many areas of science still insist on labelling a result of P {$<$} 0.05 as "significant". This practice must account for a substantial part of the lack of reproducibility in some areas of science. And this is before you get to the many other well-known problems, like multiple comparisons, lack of randomisation and P-hacking. Science is endangered by statistical misunderstanding, and by university presidents and research funders who impose perverse incentives on scientists.},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  journal = {bioRxiv},
  language = {en}
}

@article{cumming_confidence_2006,
  title = {Confidence Intervals and Replication: {{Where}} Will the next Mean Fall?},
  shorttitle = {Confidence Intervals and Replication},
  author = {Cumming, Geoff and Maillardet, Robert},
  year = {2006},
  volume = {11},
  pages = {217--227},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.3.217},
  journal = {Psychological Methods},
  language = {en},
  number = {3}
}

@article{danziger_extraneous_2011,
  title = {Extraneous Factors in Judicial Decisions},
  author = {Danziger, S. and Levav, J. and {Avnaim-Pesso}, L.},
  year = {2011},
  month = apr,
  volume = {108},
  pages = {6889--6892},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/PNAS.1018033108},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {17}
}

@book{de_groot_methodology_1969,
  title = {Methodology},
  author = {{de Groot}, Adrianus Dingeman},
  year = {1969},
  volume = {6},
  publisher = {{Mouton \& Co.}},
  address = {{The Hague}}
}

@misc{de_vrieze_meta-analyses_2018,
  title = {Meta-Analyses Were Supposed to End Scientific Debates. {{Often}}, They Only Cause More Controversy},
  author = {{de Vrieze}, Jop and {2018} and Pm, 4:15},
  year = {2018},
  month = sep,
  abstract = {Compiling the evidence from dozens of studies doesn't always bring clarity},
  howpublished = {https://www.sciencemag.org/news/2018/09/meta-analyses-were-supposed-end-scientific-debates-often-they-only-cause-more},
  journal = {Science | AAAS},
  language = {en}
}

@article{delacre_why_2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}'s {\emph{t}}-Test {{Instead}} of {{Student}}'s {\emph{t}}-Test},
  author = {Delacre, Marie and Lakens, Dani{\"e}l and Leys, Christophe},
  year = {2017},
  volume = {30},
  issn = {2119-4130},
  doi = {10.5334/irsp.82},
  abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-test and Welch's t-test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met. We argue that Welch's t-test should be used as a default strategy.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  journal = {International Review of Social Psychology},
  keywords = {homogeneity of variance,Homoscedasticity,Levene’s test,statistical power,Student’s t-test,type 1 error,type 2 error,Welch’s t-test},
  language = {eng},
  number = {1}
}

@book{dienes_understanding_2008,
  title = {Understanding Psychology as a Science: {{An}} Introduction to Scientific and Statistical Inference},
  shorttitle = {Understanding Psychology as a Science},
  author = {Dienes, Zoltan},
  year = {2008},
  publisher = {{Palgrave Macmillan}}
}

@article{ferguson_vast_2012,
  title = {A Vast Graveyard of Undead Theories Publication Bias and Psychological Science's Aversion to the Null},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  volume = {7},
  pages = {555--561},
  journal = {Perspectives on Psychological Science},
  number = {6}
}

@book{feyerabend_against_1993,
  title = {Against Method},
  author = {Feyerabend, Paul},
  year = {1993},
  edition = {3rd ed},
  publisher = {{Verso}},
  address = {{London ; New York}},
  isbn = {978-0-86091-481-5 978-0-86091-646-8},
  keywords = {Methodology,Philosophy,Rationalism,Science},
  lccn = {Q175 .F42 1993}
}

@article{fiedler_questionable_2015,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2015},
  month = oct,
  pages = {1948550615612150},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  journal = {Social Psychological and Personality Science},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology},
  language = {en}
}

@article{fiedler_tools_2004,
  title = {Tools, Toys, Truisms, and Theories: {{Some}} Thoughts on the Creative Cycle of Theory Formation},
  shorttitle = {Tools, Toys, Truisms, and Theories},
  author = {Fiedler, Klaus},
  year = {2004},
  volume = {8},
  pages = {123--131},
  journal = {Personality and Social Psychology Review},
  number = {2}
}

@article{field_minimizing_2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonz{\'e}n, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  volume = {7},
  pages = {669--675},
  journal = {Ecology Letters},
  number = {8}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  year = {1935},
  publisher = {{Oliver And Boyd; Edinburgh; London}}
}

@book{fisher_statistical_1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  year = {1956},
  volume = {viii},
  publisher = {{Hafner Publishing Co.}},
  address = {{Oxford, England}},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation.},
  copyright = {(c) 2016 APA, all rights reserved}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2014},
  volume = {345},
  pages = {1502--1505},
  doi = {10.1126/SCIENCE.1255484},
  journal = {Science},
  number = {6203}
}

@article{glockner_irrational_2016,
  title = {The Irrational Hungry Judge Effect Revisited: {{Simulations}} Reveal That the Magnitude of the Effect Is Overestimated},
  shorttitle = {The Irrational Hungry Judge Effect Revisited},
  author = {Gl{\"o}ckner, Andreas},
  year = {2016},
  volume = {11},
  pages = {601--610},
  journal = {Judgment and Decision Making},
  number = {6}
}

@article{good_c140_1982,
  title = {C140. {{Standardized}} Tail-Area Probabilities},
  author = {Good, I. J.},
  year = {1982},
  month = dec,
  volume = {16},
  pages = {65--66},
  issn = {0094-9655},
  doi = {10.1080/00949658208810607},
  journal = {Journal of Statistical Computation and Simulation},
  number = {1}
}

@article{greenland_statistical_2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  volume = {31},
  pages = {337--350},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  journal = {European Journal of Epidemiology},
  language = {en},
  number = {4}
}

@article{greenwald_consequences_1975,
  title = {Consequences of Prejudice against the Null Hypothesis.},
  author = {Greenwald, Anthony G.},
  year = {1975},
  volume = {82},
  pages = {1},
  journal = {Psychological Bulletin},
  number = {1}
}

@article{hacking_experimentation_1982,
  title = {Experimentation and {{Scientific Realism}}},
  author = {Hacking, Ian},
  year = {1982},
  volume = {13},
  pages = {71--87},
  issn = {0276-2080},
  doi = {10/fz8ftm},
  journal = {Philosophical Topics},
  number = {1}
}

@article{harms_making_2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  pages = {382--393},
  issn = {2424810X},
  doi = {10.18053/jctres.03.2017S2.007},
  abstract = {Being able to interpret `null effects' is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  journal = {Journal of Clinical and Translational Research},
  language = {en},
  number = {3}
}

@misc{heino_legacy_2016,
  title = {The Legacy of Social Psychology},
  author = {Heino, Matti TJ},
  year = {2016},
  month = nov,
  abstract = {What can we learn re-examining the classic cognitive dissonance experiment?},
  journal = {Data punk | K\"aytt\"aytymisarkkitehtuuri},
  language = {en}
}

@article{hoenig_abuse_2001,
  title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
  shorttitle = {The Abuse of Power},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  year = {2001},
  volume = {55},
  pages = {19--24},
  journal = {The American Statistician},
  number = {1}
}

@article{hung_behavior_1997,
  title = {The {{Behavior}} of the {{P}}-{{Value When}} the {{Alternative Hypothesis}} Is {{True}}},
  author = {Hung, H. M. James and O'Neill, Robert T. and Bauer, Peter and Kohne, Karl},
  year = {1997},
  volume = {53},
  pages = {11--22},
  issn = {0006-341X},
  doi = {10.2307/2533093},
  abstract = {The P-value is a random variable derived from the distribution of the test statistic used to analyze a data set and to test a null hypothesis. Under the null hypothesis, the P-value based on a continuous test statistic has a uniform distribution over the interval [0, 1], regardless of the sample size of the experiment. In contrast, the distribution of the P-value under the alternative hypothesis is a function of both sample size and the true value or range of true values of the tested parameter. The characteristics, such as mean and percentiles, of the P-value distribution can give valuable insight into how the P-value behaves for a variety of parameter values and sample sizes. Potential applications of the P-value distribution under the alternative hypothesis to the design, analysis, and interpretation of results of clinical trials are considered.},
  journal = {Biometrics},
  number = {1}
}

@article{john_measuring_2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  volume = {23},
  pages = {524--532},
  journal = {Psychological science},
  number = {5}
}

@article{johnson_revised_2013,
  title = {Revised Standards for Statistical Evidence},
  author = {Johnson, V. E.},
  year = {2013},
  month = nov,
  volume = {110},
  pages = {19313--19317},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1313476110},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{kelley_confidence_2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  year = {2007},
  volume = {20},
  issn = {1548-7660},
  doi = {10.18637/JSS.V020.I08},
  abstract = {The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F , and {$\chi$}2 distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F , and {$\chi$}2 distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {8}
}

@article{kelley_sample_2006,
  title = {Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  year = {2006},
  volume = {11},
  pages = {363},
  journal = {Psychological methods},
  number = {4}
}

@article{kerr_harking_1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  volume = {2},
  pages = {196--217},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  journal = {Personality and Social Psychology Review},
  language = {en},
  number = {3},
  pmid = {15647155}
}

@article{kirk_practical_1996,
  title = {Practical {{Significance}}: {{A Concept Whose Time Has Come}}},
  shorttitle = {Practical {{Significance}}},
  author = {Kirk, R. E.},
  year = {1996},
  month = oct,
  volume = {56},
  pages = {746--759},
  issn = {0013-1644},
  doi = {10.1177/0013164496056005002},
  journal = {Educational and Psychological Measurement},
  language = {en},
  number = {5}
}

@book{kitcher_advancement_1993,
  title = {The Advancement of Science: Science without Legend, Objectivity without Illusions},
  shorttitle = {The Advancement of Science},
  author = {Kitcher, Philip},
  year = {1993},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  isbn = {978-0-19-504628-1},
  keywords = {History,Philosophy,Science},
  lccn = {Q175 .K533 1993}
}

@article{kruschke_rejecting_2018,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  year = {2018},
  month = jun,
  volume = {1},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@book{lakatos_methodology_1978,
  title = {The Methodology of Scientific Research Programmes: {{Volume}} 1: {{Philosophical}} Papers},
  shorttitle = {The Methodology of Scientific Research Programmes},
  author = {Lakatos, Imre},
  year = {1978},
  volume = {1},
  publisher = {{Cambridge University Press}}
}

@article{lakens_calculating_2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Daniel},
  year = {2013},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  journal = {Frontiers in Psychology},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning},
  language = {English}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  volume = {44},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  journal = {European Journal of Social Psychology},
  language = {en},
  number = {7}
}

@article{lakens_reproducibility_2016,
  title = {On the Reproducibility of Meta-Analyses: Six Practical Recommendations},
  shorttitle = {On the Reproducibility of Meta-Analyses},
  author = {Lakens, Dani{\"e}l and Hilgard, Joe and Staaks, Janneke},
  year = {2016},
  volume = {4},
  pages = {24},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0126-3},
  abstract = {Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions.},
  journal = {BMC Psychology},
  keywords = {Meta-analysis,Open science,Reporting guidelines,Reproducibility}
}

@book{leamer_specification_1978,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  year = {1978},
  month = apr,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{New York usw.}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  language = {English}
}

@article{lenth_practical_2001,
  title = {Some Practical Guidelines for Effective Sample Size Determination},
  author = {Lenth, Russell V.},
  year = {2001},
  volume = {55},
  pages = {187--193},
  journal = {The American Statistician},
  number = {3}
}

@article{leon_role_2011,
  title = {The {{Role}} and {{Interpretation}} of {{Pilot Studies}} in {{Clinical Research}}},
  author = {Leon, Andrew C. and Davis, Lori L. and Kraemer, Helena C.},
  year = {2011},
  month = may,
  volume = {45},
  pages = {626--629},
  issn = {0022-3956},
  doi = {10.1016/j.jpsychires.2010.10.008},
  abstract = {Pilot studies represent a fundamental phase of the research process. The purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to be used in a larger scale study. The roles and limitations of pilot studies are described here using a clinical trial as an example. A pilot study can be used to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and implementation of the novel intervention., A pilot study is not a hypothesis testing study. Safety, efficacy and effectiveness are not evaluated in a pilot. Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples. Feasibility results do not necessarily generalize beyond the inclusion and exclusion criteria of the pilot design., A pilot study is a requisite initial step in exploring a novel intervention or an innovative application of an intervention. Pilot results can inform feasibility and identify modifications needed in the design of a larger, ensuing hypothesis testing study. Investigators should be forthright in stating these objectives of a pilot study. Grant reviewers and other stakeholders should expect no more.},
  journal = {Journal of psychiatric research},
  number = {5},
  pmcid = {PMC3081994},
  pmid = {21035130}
}

@article{mahoney_publication_1977,
  title = {Publication Prejudices: {{An}} Experimental Study of Confirmatory Bias in the Peer Review System},
  shorttitle = {Publication Prejudices},
  author = {Mahoney, Michael J.},
  year = {1977},
  month = jun,
  volume = {1},
  pages = {161--175},
  issn = {1573-2819},
  doi = {10.1007/BF01173636},
  abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
  journal = {Cognitive Therapy and Research},
  keywords = {Clinical Research,Cognitive Psychology,Experimental Study,Review System,Theoretical Perspective},
  language = {en},
  number = {2}
}

@book{maxwell_designing_2017,
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}, {{Third Edition}}},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  month = aug,
  edition = {3 edition},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  abstract = {Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd edition) offers an integrative conceptual framework for understanding experimental design and data analysis. Maxwell, Delaney, and Kelley first apply fundamental principles to simple experimental designs followed by an application of the same principles to more complicated designs. Their integrative conceptual framework better prepares readers to understand the logic behind a general strategy of data analysis that is appropriate for a wide variety of designs, which allows for the introduction of more complex topics that are generally omitted from other books. Numerous pedagogical features further facilitate understanding: examples of published research demonstrate the applicability of each chapter's content; flowcharts assist in choosing the most appropriate procedure; end-of-chapter lists of important formulas highlight key ideas and assist readers in locating the initial presentation of equations; useful programming code and tips are provided throughout the book and in associated resources available online, and extensive sets of exercises help develop a deeper understanding of the subject. Detailed solutions for some of the exercises and realistic data sets are included on the website (DesigningExperiments.com). The pedagogical approach used throughout the book enables readers to gain an overview of experimental design, from conceptualization of the research question to analysis of the data. The book and its companion website with web apps, tutorials, and detailed code are ideal for students and researchers seeking the optimal way to design their studies and analyze the resulting data.},
  isbn = {978-1-138-89228-6},
  language = {English}
}

@book{mayo_statistical_2018,
  title = {Statistical Inference as Severe Testing: How to Get beyond the Statistics Wars},
  shorttitle = {Statistical Inference as Severe Testing},
  author = {Mayo, Deborah G.},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-05413-4},
  keywords = {Deviation (Mathematics),Error analysis (Mathematics),Fallacies (Logic),Inference,Mathematical statistics},
  language = {en},
  lccn = {QA276 .M3755 2018}
}

@article{meehl_appraising_1990,
  title = {Appraising and Amending Theories: {{The}} Strategy of {{Lakatosian}} Defense and Two Principles That Warrant It},
  shorttitle = {Appraising and Amending Theories},
  author = {Meehl, Paul E.},
  year = {1990},
  volume = {1},
  pages = {108--141},
  journal = {Psychological Inquiry},
  number = {2}
}

@article{milgram_maintaining_1978,
  title = {On Maintaining Urban Norms: {{A}} Field Experiment in the Subway},
  shorttitle = {On Maintaining Urban Norms},
  author = {Milgram, Stanley and Sabini, John},
  year = {1978},
  volume = {1},
  pages = {31--40},
  journal = {Advances in environmental psychology}
}

@article{mitroff_systemic_1974,
  title = {On Systemic Problem Solving and the Error of the Third Kind},
  author = {Mitroff, Ian I. and Featheringham, Tom R.},
  year = {1974},
  month = nov,
  volume = {19},
  pages = {383--393},
  issn = {00057940, 10991743},
  doi = {10/cjqj8h},
  journal = {Behavioral Science},
  language = {en},
  number = {6}
}

@article{neyman_problem_1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses.},
  author = {Neyman, Jerzy and Pearson, E. S.},
  year = {1933},
  month = jan,
  volume = {231},
  pages = {289--337},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {694-706}
}

@book{niiniluoto_critical_1999,
  title = {Critical {{Scientific Realism}}},
  author = {Niiniluoto, Ilkka},
  year = {1999},
  publisher = {{Oxford University Press}},
  abstract = {Ilkka Niiniluoto comes to the rescue of scientific realism, showing that reports of its death have been greatly exaggerated. Philosophical realism holds that the aim of a particular discourse is to make true statements about its subject-matter. Niiniluoto surveys the different varieties ofrealism in ontology, semantics, epistemology, theory construction, and methodology. He then sets out his own original version, and defends it against competing theories in the philosophy of science. Niiniluoto's critical scientific realism is founded upon the notion of truth as correspondencebetween language and reality, and characterizes scientific progress in terms of increasing truthlikeness. This makes it possible not only to take seriously, but also to make precise, the troublesome idea that scientific theories typically are false but nevertheless close to the truth.},
  googlebooks = {Ng\_p\_3XCHxAC},
  isbn = {978-0-19-823833-1},
  keywords = {Philosophy / Metaphysics,Science / Philosophy \& Social Aspects},
  language = {en}
}

@article{nowok_synthpop_2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  year = {2016},
  month = oct,
  volume = {74},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  copyright = {Copyright (c) 2016 Beata Nowok, Gillian M. Raab, Chris Dibben},
  journal = {Journal of Statistical Software},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies},
  language = {en},
  number = {1}
}

@article{nuijten_prevalence_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985\textendash 2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2015},
  month = oct,
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  journal = {Behavior Research Methods},
  language = {en}
}

@article{obels_analysis_2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  volume = {3},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{pickett_questionable_2017,
  title = {Questionable, {{Objectionable}} or {{Criminal}}? {{Public Opinion}} on {{Data Fraud}} and {{Selective Reporting}} in {{Science}}},
  shorttitle = {Questionable, {{Objectionable}} or {{Criminal}}?},
  author = {Pickett, Justin T. and Roche, Sean Patrick},
  year = {2017},
  month = mar,
  pages = {1--21},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-017-9886-2},
  abstract = {Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public's welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public's moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies\textemdash a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)\textemdash to explore community members' judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.},
  journal = {Science and Engineering Ethics},
  language = {en}
}

@book{popper_logic_2002,
  title = {{The logic of scientific discovery}},
  author = {Popper, Karl R},
  year = {2002},
  publisher = {{Routledge}},
  address = {{London; New York}},
  abstract = {When first published in 1959, this book revolutionized contemporary thinking about science and knowledge. It remains the one of the most widely read books about science to come out of the twentieth century.},
  isbn = {978-0-203-99462-7 978-0-415-27843-0 978-0-415-27844-7},
  language = {Translated from the German.}
}

@book{ravetz_scientific_1995,
  title = {Scientific {{Knowledge}} and {{Its Social Problems}}},
  author = {Ravetz, Jerome},
  year = {1995},
  month = jan,
  edition = {Reprint edition},
  publisher = {{Transaction Publishers}},
  address = {{New Brunswick, N.J}},
  abstract = {Science is continually confronted by new and difficult social and ethical problems. Some of these problems have arisen from the transformation of the academic science of the prewar period into the industrialized science of the present. Traditional theories of science are now widely recognized as obsolete. In Scientific Knowledge and Its Social Problems (originally published in 1971), Jerome R. Ravetz analyzes the work of science as the creation and investigation of problems. He demonstrates the role of choice and value judgment, and the inevitability of error, in scientific research. Ravetz's new introductory essay is a masterful statement of how our understanding of science has evolved over the last two decades.},
  isbn = {978-1-56000-851-4},
  language = {English}
}

@article{richard_one_2003,
  title = {One {{Hundred Years}} of {{Social Psychology Quantitatively Described}}.},
  author = {Richard, F. D. and Bond, Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  volume = {7},
  pages = {331--363},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/1089-2680.7.4.331},
  journal = {Review of General Psychology},
  language = {en},
  number = {4}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  volume = {16},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {2}
}

@article{schulz_sample_2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F. and Grimes, David A.},
  year = {2005},
  volume = {365},
  pages = {1348--1353},
  journal = {The Lancet},
  number = {9467}
}

@article{shmueli_explain_2010,
  ids = {shmueli\_explain\_2010-1},
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  volume = {25},
  pages = {289--310},
  journal = {Statistical science},
  number = {3}
}

@article{skipper_sacredness_1967,
  title = {The {{Sacredness}} of .05: {{A Note}} Concerning the {{Uses}} of {{Statistical Levels}} of {{Significance}} in {{Social Science}}},
  shorttitle = {The {{Sacredness}} of .05},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  year = {1967},
  volume = {2},
  pages = {16--18},
  issn = {0003-1232},
  journal = {The American Sociologist},
  number = {1}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  month = sep,
  volume = {3},
  pages = {160384},
  issn = {2054-5703},
  doi = {10.1098/rsos.160384},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash no deliberate cheating nor loafing\textemdash by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  copyright = {\textcopyright{} 2016 The Authors.. http://creativecommons.org/licenses/by/4.0/Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  journal = {Royal Society Open Science},
  language = {en},
  number = {9}
}

@article{steiger_beyond_2004,
  title = {Beyond the {{F Test}}: {{Effect Size Confidence Intervals}} and {{Tests}} of {{Close Fit}} in the {{Analysis}} of {{Variance}} and {{Contrast Analysis}}.},
  shorttitle = {Beyond the {{F Test}}},
  author = {Steiger, James H.},
  year = {2004},
  volume = {9},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  journal = {Psychological Methods},
  language = {en},
  number = {2}
}

@article{taylor_bias_1996,
  title = {Bias in Linear Model Power and Sample Size Calculation Due to Estimating Noncentrality},
  author = {Taylor, Douglas J. and Muller, Keith E.},
  year = {1996},
  volume = {25},
  pages = {1595--1610},
  journal = {Communications in Statistics-Theory and Methods},
  number = {7}
}

@article{ter_schure_accumulation_2019,
  title = {Accumulation {{Bias}} in {{Meta}}-{{Analysis}}: {{The Need}} to {{Consider Time}} in {{Error Control}}},
  shorttitle = {Accumulation {{Bias}} in {{Meta}}-{{Analysis}}},
  author = {{ter Schure}, Judith and Gr{\"u}nwald, Peter D.},
  year = {2019},
  month = may,
  abstract = {Studies accumulate over time and meta-analyses are mainly retrospective. These two characteristics introduce dependencies between the analysis time, at which a series of studies is up for meta-analysis, and results within the series. Dependencies introduce bias --- Accumulation Bias --- and invalidate the sampling distribution assumed for p-value tests, thus inflating type-I errors. But dependencies are also inevitable, since for science to accumulate efficiently, new research needs to be informed by past results. Here, we investigate various ways in which time influences error control in meta-analysis testing. We introduce an Accumulation Bias Framework that allows us to model a wide variety of practically occurring dependencies, including study series accumulation, meta-analysis timing, and approaches to multiple testing in living systematic reviews. The strength of this framework is that it shows how all dependencies affect p-value-based tests in a similar manner. This leads to two main conclusions. First, Accumulation Bias is inevitable, and even if it can be approximated and accounted for, no valid p-value tests can be constructed. Second, tests based on likelihood ratios withstand Accumulation Bias: they provide bounds on error probabilities that remain valid despite the bias. We leave the reader with a choice between two proposals to consider time in error control: either treat individual (primary) studies and meta-analyses as two separate worlds --- each with their own timing --- or integrate individual studies in the meta-analysis world. Taking up likelihood ratios in either approach allows for valid tests that relate well to the accumulating nature of scientific knowledge. Likelihood ratios can be interpreted as betting profits, earned in previous studies and invested in new ones, while the meta-analyst is allowed to cash out at any time and advise against future studies.},
  archivePrefix = {arXiv},
  eprint = {1905.13494},
  eprinttype = {arxiv},
  journal = {arXiv:1905.13494 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  primaryClass = {math, stat}
}

@book{tukey_exploratory_1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1977},
  edition = {1 edition},
  publisher = {{Pearson}},
  address = {{Reading, Mass}},
  abstract = {The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator.},
  isbn = {978-0-201-07616-5},
  language = {English}
}

@article{tukey_future_1962,
  ids = {tukey\_future\_1962-1},
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1962},
  month = mar,
  volume = {33},
  pages = {1--67},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177704711},
  abstract = {Project Euclid - mathematics and statistics online},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR133937},
  number = {1},
  zmnumber = {0107.36401}
}

@book{van_fraassen_scientific_1980,
  title = {The Scientific Image},
  author = {Van Fraassen, Bas C.},
  year = {1980},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford : New York}},
  isbn = {978-0-19-824424-0 978-0-19-824427-1},
  keywords = {Philosophy,Science},
  lccn = {Q175 .V335 1980},
  series = {Clarendon Library of Logic and Philosophy}
}

@article{wacholder_assessing_2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  volume = {96},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  journal = {JNCI Journal of the National Cancer Institute},
  language = {en},
  number = {6}
}

@article{wald_sequential_1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, Abraham},
  year = {1945},
  volume = {16},
  pages = {117--186},
  journal = {The Annals of Mathematical Statistics},
  number = {2}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} ``p {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  volume = {73},
  pages = {1--19},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913},
  journal = {The American Statistician},
  number = {sup1}
}

@article{wigboldus_encourage_2016,
  title = {Encourage {{Playing}} with {{Data}} and {{Discourage Questionable Reporting Practices}}},
  author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
  year = {2016},
  month = mar,
  volume = {81},
  pages = {27--32},
  issn = {1860-0980},
  doi = {10.1007/s11336-015-9445-1},
  journal = {Psychometrika},
  keywords = {Confirmatory Analysis,Data Analysis Phase,Data Analysis Plan,Reporting Practice,Research Practice},
  language = {en},
  number = {1}
}


