<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="12 Preregistration and Transparency | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-07-25" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>12 Preregistration and Transparency | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a href="power.html"><span class="toc-section-number">2</span> Power analysis.</a>
<a href="questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a>
<a href="errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a>
<a href="effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a>
<a href="equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a>
<a href="severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a>
<a href="sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a>
<a href="meta.html"><span class="toc-section-number">9</span> Meta-analysis</a>
<a href="bias.html"><span class="toc-section-number">10</span> Bias detection</a>
<a href="computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a>
<a id="active-page" href="prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a><ul class="toc-sections">
<li class="toc"><a href="#trust-in-scientists"> Trust in scientists</a></li>
<li class="toc"><a href="#the-value-of-preregistration"> The value of preregistration</a></li>
<li class="toc"><a href="#preregister-your-study"> Preregister your study?</a></li>
<li class="toc"><a href="#what-does-a-formalized-test-of-a-prediction-look-like"> What Does a Formalized Test of a Prediction Look Like?</a></li>
</ul>
<a href="bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a>
<a href="references.html"><span class="toc-section-number">14</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="prereg" class="section level1">
<h1>
<span class="header-section-number">12</span> Preregistration and Transparency</h1>
<div id="trust-in-scientists" class="section level2">
<h2>
<span class="header-section-number">12.1</span> Trust in scientists</h2>
<p>For as long as data has been used to support scientific claims people have tried to selectively present data in line with what they wish to be true. In his treatise ‘On the Decline of Science in England: And on Some of its Cases’ Babbage <span class="citation">Babbage (<label for="tufte-mn-37" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-37" class="margin-toggle">1830<span class="marginnote">Babbage, C. (1830). <em>Reflections on the Decline of Science in England: And on Some of Its Causes</em>. B. Fellowes.</span>)</span> discusses what he calls cooking:</p>
<div class="figure">
<span id="fig:babbage"></span>
<p class="caption marginnote shownote">
Figure 12.1: Excerpt from Babbage, 1830.
</p>
<img src="images/babbage.jpg" alt="Excerpt from Babbage, 1830." width="396">
</div>
<p>The practice Babbage describes is still a problem, almost 200 years later. Researchers still make ‘multitudes of observations’ only to present those that support the story they want to tell. An example of a scientist who does this is Daryl Bem, a parapsychologist who studies whether people have extra-sensory perception that allows them to predict the future. In Figure <a href="prereg.html#fig:bem">12.2</a> you see the results and discussion from a study he performed <span class="citation">(D. J. Bem, <label for="tufte-mn-38" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-38" class="margin-toggle">2011<span class="marginnote">Bem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. <em>Journal of Personality and Social Psychology</em>, <em>100</em>(3), 407–425. <a href="https://doi.org/10.1037/a0021524">https://doi.org/10.1037/a0021524</a></span>)</span>. In this study participants pressed a left or right button to predict whether a picture was hidden behind a left or right curtain. At the moment they made the decision, not even the computer had randomly determined where this picture would appear, so any performence better than average would be very surprising.</p>
<div class="figure">
<span id="fig:bem"></span>
<p class="caption marginnote shownote">
Figure 12.2: Screenshot from the Results and Discussion section of Bem, 2011.
</p>
<img src="images/bem.png" alt="Screenshot from the Results and Discussion section of Bem, 2011." width="622">
</div>
<p>If we take this study as it is (without pre-registration) it is clear there are 5 tests against guessing average (for erotic, neutral, negative, positive, and ‘romantic but non-erotic’ pictures). A Bonferroni correction would lead us to use an alpha level of 0.01 (an alpha of 0.05/5 tests) and the main result, that participants guessed the future position of erotic pictures above guessing average, with a <em>p</em>-value of 0.013, would not have allowed Bem to reject the null-hypothesis, given the pre-specified alpha level.</p>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<p>
Which of the five categories (erotic, neutral, negative, positive, and romantic but non-erotic pictures) would you have predicted that people would perform better than guessing average at, if we had evolved the ability to predict the future? Do you think Bem predicted an effect for the erotic pictures? If you don’t trust Bem predicted this specific effect, why do you think others would trust you in a similar situation?
</p>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:bemmeta"></span>
<img src="images/bemmeta.png" alt="Screenshot from the Results and Discussion section of Bem, 2011." width="617"><!--
<p class="caption marginnote">-->Figure 12.3: Screenshot from the Results and Discussion section of Bem, 2011.<!--</p>-->
<!--</div>--></span>
</p>
<p>Bem <span class="citation">D. J. Bem et al. (<label for="tufte-mn-39" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-39" class="margin-toggle">2011<span class="marginnote">Bem, D. J., Utts, J., &amp; Johnson, W. O. (2011). Must psychologists change the way they analyze their data? <em>Journal of Personality and Social Psychology</em>, <em>101</em>(4), 716–719. <a href="https://doi.org/10.1037/a0024777">https://doi.org/10.1037/a0024777</a></span>)</span> explicitly says he predicted this specific test. I personally think Bem isn’t telling the truth. I know this is a harsh thing to say, but in a meta-analysis of precognition studies <span class="citation">(Bem et al., <label for="tufte-mn-40" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-40" class="margin-toggle">2015<span class="marginnote">Bem, D., Tressoldi, P., Rabeyron, T., &amp; Duggan, M. (2015). Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. <em>F1000Research</em>. <a href="https://doi.org/10.12688/f1000research.7177.1">https://doi.org/10.12688/f1000research.7177.1</a></span>)</span> the only studies Bem shares are all significant or marginally significant, and all confidence intervals in Figure <a href="prereg.html#fig:bemmeta">12.3</a> show a pattern you will recognize from the chapter on <a href="bias.html#bias">publication bias</a>. In other words, even when he could have emptied his filedrawer in his own meta-analyses, he wasn’t honest. Furthermore, in this <a href="https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html">interview</a> he also admitted that “I used data as a point of persuasion, and I never really worried about, ‘Will this replicate or will this not?” (see also <a href="https://skepticalinquirer.org/2019/11/another-scandal-for-psychology-daryl-bems-data-massage/">this post</a> by Susan Blackmore).</p>
<p>I see absolutely no reason to believe Bem without a preregistration document for the study he performed (or for any studies he will perform in the future, for that matter). It would be great if we could trust all scientists, but cases like Daryl Bem show that we can’t. As Vazire <span class="citation">Vazire (<label for="tufte-mn-41" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-41" class="margin-toggle">2017<span class="marginnote">Vazire, S. (2017). Quality Uncertainty Erodes Trust in Science. <em>Collabra: Psychology</em>, <em>3</em>(1), 1. <a href="https://doi.org/10.1525/collabra.74">https://doi.org/10.1525/collabra.74</a></span>)</span> writes: “Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars – they cannot reliably tell the difference between lemons and high quality findings.” So being transparent is a good way to allow others to evaluate the quality of your findings.</p>
</div>
<div id="the-value-of-preregistration" class="section level2">
<h2>
<span class="header-section-number">12.2</span> The value of preregistration</h2>
<p>In the past researchers have proposed solutions to prevent bias in the literature due to inflated Type 1 error rates as a result of selective reporting. For example, Bakan (1966) discussed the problematic aspects of choosing whether or not to perform a directional hypothesis test after looking at the data. If a researcher chooses to perform a directional hypothesis test only when the two-sided hypothesis test yields a <em>p</em>-value between 0.05 and 0.10 (i.e., when a test yields <em>p</em> = 0.08, the researcher decides after seeing the result that a one-sided test was also warranted, and reports the <em>p</em>-value as 0.04, one-sided) then in practice the Type 1 error rate is doubled (i.e., is 0.10 instead of 0.05). Bakan (p. 431) writes: “How should this be handled? Should there be some central registry in which one registers one’s decision to run a one- or two-tailed test before collecting the data? Should one, as one eminent psychologist once suggested to me, send oneself a letter so that the postmark would prove that one had pre-decided to run a one-tailed test?”</p>
<p>With the rise of the internet it has become feasible to create online registries that allow researchers to specify their research design, data collection, and the planned analyses before the data is collected. This makes it possible to see which predictions are confirmed, based on statistical analyses that are not influences by looking at the results they give. Instead of choosing which of 5 dependent variables yield a significant result, and than writing an introduction that pretends to have predicted this effect, a practice known as HARKing <span class="citation">(Kerr, <label for="tufte-mn-42" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-42" class="margin-toggle">1998<span class="marginnote">Kerr, N. L. (1998). HARKing: Hypothesizing After the Results are Known. <em>Personality and Social Psychology Review</em>, <em>2</em>(3), 196–217. <a href="https://doi.org/10.1207/s15327957pspr0203_4">https://doi.org/10.1207/s15327957pspr0203_4</a></span>)</span>, the reported analyses actually test the predictions researchers had before they looked at the data.</p>
<p>Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction. When effect size estimates are biased, for example due to the desire to obtain a <em>significant</em> result, hypothesis tests performed on these estimates have inflated Type 1 error rates. When bias emerges due to the desire to obtain a <em>non-significant</em> test result hypothesis tests have reduced statistical power. The goal of preregistration is not simply to control the Type 1 error rate in hypothesis tests, but to prevent researchers from non-transparently reducing the capacity of the test to falsify a prediction in general.</p>
<p>Preregistration adds value for people who, based on their philosophy of science, increase their trust in claims that are supported by severe tests and predictive successes. Preregistration itself does not make a study better or worse compared to a non-preregistered study. Instead, it merely allows researchers to transparently evaluate the <a href="severity.html#severity">severity</a> of a test <span class="citation">(Lakens, <label for="tufte-mn-43" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-43" class="margin-toggle">2020<span class="marginnote">Lakens, D. (2020). The Value of Preregistration for Psychological Science: A Conceptual Analysis. <em>Japanese Psychological Review</em>. <a href="https://doi.org/10.31234/osf.io/jbh4w">https://doi.org/10.31234/osf.io/jbh4w</a></span>)</span>. The severity of a test is in theory unrelated to whether it is preregistered. However, in practice there will almost always be a correlation between the ability to transparently evaluate the severity of a test and preregistration, both because researchers can often selectively report results, use optional stopping, or come up with a plausible hypothesis after the results are known, and because theories rarely completely constrain the test of predictions.</p>
<p>Preregistration is a tool, and researchers who use it should do so because they have a goal that preregistration facilitates. If the use of a tool is detached from a philosophy of science it risks becoming a heuristic. Researchers should not choose to preregister because it has become a new norm, but they should preregister because they can justify based on their philosophy of science how preregistration supports their goals. There are many types of research for which preregistration is not necessary. Although it is always good to be as transparent as possible when doing research, from a philosophy of science perspective, the unique value of preregistration is limited to research which aims to severely test predictions.</p>
</div>
<div id="preregister-your-study" class="section level2">
<h2>
<span class="header-section-number">12.3</span> Preregister your study?</h2>
<p>If the previous sections have convinced you that, for at least some of the hypothesis testing studies you perform, it is useful to preregister your research, logical questions are: <em>How</em> and <em>Where</em>?</p>
<div id="how-to-preregister" class="section level3">
<h3>
<span class="header-section-number">12.3.1</span> How to preregister</h3>
<p>The more detail a preregistration document has, the easier it is for others to transparently evaluate the severity of the tests that are performed. Because it is difficult to come up with all aspects that one should include, researchers have created websites to guide researchers through this process (e.g., <a href="https://aspredicted.org/" class="uri">https://aspredicted.org/</a>), submission guidelines, as well as templates <span class="citation">(van ’t Veer &amp; Giner-Sorolla, <label for="tufte-mn-44" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-44" class="margin-toggle">2016<span class="marginnote">van ’t Veer, A. E., &amp; Giner-Sorolla, R. (2016). Pre-registration in social psychologyA discussion and suggested template. <em>Journal of Experimental Social Psychology</em>, <em>67</em>, 2–12. <a href="https://doi.org/10.1016/j.jesp.2016.03.004">https://doi.org/10.1016/j.jesp.2016.03.004</a></span>)</span>. The template by Van ’t Veer and Giner-Sorolla is an excellent start, and is intended to be less ambitious than the Journal Article Reporting Standards (JARS) developed by the American Psychological Association. However, I like being a bit ambitious, and I think the 2018 update of JARS <span class="citation">(Appelbaum et al., <label for="tufte-mn-45" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-45" class="margin-toggle">2018<span class="marginnote">Appelbaum, M., Cooper, H., Kline, R. B., Mayo-Wilson, E., Nezu, A. M., &amp; Rao, S. M. (2018). Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. <em>American Psychologist</em>, <em>73</em>(1), 3. <a href="https://doi.org/10.1037/amp0000191">https://doi.org/10.1037/amp0000191</a></span>)</span> should be more widely used. The reporting guidelines encompass more suggestions than needed for a preregistration document, but I would recommend using JARS both for your preregistration document, as when writing up the final report.</p>
<p>The Journal Article Reporting Standards inform you about information that needs to be present on the title page (such as an Author Note, that includes “Registration information if the study has been registered”), the abstract, the introduction, the method, results, and discussion. The method and result sections receive a lot of attention, and these two sections are also the most important in a preregistration if we want to allow others to evaluate the severity with which we tested hypotheses. Remember that a severe test has a high probability of finding a predicted effect if the prediction is correct, and a high likelihood of not finding a predicted effect if the prediction is incorrect. Practices that inflate the Type 1 error rate increase the possibility of finding a predicted effect if a prediction is actually wrong. Low power, unreliable measures, a flawed procedure, or a bad design increase the possibility of not finding an effect when the prediction was actually correct. Incorrect analyses risk answering a question that is unrelated to the prediction researchers set out to test (sometimes referred to as a <a href="https://en.wikipedia.org/wiki/Type_III_error#Kimball">Type 3 error</a>). As we see, JARS aims to address these threats to the severity of a test by asking authors to specify a wide range of aspects in their methods and results.</p>
<p>I will highlight those aspects that should be included in a preregistration document. However, I want to recommend reading through all information that is recommended to include in manuscripts. As you will see, the requirements to report validity evidence for instruments (or admit that you are using an ad-hoc measure with unknown validity), awareness about the conditions participants were assigned to, and the reliability of measures might all be easier if you have already collected information about this previously (or if this information is available in the literature). I will focus on quantitative experimental studies with random assignment to conditions below, but JARS includes tables for experiments without randomization, clinical trials, longitudinal designs, and replication studies.</p>
<ol style="list-style-type: decimal">
<li><p><em>Describe the unit of randomization and the procedure used to generate the random assignment sequence, including details of any restriction (e.g., blocking, stratification).</em></p></li>
<li><p><em>Report inclusion and exclusion criteria, including any restrictions based on demographic characteristics.</em></p></li>
</ol>
<p>This prevents flexibility concerning the participants that will be included in the final analysis.</p>
<ol start="3" style="list-style-type: decimal">
<li>
<em>Describe procedures for selecting participants, including</em>
<ul>
<li><em>Sampling method if a systematic sampling plan was implemented</em></li>
<li><em>Percentage of sample approached that actually participated</em></li>
</ul>
</li>
</ol>
<p>You might often not know which percentage you approach will participate, which might require some pilot data, as you might not be able to reach the desired final sample size (see below) with the sampling plan.</p>
<ol start="4" style="list-style-type: decimal">
<li>
<em>Describe the sample size, power, and precision, including</em>
<ul>
<li><em>Intended sample size</em></li>
<li>
<em>Determination of sample size, including</em>
<ul>
<li><em>Power analysis, or methods used to determine precision of parameter estimates</em></li>
<li><em>Explanation of any interim analyses and stopping rules employed</em></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>Clearly stating the intended sample size prevents practices such as optional stopping, which inflate the Type 1 error rate. Be aware (or if not, JARS Will remind you) that you might end up with an achieved sample size that differs from the intended sample size, and consider possible reasons why you might not manage to collect the intended sample size. We discussed sample size justifications in the chapter on <a href="power.html#power">power</a>. A sample size needs to be justified, as do the assumptions in a power analysis (e.g., is the expected effect size realistic, or is the <a href="sesoi.html#sesoi">smallest effect size of interest</a> indeed of interest to others?). If you used <a href="errorcontrol.html#sequential">sequential analyses</a>, specify how you controlled the Type 1 error rate while analyzing the data repeatedly as it came in.</p>
<ol start="5" style="list-style-type: decimal">
<li>
<em>Describe planned data diagnostics, including</em>
<ul>
<li><em>Criteria for post-data collection exclusion of participants, if any</em></li>
<li><em>Criteria for deciding when to infer missing data and methods used for imputation of missing data</em></li>
<li><em>Defining and processing of statistical outliers</em></li>
<li><em>Analyses of data distributions</em></li>
<li><em>Data transformations to be used, if any</em></li>
</ul>
</li>
</ol>
<p>After collecting the data, the first step is to examine the data quality, and test assumptions for the planned analytic methods. It is common to exclude data from participants who did not follow instructions, and these decision procedures should be prespecified. Each preregistration you will discover additional unforeseen consequences that will be added to these sections. If data is missing, you might not want to remove a participant entirely, but use a method to impute missing data. Because outliers can have an undue influence on the results, you might want to preregister ways to mitigate the impact of outliers. For practical recommendations on how to classify, detect, and manage outliers, see <span class="citation">(Leys et al., <label for="tufte-mn-46" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-46" class="margin-toggle">2019<span class="marginnote">Leys, C., Delacre, M., Mora, Y. L., Lakens, D., &amp; Ley, C. (2019). How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration. <em>International Review of Social Psychology</em>, <em>32</em>(1), 5. <a href="https://doi.org/10.5334/irsp.289">https://doi.org/10.5334/irsp.289</a></span>)</span>. If you are planning to perform statistical tests that have assumptions (e.g., the assumption of normality for Welch’s <em>t</em>-test) you need to preregister how you will decide whether these assumptions are met, and if not, what you will do.</p>
<ol start="6" style="list-style-type: decimal">
<li>
<em>Describe the analytic strategy for inferential statistics and protection against experiment-wise error for</em>
<ul>
<li><em>Primary hypotheses</em></li>
<li><em>Secondary hypotheses</em></li>
<li><em>Exploratory hypotheses</em></li>
</ul>
</li>
</ol>
<p>The difference between these three levels of hypotheses is not adequately explained in Appelbaum et al., <span class="citation">Appelbaum et al. (<label for="tufte-mn-47" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-47" class="margin-toggle">2018<span class="marginnote">Appelbaum, M., Cooper, H., Kline, R. B., Mayo-Wilson, E., Nezu, A. M., &amp; Rao, S. M. (2018). Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. <em>American Psychologist</em>, <em>73</em>(1), 3. <a href="https://doi.org/10.1037/amp0000191">https://doi.org/10.1037/amp0000191</a></span>)</span> but Cooper <span class="citation">Cooper (<label for="tufte-mn-48" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-48" class="margin-toggle">2020<span class="marginnote">Cooper, H. (2020). <em>Reporting quantitative research in psychology: How to meet APA Style Journal Article Reporting Standards (2nd ed.).</em> American Psychological Association. <a href="https://doi.org/10.1037/0000178-000">https://doi.org/10.1037/0000178-000</a></span>)</span> explains the distinction a <em>bit</em> more, but it remains quite vague. The way I would distinguish these three categories is as follows. First, a study is designed to answer a <strong>primary hypothesis</strong>. The Type 1 and Type 2 error rates for this primary hypothesis are as low as the researcher can afford to make them. <strong>Secondary hypotheses</strong> are questions that a researchers considers interesting when planing the study, but that are not the main goal of the study. A secondary hypotheses might concern additional variables that are collected, or even sub-group analyses that are deemed interesting from the outset. For these hypotheses, the Type 1 error rate is still controlled at a level the researchers considers justifiable. However, the Type 2 error rate is not controlled for secondary analyses. The effect that is expected on additional variables might be much smaller than the effect for the primary hypothesis, or analyses on subgroups will have smaller sample sizes. Finally,there is a left-over category of analyses that are performed in an article. I would refer to this category as <strong>exploratory results</strong>, not exploratory hypotheses, because a researcher might not have hypothesized these analyses at all, but comes up with these tests during data analysis. JARS requires researchers to report such results ‘in terms of both substantive findings and error rates that may be uncontrolled’. An exploratory result might be deemed impressive by readers, or not, depending on their prior belief, but it has not been severely tested. All findings need to be independently replicated if we want to be able to build on them - but all else equal, this requirement is more immenent for exploratory results.</p>
</div>
</div>
<div id="what-does-a-formalized-test-of-a-prediction-look-like" class="section level2">
<h2>
<span class="header-section-number">12.4</span> What Does a Formalized Test of a Prediction Look Like?</h2>
<p>A hypothesis test is a methodological procedure to evaluate a prediction that can be described on a conceptual level (e.g., people exhibit higher levels of prosocial behavior towards those who physically resemble them), an operationalized level (e.g., people playing a trust game make more trusting decisions when the person they play against is a self morph versus an other morph), and a statistical level. In a preregistration, the hypothesis should be specified in detail at this statistical level, and each statistical hypothesis should be clearly linked to the conceptual and operationalized level.</p>
<p>Let’s start by identifying the individual components that make it possible to evaluate a hypothesis test. A <em>hypothesis</em> is tested in an <em>analysis</em> that takes <em>data</em> as input and returns test <em>results</em>. Some of these tests results will be compared to <em>criteria</em>, used in the <em>evaluation</em> of the test result.XXXX</p>
<p>For example, imagine a hypothesis predicts a certain difference in means between conditions. The data is analyzed with a Bayesian <em>t</em>-test <span class="citation">(<span class="citeproc-not-found" data-reference-id="dienes2019"><strong>???</strong></span>)</span>, and if the resulting Bayes factor is greater than some specified criterion (e.g., 6, used by the journal <em>Cortex</em>), the prediction is considered corroborated. If the Bayes factor is less than the reciprocal of the criterion (e.g., 1/6), this is interpreted as evidence for the null model, and the prediction is falsified. All other values are interpreted as inconclusive evidence.</p>
<p>When we evaluate the result of a statistical prediction, we need to perform a statistical test, retrieve the test result, and compare the test result to one or more criterion values. For example, our statistical prediction might be that we will observe a positive difference in the means between two measurements, which will be examined in a dependent <em>t</em>-test, from which we will determine the lower and upper 97.5% confidence interval around the mean difference, which we will compare against a value of 0. Statistical hypotheses are probabilistic, and probabilistic hypotheses can be made falsifiable “by specifying certain rejection rules which may render statistically interpreted evidence ‘inconsistent’ with the probabilistic theory” <span class="citation">(<span class="citeproc-not-found" data-reference-id="lakatos1978"><strong>???</strong></span>)</span>. A hypothesis test thus requires researchers to specify when the observed results of a statistical test will lead them to act as if their prediction is consistent with the data, inconsistent with the data, or inconclusive <span class="citation">(<span class="citeproc-not-found" data-reference-id="neyman1933"><strong>???</strong></span>)</span>.</p>
<p>As highlighted above, one limitation of current practice when testing hypotheses is that researchers often do not explicitly state what would corroborate or falsify their prediction. To be able to unambiguously evaluate a hypothesis, researchers need to specify the rules they will use to evaluate whether statistical results corroborate a prediction, falsify it, or when the results are inconclusive. For example, in a 2x2 design, many different patterns of means across the four cells could be predicted (e.g., one of two main effects, or a specific pattern of the observed interaction effect), but the full pattern of possible results that would corroborate or falsify a prediction is seldom made explicit.</p>
<p>An important requirement to make statistical hypothesis machine readable is to identify the individual components that make it possible to evaluate a hypothesis test. Our example relies on a <em>hypothesis</em> that is tested in an <em>analysis</em> that takes <em>data</em> as input and returns test <em>results</em>. Some of these tests results will be compared to <em>criteria</em>, used in the <em>evaluation</em> of the test result. For example, imagine a hypothesis predicts a certain difference in means between conditions. The data is analyzed with a Bayesian <em>t</em>-test <span class="citation">(<span class="citeproc-not-found" data-reference-id="dienes2019"><strong>???</strong></span>)</span>, and if the resulting Bayes factor is greater than some specified criterion (e.g., 6, used by the journal <em>Cortex</em>), the prediction is considered corroborated. If the Bayes factor is less than the reciprocal of the criterion (e.g., 1/6), this is interpreted as evidence for the null model, and the prediction is falsified. All other values are interpreted as inconclusive evidence.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="computationalreproducibility.html"><button class="btn btn-default">Previous</button></a>
<a href="bayes.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-25
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-07-25 22:44:16
</span>
</p>
</div>


</body>
</html>
