--- 
knit: "bookdown::preview_chapter"
---

```{r, include = FALSE}
library(magrittr)
library(kableExtra)
library(tidyverse)
library(pwr)
library(Superpower)
library(ggplot2)
library(patchwork)
library(MASS)
library(viridis)

knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE)
```

# What is power analysis? {-}

Statistical power is the probability of a test to yield a statistically significant result [@aberson_applied_2019; @cohen_statistical_1988]. Power depends on the Type 1 error rate (α), the true effect size in the population, and the number of observations. Because the true effect size is typically unknown, it makes most sense to speak about the power function. In the figure below, you see a power curve for an independent *t*-test, with an alpha level of 0.05. We see that as the effect size (in Cohen's d) increases, power increases. If the effect you study has an effect size of 0.5, you would have almost 70% power with 50 observations in each independent group (indicated by the red dot). If the true effect size is smaller, power is lower, and as the effect size is larger, power is larger. 

```{r, power-1, fig.margin=TRUE, echo=FALSE, fig.cap="Power curve for an independent *t*-test as a function of the true effect size."}
N <- 50
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
plot_power_d <- (function(d, N, p_upper) {
  ncp <- d * (N * N / (N + N))^0.5 # formula to calculate t from d from Dunlap, Cortina, Vaslow, & Burke, 1996, Appendix B
  t <- qt(1 - (p_upper / 2), df = (N * 2) - 2)
  1 - (pt(t, df = N * 2 - 2, ncp = ncp) - pt(-t, df = N * 2 - 2, ncp = ncp))
})
par(bg = "aliceblue")
plot(-10,
  xlab = expression(paste("Cohen's ", delta)), ylab = "Power", axes = FALSE,
  main = substitute(paste("Power for independent t-test with N = 50 per group")), xlim = c(0, 2), ylim = c(0, 1)
)
abline(v = seq(0, 2, 0.2), h = seq(0, 1, 0.1), col = "lightgray", lty = 1)
axis(side = 1, at = seq(0, 2, 0.2), labels = seq(0, 2, 0.2))
axis(side = 2, at = seq(0, 1, 0.2), labels = seq(0, 1, 0.2))
curve(plot_power_d(d = x, N = N, p_upper = p_upper), 0, 2, type = "l", lty = 1, lwd = 2, ylim = c(0, 1), xlim = c(0, 2), add = TRUE)
points(x = d, y = (1 + pt(qt(p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp) - pt(qt(1 - p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp)), cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5))

```

When designing a study where the goal is to observe a statistically significant effect, researchers often want to make sure they have enough power to detect effects they expect or are interested in observing. This is done by performing an *a-priori* power analysis. The true effect size is what it is, so if researchers want to increase the statistical power of their test, they either have to increase the alpha level, or increase the number of observations. In the figure below you see how the statistical power increases as the number of observations (per group) in an independent *t*-test with an alpha level of 0.05 increases.  

```{r, power-2, fig.margin=TRUE, echo=FALSE, fig.cap="Power curve for an independent *t*-test as a function of the sample size."}
N <- 50
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
plot_power_custom <- (function(d, N, p_upper) {
  ncp <- d * (N * N / (N + N))^0.5 # formula to calculate t from d from Dunlap, Cortina, Vaslow, & Burke, 1996, Appendix B
  t <- qt(1 - (p_upper / 2), df = (N * 2) - 2)
  1 - (pt(t, df = N * 2 - 2, ncp = ncp) - pt(-t, df = N * 2 - 2, ncp = ncp))
})
par(bg = "aliceblue")
plot(-10,
  xlab = "sample size (per group)", ylab = "Power", axes = FALSE,
  main = expression(paste("Power for independent t-test and ", delta, " = ", 0.5)), xlim = c(0, N * 2), ylim = c(0, 1)
)
abline(v = seq(0, N * 2, (2 * N) / 10), h = seq(0, 1, 0.1), col = "lightgray", lty = 1)
axis(side = 1, at = seq(0, 2 * N, (2 * N) / 10), labels = seq(0, 2 * N, (2 * N) / 10))
axis(side = 2, at = seq(0, 1, 0.2), labels = seq(0, 1, 0.2))
curve(plot_power_custom(d=d, N=x, p_upper=p_upper), 3, 2*N, type="l", lty=1, lwd=2, ylim=c(0,1), xlim=c(0,N), add=TRUE)
points(x = N, y = (1 + pt(qt(p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp) - pt(qt(1 - p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp)), cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5))

```

**Types of power analysis**

In an a-priori power analysis one uses the effect size, desired power, and alpha level to compute the required sample size. If we plan to perform a two-sided *t*-test, want to have 90% power for an effect size of d = 0.5, given an alpha level of 0.01, we will need a sample size of 121 in each independent group. 

```{r, echo = TRUE}
power.t.test(delta = 0.5, 
             sig.level = 0.01,
             power = 0.9,
             type = "two.sample",
             alternative = "two.sided")
```

Sometimes the sample size you can collect is fixed. In that case you can perform a sensitivity power analysis to examine the effect sizes you can detect with a desired power. If we plan to perform a two-sided *t*-test, and can collect at most 70 observations in each independent group, and we want to have 90% power, given an alpha level of 0.01, we only have 90% power for effects of d = 0.66. We could still get lucky find a significant effect if the population effect size is smaller than 0.66, but we would no longer control our Type 2 error rate (1-power) at 10%. 

```{r, echo = TRUE}
power.t.test(n = 70, 
             sig.level = 0.01,
             power = 0.9,
             type = "two.sample",
             alternative = "two.sided")
```

In a criterion power analysis one computes the alpha level one should choose to achieve a desired power, given a sample size and an expectation of the population effect size. If we plan to perform a two-sided *t*-test, and can collect at most 70 observations in each independent group, and we want to have 90% power, and expect a population effect size of 0.5, we would need to set the alpha level to 0.0966. We can use a lower alpha level, but then our power would be smaller. If we assume we can not increase our number of observations beyond 70 per group, we need to make a trade-off between increasing our Type 1 error rate, or our Type 2 error rate. As Neyman and Pearson @neyman_problem_1933 write: "The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator."

```{r, echo = TRUE}
power.t.test(n = 70, 
             delta = 0.5,
             sig.level = NULL,
             power = 0.9,
             type = "two.sample",
             alternative = "two.sided")

```

Note that power calculations are performed under the assumption that there is an effect. In practice, it is of course also possible that there is no effect, or d = 0. In that case, regardless of the sample size, you will observe significance results at your chosen alpha level. 

Therefore, probability of observing a significant or non-significant result in a study depends on the Type 1 error rate (α), the statistical power of the test (1-β), and the probability that the null-hypothesis is true. A study might examine a true effect, which means the alternative hypothesis (H1) is true (e.g., a correlation that differs from zero) or it might examine a null effect, which means the null-hypothesis (H0) is true (e.g., a correlation that is zero). When performing a statistical test on data, the test result might be statistically significant at a specified alpha level (p < α) or not. Thus, there are four possible outcomes of a study:

1. False positives or Type 1 errors (you observe a significant test result when H0 is true)
2. False negatives or Type 2 errors (you observe a non-significant result when H1 is true)
3. True negatives (a non-significant result when H0 is true) 
4. True positives (a significant test result when H1 is true)

```{r NHST-outcomes, fig.margin=TRUE, echo=FALSE, fig.cap="Four possible outcomes in a null hypothesis significance test."}
knitr::include_graphics("images/2.1.1.png")
```

The goal of an a-priori power analysis is to increase the sample size up to the level that the desired power is achieved for an effect size one is interested in detecting, but one should always consider the possibility that the effect size is 0. In practice, one can design a study assuming both the presence and the absence of an effect by performing an a-priori power analysis for a null-hypothesis significance test, assuming there is an effect, and an a-priori power analysis for an equivalence test, assuming the true effect size is 0. We will demonstrate some practical examples in the next sections. 

# How to justify your sample size based on a power analysis. {-}

One challenge in power analysis is that you never know the true effect size. This leads to the ‘sample size samba’ [@schulz_sample_2005]. Researchers go back and forth between the effect size they expect, and the sample size they are willing to collect, until they 'expect' the effect size that, in an a-priori power analysis, leads to the sample size they are willing to collect. This practice obviously makes a power analysis a useless procedure. 

One might be tempted to perform a small power analysis to estimate the effect size, and use this effect size estimate in an a-priori power analysis. Regrettably, this is not a recommended solution. First, effect size estimates from small pilot studies are highly uncertain , which means the estimate can easily be much smaller or larger than the population effect size (which can be seen by the width of the confidence interval around the effect size estimate). Second, such a procedure inevitably leads to bias, because you will only perform studies when the pilot provided an effect size estimates that, when entered in an a-priori power analysis, yielded a sample size that was feasible to collect. Since you only follow up on pilot studies when the effect size estimate is sufficiently large, this inevitably leads to 'follow-up bias' 

What can be done? The recommended best practice is to not enter the effect size you expect, but the smallest effect size you would still be interested in. By doing this, you will be able to design a study that has sufficient power for the smallest effect you find worthwhile to observe, even when you do not know what the true effect size is. By determining the required sample size for the smallest effect size of interest, you can guarantee you have designed an informative study. However, in case the true effect size is larger than your smallest effect size of interest, you might collect many more participants than required. This can be solved by performing sequential analyses, where you analyze the data intermittently, while controlling the Type 1 error rate for multiple looks at the data. We will explain sequential analyses later.


# Why Within-Subject Designs Typically Require Fewer Participants than Between-Subject Designs {-}

 One widely recommended approach to increase power is using a within subject design. Indeed, you need fewer participants to detect a mean difference between two conditions in a within-subjects design (in a dependent t-test) than in a between-subjects design (in an independent t-test). The reason is straightforward, but not always explained, and even less often expressed in the easy equation below. The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell & Delaney, 2004, p. 561, formula 45): 

NW = NB (1-ρ)/2

The “/2” part of the equation is due to the fact that in a two-condition within design every participant provides two data-points. The extent to which this reduces the sample size compared to a between-subject design depends on the correlation between the two dependent variables, as indicated by the (1-ρ) part of the equation. If the correlation is 0, a within-subject design simply needs half as many participants as a between-subject design (e.g., 64 instead 128 participants). The higher the correlation, the larger the relative benefit of within designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Note than when the correlation is -1, you need 128 participants in a within-design and 128 participants in a between-design, but in a within-design you will need to collect two measurements from each participant, making a within design more work than a between-design. However, negative correlations between dependent variables in psychology are rare, and perfectly negative correlations will probably never occur.

So what does the correlation do so that it increases the power of within designs, or reduces the number of participants you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots.

We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur). 

```{r, plot-1, fig.margin=TRUE, echo=FALSE, fig.cap="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15."}
set.seed(419)
#Set color palette
cbbPalette<-c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

n<-10000  #set the sample size for each group
mx<-100  #set the mean in group 1
sdx<-15  #set the standard deviation in group 1
my<-106  #set the mean in group 2
sdy<-15  #set the standard deviation in group 2
cor.true <- 0.0 #set true correlation
#randomly draw data
cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T)
mu <- c(0,0)
mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE)
x<-mat[,1]*sdx+mx
y<-mat[,2]*sdy+my
dif<-x-y
dataset<-data.frame(x,y)
DV<-c(x,y) #combine the two samples into a single variable
IV<-as.factor(c(rep("1", n), rep("2", n))) #create the independent variable (1 and 2) 
datasetplot<-data.frame(IV,DV) #create a dataframe (to make the plot)

#plot graph two groups
p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV)))  + 
  geom_histogram(alpha=0.4, binwidth=2, position="identity", colour="black", aes(y = ..density..)) +
  scale_fill_manual(values=cbbPalette, name = "Condition") +
  stat_function(fun=dnorm, args=c(mean=mx,sd=sdx), size=1, color="#E69F00", lty=2) +
  stat_function(fun=dnorm, args=c(mean=my,sd=sdy), size=1, color="#56B4E9", lty=2) +
  xlab("IQ") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(x), colour="black", linetype="dashed", size=1) + 
  geom_vline(xintercept=mean(y), colour="black", linetype="dashed", size=1) + 
  coord_cartesian(xlim=c(50,150)) + scale_x_continuous(breaks=c(50,60,70,80,90,100,110,120,130,140,150)) +
  annotate("text", x = 70, y = 0.02, label = paste("Mean X = ",round(mean(x)),"\n","SD = ",round(sd(x)),sep="")) +
  annotate("text", x = 130, y = 0.02, label = paste("Mean Y = ",round(mean(y)),"\n","SD = ",round(sd(y)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))


#plot data differences
p2 <- ggplot(as.data.frame(dif), aes(dif))  + 
  geom_histogram(colour="black", fill="grey", aes(y=..density..), binwidth=2) +
  #  geom_density(fill=NA, colour="black", size = 1) +
  xlab("IQ dif") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(dif), colour="gray20", linetype="dashed") + 
  coord_cartesian(xlim=c(-80,80)) + scale_x_continuous(breaks=c(seq(-80, 80, 10))) +
  annotate("text", x = mean(dif), y = 0.01, label = paste("Mean = ",round(mean(dif)),"\n","SD = ",round(sd(dif)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))

#Plot correlation
p3 <- ggplot(dataset, aes(x=x, y=y)) +
  geom_point(size=2) +    # Use hollow circles
  geom_smooth(method=lm, colour="#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line
  coord_cartesian(xlim=c(40,160), ylim=c(40,160)) +
  scale_x_continuous(breaks=c(seq(40, 160, 20))) + scale_y_continuous(breaks=c(seq(40, 160, 20))) +
  xlab("IQ twin 1") + ylab("IQ twin 2")  + ggtitle(paste("Correlation = ",round(cor(x,y),digits=2),sep="")) + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  coord_fixed(ratio = 1)  +
  theme(plot.title = element_text(hjust = 0.5))


p1
```
In the scatter plot, we can see that the correlation between the measurements is indeed 0.
```{r, plot-2, fig.margin=TRUE, echo=FALSE, fig.cap="Correlation between two dependent groups."}
p3
```
Now, let’s look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is √2 times as large as the standard deviation in each measurement, and indeed, 15*√2 = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent t-test, where the correlation between measurements is not taken into account. 

```{r, plot-3, fig.margin=TRUE, echo=FALSE, fig.cap="Distributions of difference scores between two dependent groups."}
p2
```

Now let’s increase the correlation between dependent variables to 0.7.

Nothing has changed when we plot the means:


```{r, plot-4, fig.margin=TRUE, echo=FALSE, fig.cap="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15."}
set.seed(419)
#Set color palette
cbbPalette<-c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

n<-10000  #set the sample size for each group
mx<-100  #set the mean in group 1
sdx<-15  #set the standard deviation in group 1
my<-106  #set the mean in group 2
sdy<-15  #set the standard deviation in group 2
cor.true <- 0.0 #set true correlation
#randomly draw data
cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T)
mu <- c(0,0)
mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE)
x<-mat[,1]*sdx+mx
y<-mat[,2]*sdy+my
dif<-x-y
dataset<-data.frame(x,y)
DV<-c(x,y) #combine the two samples into a single variable
IV<-as.factor(c(rep("1", n), rep("2", n))) #create the independent variable (1 and 2) 
datasetplot<-data.frame(IV,DV) #create a dataframe (to make the plot)
t.test(x, y, alternative = "two.sided", paired = FALSE, var.equal = TRUE, conf.level = 0.95) #t-test

#plot graph two groups
p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV)))  + 
  geom_histogram(alpha=0.4, binwidth=2, position="identity", colour="black", aes(y = ..density..)) +
  scale_fill_manual(values=cbbPalette, name = "Condition") +
  stat_function(fun=dnorm, args=c(mean=mx,sd=sdx), size=1, color="#E69F00", lty=2) +
  stat_function(fun=dnorm, args=c(mean=my,sd=sdy), size=1, color="#56B4E9", lty=2) +
  xlab("IQ") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(x), colour="black", linetype="dashed", size=1) + 
  geom_vline(xintercept=mean(y), colour="black", linetype="dashed", size=1) + 
  coord_cartesian(xlim=c(50,150)) + scale_x_continuous(breaks=c(50,60,70,80,90,100,110,120,130,140,150)) +
  annotate("text", x = 70, y = 0.02, label = paste("Mean X = ",round(mean(x)),"\n","SD = ",round(sd(x)),sep="")) +
  annotate("text", x = 130, y = 0.02, label = paste("Mean Y = ",round(mean(y)),"\n","SD = ",round(sd(y)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))


#plot data differences
p2 <- ggplot(as.data.frame(dif), aes(dif))  + 
  geom_histogram(colour="black", fill="grey", aes(y=..density..), binwidth=2) +
  #  geom_density(fill=NA, colour="black", size = 1) +
  xlab("IQ dif") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(dif), colour="gray20", linetype="dashed") + 
  coord_cartesian(xlim=c(-80,80)) + scale_x_continuous(breaks=c(seq(-80, 80, 10))) +
  annotate("text", x = mean(dif), y = 0.01, label = paste("Mean = ",round(mean(dif)),"\n","SD = ",round(sd(dif)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))

#Plot correlation
p3 <- ggplot(dataset, aes(x=x, y=y)) +
  geom_point(size=2) +    # Use hollow circles
  geom_smooth(method=lm, colour="#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line
  coord_cartesian(xlim=c(40,160), ylim=c(40,160)) +
  scale_x_continuous(breaks=c(seq(40, 160, 20))) + scale_y_continuous(breaks=c(seq(40, 160, 20))) +
  xlab("IQ twin 1") + ylab("IQ twin 2")  + ggtitle(paste("Correlation = ",round(cor(x,y),digits=2),sep="")) + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  coord_fixed(ratio = 1)  +
  theme(plot.title = element_text(hjust = 0.5))


p1
```

The correlation between measurements is now strongly positive:

```{r, plot-5, fig.margin=TRUE, echo=FALSE, fig.cap="Correlation between two dependent groups."}
p3
```

The important difference lies in the standard deviation of the difference scores. The SD = 11 instead of 21 in the simulation above. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above.

```{r, plot-6, fig.margin=TRUE, echo=FALSE, fig.cap="Difference scores between two dependent groups."}
p2
```

if you set the correlation to a negative value, the standard deviation of the difference scores actually increases. 

I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor.

You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (M = 390, SD = 44) when they used their feet than when they used their hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p < 0.001, Hedges' g = 0.43, Mdiff = 19, 95% CI [12; 26]). 

Since most dependent variables in within designs in psychology are positively correlated, within designs will greatly increase the power you can achieve given the sample size you have available. Use within-designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-subject design. Maxwell and Delaney's book (Chapter 11) has a good discussion of this topic.

You can use this Shiny app to play around with different means, sd's, and correlations, and see the effect of the distribution of the difference scores.

```{r shiny2, echo=F}
knitr::include_app('http://shiny.ieis.tue.nl/within_between/', height = '4000px')
```

# Power analysis for interactions {-}

Based on our recent [preprint](https://psyarxiv.com/baxsf/) explaining power analysis for ANOVA designs, in this post I want provide a step-by-step mathematical overview of power analysis for interactions. These details often do not make it into tutorial papers because of word limitations, and few good free resources are available (for a paid resource worth your money, see [Maxwell, Delaney, & Kelley, 2018](https://designingexperiments.com/)). This post is a bit technical, but nothing in this post requires more knowedge than multiplying and dividing numbers, and I believe that for anyone willing to really understand effect sizes and power in ANOVA designs digging in to these details will be quite beneficial. There are three take-home messages in this section. 

1) In power analyses for ANOVA designs, you should always think of the predicted pattern of means. Different patterns of means will have the same effect size, and your intuition can not be relied on when predicting an effect size for ANOVA designs.
2) Understanding how patterns of means relate to the effect you predict is essential to design an informative study.
3) Always perform a power analysis if you want to test a predicted interaction effect, and always calculate the effect size based on means, sd's, and correlations, instead of plugging in a 'medium' partial eta squared. 
4) Crossover interaction effects have large effects and can thus be studies with high power in smaller samples, and if your theory can predict crossover interactions, such experiments might be worthwhile to design.
5) There are some additional benefits of examining interactions (risky predictions, generalizability, efficiently examining multiple main effects) and it would be a shame if the field is turned away from examining interactions because they sometimes require large samples.

**Getting started: Comparing two groups**

We are planning a two independent group experiment. We are using a validated measure, and we know the standard deviation of our measure is approximately 2. Psychologists are generaly horribly bad at knowing the standard deviation of their measures, even though a very defensible position is that you are not ready to perform a power analysis without solid knowledge of the standard deviation of your measure. We are interested in observing a mean difference of 1 or more, because smaller effects would not be practically meaningful. We expect the mean in the control condition to be 0, and therefore want the mean in the intervention group to be 1 or higher.

This means the standardized effect size is the mean difference, divided by the standard deviation, or 1/2 = 0.5. This is the Cohen's d we want to be able to detect in our study:

\begin{equation}
d = \frac{m_1-m_2}{\sigma} =  \frac{1-0}{2} = 0.5.
\end{equation}

An independent *t*-test is mathematically identical to an *F*-test with two groups. For an *F*-test, the effect size used for power analyses is Cohen's *f*, which is a generalization
of Cohen’s d to more than two groups (Cohen, 1988). It is calculated based on the standard deviation of the population means divided by the population standard deviation which we know for our measure is 2), or: 

\begin{equation}
f = \frac{\sigma _{ m }}{\sigma}
\end{equation}
where for equal sample sizes,
\begin{equation}
\sigma _{ m } = \sqrt { \frac { \sum_ { i = 1 } ^ { k } ( m _ { i } - m ) ^ { 2 } } { k } }.
\end{equation}

In this formula *m* is the grand mean, k is the number of means, and m_i is the mean in each group. The formula above might look a bit daunting, but calculating Cohen's f is not that difficult for two groups. 

If we take the expected means of 0 and 1, and a standard deviation of 2, the grand mean (the *m* in the formula above) is (0 + 1)/2 = 0.5. The formula says we should subtract this grand mean from the mean of each group, square this value, and sum them. So we have (0-0.5)^2 and (1-0.5)^2, which are both 0.25. We sum these values (0.25 + 0.25 = 0.5), divide them by the number of groups (0.5/2 = 0.25) and take the square root, we find that $\sigma_{ m }$ = 0.5. We can now calculate Cohen's f (remember than we know $\sigma$ = 2 for our measure): 

\begin{equation}
f = \frac{\sigma _{ m }}{\sigma} = \frac{0.5}{2} = 0.25
\end{equation}

We see that for two groups Cohen's f is half as large as Cohen's d, or $f = \frac{1}{2}d$, which always holds for an *F*-test with two independent groups. 

Although calculating effect sizes by hand is obviously an incredibly enjoyable thing to do, you might prefer using software that performs these calculations for you. Here, I will use our Superpower power analysis package (developed by Aaron Caldwell and me). The code below uses a function from the package that computes power analytically for a one-way ANOVA where all conditions are manipulated between participants. In addition to the effect size, the function will compute power for any sample size per condition you enter. Let's assume you have a friend who told you that they heard from someone else that you now need to use 50 observations in each condition (n = 50), so you plan to follow this trustworthy advice. We see the code below returns a Cohen's *f* of 0.25, and also tells us we would have 61.78% power if we use a preregistered alpha level of 0.03.

```{r eval=T, echo=T}
library(Superpower)

design <- ANOVA_design(
  design = "2b", 
  n = 50, 
  mu = c(1, 0), 
  sd = 2)


power_oneway_between(design, alpha_level = 0.03)$Cohen_f
power_oneway_between(design, alpha_level = 0.03)$power
```

We therefore might want to increase our sample size for our planned study. Using the `plot_power` function, we can see we would pass 90% power with 100 observations per condition.

```{r, echo=T}

plot_power(design, alpha_level = 0.03, min_n = 45, max_n = 150)$plot_ANOVA
```

**Interaction Effects**

So far we have explained the basics for effect size calculations (and we have looked at statistical power) for 2 group ANOVA designs. Now we have the basis to look at interaction effects.

One of the main points in this blog post is that it is better to talk about interactions in ANOVAs in terms of the pattern of means, standard deviations, and correlations, than in terms of a standarized effect size. The reason for this is that, while for two groups a difference between means directly relates to a Cohen's d, wildly different patterns of means in an ANOVA will have the same Cohen's *f*. In my experience helping colleagues out their with power analyses for ANOVA designs, talking about effects in terms of a Cohen's *f* is rarely a good place to start when thinking about what your hypothesis predicts. Instead, you need to specify the predicted pattern of means, have some knowledge about the standard deviation of your measure, and then calculate your predicted effect size. 

There are two types of interactions, as visualized below. In an ordinal interaction, the mean of one group ("B1") is always higher than the mean for the other group ("B2"). Disordinal interactions are also known as 'cross-over' interactions, and occur when the group with the larger mean switches over. The difference is important, since another main takeaway of this blog post is that, in two studies where the largest simple comparison has the same effect size, a study with a disordinal interaction has much higher power than a study with an ordinal interaction. Thus, if possible, you will want to design experiments where an effect in one condition flips around in the other condition, instead of an experiment where the effect in the other condition just disappears. I personally never realized this before I learned how to compute power for interactions, and never took this simple but important fact into account. Let's see why it is important.

**Calculating effect sizes for interactions**

```{r, echo=F}

df <- data.frame(
    A = factor(c("A1","A1","A2","A2")),
    B = factor(c("B1","B2","B1","B2")),
    Y = c(1, 0.0, 0.1, 1)
)

p1 <- ggplot(data=df, aes(x=A, y=Y, group=B, shape=B)) +
    geom_line(size = 2) +
    geom_point(size = 4, fill = "white") +
  scale_shape_manual(values=c(22,21)) +
  ggtitle("disordinal interaction") +
  theme_bw()

df <- data.frame(
    A = factor(c("A1","A1","A2","A2")),
    B = factor(c("B1","B2","B1","B2")),
    Y = c(1, 0, 0.1, 0)
)

p2 <- ggplot(data=df, aes(x=A, y=Y, group=B, shape=B)) +
    geom_line(size = 2) +
    geom_point(size = 4, fill = "white") +
  scale_shape_manual(values=c(22,21)) +
  ggtitle("ordinal interaction") +
  theme_bw()

# Use patchwork to combine and plot only 1 legend without title.
combined <- p1 + p2 & theme(legend.position = "bottom", 
                            legend.title = element_blank())
combined + plot_layout(guides = "collect")
```

Mathematically the interaction effect is computed as the cell mean minus the sum of the grand mean, the marginal mean in each condition of one factor minus the grand mean, and the marginal mean in each condition for the other factor minus grand mean (see Maxwell et al., 2017).

Let's consider two cases comparable to the figure above, one where we have a perfect disordinal interaction (the means of 0 and 1 flip around in the other condition, and are 1 and 0) or an ordinal interaction (the effect is present in one condition, with means of 0 and 1, but there is no effect in the other condition, and both means are 0). We can calcuate the interaction effect as follows. First, let's look at the interaction in a 2x2 matrix:

```{r eval=T, echo=F}
design <- ANOVA_design(
  design = "2b*2b", 
  n = 50, 
  mu = c(1, 0, 0, 1), 
  sd = 2)

power_twoway_between(design, alpha_level = 0.03)$mean_mat
```

The grand mean is (1 + 0 + 0 + 1) / 4 = 0.5.

We can compute the marginal means for A1, A2, B1, and B2, which is simply averaging per row and column, which gets us for the A1 column (1+0)/2=0.5. For this perfect disordinal interaction, all marginal means are 0.5. This means there are no main effects. There is no main effect of factor A (because the marginal means for A1 and A2 are both exactly 0.5), nor is there a main effect of B.

We can also calculate the interaction effect. For each cell we take the value in the cell (e.g., for a1b1 this is 1) and compute the difference between the cell mean and the additive effect of the two factors as:

1 - (the grand mean of 0.5 + (the marginal mean of a1 minus the grand mean, or 0.5 - 0.5 = 0) + (the marginal mean of b1 minus the grand mean, or 0.5 - 0.5 = 0)). Thus, for each cell we get:

a1b1: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5

a1b2: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5

a2b1: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5

a2b2: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5

Cohen's $f$ is then $f = \frac { \sqrt { \frac { 0.5^2 +-0.5^2 + -0.5^2 + 0.5^2 } { 4 } }}{ 2 } = 0.25$

or in R code: `sqrt(((0.5)^2 +(-0.5)^2 + (-0.5)^2 + (0.5)^2)/4)/2 = 0.25`.

For the ordinal interaction the grand mean is (1 + 0 + 0 + 0) / 4, or 0.25. The marginal means are a1: 0.5, a2: 0, b1: 0.5, and b2: 0.

Completing the calculation for all four cells for the ordinal interaction gives:

a1b1: 1 - (0.25 + (0.5 -0.25) + (0.5 -0.25)) = 0.25

a1b2: 0 - (0.25 + (0.5 -0.25) + (0.0 -0.25)) = -0.25

a2b1: 0 - (0.25 + (0.0 -0.25) + (0.5 -0.25)) = -0.25

a2b2: 0 - (0.25 + (0.0 -0.25) + (0.0 -0.25)) = 0.25

Cohen's $f$ is then $f = \frac { \sqrt { \frac { 0.25^2 +-0.25^2 + -0.25^2 + 0.25^2 } { 4 } }}{ 2 } = 0.125$.

or in R code: `sqrt(((0.25)^2 +(-0.25)^2 + (-0.25)^2 + (0.25)^2)/4)/2 = 0.125`.


We see the effect size of the cross-over interaction (*f* = 0.25) is twice as large as the effect size of the ordinal interaction (*f* = 0.125). 

If the math so far was a bit too much to follow, there is an easier way to think of why the effect sizes are halved. In the disordinal interaction we are comparing cells a1b1 and a2b2 against a1b2 and a2b1, or (1+1)/2 vs. (0+0)/2. Thus, if we see this as a *t*-test for a contrast, it is clear the mean difference is 1, as it was in the simple effect we started with. For the ordinal interaction, we have (1+0)/2 vs. (0+0)/2, so the mean difference is halved, namely 0.5. 

**Power for interactions**

All of the above obviously matters for the statistical power we will have when we examine interaction effects in our experiments. Let's use Superpower to perform power analyses for the disordinal interaction first, if we would collect 50 participants in each condition. 

```{r eval=T, echo=T}
design <- ANOVA_design(
  design = "2b*2b", 
  n = 50, 
  mu = c(1, 0, 0, 1), 
  sd = 2)

ANOVA_exact(design, alpha_level = 0.03)
```

First let's look at the Power and Effect size for the pairwise comparisons. Not surprisingly, these are just the same as our original t-test, given that we have 50 observations per condition, and our mean difference is either 1, or a Cohen's d of 0.5 (in which case we have 61.78% power) or the mean difference is 0, and we have no power (because there is no true effect) but we wil observe significant results 3% of the time because we set our alpha level to 0.03.

Then, let's look at the results for the ANOVA. Since there are no main effects in a perfect crossover interaction, we have a 3% Type 1 error rate. We see the power for the crossover interaction between factor a and b is 91.06%. This is much larger than the power for the simple effects. The reason is that the contrast that is equal to the test of the interaction is based on all 200 observations. Unlike the pairwise comparisons with 50 vs 50 observations, the contrast for the interaction has 100 vs 100 observations. Given that the effect size is the same (*f* = 0.25) we end up with much higher power. 

If you current think it is impossible to find a statistically significant interaction without a huge sample size, you clearly see this is wrong. Power *can* be higher for an interaction than for the simpe effect - but this depends on the pattern of means underlying the interaction. If possible, design studies where your theory predicts a perfect crossover interaction.

For the ordinal interaction, our statistical power does not look that good based on an a-priori power analysis. Superpower tells us we have 33.99% power for the main effects and interaction (yes, we have exactly the same power for all three - if you think about the three contrasts that are tested, these have the same effect size). 

```{r eval=T, echo=T}
design <- ANOVA_design(
  design = "2b*2b", 
  n = 50, 
  mu = c(1, 0, 0, 0), 
  sd = 2)

ANOVA_exact(design, alpha_level = 0.03)
```

If you have heard people say you should be careful when designing studies predicting interaction patterns because you might have very low power, this is the type of pattern of means they are warning about. Maxwell, Delaney, and Kelley (2018) discuss why power for interactions is often smaller, and note interactions effects are often smaller in the real world, and we often examine ordinal interactions. This might be true. But in experimental psychology it might be possile to think about hypotheses that predict disordinal interactions. In addition to the fact that such predictions are often theoretically riskier and more impressive (after all, many things can make an effect go away, but without your theory it might be difficult to explain why an effect flips around) they also have larger effects and are easier to test with high power.

Some years ago other blog posts by [Uri Simonsohn](http://datacolada.org/17) and [Roger Giner-Sorolla](https://approachingblog.wordpress.com/2018/01/24/powering-your-interaction-2/) did a great job in warning researchers they need large sample sizes for ordinal interactions, and my post repeats this warning. But it would be a shame if researchers would stop examining interaction effects. There are some nice benefits studying interactions, such as 1) making riskier theoretical predictions, 2) greater generalizability (if there is no interaction effect, you might show a main effect operates across different conditions of a second factor) and 3) if you want to study two main effects it is more efficient to do this in a 2x2 design than in two seperate designs (Maxwell, Delaney, & Kelley, 2018). So maybe this blog post has been able to highlight some scenarios where examining interaction effects is still beneficial.


