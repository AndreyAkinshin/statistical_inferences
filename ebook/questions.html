<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3 Asking Statistical Questions | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-08-01" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>3 Asking Statistical Questions | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a href="power.html"><span class="toc-section-number">2</span> Justifying the Number of Observations</a>
<a id="active-page" href="questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a><ul class="toc-sections">
<li class="toc"><a href="#do-you-really-want-to-test-a-hypothesis"> Do You Really Want to Test a Hypothesis?</a></li>
<li class="toc"><a href="#goals-of-tests"> Goals of tests</a></li>
</ul>
<a href="errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a>
<a href="effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a>
<a href="equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a>
<a href="severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a>
<a href="sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a>
<a href="meta.html"><span class="toc-section-number">9</span> Meta-analysis</a>
<a href="bias.html"><span class="toc-section-number">10</span> Bias detection</a>
<a href="computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a>
<a href="prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a>
<a href="bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a>
<a href="references.html"><span class="toc-section-number">14</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="questions" class="section level1">
<h1>
<span class="header-section-number">3</span> Asking Statistical Questions</h1>
<div id="do-you-really-want-to-test-a-hypothesis" class="section level2">
<h2>
<span class="header-section-number">3.1</span> Do You Really Want to Test a Hypothesis?</h2>
<p>A hypothesis test is a very specific answer to a very specific question. We can use a dart game as a metaphor for the question a hypothesis test aims to answer. In essence, both a dart game and a hypothesis test are a methodological procedure to make a directional prediction: Is A better or worse than B?, In a dart game we very often compare two players, and the question is whether we should act as is player A is the best, or player B is the best. In a hypothesis test, we compare two hypotheses, and the question is whether we should act as if the null hypothesis is true, or whether the alternative hypothesis is true.</p>
<p>Dart games are specifically designed to decide who of two players is better than the other. The rules around a dart game are implemented in such a way that they give you the best possibility to distinguish which of the two players is better. For example, we don’t ask players to throw a single dart, because that would be too noisy information to make a final decision, and similarly, we don’t test hypotheses based on a single datapoint in each condition.</p>
<p>Some of the rules have a certain level of arbitrariness to them. A dart match could be decided by whether a single dart ends up one millimeter to the left or the right of the divisor between 1 and 20, leading a player to win or lose. This is the nature of a game that is set up to determine a winner. The difference might be trivially small, but a decision is made.</p>
<p>In hypothesis tests, one criticism has focused on the fact that the goal of a test is a dichotomous decision. For example, in Rosnow and Rosenthal <span class="citation">Rosnow &amp; Rosenthal (<label for="tufte-mn-17" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle">1989<span class="marginnote">Rosnow, R. L., &amp; Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. <em>American Psychologist</em>, <em>44</em>(10), 1276. <a href="https://doi.org/10.1037/0003-066X.44.10.1276">https://doi.org/10.1037/0003-066X.44.10.1276</a></span>)</span> the authors write:</p>
<blockquote>
<p>dichotomous significance testing has no ontological basis. That is, we want to underscore that, surely, God loves the .06 nearly as much as the .05.</p>
</blockquote>
<p>However, the basis for dichotomous claims was never ontological, but based on a philosophy of science <span class="citation">(Neyman &amp; Pearson, <label for="tufte-mn-18" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle">1933<span class="marginnote">Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</em>, <em>231</em>(694-706), 289–337. <a href="https://doi.org/10.1098/rsta.1933.0009">https://doi.org/10.1098/rsta.1933.0009</a></span>; Popper, <label for="tufte-mn-19" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle">2002<span class="marginnote">Popper, K. R. (2002). <em>The logic of scientific discovery</em>. Routledge.</span>)</span>. The idea behind hypothesis tests is that researchers make claims, based on whether predictions are corroborated, or not. This claim, a basic statement about for example the existence of a directional effect, is put forward for others to falsify. If others fail to falsify a claim, it is tentatively accepted as a fact that we can build on in future scientific work. Scientists often need to make dichotomous decisions (e.g., do we use manipulation X or Y, do we abandon a research line, or continue it?) and hypothesis tests are a methodological procedure to make claims that these decisions are based on.</p>
<p>The whole point of a hypothesis test is to choose between two possible hypotheses. When we design a hypothesis test we divide all possible states of the world into what we predict and what we don’t predict. Sometimes this is very easy to do, especially when we make predictions about concrete objects. The best-known example is the prediction that all swans are white. You only need to observe one black swan to falsify this prediction. So the state of the world that is predicted is a world with only white swans, and the state of the world that is not predicted is a black swan. If you observe one of these black swans we know that the situation in the real-world represents something that we did not predict.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:blackwhite"></span>
<img src="images/blackwhite.png" alt="Some fields make black and white predictions about the presence or absence of obervables, but in many sciences, predictions are probabilistic, and shades of grey." width="442"><!--
<p class="caption marginnote">-->Figure 3.1: Some fields make black and white predictions about the presence or absence of obervables, but in many sciences, predictions are probabilistic, and shades of grey.<!--</p>-->
<!--</div>--></span>
</p>
<p>Tests are more difficult if we have a probabilistic environment. Many situations where we rely on statistical inferences are situations where we can’t divide the world into concrete different states. The world is probabilistic. Things are more or less likely to happen. In these situations we can still divide the world into a area that we predicted and that we didn’t predict. We can make our predictions falsifiable by specifying certain rejection rules which may render statistically interpreted evidence inconsistent with probabilistic theory. So this is a statement by Lakatos, a philosopher of science, who says this is one way in which we can actually make theoretical predictions falsifiable even when these are statements about probabilistic events. Lakatos also writes that the Neyman-Pearson approach to hypothesis testing rests completely on methodological falsificationism. It’s good to remember that there’s a difference between a Fisherian significance test and a Neyman-Pearson hypothesis test. The goal of the letters to prevent incorrect decisions or type one and type two errors as Neyman and Pearson themselves right the goal of their approach is to not be too often wrong. It’s interesting to think back about how these people wrote about statistics and if you’ve never read their original work, I can highly recommend it: some of the statements that they write about are basically poetry. Let’s take a look at one specific page from the book by Fisher on how to design experiments. Here he writes, “It is usual and convenient for experimenters to take five percent as a standard level of significance, in the sense that they are prepared to ignore all results which failed to reach this standard and by this means to eliminate from further discussion the greater part of the fluctuations which chance causes have introduced into their experimental results. No such selection can eliminate the whole of the possible effects of chance coincidence and if we accept this convenience convention with which he means the convenient convention to set the alpha level at five percent something we’ll return to in later modules.” So he writes, “And if we accept this convenience convention and agree that an event which would occur by chance only once in 70 trials is decidedly significant in the statistical sense, we thereby admit that no isolated experiment however significant in itself can suffice for the experimental demonstration of any natural phenomenon for the one chance in a million will undoubtedly occur with no less and no more than its appropriate frequency. However surprised we may be that it should occur to us.” So he’s basically saying that we’re always observe flux in single studies. This possibility always exists and this is why we need to make repeated observations and do experiments multiple times. He writes, “In order to assert that a natural phenomenon is experimentally demonstrable, we need not an isolated record but a reliable methods of procedure.” In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result. So it’s important to note that Fisher is talking here about a reliable method of procedure. Neyman and Pearson themselves say such a rule or such a methodological procedure tells us nothing as to whether in a particular case the hypothesis is true when we observe data that is smaller than some threshold, or false when we observe data that is larger than some threshold. So we don’t make statements about any particular case but we have a method of procedure that allows us to with some controlled error rate distinguish between two players and regularly determine the correct winner. Now you want to reflect on why you would want to test a hypothesis with such a methodological rule. When would you want to do something like this. First, it makes sense that you need to make a decision how to act. In a dart game, we declare one person the world champion, the other person the second best in the world. So this is the decision on how to act. In a hypothesis test this act can be to either accept or reject the null hypothesis which means act as if the null hypothesis is true or act as if the null hypothesis is false. You can also, if you want to, remain in doubt. Very often people criticize hypothesis testing as a methodological procedure because it’s dichotomous, but this never was the intention. Neyman already wrote that the region of doubt may be obtained by further subdivision of the region of acceptance. You can divide the probabilistic space up in any way that you want, a rejection region, an acceptance region, and a region in which you would remain in doubt and would need to collect more data. So reflect on whether you actually need to make a decision, sometimes yes, sometimes no. In a cumulative science, I would say that very often it is unavoidable to make some sorts of decisions based on a pilot study or an experiment that somebody else did or the validation of some measure that you want to use. The second thing to consider is whether your hypotheses are good players; is it actually sensible to pit these two against each other or do you know which of these is going to win to begin with? There is for example no need to let me play the world champion in darts, we already know who’s going to win: the World Champion, because I don’t know how to play darts. In the case of a null hypothesis significance test, randomization is a crucial factor that contributes to the plausibility of the null hypothesis, because we randomly assign, for example, people to an experimental condition and to a control condition. It’s actually somewhat likely that the null hypothesis is true. Remember of the statement by Cohen earlier, that a null hypothesis significance test might be an interesting question to ask in a carefully controlled experiment. The reason for this is that randomization makes the null a plausible value and thus a good player in a hypothesis test. It actually makes sense to test against it. This is not automatically true in correlational research. Sometimes people make the statement that everything is correlated with everything else. There’s all sorts of systematic variation among measurements that can lead answers to be slightly higher or slightly lower between different groups of people just because there is systematic variation in the world. In these cases, it’s not so exciting to actually test against the value of exactly zero. This systematic noise is sometimes referred to as crud. Crud is actually a strong argument against testing against the null hypothesis in non-experimental research, where there was no randomization to conditions. Luckily, the null-hypothesis doesn’t need to be a nil-hypothesis. You don’t need to test against a value of exactly zero. You can divide the world into a range where you accept certain values and reject certain values in any way that you like. So it’s perfectly acceptable to have a null hypothesis that’s another point, but a small range around zero, where you say, "Well, there will be some systematic variation but any value within this small range around zero, those are values that I don’t predict, and anything that’s large enough outside of this range, that’s the stuff that I’m predicting in my hypothesis test. Finally, the last thing to take into account is whether you can actually control the error rate in the study that you are designing. We have carefully constructed a game of darts so that the error rates are actually nicely balanced. We don’t throw darts from maybe 100 feet away from the board, because in those cases even the best players would very likely miss the board altogether. We also don’t throw a dart from one foot away from the dartboard, because then we could just stick it in and we’d would never make any errors. We can’t make decisions when error rate are huge, when our error rates are so high that we’re looking at a chaotic pattern. So you want to be able to control the error rate in some sensible way so that you’re not wrong too often. The goal of making a successful prediction in this game or in a hypothesis test is to show the predictive validity of a theory; this theory has something going for it. A more fancy terms and for a word you might want to remember next time that you’re playing Scrabble, we’re talking in terms of verisimilitude or truth likeness. If your theory can make good predictions, I should be impressed by the verisimilitude of the theory that you came up with. So this is the goal of a hypothesis test. When a dart player wants to demonstrate that they know what they’re doing, they might make a prediction such as saying that the dart will go into the bulls-eye. If they then throw a dart and it actually ends up in the bulls-eye, we as an audience are impressed. We think that this dart player knows what they are doing. A hypothesis test should work in a similar manner. We make a prediction, then we collect some data, and if the data support our predictions, we should be impressed by the quality of our hypothesis. Now of course, there’s something slightly peculiar about how we test hypothesis in science. Very often, we don’t state in advance what we want to do. It’s more like we here dart players who say, ‘’Sure, I’m predicting that my next dart is going to be in the bulls-eye, would you now please leave room, come back in a minute.’’ Then when we come back in a minute we see the end result. We see that the dart is sticking in the bulls-eye, but we don’t really know what happened. If we would preregister our predictions, we see that actually confirming them is much more impressive, than if the whole approach to the scientific process is much less transparent. Now, if we think about the cases where you collect data, you do research, there should be situations where maybe you don’t need to make a decision, you don’t have very good hypotheses to begin with, maybe you didn’t do the exploratory research to create good hypotheses, you don’t have two good players, or you can’t really set the error rates because you can’t control the number of observations for example. So in these cases, it should make sense that you might not want to test the hypothesis. A hypothesis test in these situations might not be the very specific answer to the question that you’re asking. So I think that in general we should really think whether what we want to do is actually test a hypothesis and stop overusing hypothesis tests. Hypothesis tests are a very specific tool that answer a very specific question under very specific conditions.</p>
</div>
<div id="goals-of-tests" class="section level2">
<h2>
<span class="header-section-number">3.2</span> Goals of tests</h2>
<p>What is the goal of data collection? This is a simple question, and as researchers we collect data all the time. But the answer to this question is not straightforward. It depends on the question that you are asking of your data. There are different questions you can ask from your data, and therefore, you can have different goals when collecting data. Here, I want to focus on collecting data to test scientific theories. I will be quoting a lot from De Groot’s book Methodology (1969), especially Chapter 3. If you haven’t read it, you should – I think it is the best book about doing good science that has ever been written.</p>
<p>When you want to test theories, the theory needs to make a prediction, and you need to have a procedure that can evaluate verification criteria. As De Groot writes: “A theory must afford at least a number of opportunities for testing. That is to say, the relations stated in the model must permit the deduction of hypotheses which can be empirically tested. This means that these hypotheses must in turn allow the deduction of verifiable predictions, the fulfillment or non-fulfillment of which will provide relevant information for judging the validity or acceptability of the hypotheses” (§ 3.1.4).</p>
<p>This last sentence is interesting – we collect data, to test the ‘validity’ of a theory. We are trying to see how well our theory works when we want to predict what unobserved data looks like (whether these are collected in the future, or in the past, as De Groot remarks). As De Groot writes: “Stated otherwise, the function of the prediction in the scientific enterprise is to provide relevant information with respect to the validity of the hypothesis from which it has been derived.” (§ 3.4.1).</p>
<p>To make a prediction that can be true or false, we need to forbid certain states of the world and allow others. As De Groot writes: “Thus, in the case of statistical predictions, where it is sought to prove the existence of a causal factor from its effect, the interval of positive outcomes is defined by the limits outside which the null hypothesis is to be rejected. It is common practice that such limits are fixed by selecting in advance a conventional level of significance: e.g., 5 %, 1 %, or .1 % risk of error in rejecting the assumption that the null hypothesis holds in the universe under consideration. Though naturally a judicious choice will be made, it remains nonetheless arbitrary. At all events, once it has been made, there has been created an interval of positive outcome, and thus a verification criterion. Any outcome falling within it stamps the prediction as ’proven true.” (§ 3.4.2). Note that if you prefer, you can predict an effect size with some accuracy, calculate a Bayesian highest density interval that excludes some value, or a Bayes factor that is larger than some cut-off – as long as your prediction can be either confirmed or not confirmed.</p>
<p>Note that the prediction gets a ‘proven true’ stamp – the theory does not. In this testing procedure, there is no direct approach from the ‘proven true’ stamp to a ‘true theory’ conclusion. Indeed, the latter conclusion is not possible in science. We are mainly indexing the ‘track record’ of a theory, as Meehl (1990) argues: “The main way a theory gets money in the bank is by predicting facts that, absent the theory, would be antecedently improbable.” Often (e.g., in non-experimental settings) rejecting a null hypothesis with large sample sizes is not considered a very improbable event, but that is another issue (see also the definition of a severe test by Mayo (1996, 178): a passing result is a severe test of hypothesis H just to the extent that it is very improbable for such a passing result to occur, were H false).</p>
<p>Regardless of how risky the prediction we made was, when we then collect data, and test the hypothesis, we either confirm our prediction, or we do not confirm our prediction. In frequentist statistics, we add the outcome of this prediction to the ‘track record’ of our theory, but we can not draw conclusions based on any single study. As Fisher (1926, 504) writes: “if one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 per cent point), or one in a hundred (the 1 per cent point). Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance” (italics added).</p>
<p>The study needs to be ‘properly designed’ to ‘rarely’ fail to give a level of evidence – which despite Fisher’s dislike for Neyman-Pearson statistics, I can read in no other way than to make sure you run well-powered studies for whatever happens to be your smallest effect size of interest. In other words: When testing the validity of theories through predictions, where you keep track of a ‘track record’ of predictions, you need to control your error rates to efficiently distinguish hits from misses. Design well-powered studies, and do not fool yourself by inflating the probability of observed in false positive.</p>
<p>Bayesian statistics do not provide a solution when analyzing this pre-cognition experiment. As Gelman and Loken (2013) write about this study (I just realized this ‘Garden of Forking paths’ paper is unpublished, but has 150 citations!): “we can still take this as an observation that 53.1% of these guesses were correct, and if we combine this with a flat prior distribution (that is, the assumption that the true average probability of a correct guess under these conditions is equally likely to be anywhere between 0 and 1) or, more generally, a locally-flat prior distribution, we get a posterior probability of over 99% that the true probability is higher than 0.5; this is one interpretation of the one-sided p-value of 0.01.” The use of Bayes factors that quantify model evidence provides no solution. Where Wagenmakers and colleagues argue based on ‘default’ Bayesian t-tests that the null-hypothesis is supported, Bem, Utts, and Johnson (2011) correctly point out this criticism is flawed, because the default Bayesian t-tests use completely unrealistic priors for pre-cognition research (and most other studies published in psychology, for that matter).</p>
<p>It is interesting that the best solution Gelman and Loken come up with is that “perhaps researchers can perform half as many original experiments in each paper and just pair each new experiment with a preregistered replication”. What matters is not just the data, but the procedure used to collect the data. The procedure needs to be able to demonstrate a strong predictive validity, which is why pre-registration is such a great solution to many problems science faces. Pre-registered studies are the best way we have to show you can actually predict something – which gets your theory money in the bank.</p>
<p>If people ask me if I care about evidence, I typically say: ‘mwah’. For me, evidence is not a primary goal of doing research. Evidence is a consequence of demonstrating that my theories have high validity as I test predictions. It is important to end up with, and it can be useful to try to quantify model evidence through likelihoods or Bayes factors, if you have good models. But if I am able to show that I can confirm predictions in a line of pre-registered studies, either by showing my p-value is smaller than an alpha level, a Bayesian highest density interval excludes some value, a Bayes factor is larger than some cut-off, or by showing the effect size is close enough to some predicted value, I will always end up with strong evidence for the presence of some effect. As De Groot (1969) writes: “If one knows something to be true, one is in a position to predict; where prediction is impossible, there is no knowledge.”</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="power.html"><button class="btn btn-default">Previous</button></a>
<a href="errorcontrol.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-01
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-08-01 13:29:19
</span>
</p>
</div>


</body>
</html>
