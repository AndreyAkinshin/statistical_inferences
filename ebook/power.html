<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2 Sample size justification | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-08-11" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>2 Sample size justification | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a id="active-page" href="power.html"><span class="toc-section-number">2</span> Sample size justification</a><ul class="toc-sections">
<li class="toc"><a href="#measuring-the-entire-population"> Measuring the Entire Population</a></li>
<li class="toc"><a href="#feasibility"> Feasibility</a></li>
<li class="toc"><a href="#a-priori-power-analysis"> A-priori power analysis</a></li>
<li class="toc"><a href="#compromisepower"> Compromise power analysis</a></li>
<li class="toc"><a href="#observedpower"> Observed (post-hoc) power analysis</a></li>
<li class="toc"><a href="#why-within-subject-designs-typically-require-fewer-observations-than-between-subject-designs"> Why Within-Subject Designs Typically Require Fewer Observations than Between-Subject Designs</a></li>
<li class="toc"><a href="#designing-efficient-studies"> Designing efficient studies</a></li>
<li class="toc"><a href="#planning-for-precision"> Planning for precision</a></li>
</ul>
<a href="questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a>
<a href="errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a>
<a href="effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a>
<a href="equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a>
<a href="severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a>
<a href="sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a>
<a href="meta.html"><span class="toc-section-number">9</span> Meta-analysis</a>
<a href="bias.html"><span class="toc-section-number">10</span> Bias detection</a>
<a href="computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a>
<a href="prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a>
<a href="bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a>
<a href="sequential.html"><span class="toc-section-number">14</span> Sequential Analysis</a>
<a href="references.html"><span class="toc-section-number">15</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="power" class="section level1">
<h1>
<span class="header-section-number">2</span> Sample size justification</h1>
<p>When you perform an experiment, you want it to provide an answer to your research question that is as informative as possible. However, since all scientists are faced with resource limitations, you need to balance the cost of collecting each additional datapoint against the increase in information that datapoint provides. In economics, this is known as the Value of Information <span class="citation">(Eckermann et al., <label for="tufte-mn-7" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">2010<span class="marginnote">Eckermann, S., Karnon, J., &amp; Willan, A. R. (2010). The Value of Value of Information. <em>PharmacoEconomics</em>, <em>28</em>(9), 699–709. <a href="https://doi.org/10.2165/11537370-000000000-00000">https://doi.org/10.2165/11537370-000000000-00000</a></span>)</span>. Calculating the value of information is notoriously difficult. You need to specify the costs and benefits of possible outcomes of the study, and quantifying a utility function for scientific research is not easy.</p>
<p>Because of the difficulty of quantifying the value of information, scientists use less formal approaches to justify the amount of data they set out to collect. That is, if they provide a justification for the number of observations to begin with. Even though in some fields a justification for the number of observations is required when submitting a grant proposal to a science funder, a research proposal to an ethical review board, or a manuscript for submission to a journal. In some research fields, the number of observations is <em>stated</em>, but not <em>justified</em>. This makes it difficult to evaluate how informative the study was. Referees can’t just assume the number of observations is sufficient to provide an informative answer to your research question, so leaving out a justification for the number of observations is not best practice, and a reason reviewers can criticize your submitted article.</p>
<p>Researchers often find it difficult to justify their sample size, and a simple and efficient alternative is to only specify a maximum sample size, and analyse results as data comes in using <a href="sequential.html#sequential">sequential analyses</a> instead. Below, several possible justifications to decide upon a specific sample size when designing an experiment are discussed.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-pow-just">Table 2.1: </span>Overview of recommendations when reporting an a-prior power analysis.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
Type of Justification
</th>
<th style="text-align:left;">
In which situations is this justification relevant?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Measure the entire population
</td>
<td style="text-align:left;">
You can specify the entire population, it is finite, and you are able to measure every entity in the population.
</td>
</tr>
<tr>
<td style="text-align:left;">
Feasibility
</td>
<td style="text-align:left;">
Limited resources are the primary reason for your choice of the sample size you can collect.
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy
</td>
<td style="text-align:left;">
Your research question focusses on the size of a parameter, and you plan to collect sufficient data to have an estimate with a desired level of accuracy.
</td>
</tr>
<tr>
<td style="text-align:left;">
A-priori power analysis
</td>
<td style="text-align:left;">
Your research question focusses on rejecting certain effect sizes (e.g., effects = 0 in a directional NHST) and you want to achieve a desired probability of a correct rejection.
</td>
</tr>
<tr>
<td style="text-align:left;">
Heuristics
</td>
<td style="text-align:left;">
You rely on a benchmark, heuristic, general rule, or other convention that is either described in the literature or communicated orally.
</td>
</tr>
<tr>
<td style="text-align:left;">
No justification
</td>
<td style="text-align:left;">
You do not have any reason to choose a specific sample size, and you are simply collecting some data that seems sufficient to increase your understanding of the topic you are studying.
</td>
</tr>
</tbody>
</table>
<p>All of these sample size justification, even the ‘no justification’ category, are valid justifications in the sense that they give others insight into the underlying reasons that lead to your decision for a sample size in your study. However, it should not be surprising that the ‘heuristics’ and ‘no justification’ categories are unlikely to impress other scholars. What is perhaps more surprising is that, except for measuring the entire population, the other justifications might also not impress other scholars. The extent to which these justifications make other researchers judge the data you have collected as <em>informative</em> depends on the details of the question you aimed to answer, and the parameters you choose when determining the sample size for your study.</p>
<div id="measuring-the-entire-population" class="section level2">
<h2>
<span class="header-section-number">2.1</span> Measuring the Entire Population</h2>
<p>In some instances it might be possible to collect data from the entire population under examination. For example, you might be interested in the average amount of time people spend on the moon, whenever they visit it. Since there are only 12 people who have ever been on the moon, we can calculate the average time people have spent on the moon by collecting data from the entire population. Whenever it is possible to measure the entire population, the justification for the sample size is that you collected all the data that is available. This is the simplest, and most straightforward, justification for the sample size. However, it is also quite rare that we are able to measure the entire population.</p>
<p>The the entire population is measured, there is no need to perform a hypothesis test. After all, there is no population to generalize to, and any differences between two populations are, except for the possibility of measurement inaccuracy, true differences. In these cases, the question whether there is a true difference, or if the difference is noise becomes uninteresting, and we should mainly be interested in whether the observed difference is large enough to be meaningful.</p>
<p>If we have sampled the entire population, the mean is measured with absolute precision, and there is no confidence interval. If the total population size is known, but not measured completely, then the confidence interval width interval should shrink to zero the closer we get to measuring the entire population. This is known as the <strong>finite population correction factor</strong> for the variance of the estimator <span class="citation">(Thompson, <label for="tufte-mn-8" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2012<span class="marginnote">Thompson, S. K. (2012). <em>Sampling</em> (3rd ed). Wiley.</span>)</span>. The variance of a sample mean is <span class="math inline">\(\frac{\sigma^2}{n}\)</span>, and the finite population correction factor is <span class="math inline">\(1 - \frac{n}{N}\)</span>. When N is massively larger than n, the correction factor will be close to 1, and not have a noticeable effect on the variance. However, is the total population is measured, the correction factor is 0. The <code>superb</code> R package can compute population corrected confidence intervals <span class="citation">(Cousineau &amp; Chiasson, <label for="tufte-mn-9" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">2019<span class="marginnote">Cousineau, D., &amp; Chiasson, F. (2019). <em>Superb: Computes standard error and confidence interval of means under various designs and sampling schemes</em> [Manual].</span>)</span>.</p>
</div>
<div id="feasibility" class="section level2">
<h2>
<span class="header-section-number">2.2</span> Feasibility</h2>
<p>A common reason why a specific number of observations is collected is because collecting more data was not feasible. Note that all decisions for the sample size we collect in a study are based on the resources we have available in some way. A <strong>feasibility justification</strong> makes these resource limitations the primary reason for the sample size that is collected. Because we always have resource limitations in science, even when feasibility is not our primary justification for the number of observations we plan to collect, it is always a secondary reason. Despite the omnipresence of resource limitations, the topic often receives very little attention in texts on experimental design. This might make it feel like a feasibility justification is not appropriate, and you should perform an a-priori power analysis or plan for a desired precision instead. But feasibility always plays a secondary role, and therefore regardless of which justification for the sample size you provide, you will almost always need to include a feasibility justification as well.</p>
<p>Time and money are the two main resource limitations a scientist faces. Our master students write their thesis in 6 months, and therefore their data collection is necessarily limited in whatever can be collected in 6 months, minus the time needed to formulate a research question, design an experiment, analyze the data, and write up the thesis. A PhD student at our department would have 4 years to complete their thesis, but is also expected to complete multiple research lines in this time. In addition to limitations on time, we have limited financial resources. Although nowadays it is possible to collect data online quickly, if you offer participants a decent pay (as you should) most researchers do not have the financial means to collect thousands of datapoints.</p>
<p>A feasibility justification puts the limited resources at the center of the justification for the sample size that will be collected. For example, one might argue that 120 observations is the most that can be collected in the three weeks a master student has available to collect data, when each observation takes an hour to collect. A PhD student might collect data until the end of the academic year, and then needs to write up the results over the summer to stay on track to complete the thesis in time.</p>
<p>A feasibility justification thus <em>starts</em> with the expected number of observations (N) that a researcher expects to be able to collect. The challenge is to evaluate whether collecting N observations is worthwhile. The answer should sometimes be that data collection is <em>not</em> worthwhile. For example, assume I plan to manipulate the mood of participants using funny cartoons and then measure the effect of mood on some dependent variable - say the amount of money people donate to charity. I should expect an effect size around d = 0.31 for the mood manipulation <span class="citation">(Joseph et al., <label for="tufte-mn-10" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">2020<span class="marginnote">Joseph, D. L., Chan, M. Y., Heintzelman, S. J., Tay, L., Diener, E., &amp; Scotney, V. S. (2020). The manipulation of affect: A meta-analysis of affect induction procedures. <em>Psychological Bulletin</em>, <em>146</em>(4), 355–375. <a href="https://doi.org/10.1037/bul0000224">https://doi.org/10.1037/bul0000224</a></span>)</span>, and seems unlikely that the effect on donations will be larger than the effect size of the manipulation. If I can only collect mood data from 30 participants in total, how do we decide if this study will be informative?</p>
<p>If we want to evaluate whether the feasibility limitations make data collection uninformative, we need to think about what the goal of data collection is. First of all, having data always provide more knowledge than not having data, so in an absolute sense, all additional data that is collected is better than not collecting data. However, in line with the idea that we need to take into account costs and benefits, it is possible that the cost of data collection outweighs the benefits. To determine this, one needs to think about what the benefits of having the data are. The benefits are clearest when we know for certain that someone is going to make a decision, with or without data. If this is the case, then any data you collect will reduce the error rates of a well-calibrated decision process, even if only ever so slightly. In these cases, the value of information might be positive, as long as the reduction in error rates is more beneficial than the costs of data collection. If your sample size is limited and you know you will make a decision anyway, perform a <a href="power.html#compromisepower">compromise power analysis</a>, where you balance the error rates, given a specified effect size and sample size.</p>
<p>Another way in which a small dataset can be valuable is if its existence makes it possible to combine several small datasets into a meta-analysis. This argument in favor of collecting a small dataset requires 1) that you share the results in a way that a future meta-analyst can find them, and 2) that there is a decent probability that someone will perform a meta-analysis in the future which inclusion criteria would contain your study, because a sufficient number of small studies exist. The uncertainty about whether there will ever be such a meta-analysis should be weighed against the costs of data collection. One way to increase the probability of a future meta-analysis is if you commit to performing this yourself in the future. For example, you might plan to repeat a study for the next 12 years in a class you teach, with the expectation that a meta-analysis of 360 participants would be sufficient to achieve around 90% power for d = 0.31. If it is not plausible you will collect all the required data by yourself, you can attempt to set up a collaboration, where fellow researchers in your field commit to collecting similar data, with identical measures, over the next years. If it is not likely sufficient data will emerge over time, we will not be able to draw informative conclusions from the data, and it might be more beneficial to not collect the data to begin with, and examine an alternative research question with a larger effect size instead.</p>
<p>Even if you believe over time sufficient data will emerge, you will most likely compute statistics after collecting a small sample size. Before embarking on a study where your main justification for the sample size is based on feasibility, you can expect. I propose that a feasibility justification for the sample size is always accompanied by three statistics, detailed in the following three sections.</p>
<div id="the-smallest-effect-size-that-can-be-statistically-significant" class="section level3">
<h3>
<span class="header-section-number">2.2.1</span> The smallest effect size that can be statistically significant</h3>
<p>In Figure <a href="power.html#fig:power-effect1">2.1</a> the distribution of Cohen’s d given 15 participants per group is plotted when the true effect size is 0 (or the null-hypothesis is true), and when the true effect size is d = 0.5. The blue area is the Type 2 error rate (the probability of not finding p &lt; α, when there is a true effect, and α = 0.05). 1 - Type 2 error is the <strong>statistical power</strong>, or the probability of a test to yield a statistically significant result if the alternative hypothesis is true <span class="citation">(Aberson, <label for="tufte-mn-11" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">2019<span class="marginnote">Aberson, C. L. (2019). <em>Applied Power Analysis for the Behavioral Sciences: 2nd Edition</em> (2 edition). Routledge.</span>; Cohen, <label for="tufte-mn-12" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">1988<span class="marginnote">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed). L. Erlbaum Associates.</span>)</span>. Power depends on the Type 1 error rate (α), the true effect size in the population, and the number of observations.</p>
<div class="figure">
<span id="fig:power-effect1"></span>
<p class="caption marginnote shownote">
Figure 2.1: Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 15 per group.
</p>
<img src="Statistical_Inferences_files/figure-html/power-effect1-1.png" alt="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 15 per group." width="672">
</div>
<p>Because the true effect size is typically unknown, it makes most sense to speak about the power function. In Figure <a href="power.html#fig:power-1">2.2</a> you see a power curve for an independent <em>t</em>-test, with an alpha level of 0.05. We see that as the effect size (in Cohen’s d) increases, power increases. If the effect you study has an effect size of 0.5, you would have almost 70% power with 50 observations in each independent group (indicated by the red dot). If the true effect size is smaller, power is lower, and as the effect size is larger, power is larger.</p>
<div class="figure">
<span id="fig:power-1"></span>
<p class="caption marginnote shownote">
Figure 2.2: Power curve for an independent <em>t</em>-test as a function of the true effect size.
</p>
<img src="Statistical_Inferences_files/figure-html/power-1-1.png" alt="Power curve for an independent *t*-test as a function of the true effect size." width="672">
</div>
<p><strong>Power analysis</strong> is the practice of computing the effect size, desired power, sample size, or alpha level from the other three values. Computing the required sample size is commonly referred to as <strong>a-priori</strong> power analysis, computing the smallest effect size you have the desired power for is called <strong>sensitivity</strong> power analysis, computing the alpha level is called <strong>criterion</strong> power analysis, and computing the achieved power is called <strong>post-hoc</strong> power analysis.</p>
<p>You might seen graphs like Figure <a href="power.html#fig:power-effect1">2.1</a> before. The only thing I have done is to transform the <em>t</em>-value distribution that is commonly used in these graphs, and calculated the distribution for Cohen’s d. This is a straightforward transformation, but instead of presenting the critical <em>t</em>-value the figure provides the critical <em>d</em>-value. For a two-sided independent <em>t</em>-test, this is calculated as:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="power.html#cb16-1"></a><span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span>(a <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), (n1 <span class="op">+</span><span class="st"> </span>n2) <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span>n1 <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n2)</span></code></pre></div>
<p>where ‘a’ is the alpha level (e.g., 0.05) and N is the sample size in each independent group. For the example above, where alpha is 0.05 and n = 15:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="power.html#cb17-1"></a><span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span>(<span class="fl">0.05</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), (<span class="dv">15</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">15</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">15</span>)</span></code></pre></div>
<pre><code>## [1] 0.7479725</code></pre>
<p>The critical <em>t</em>-value (2.0484071) is also provided in commonly used power analysis software such as G*Power. We can compute the critical Cohen’s d from the <em>t</em>-value and sample size using: <span class="math inline">\(d = t{\sqrt{1/n_1 + 1/n_2}}\)</span>.</p>
<div class="figure">
<span id="fig:gcrit1"></span>
<p class="caption marginnote shownote">
Figure 2.3: The critical <em>t</em>-value is provided by G*Power software.
</p>
<img src="images/gpowcrit.png" alt="The critical *t*-value is provided by G\*Power software." width="378">
</div>
<p>When you will test an association between variables with a correlation, G*Power will directly provide you with the critical effect size. When you compute a correlation based on a two-sided test, your alpha level is 0.05, and you have 30 observations, only effects larger than r = 0.361 will be statistically significant. In other words, the effect needs to be quite large to even have the mathematical possibility of becoming statistically significant.</p>
<div class="figure">
<span id="fig:gcrit2"></span>
<p class="caption marginnote shownote">
Figure 2.4: The critical r is provided by G*Power software.
</p>
<img src="images/gpowcrit2.png" alt="The critical r is provided by G\*Power software." width="378">
</div>
<p>The critical effect size gives you information about the smallest effect size that, if observed, would by statistically significant. If you observe a smaller effect size, the <em>p</em>-value will be larger than your significance threshold. You always have some probability of observing effects larger than the critical effect size. After all, even if the null hypothesis is true, 5% of your tests will yield a significant effect. But what you should ask yourself is whether the effect sizes that could be statistically significant are realistically what you would expect to find. If this is not the case, it should be clear that there is little (if any) use in performing a significance test. Mathematically, when the critical effect size is larger than effects you expect, your statistical power will be less than 50% (see the section on <a href="power.html#observedpower">observed power</a>). If you perform a statistical test with less than 50% power, your single study is not very informative. Reporting the critical effect size in a feasibility justification should make you reflect on whether a hypothesis test will yield an informative answer to your research question.</p>
</div>
<div id="compute-the-width-of-the-confidence-interval-around-the-effect-size" class="section level3">
<h3>
<span class="header-section-number">2.2.2</span> Compute the width of the confidence interval around the effect size</h3>
<p>The second statistic to report alongside a feasibility justification is the width of the 95% confidence interval around the effect size. As we saw in the section on <a href="effectsizesCI.html#confint">confidence intervals</a> they represent a range that is wide enough so that in the long run in 95% of repetitions of the same experiment the true population parameter falls within each confidence interval calculated around the observed effect size. Cumming <span class="citation">(<label for="tufte-mn-13" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">2013<span class="marginnote">Cumming, G. (2013). <em>Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis</em>. Routledge.</span>)</span> calls the difference between the observed effect size and the upper 95% confidence interval (or the lower 95% confidence interval) the margin of error (MOE).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="power.html#cb19-1"></a><span class="co"># Compute the effect size d and 95% CI</span></span>
<span id="cb19-2"><a href="power.html#cb19-2"></a>res &lt;-<span class="st"> </span><span class="kw">d.ind.t</span>(<span class="dt">m1 =</span> <span class="dv">0</span>, <span class="dt">m2 =</span> <span class="dv">0</span>, <span class="dt">sd1 =</span> <span class="dv">1</span>, <span class="dt">sd2 =</span> <span class="dv">1</span>, <span class="dt">n1 =</span> <span class="dv">15</span>, <span class="dt">n2 =</span> <span class="dv">15</span>, <span class="dt">a =</span> <span class="fl">.05</span>)</span>
<span id="cb19-3"><a href="power.html#cb19-3"></a><span class="co"># Print the result</span></span>
<span id="cb19-4"><a href="power.html#cb19-4"></a>res<span class="op">$</span>estimate</span></code></pre></div>
<pre><code>## [1] "$d_s$ = 0.00, 95\\% CI [-0.72, 0.72]"</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="power.html#cb21-1"></a><span class="co"># Compute the average Margin of Error (MOE)</span></span>
<span id="cb21-2"><a href="power.html#cb21-2"></a>(res<span class="op">$</span>dhigh <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>dlow)<span class="op">/</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="power.html#cb23-1"></a><span class="co"># For non-zero effects the CI is based on the non-central t</span></span>
<span id="cb23-2"><a href="power.html#cb23-2"></a><span class="co"># The MOE is then asymmetric (but differences are very small)</span></span>
<span id="cb23-3"><a href="power.html#cb23-3"></a>(res<span class="op">$</span>d <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>dlow)</span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="power.html#cb25-1"></a>(res<span class="op">$</span>dhigh <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>d)</span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<p>If we compute the 95% CI for an effect size of 0, we see that with 15 observations in each condition of an independent <em>t</em>-test the 95% CI ranges from -0.72 to 0.72. The MOE is half the width of the 95% CI, 0.72. This clearly shows we have a very imprecise estimate. A Bayesian estimator who uses an uninformative prior would compute a credible interval with the same upper and lower bound <span class="citation">(Albers et al., <label for="tufte-mn-14" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-14" class="margin-toggle">2018<span class="marginnote">Albers, C. J., Kiers, H. A. L., &amp; Ravenzwaaij, D. van. (2018). Credible Confidence: A Pragmatic View on the Frequentist vs Bayesian Debate. <em>Collabra: Psychology</em>, <em>4</em>(1), 31. <a href="https://doi.org/10.1525/collabra.149">https://doi.org/10.1525/collabra.149</a></span>)</span>, and might conclude they personally believe there is a 95% chance the true effect size lies in this interval. A frequentist would reason more hypothetically: If the observed effect size in the data I plan to collect is 0, I could only reject effects more extreme than d = 0.72 in an equivalence test with a 5% alpha level (even though if such a test would be performed, power might be low, depending on the true effect size). Regardless of the statistical philosophy you plan to rely on when analyzing the data, our evaluation of what we can conclude based on the width of our interval tells us we will not learn a lot. Effect sizes in the range of d = 0.7 are findings such as “People become aggressive when they are provoked”, “People prefer their own group to other groups”, and “Romantic partners resemble one another in physical attractiveness” <span class="citation">(Richard et al., <label for="tufte-mn-15" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-15" class="margin-toggle">2003<span class="marginnote">Richard, F. D., Bond, C. F., &amp; Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. <em>Review of General Psychology</em>, <em>7</em>(4), 331–363. <a href="https://doi.org/10.1037/1089-2680.7.4.331">https://doi.org/10.1037/1089-2680.7.4.331</a></span>)</span>. The width of the confidence interval tells you that you can only reject the presence of effects that are so large, if they existed, you would probably already have noticed them. It might still be important to establish these large effects in a well-controlled experiment. But since most effect sizes in we should realistically expect are much smaller, we do not learn something we didn’t already know from the data that plan to collect. Even without data, we would exclude effects larger than d = 0.7 in most research lines.</p>
<p>We see this the MOE is almost, but not exactly, the same as the critical effect size d we observed above (d = 0.7479725). The reason for this is that the 95% confidence interval is calculated based on the <em>t</em>distribution. If the true effect size is not zero, the confidence interval is calculated based on the non-central <em>t</em>distribution, and the 95% CI is asymmetric. Figure <a href="power.html#fig:noncentralt">2.5</a> vizualizes three <em>t</em>-distributions, one symmetric at 0, and two asymmetric distributions with a noncentrality parameter of 2 and 3. The asymmetry is most clearly visible in very small samples (the distribution in the plot have 5 degrees of freedom) but remain noticeable when calculating confidence intervals and statistical power. For example, for a true effect size of d = 0.5 the 95% CI is <span class="math inline">\(d_s\)</span> = 0.50, 95\% CI [-0.23, 1.22]. The MOE based on the lower bound is 0.7317584 and based on the upper bound is 0.7231479. If we compute the 95% CI around the critical effect size (d = 0.7479725) we see the 95% CI ranges from exactly 0.00 to 1.48. As explained in the section on the relation between a <a href="effectsizesCI.html#relatCIp">confidence interval and a <em>p</em>-value</a>, if the 95% CI excludes zero, the test is statistically significant. In this case the lowerbound of the confidence interval exactly touches 0, which means we would observe a <em>p</em> = 0.05 if we exactly observed the critical effect size.</p>
<div class="figure">
<span id="fig:noncentralt"></span>
<p class="caption marginnote shownote">
Figure 2.5: Central (black) and 2 non-central (red and blue) <em>t</em>-distributions.
</p>
<img src="Statistical_Inferences_files/figure-html/noncentralt-1.png" alt="Central (black) and 2 non-central (red and blue) *t*-distributions." width="672">
</div>
<p>Where computing the critical effect size can make it clear that a <em>p</em>-value is of little interest, computing the 95% CI around the effect size can make it clear that the effect size estimate is of little value. It will often be so uncertain, and the range of effect sizes you will not be able to reject if there is no effect is so large, the effect size estimate is not very useful. This is also the reason why performing a pilot study to estimate an effect size for an a-priori power analysis is not a sensible strategy <span class="citation">(Albers &amp; Lakens, <label for="tufte-mn-16" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-16" class="margin-toggle">2018<span class="marginnote">Albers, C. J., &amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. <em>Journal of Experimental Social Psychology</em>, <em>74</em>, 187–195. <a href="https://doi.org/10.1016/j.jesp.2017.09.004">https://doi.org/10.1016/j.jesp.2017.09.004</a></span>; Leon et al., <label for="tufte-mn-17" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle">2011<span class="marginnote">Leon, A. C., Davis, L. L., &amp; Kraemer, H. C. (2011). The Role and Interpretation of Pilot Studies in Clinical Research. <em>Journal of Psychiatric Research</em>, <em>45</em>(5), 626–629. <a href="https://doi.org/10.1016/j.jpsychires.2010.10.008">https://doi.org/10.1016/j.jpsychires.2010.10.008</a></span>)</span>. However, it is possible that the sample size is large enough to exclude some effect sizes that are still a-priori plausible. For example, with 50 observations in each independent group, you have 82% <a href="equivalencetest.html#powerequiv">power for an equivalence test</a> with bounds of -0.6 and 0.6, and if effect larger than 0.6 can be rejected, this might be sufficient to tentatively start to question claims of even larger effects in the literature.</p>
</div>
<div id="sensitivitypower" class="section level3">
<h3>
<span class="header-section-number">2.2.3</span> Plot a sensitivity power analysis</h3>
<p>In a <strong>sensitivity power analysis</strong> the sample size and the alpha level are fixed, and you compute the effect size you have the desired statistical power to detect. For example, in Figure <a href="power.html#fig:gsens0">2.6</a> the sample size in each group is set to 15, the alpha level is 0.05, and the desired power is set to 90%. The sensitivity power analysis shows we have 90% power to detect an effect of d = 1.23.</p>
<div class="figure">
<span id="fig:gsens0"></span>
<p class="caption marginnote shownote">
Figure 2.6: Sensitivity power analysis in G*Power software.
</p>
<img src="images/sensitivity0.png" alt="Sensitivity power analysis in G\*Power software." width="378">
</div>
<p>Perhaps you feel a power of 90% is a bit high, and you would be happy with 80% power. We can plot a sensitivity curve across all possible levels of statistical power. In Figure <a href="power.html#fig:gsens1">2.7</a> we see that if we desire 80% power, the effect size should be d = 1.06. The smaller the true effect size, the lower the power we have. This plot should again remind us not to put too much faith in a significance test when are sample size is small, since for 15 observations in each condition, statistical power is very low for anything but extremely large effect sizes.</p>
<div class="figure">
<span id="fig:gsens1"></span>
<p class="caption marginnote shownote">
Figure 2.7: Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05.
</p>
<img src="images/sensitivity1.png" alt="Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05." width="485">
</div>
<p>If we look at the effect size that we would have 50% power for, we see it is d = 0.7411272. This is very close to our critical effect size of d = 0.7479725 (the smallest effect size that, if observed, would be significant). The difference is due to the non-central <em>t</em>-distribution (see Figure <a href="power.html#fig:noncentralt">2.5</a>. If the distribution was symmetric, observing an effect size exactly on the critical value would mean half of the distribution is smaller than the critical effect size, and half of the distribution is larger, and we would have exactly 50% power for the critical effect size. Because the distribution is not symmetrical, we need to find the critical effect size for which it <em>is</em> true that half the distribution falls below it, and half the distribution falls above it. This value can’t be calculated directly, and requires an iterative procedure that optimizes the values of the non-centrality parameter such that power is exactly 50% <span class="citation">(Smithson, <label for="tufte-mn-18" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle">2003<span class="marginnote">Smithson, M. (2003). <em>Confidence intervals</em>. Sage Publications.</span>)</span>. The <code>pwr</code> package in R can calculate this effect size in a sensitivity analysis where we enter the sample size (per group), the alpha level, and the desired power for a two-sided independent <em>t</em>-test, which will return d = 0.7411272. We can check this true effect size would indeed give with 50% power.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="power.html#cb27-1"></a><span class="co"># Sensitivity power analysis</span></span>
<span id="cb27-2"><a href="power.html#cb27-2"></a>pwr<span class="op">::</span><span class="kw">pwr.t.test</span>(<span class="dt">n =</span> <span class="dv">15</span>, </span>
<span id="cb27-3"><a href="power.html#cb27-3"></a>                <span class="dt">sig.level =</span> <span class="fl">0.05</span>, </span>
<span id="cb27-4"><a href="power.html#cb27-4"></a>                <span class="dt">power =</span> <span class="fl">0.5</span>, </span>
<span id="cb27-5"><a href="power.html#cb27-5"></a>                <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb27-6"><a href="power.html#cb27-6"></a>                <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 15
##               d = 0.7411272
##       sig.level = 0.05
##           power = 0.5
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="power.html#cb29-1"></a><span class="co"># d = 0.7411272 - let's check this indeed gives 50% power</span></span>
<span id="cb29-2"><a href="power.html#cb29-2"></a><span class="co"># We compute the non-centrality parameter delta as 2.0296604</span></span>
<span id="cb29-3"><a href="power.html#cb29-3"></a><span class="co"># delta &lt;- 0.7411272 * sqrt(15/2)</span></span>
<span id="cb29-4"><a href="power.html#cb29-4"></a></span>
<span id="cb29-5"><a href="power.html#cb29-5"></a><span class="co"># For a 2-sided test power is the probability of finding a result that is</span></span>
<span id="cb29-6"><a href="power.html#cb29-6"></a><span class="co"># 1) larger than the critical value of 2.0484071, which is: 0.4999563</span></span>
<span id="cb29-7"><a href="power.html#cb29-7"></a><span class="co"># 2) We need to add the tiny probability that we find a significant result in</span></span>
<span id="cb29-8"><a href="power.html#cb29-8"></a><span class="co"># the opposite direction. This will rarely happen: 0.00004372648</span></span>
<span id="cb29-9"><a href="power.html#cb29-9"></a><span class="co"># Together these two probabilities make exactly 50%:</span></span>
<span id="cb29-10"><a href="power.html#cb29-10"></a><span class="co"># 1-pt(2.0484071, 28, 2.0296604) + pt(-2.0484071, 28, 2.0296604)</span></span></code></pre></div>
</div>
<div id="reporting-a-feasibility-justification." class="section level3">
<h3>
<span class="header-section-number">2.2.4</span> Reporting a feasibility justification.</h3>
<p>To summarize, I recommend addressing the following components in a feasibility sample size justification. Addressing these points explicitly will allow you to evaluate for yourself if collecting the data will have scientific value. If not, there might be other reasons to collect the data. For example, at our department, students often collect data as part of their education. However, if the primary goal of data collection is educational, the sample size that is collected can be very small. It is often educational to collect data from a small number of participants to experience what data collection looks like in practice, but there is often no educational value in collecting data from more than 10 participants. Despite the small sample size, we often require students to report statistical analyses as part of their education, which is fine as long as it is clear the numbers that are calculated can not meaningfully be interpreted. Table <a href="power.html#tab:table-pow-rec">2.2</a> should help to evaluate if the interpretation of statistical tests has any value, or not.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-pow-rec">Table 2.2: </span>Overview of recommendations when reporting a sample size justification based on feasibility.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to address
</th>
<th style="text-align:left;">
Example?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Will a future meta-analysis be performed?
</td>
<td style="text-align:left;">
Consider the plausibility that sufficient highly similar studies will be performed in the future to, eventually, make a meta-analysis possible
</td>
</tr>
<tr>
<td style="text-align:left;">
Will a decision be made, regardless of the amount of data that is available?
</td>
<td style="text-align:left;">
If it is known that a decision will be made, with or without data, then any data you collect will reduce error rates.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the critical effect size?
</td>
<td style="text-align:left;">
Report and interpret the critical effect size, with a focus on whether a hypothesis test would even be significant for expected effect sizes. If not, indicate you will not interpret the data based on <em>p</em>-values.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the width of the confidence interval?
</td>
<td style="text-align:left;">
Report and interpret the width of the confidence interval. What will an estimate with this much uncertainty be useful for? If the null hypothesis is true, would rejecting effects outside of the confidence interval be worthwhile (ignoring you might have low power to actually test against these values)?
</td>
</tr>
<tr>
<td style="text-align:left;">
Which effect sizes would you have decent power to detect?
</td>
<td style="text-align:left;">
Report a sensitivity power analysis, and report the effect sizes you could detect across a range of desired power levels (e.g., 80%, 90%, and 95%), or plot a sensitivity curve of effect sizes against desired power.
</td>
</tr>
</tbody>
</table>
<p>If the study is not performed for educational purposes, but the goal is answer a research question, the feasibility justification might indicate that there is no value in collecting the data. If it wasn’t possible to conclude that one should not proceed with the data collection, there is no use of justifying the sample size. There should be cases where it is unlikely there will ever be enough data to perform a meta-analysis (for example because of a lack of general interest in the topic), the information will not be used to make any decisions, and the statistical tests do not allow you to test a hypothesis or estimate an effect size estimate with any useful accuracy. It should be a feasibility justification - not a feasibility excuse. If there is no good justification to collect the maximum number of observations that is feasible, performing the study nevertheless is a waste of participants time, and/or a waste of money if data collection has associated costs. Collecting data without a good justification why the planned sample size will yield worthwhile information has an ethical component. As Button and colleagues <span class="citation">Button et al. (<label for="tufte-mn-19" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle">2013<span class="marginnote">Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. <em>Nature Reviews Neuroscience</em>, <em>14</em>(5), 365–376. <a href="https://doi.org/10.1038/nrn3475">https://doi.org/10.1038/nrn3475</a></span>)</span> write:</p>
<blockquote>
<p>Low power therefore has an ethical dimension — unreliable research is inefficient and wasteful. This applies to both human and animal research".</p>
</blockquote>
<p>Think carefully if you can defend data collection based on a feasibility justification. Sometimes data collection is just not feasible, and we should accept this.</p>
</div>
</div>
<div id="a-priori-power-analysis" class="section level2">
<h2>
<span class="header-section-number">2.3</span> A-priori power analysis</h2>
<p>When designing a study where the goal is to observe a statistically significant effect, researchers often want to make sure their sample size is large enough to have sufficient power to detect effects they expect, or effects they are interested in observing. This is done by performing an <em>a-priori</em> power analysis. Given a specified effect size, alpha level, and desired power, an a-priori power analysis will inform you about the sample size you need to collect. In Figure <a href="power.html#fig:power-2">2.8</a> you see how the statistical power increases as the number of observations (per group) in an independent <em>t</em>-test with an alpha level of 0.05 increases.</p>
<div class="figure">
<span id="fig:power-2"></span>
<p class="caption marginnote shownote">
Figure 2.8: Power curve for an independent <em>t</em>-test as a function of the sample size.
</p>
<img src="Statistical_Inferences_files/figure-html/power-2-1.png" alt="Power curve for an independent *t*-test as a function of the sample size." width="672">
</div>
<p>A-priori power calculations are performed under the assumption that there is an effect. In practice, it is of course also possible that there is no effect (e.g., d = 0). If there is no true effect, and you want to use formally correct language, power is <em>undefined</em>. Statistical power is a concept that can only be computed assuming there is a true effect. If there is no true effect, you will still observe significant effects, but these ar <em>Type 1 errors</em>, or false positives. These occur at your chosen alpha level (e.g., 5% of the time). If you perform a hypothesis test, there are four possible outcomes:</p>
<ol style="list-style-type: decimal">
<li>False positives or Type 1 errors, indicated by α (you observe a significant test result when H0 is true)</li>
<li>False negatives or Type 2 errors, indicated by β (you observe a non-significant result when H1 is true)</li>
<li>True negatives, indicated by 1-α (a non-significant result when H0 is true)</li>
<li>True positives, indicated by 1-β (a significant test result when H1 is true)</li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:NHST-outcomes"></span>
<img src="images/2.1.1.png" alt="Four possible outcomes in a null hypothesis significance test." width="961"><!--
<p class="caption marginnote">-->Figure 2.9: Four possible outcomes in a null hypothesis significance test.<!--</p>-->
<!--</div>--></span>
</p>
<p>The goal of an a-priori power analysis is to increase the sample size up to the level that the desired power, the probability of finding a significant result if there is a true effect, or 1-β, is at a desired level for an effect size one is interested in detecting, given a specific alpha level.</p>
<p>In Figure <a href="power.html#fig:power-3">2.10</a> you see two distributions. The left (grey) distribution is centered at 0. This is our model for the null hypothesis. If the null hypothesis is true we can find statistically significant results if the effect size is extreme enough (in a two-sided test either in the positive or negative direction), but these would be Type 1 errors (the red areas under the curve).</p>
<p>The right (black) distribution is centered at an effect of d = 0.5. This is our model for the alternative hypothesis, where we expect a true effect in the population of d = 0.5. Even though there is a true effect, we will not always find a statistically significant result. This happens when, due to random variation, the observed effect size is too close to 0 to be statistically significant. These results would be false negative results (the blue area under the curve).</p>
<div class="figure">
<span id="fig:power-3"></span>
<p class="caption marginnote shownote">
Figure 2.10: Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group.
</p>
<img src="Statistical_Inferences_files/figure-html/power-3-1.png" alt="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group." width="672">
</div>
<p>If we want to increase the statistical power, we are trying to reduce the size of the blue area. One way to do this would be to increase the effect size. This would shift the entire distribution to the right, and reduce the size of the blue area. Although there are ways in controlled experiments to increase the standardized effect size (e.g., by reducing statistical variation in the data), often the effect size is what it is. All we can do is increase the sample size. This will make the distribution around 0.5 more narrow, and this reduces the blue area. You can check this in an online shiny app that <a href="http://shiny.ieis.tue.nl/d_p_power/">reproduces the plot</a>.</p>
<div id="performing-a-power-analysis." class="section level3">
<h3>
<span class="header-section-number">2.3.1</span> Performing a power analysis.</h3>
<p>There is a wide range of software tools that can help you to perform an a-priori power analysis. Sometimes statistical power can be analytically computed based on closed formulas, and in other situations power can be computed by performing simulations. If we simulate thousands of studies, and count the percentage of studies that are statistically significant, we have an estimate of the statistical power of the test. The code below computes the statistical power (using the <code>power.t.test</code> in base R) assuming an effect size of d = 0.5, and alpha level of 0.05, and a desired power of 90%.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="power.html#cb30-1"></a><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="fl">0.5</span>, </span>
<span id="cb30-2"><a href="power.html#cb30-2"></a>             <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb30-3"><a href="power.html#cb30-3"></a>             <span class="dt">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb30-4"><a href="power.html#cb30-4"></a>             <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb30-5"><a href="power.html#cb30-5"></a>             <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 85.03129
##           delta = 0.5
##              sd = 1
##       sig.level = 0.05
##           power = 0.9
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>The output tells us we need to collect 85.03 observations. Because observations do not come in decimals (we can hardly cut 0.03 from a participant) we need to collect 86 observations. We could also get this answer through simulations.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="power.html#cb32-1"></a><span class="kw">set.seed</span>(<span class="dv">600746</span>) <span class="co">#set a seed for reproducible simulations</span></span>
<span id="cb32-2"><a href="power.html#cb32-2"></a>p &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">100000</span>) <span class="co">#set up empty variable to store all simulated p-values</span></span>
<span id="cb32-3"><a href="power.html#cb32-3"></a></span>
<span id="cb32-4"><a href="power.html#cb32-4"></a><span class="co">#Run simulation</span></span>
<span id="cb32-5"><a href="power.html#cb32-5"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100000</span>){ <span class="co">#for each simulated experiment</span></span>
<span id="cb32-6"><a href="power.html#cb32-6"></a>  x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 1</span></span>
<span id="cb32-7"><a href="power.html#cb32-7"></a>  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="fl">0.5</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 2</span></span>
<span id="cb32-8"><a href="power.html#cb32-8"></a>  z &lt;-<span class="st"> </span><span class="kw">t.test</span>(x, y) <span class="co">#perform the t-test</span></span>
<span id="cb32-9"><a href="power.html#cb32-9"></a>  p[i] &lt;-<span class="st"> </span>z<span class="op">$</span>p.value <span class="co">#get the p-value and store it</span></span>
<span id="cb32-10"><a href="power.html#cb32-10"></a>}</span>
<span id="cb32-11"><a href="power.html#cb32-11"></a></span>
<span id="cb32-12"><a href="power.html#cb32-12"></a><span class="co">#Calculate power: sum significant p-values, divide by 100000 simulations</span></span>
<span id="cb32-13"><a href="power.html#cb32-13"></a>(<span class="kw">sum</span>(p <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>)<span class="op">/</span><span class="dv">100000</span>) <span class="co">#power</span></span></code></pre></div>
<pre><code>## [1] 0.90015</code></pre>
<p>We see that after 100000 simulations with 85 participants, the estimated power is 0.90015 with 85 participants in each group. The analytic solution tells us power is 0.899894. The more simulations you perform, the more accurate the power estimate will be (as we see, 100000 simulations gives very accurate results).</p>
<p>The way you perform the power analysis depends on the software you use. There are excellent software packages for power analysis, such as <a href="https://www.ncss.com/software/pass/">PASS</a> or <a href="https://stats.idre.ucla.edu/sas/seminars/proc-power/">SAS</a>, but these solutions are rather expensive. There are also freeware solutions, such as the widely used <a href="https://gpower.hhu.de">G*Power</a>, <a href="https://wiki.usask.ca/pages/viewpageattachments.action?pageId=420413544">MorePower</a> by Campbell and Thompson, and <a href="https://jakewestfall.shinyapps.io/pangea/">PANGEA</a> by Jake Westfall.
There is also a range of options in R. The default <code>stats</code> package has power functions for <em>t</em>-tests, proportions, and ANOVA, the <code>pwr</code> package has a wider range of options, <code>pwr2ppl</code> by Chris Aberson accompanies his excellent book on power analyses <span class="citation">(Aberson, <label for="tufte-mn-20" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle">2019<span class="marginnote">Aberson, C. L. (2019). <em>Applied Power Analysis for the Behavioral Sciences: 2nd Edition</em> (2 edition). Routledge.</span>)</span>, <code>powerlmm</code> by Kristoffer Magnusson performs power analyses for <a href="https://github.com/rpsychologist/powerlmm">two- and three-level linear mixed models</a>, and <a href="https://aaroncaldwell.us/SuperpowerBook/">Superpower</a> created by Aaron Caldwell and myself does power analyses for complex ANOVA designs.</p>
<p>It takes time to learn to use this software correctly. You will need to sit down for a few hours and go through vignettes or read the accompanying publications, before you know what to do. In our online manual for Superpower, we compare Superpower against G*Power, <code>pwr</code>, <code>pwr2ppl</code> and MorePower. Each software package will have slightly different design philosophies. In G*Power you are expected to enter the standardized effect size d, while in MorePower we enter the mean difference and the standard deviation, and in Superpower we need to enter the means and standard deviations for each condition. In general I recommend to always think about the raw pattern of means you expect, before you perform a power analysis. This typically leads to more realistic effect sizes estimates than the use of standardized effect sizes (where researchers all too often ‘expect’ a d = 0.5 just because that is the default value in G*Power).</p>
<div class="figure">
<span id="fig:powerex1"></span>
<p class="caption marginnote shownote">
Figure 2.11: Example of a power analysis in G*Power.
</p>
<img src="images/powerex1.png" alt="Example of a power analysis in G\*Power." width="378">
</div>
<div class="figure">
<span id="fig:powerex2"></span>
<p class="caption marginnote shownote">
Figure 2.12: Example of a power analysis in MorePower.
</p>
<img src="images/powerex2.png" alt="Example of a power analysis in MorePower." width="222">
</div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="power.html#cb34-1"></a>pwr<span class="op">::</span><span class="kw">pwr.t.test</span>(<span class="dt">d =</span> <span class="fl">0.5</span>, </span>
<span id="cb34-2"><a href="power.html#cb34-2"></a>                <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb34-3"><a href="power.html#cb34-3"></a>                <span class="dt">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb34-4"><a href="power.html#cb34-4"></a>                <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb34-5"><a href="power.html#cb34-5"></a>                <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 63.76561
##               d = 0.5
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="power.html#cb36-1"></a>design_result &lt;-<span class="st"> </span>Superpower<span class="op">::</span><span class="kw">ANOVA_design</span>(<span class="dt">design =</span> <span class="st">"2b"</span>,</span>
<span id="cb36-2"><a href="power.html#cb36-2"></a>                                          <span class="dt">n =</span> <span class="dv">86</span>,</span>
<span id="cb36-3"><a href="power.html#cb36-3"></a>                                          <span class="dt">mu =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>),</span>
<span id="cb36-4"><a href="power.html#cb36-4"></a>                                          <span class="dt">sd =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb36-5"><a href="power.html#cb36-5"></a>                                          <span class="dt">plot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb36-6"><a href="power.html#cb36-6"></a></span>
<span id="cb36-7"><a href="power.html#cb36-7"></a><span class="co"># Plot power curve (from 5 to 200)</span></span>
<span id="cb36-8"><a href="power.html#cb36-8"></a>Superpower<span class="op">::</span><span class="kw">plot_power</span>(design_result, <span class="dt">max_n =</span> <span class="dv">200</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:powerex3"></span>
<img src="Statistical_Inferences_files/figure-html/powerex3-1.png" alt="Example of a power analysis plot in Superpower." width="672"><p class="caption marginnote shownote">
Figure 2.13: Example of a power analysis plot in Superpower.
</p>
</div>
</div>
<div id="justifying-the-effect-size-used-in-an-a-priori-power-analysis" class="section level3">
<h3>
<span class="header-section-number">2.3.2</span> Justifying the effect size used in an a-priori power analysis</h3>
<p>One challenge in power analysis is that you never know the true effect size. This leads to the ‘sample size samba’ <span class="citation">(Schulz &amp; Grimes, <label for="tufte-mn-21" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle">2005<span class="marginnote">Schulz, K. F., &amp; Grimes, D. A. (2005). Sample size calculations in randomised trials: Mandatory and mystical. <em>The Lancet</em>, <em>365</em>(9467), 1348–1353.</span>)</span>. Researchers go back and forth between the effect size they expect, and the sample size they are willing to collect, until they ‘expect’ the effect size that, in an a-priori power analysis, leads to the sample size they are willing to collect. This practice obviously makes a power analysis a useless procedure. If you perform an a-priori power analysis, it is essential that you have a good justification for the effect size you rely on, or the result of the power analysis has no value.</p>
<div id="using-a-smallest-effect-size-of-interest" class="section level4">
<h4>
<span class="header-section-number">2.3.2.1</span> Using a smallest effect size of interest</h4>
<p>Best practice is to justify an a-priori power analysis based on a smallest effect size of interest. Because the population effect size is always uncertain (indeed, estimating this is typically one of the goals of the study) this means there is considerable uncertainty about the achieved power in any study (see Figure <a href="power.html#fig:power-1">2.2</a>). However, there the smallest effect size of interest is not uncertain. A smallest effect of interest may be subjective (one researcher might find effect sizes smaller than d = 0.3 meaningless, while another researcher might still be interested in effects up to d = 0.1), but it does not have uncertainty. This means that if you enter it in an a-priori power analyses, you will be guaranteed to have the desired power (or higher power) for the smallest effect size you care about. The topic of how to justify a smallest effect size of interest is discussed in a <a href="sesoi.html#sesoi">dedicated chapter</a>.</p>
</div>
<div id="using-an-estimate-of-a-meta-analysis" class="section level4">
<h4>
<span class="header-section-number">2.3.2.2</span> Using an estimate of a meta-analysis</h4>
<p>As we will see in the chapter on <a href="bias.html#bias">bias</a> it is regrettably not true that effect size estimates from meta-analyses are always accurate. They can be biased, sometimes substantially so. Furthermore, meta-analyses typically have considerable <a href="meta.html#heterogeneity">heterogeneity</a>, which means that the meta-analytic effect size estimate differs for subsets of studies that make up the meta-analysis. So, although it might seem useful to enter a meta-analytic effect size estimate in your power analysis, you need to take great care before doing so.</p>
<p>If you want to enter a meta-analytic effect size estimate in an a-priori power analysis, you need to consider three things. First, the studies included in the meta-analysis should be similar enough to the study you are performing that it is reasonable to expect a similar effect size. In essence, this requires evaluating the generalizability of the effect size estimate to your study. Carefully consider differences between the meta-analysed studies and your study, with respect to the manipulation, the measure, the population, and any other relevant variables. Second, check whether the effect sizes reported in the meta-analysis are homogeneous. If not, and there is considerable heterogeneity in the meta-analysis, it means not all included studies can be expected to have the same true effect size estimate. This is almost always true. Choose the meta-analytic estimate for the subset of studies that most closely represent your planned study (this is why it is important to make sure others can <a href="meta.html#metareporting">reproduce your meta-analysis</a>. Third, the meta-analytic effect size estimate should not be biased. Check the bias detection tests that are reported in the meta-analysis, and consider bias corrected effect size estimates (even though these estimates might still be biased, and not reflect the true population effect size).</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:tablemetajust">Table 2.3: </span>Overview of recommendations when justifying the use of a meta-analytic effect size estimate for a power analysis.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to take into account
</th>
<th style="text-align:left;">
How to take it into account?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Are the studies in the meta-analysis similar?
</td>
<td style="text-align:left;">
Are the studies in the meta-analyses very similar in design, measure, an population to the study you are planning? Evalaute the generalizability of the effect size estimate to your study.
</td>
</tr>
<tr>
<td style="text-align:left;">
Are the studies in the meta-analysis homogeneous?
</td>
<td style="text-align:left;">
Is there heterogeneity in the meta-analysis? If so, use the meta-analytic effect size estimate of the most relevant homogenous subsample.
</td>
</tr>
<tr>
<td style="text-align:left;">
Is the effect size estimate unbiased?
</td>
<td style="text-align:left;">
Did the original study report bias detection tests, and was there bias? If so, it might be wise to use a more conservative effect size estimate, based on bias correction techniques (e.g., PET-PEESE) while acknowledging these corrected effect size estimates might not represent the true meta-analytic effect size estimate.
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-an-estimate-from-a-previous-study" class="section level4">
<h4>
<span class="header-section-number">2.3.2.3</span> Using an estimate from a previous study</h4>
<p>Statisticians have warned against using effect size estimates from small samples in power analyses. Leon, Davis, and Kraemer <span class="citation">(<label for="tufte-mn-22" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-22" class="margin-toggle">2011<span class="marginnote">Leon, A. C., Davis, L. L., &amp; Kraemer, H. C. (2011). The Role and Interpretation of Pilot Studies in Clinical Research. <em>Journal of Psychiatric Research</em>, <em>45</em>(5), 626–629. <a href="https://doi.org/10.1016/j.jpsychires.2010.10.008">https://doi.org/10.1016/j.jpsychires.2010.10.008</a></span>)</span> write:</p>
<blockquote>
<p>Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples.</p>
</blockquote>
<p>The two main reasons researchers should be careful when using effect sizes from the published literature in power analyses is that effect size estimates from small studies are inaccurate, and that publication bias inflates effect sizes. Bias can even emerge when researchers do not take an effect size estimate from the literature, but perform the pilot study themselves, due to <strong>follow-up bias</strong> <span class="citation">(Albers &amp; Lakens, <label for="tufte-mn-23" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-23" class="margin-toggle">2018<span class="marginnote">Albers, C. J., &amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. <em>Journal of Experimental Social Psychology</em>, <em>74</em>, 187–195. <a href="https://doi.org/10.1016/j.jesp.2017.09.004">https://doi.org/10.1016/j.jesp.2017.09.004</a></span>)</span>. Figure <a href="power.html#fig:follow-up-bias">2.14</a> illustrates that for a 3 group ANOVA with 15 participants in each group, only effects with a partial eta squared larger than 0.133 will be statistically significant. But even if researchers are willing to follow up on smaller effect sizes, entering an effect size estimate of <span class="math inline">\(\eta_p^2\)</span> = 0.01 in an a-priori power analysis would require a total sample size of 957 observations for 80% power. If researchers only follow up on pilot studies when they leads to feasible sample sizes, their effect size estimates will be biased.</p>
<div class="figure">
<span id="fig:follow-up-bias"></span>
<p class="caption marginnote shownote">
Figure 2.14: Distribution of partial eta squared under H0 (grey curve) and a true effect of 0.0588 (black curve).
</p>
<img src="Statistical_Inferences_files/figure-html/follow-up-bias-1.png" alt="Distribution of partial eta squared under H0 (grey curve) and a true effect of 0.0588 (black curve)." width="672">
</div>
<p>In essence, the problem illustrated in Figure <a href="power.html#fig:follow-up-bias">2.14</a> is that the effect sizes we end up using four our power analysis do not come from a full <em>f</em>-distribution, but that we ignore small effect sizes, and thus perform power analyses based on a <strong>truncated <em>F</em>-distribution</strong>. It is possible to correct for bias, using some assumptions about bias. For example, imagine we observe a result in the literature for a One-Way ANOVA with 3 conditions, reported as <em>F</em>(2, 42) = 0.017, <span class="math inline">\(\eta_p^2\)</span> = 0.176. Taking this effect size at face value, entering it in an a-priori power analysis would suggest we need to collect 27 observations in each condition. However, using the <code>BUCSS</code> R package <span class="citation">(Anderson et al., <label for="tufte-mn-24" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-24" class="margin-toggle">2017<span class="marginnote">Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. <em>Psychological Science</em>, <em>28</em>(11), 1547–1562.</span>)</span>, we can perform a power analysis that attempts to correct for bias.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="power.html#cb37-1"></a><span class="kw">library</span>(BUCSS)</span>
<span id="cb37-2"><a href="power.html#cb37-2"></a><span class="kw">ss.power.ba</span>(</span>
<span id="cb37-3"><a href="power.html#cb37-3"></a>  <span class="dt">F.observed =</span> <span class="fl">4.5</span>,</span>
<span id="cb37-4"><a href="power.html#cb37-4"></a>  <span class="dt">N =</span> <span class="dv">45</span>,</span>
<span id="cb37-5"><a href="power.html#cb37-5"></a>  <span class="dt">levels.A =</span> <span class="dv">3</span>,</span>
<span id="cb37-6"><a href="power.html#cb37-6"></a>  <span class="dt">effect =</span> <span class="kw">c</span>(<span class="st">"factor.A"</span>),</span>
<span id="cb37-7"><a href="power.html#cb37-7"></a>  <span class="dt">alpha.prior =</span> <span class="fl">0.05</span>,</span>
<span id="cb37-8"><a href="power.html#cb37-8"></a>  <span class="dt">alpha.planned =</span> <span class="fl">0.05</span>,</span>
<span id="cb37-9"><a href="power.html#cb37-9"></a>  <span class="dt">assurance =</span> <span class="fl">0.5</span>,</span>
<span id="cb37-10"><a href="power.html#cb37-10"></a>  <span class="dt">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb37-11"><a href="power.html#cb37-11"></a>  <span class="dt">step =</span> <span class="fl">0.001</span></span>
<span id="cb37-12"><a href="power.html#cb37-12"></a>)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 73
## 
## [[2]]
## [1] 2.012</code></pre>
<p>This analysis suggests collecting 73 participants in each condition, based on a bias corrected (under a specific model of publication bias assuming only significant effects ( with <em>p</em> &lt; 0.05) being published) non-centrality parameter of 2.012. It is possible that the bias corrected non-centrality parameter is zero, in which case it is not possible to correct for bias (or the true effect size might be 0).</p>
<p>Instead of formally modeling a correction for publication bias, researchers can simply use a more conservative effect size estimate, for example by computing power based on the lower limit of 60% two-sided confidence interval around the effect size estimate <span class="citation">(Perugini et al., <label for="tufte-mn-25" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-25" class="margin-toggle">2014<span class="marginnote">Perugini, M., Gallucci, M., &amp; Costantini, G. (2014). Safeguard power as a protection against imprecise power estimates. <em>Perspectives on Psychological Science</em>, <em>9</em>(3), 319–332.</span>)</span>, which they refer to as <strong>safeguard power</strong>.</p>
<p>Both these approaches lead to a more conservative power analysis, but not necessarily a more accurate power analysis. It is simply not possible to perform an accurate power analysis on the basis of an effect size estimate from a study that might be biased and had a small sample size.</p>
<p>A sample size from a single study can be used if two conditions are met. First, there was a low risk of bias (e.g., the effect size estimate comes from a Registered Report, or from an analysis which results would not have impacted the likelihood of publication). Second, the sample size is large enough to yield relatively accurate effect size estimate, based on the width of a 95% CI around the observed effect size estimate. There is always uncertainty around the effect size estimate, and entering the upper and lower limit of the 95% CI around the effect size estimate might be educational about the consequences of the uncertainty that is present for an a-priori power analysis.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-es-just">Table 2.4: </span>Overview of recommendations when justifying the use of an effect size estimate from a single study.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to take into account
</th>
<th style="text-align:left;">
How to take it into account?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Is there a risk of bias?
</td>
<td style="text-align:left;">
Evaluate the possibility that you would not have used, or had access to, the effect size estimate if had been smaller. Examine the difference when entering the reported, and a bias corrected, effect sizes estimate in a power analysis.
</td>
</tr>
<tr>
<td style="text-align:left;">
How large is the uncertainty?
</td>
<td style="text-align:left;">
Studies with a small number of observations have large uncertainty. Consider the possibility of using a more conservative effect sizes estimate to reduce the possibility of an underpowered study for the true effect size (such as a safeguard power analysis).
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-a-heuristic" class="section level4">
<h4>
<span class="header-section-number">2.3.2.4</span> Using a heuristic</h4>
<p>The most commonly entered effect size estimate in an a-priori power analysis for an independent <em>t</em>-test is d = 0.5. The reason is that it is the default in G*Power. It is also an effect size that Cohen <span class="citation">(<label for="tufte-mn-26" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-26" class="margin-toggle">1988<span class="marginnote">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed). L. Erlbaum Associates.</span>)</span> referred to as a ‘medium’ effect size, which seems less pretentious than expecting a ‘large’ effect size (d = 0.8) and leads to a required sample size that is substantially more manageable than when entering a ‘small’ effect size (d = 0.2). Cohen would probably not have chosen d = 0.5 as a default option for a power analysis tool designed for psychologists, as he wrote (1988, p. 13):</p>
<blockquote>
<p>Many effects sought in personality, social, and clinical-psychological research are likely to be small effects as here defined, both because of the attenutation in validity of the measures employed and the subtlety of the issues frequently involved.</p>
</blockquote>
<p>The large variety in research topics in psychology means that any ‘default’ or ‘heuristic’ you use in an a-priori power analysis is wrong, and more importantly, it is likely to be more substantially wrong than using any of the other strategies outlines here. Cohen’s benchmarks should not be used in a-priori power analysis <span class="citation">(Correll et al., <label for="tufte-mn-27" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-27" class="margin-toggle">2020<span class="marginnote">Correll, J., Mellinger, C., McClelland, G. H., &amp; Judd, C. M. (2020). Avoid Cohen’s “Small”, “Medium”, and “Large” for Power Analysis. <em>Trends in Cognitive Sciences</em>, <em>24</em>(3), 200–207. <a href="https://doi.org/10.1016/j.tics.2019.12.009">https://doi.org/10.1016/j.tics.2019.12.009</a></span>)</span>.</p>
<p>Some researchers have wondered what a better default would be, if researchers have no other basis to decide upon an effect size for an a-priori power analysis. For example, sometimes it is recommended to perform a power analysis based on an average effect size in psychology. For example, Brysbaert <span class="citation">(<label for="tufte-mn-28" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-28" class="margin-toggle">2019b<span class="marginnote">Brysbaert, M. (2019b). How many participants do we have to include in properly powered experiments? A tutorial of power analysis with reference tables. <em>Journal of Cognition</em>, <em>2</em>(1), 16. <a href="https://doi.org/10.5334/joc.72">https://doi.org/10.5334/joc.72</a></span>)</span> recommends d = 0.4 as a default, which is approximately the average effect size in meta-meta-analyses, analysing effect sizes in entire scientific disciplines. However, such meta-analytic effect sizes estimated might be inflated due to publication bias, and there is huge heterogeneity in each field, so the average will often not be close to the effect size in the research line your are performing. As such, the use of such estimates is also not recommended.</p>
</div>
</div>
<div id="justifying-the-error-rates-used-in-an-a-priori-power-analysis" class="section level3">
<h3>
<span class="header-section-number">2.3.3</span> Justifying the error rates used in an a-priori power analysis</h3>
<p>TOO BE COMPLETED</p>
</div>
<div id="some-advice-when-using-gpower" class="section level3">
<h3>
<span class="header-section-number">2.3.4</span> Some advice when using G*Power</h3>
<p>G*Power is one of the most widely used software tools for power analysis <span class="citation">(Bakker et al., <label for="tufte-mn-29" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-29" class="margin-toggle">2020<span class="marginnote">Bakker, M., Veldkamp, C. L. S., Akker, O. R. van den, Assen, M. A. L. M. van, Crompvoets, E., Ong, H. H., &amp; Wicherts, J. M. (2020). Recommendations in pre-registrations and internal review board proposals promote formal power analyses but do not increase sample size. <em>PLOS ONE</em>, <em>15</em>(7), e0236079. <a href="https://doi.org/10.1371/journal.pone.0236079">https://doi.org/10.1371/journal.pone.0236079</a></span>)</span>. The option for power analysis for a Pearson’s correlation coefficient is under the Exact test family (Correlation: Bivariate normal model). The “Correlation: Point biserial model” option under the <em>t</em>-tests family is for correlations where one variable is dichotomous. The difference is small, but the required sample size is typically a few observations larger for Pearson’s correlation coefficient.</p>
<div class="figure">
<span id="fig:gpowcor"></span>
<p class="caption marginnote shownote">
Figure 2.15: The options for a power analysis for Pearson’s correlation(above) and the point biserial correlation (when one variable is dichotomous).
</p>
<img src="images/gpowcor.png" alt="The options for a power analysis for Pearson's correlation(above) and the point biserial correlation (when one variable is dichotomous)." width="377">
</div>
<p>Although the effect size for an independent <em>t</em>-test and dependent <em>t</em>-test are often both referred to as Cohen’s d, they differ, and are calculated in different ways. Cohen himself distinguished between <span class="math inline">\(d_s\)</span> and <span class="math inline">\(d_z\)</span> <span class="citation">(Lakens, <label for="tufte-mn-30" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-30" class="margin-toggle">2013<span class="marginnote">Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. <em>Frontiers in Psychology</em>, <em>4</em>. <a href="https://doi.org/10.3389/fpsyg.2013.00863">https://doi.org/10.3389/fpsyg.2013.00863</a></span>)</span>. You should not enter Cohen’s d for a power analysis for a dependent <em>t</em>-test (nor should you directly compared Cohen’s d from a between design with Cohen’s d for a within design, nor should you use the benchmarks Cohen provided for a small (0.2), medium (0.5), and large (0.8) effect be used for Cohen’s <span class="math inline">\(d_z\)</span>). Make sure you are entering the correct effect size. Cohen’s <span class="math inline">\(d_z\)</span> can be calculated from the <em>t</em>-value for a dependent <em>t</em>-test and the sample size as follows:</p>
<p><span class="math display">\[d_z = {\frac{t}{\sqrt{n}}} \]</span></p>
<div class="figure">
<span id="fig:gpowdzd"></span>
<p class="caption marginnote shownote">
Figure 2.16: Power for a dependent and independent <em>t</em>-test require entering Cohen’s dz and d, respectively.
</p>
<img src="images/gpowdzd.png" alt="Power for a dependent and independent *t*-test require entering Cohen's dz and d, respectively." width="377">
</div>
<p>The third issue that researchers often miss is that G*Power has a very unfortunate default setting for power analyses for within subject ANOVA tests. In Figure <a href="power.html#fig:gpowdwithin2">2.17</a> we see on the left how we can directly calculate Cohen’s <span class="math inline">\(f\)</span> (the effect size one needs to enter for ANOVA power analyses) from partial eta squared. A medium effect size for <span class="math inline">\(\eta^2_p\)</span> of 0.588 equals a Cohen’s <span class="math inline">\(f\)</span> of 0.25. If we specify an expected correlation between dependent variables of 0.8, have 3 repeated measurements, one condition, and want to achieve 95% power with an alpha level of 0.05, we need a sample size of 19. However, if out value for <span class="math inline">\(\eta^2_p\)</span> comes from the scientific literature or statistical software such as SPSS, this effect size measure already has the correlation between observation incorporated. If we would simply enter it in G*Power, we take into account the correlation twice, which leads to a massively smaller sample size. Instead, we need to click on the “Options” button and check the radiobutton before “As in SPSS”. We see that “Effect size f” changes into “Effect size f(U)”, and the box “Corr among rep measures” has disappeared on the right. This is because the correlation no longer needs to be entered separately - it is already taken into account in the <span class="math inline">\(\eta^2_p\)</span> as SPSS computes it. Most importantly, we now see that the sample size we need to achieve 95% power has changed to 127. This is a big difference, and I have seen people make this mistake very often. If you use G*power for power analyses for ANOVA designs, I recommend always double checking the Options setting (and maybe repeat your analysis in software such as Superpower just to double check).</p>
<div class="figure">
<span id="fig:gpowdwithin2"></span>
<p class="caption marginnote shownote">
Figure 2.17: Power analysis for repeated ANOVA in G*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account.
</p>
<img src="images/gpowwithin2.png" alt="Power analysis for repeated ANOVA in G\*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account." width="956">
</div>
</div>
<div id="reporting-an-a-priori-power-analysis." class="section level3">
<h3>
<span class="header-section-number">2.3.5</span> Reporting an a-priori power analysis.</h3>
<p>As with all the analyses you rely on in your work, make sure the power analysis is computationally reproducible. If you performed power analyses in R, you can store the script. In G*Power, you can save the several power analyses you performed consecutively in the app under the ‘protocol of power analysis’ tab by saving them as a .rtf file or printing them as a pdf file.</p>
<div class="figure">
<span id="fig:gpowprotocol"></span>
<p class="caption marginnote shownote">
Figure 2.18: G*Power allows you to easily save all details about the power analysis you performed.
</p>
<img src="images/gpowprotocol.png" alt="G\*Power allows you to easily save all details about the power analysis you performed." width="375">
</div>
<p>If you use the <a href="https://arcstats.io/shiny/anova-exact/">Superpower Shiny app</a> for more complex ANOVA designs, you can print a pdf file with all detailed about the power analysis you performed. Sharing the code or a printout of the power analysis will capture all required information about the power analysis you performed, which is often preferable over attempting to communicate all this information verbally.</p>
<div class="figure">
<span id="fig:superpowerreport"></span>
<p class="caption marginnote shownote">
Figure 2.19: Superpower allows you to print a report with all information about the power analysis you performed.
</p>
<img src="images/superpowerreport.png" alt="Superpower allows you to print a report with all information about the power analysis you performed." width="738">
</div>
<p>Providing a reproducible report is sufficient to clarify the power calculation, but not the reasons behind the power calculations. Your decision for the desired Type 1 and Type 2 error rate needs a justification, and most importantly, the effect size you have powered for needs to be justified. If your effect size estimate is based on the existing literature, provide a full citation, and preferably a direct quote from the article. If your effect size is based on an empirical estimate from the scientific literature, you will need to address 1) uncertainty, and 2) bias <span class="citation">(Anderson et al., <label for="tufte-mn-31" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-31" class="margin-toggle">2017<span class="marginnote">Anderson, S. F., Kelley, K., &amp; Maxwell, S. E. (2017). Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. <em>Psychological Science</em>, <em>28</em>(11), 1547–1562.</span>; Taylor &amp; Muller, <label for="tufte-mn-32" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-32" class="margin-toggle">1996<span class="marginnote">Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and sample size calculation due to estimating noncentrality. <em>Communications in Statistics-Theory and Methods</em>, <em>25</em>(7), 1595–1610.</span>)</span>. If your effect size is based on a smallest effect size of interest, this value should not just be stated, but <a href="equivalencetest.html#justifysesoi">justified</a> (e.g., based on theoretical predictions, practical implications, or effect sizes observed or detectable in the literature).</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-pow-rec-2">Table 2.5: </span>Overview of recommendations when reporting an a-prior power analysis.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to address?
</th>
<th style="text-align:left;">
How to address it?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
List all primary analyses you plan to do.
</td>
<td style="text-align:left;">
Following JARS guidelines, specify all primary analyses for which you want to control Type 1 and Type 2 error rates
</td>
</tr>
<tr>
<td style="text-align:left;">
Specify the alpha level for each analysis
</td>
<td style="text-align:left;">
List and justify the chosen alpha level for each analysis. Make sure to correct for multiple comparisons where needed.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the desired power?
</td>
<td style="text-align:left;">
List and justify the desired power for each analysis
</td>
</tr>
<tr>
<td style="text-align:left;">
For each power analysis, specify the effect size metric, the effect size, and the justification for powering for the effect size.
</td>
<td style="text-align:left;">
Report the effect size metric (e.g., Cohen’s <span class="math inline">\(d_z\)</span>, Cohen’s f), the effect size (e.g., 0.3). and the justification for the effect size, whether it is based on a smallest effect size of interest, a meta-analytic effect size estimate, or the estimate of a single previous study.
</td>
</tr>
<tr>
<td style="text-align:left;">
Make sure the power analysis is reproducible.
</td>
<td style="text-align:left;">
Include the code used to run the power analysis, or print a report containing the details about the power analyses you performed.
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="compromisepower" class="section level2">
<h2>
<span class="header-section-number">2.4</span> Compromise power analysis</h2>
<p>Sometimes researchers are brave enough <em>not</em> to fix their alpha level to 0.05. There are two situations where it is sensible to use a different alpha level, even if you want to follow current conventions. The first situation is when you are fortunate enough to be able to collect so many observations that the statistical power for your test is very high. For example, imagine you have access to 2000 employees who are all required to answer questions during a yearly evaluation in a company where you are testing an intervention that should reduce subjectively reported stress levels. You are quite confident that an effect smaller than d = 0.2 is not large enough to be subjectively noticeable for individuals <span class="citation">(Jaeschke et al., <label for="tufte-mn-33" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-33" class="margin-toggle">1989<span class="marginnote">Jaeschke, R., Singer, J., &amp; Guyatt, G. H. (1989). Measurement of health status: Ascertaining the minimal clinically important difference. <em>Controlled Clinical Trials</em>, <em>10</em>(4), 407–415. <a href="https://doi.org/10.1016/0197-2456(89)90005-6">https://doi.org/10.1016/0197-2456(89)90005-6</a></span>)</span>. With an alpha level of 0.05 you would have a statistical power of 0.994, or a Type 2 error rate of 0.006. This means that for a smallest effect size of interest of d = 0.2 we are 8.3 times more likely to make a Type 1 error than a Type 2 error.</p>
<p>This is a reversal of the normal balance of Type 1 and Type 2 error rates, where the minimum desired power is 0.8, and a Type 1 error (0.05) is 4 times as unlikely as a Type 2 error (20%). The default use of 80% power (or a 20% Type 2, or beta (<span class="math inline">\(\beta\)</span>) error) is based on nothing more than a personal preference by <span class="citation">Cohen (<label for="tufte-mn-34" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-34" class="margin-toggle">1988<span class="marginnote">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed). L. Erlbaum Associates.</span>)</span>, who writes:</p>
<blockquote>
<p>It is proposed here as a convention that, when the investigator has no other basis for setting the desired power value, the value .80 be used. This means that <span class="math inline">\(\beta\)</span> is set at .20. This arbitrary but reasonable value is offered for several reasons (Cohen, 1965, pp. 98-99). The chief among them takes into consideration the implicit convention for <span class="math inline">\(\alpha\)</span> of .05. The <span class="math inline">\(\beta\)</span> of .20 is chosen with the idea that the general relative seriousness of these two kinds of errors is of the order of .20/.05, i.e., that Type I errors are of the order of four times as serious as Type II errors. This .80 desired power convention is offered with the hope that it will be ignored whenever an investigator can find a basis in his substantive concerns in his specific research investigation to choose a value ad hoc.</p>
</blockquote>
<p>We see that conventions are built on conventions: the norm to aim for 80% power is built on the norm to set the alpha level at 5%. What we should take away from Cohen is not that we should aim for 90% power, but that we should justify our error rates based on the relative seriousness of each error. If our Type 1 error is 4 times as serious as a Type 2 error, and our Type 2 error is very low for all effects we care about, we should adjust our Type 1 error rate <span class="citation">(Erdfelder et al., <label for="tufte-mn-35" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-35" class="margin-toggle">1996<span class="marginnote">Erdfelder, E., Faul, F., &amp; Buchner, A. (1996). GPOWER: A general power analysis program. <em>Behavior Research Methods, Instruments, &amp; Computers</em>, <em>28</em>(1), 1–11. <a href="https://doi.org/10.3758/BF03203630">https://doi.org/10.3758/BF03203630</a></span>)</span>. G*Power allows us to perform such a <strong>compromise power analysis</strong>, as illustrated in In Figure <a href="power.html#fig:gpowcompromise">2.20</a></p>
<div class="figure">
<span id="fig:gpowcompromise"></span>
<p class="caption marginnote shownote">
Figure 2.20: Compromise power analysis in G*Power.
</p>
<img src="images/compromise1.png" alt="Compromise power analysis in G\*Power." width="378">
</div>
<p>Of course you do not need to follow conventions. If you believe Type 1 errors and Type 2 errors are equally problematic (which is often at least as defensible as arguing a Type 1 error is exactly 4 times as bad as a Type 2 error) you would weigh both types of errors equally, and use α = 0.01785.</p>
<p>As Faul, Erdfelder, Lang, and Buchner <span class="citation">(<label for="tufte-mn-36" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-36" class="margin-toggle">2007<span class="marginnote">Faul, F., Erdfelder, E., Lang, A.-G., &amp; Buchner, A. (2007). GPower 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. <em>Behavior Research Methods</em>, <em>39</em>(2), 175–191. <a href="https://doi.org/10.3758/BF03193146">https://doi.org/10.3758/BF03193146</a></span>)</span> write:</p>
<blockquote>
<p>Of course, compromise power analyses can easily result in unconventional significance levels greater than <span class="math inline">\(\alpha\)</span> = .05 (in the case of small samples or effect sizes) or less than <span class="math inline">\(\alpha\)</span> = .001 (in the case of large samples or effect sizes). However, we believe that the benefit of balanced Type I and Type II error risks often offsets the costs of violating significance level conventions</p>
</blockquote>
<p>This brings us to the second situation where we want to deviate from α = 0.05, while still following the underlying convention proposed by Cohen to weigh a Type 1 error 4 times as serious as a Type 2 error: The situation where power is lower than 80%. Although it is highly undesireable to make decisions when error rates are so very high, if one finds oneself in a situation where a decision must be made based on little data, <span class="citation">Winer (<label for="tufte-mn-37" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-37" class="margin-toggle">1962<span class="marginnote">Winer, B. J. (1962). <em>Statistical principles in experimental design</em>. New York : McGraw-Hill.</span>)</span> writes:</p>
<blockquote>
<p>The frequent use of the .05 and .01 levels of significance is a matter of convention having little scientific or logical basis. When the power of tests is likely to be low under these levels of significance, and when Type 1 and Type 2 errors are of approximately equal importance, the .30 and .20 levels of significance may be more appropriate than the .05 and .01 levels.</p>
</blockquote>
<p>If we plan to perform a two-sided <em>t</em>-test, and can collect at most 50 observations in each independent group, and we want to have 90% power, and expect a population effect size of 0.5, we would have 70% power if we set our alpha level to 0.05. If we can not increase our sample size beyond 50 per group, and we need to make a decision based on the data we will collect, we could set our alpha level to 0.065 to have a Type 1 error rate 4 times as high as our Type 2 error rate (0.262). Alternatively, we can choose to weigh both types of error equally, and set the alpha level to 0.149. The choice of α and β can be further improved by taking priors probabilities of H0 and H1 into account <span class="citation">(Miller &amp; Ulrich, <label for="tufte-mn-38" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-38" class="margin-toggle">2019<span class="marginnote">Miller, J., &amp; Ulrich, R. (2019). The quest for an optimal alpha. <em>PLOS ONE</em>, <em>14</em>(1), e0208631. <a href="https://doi.org/10.1371/journal.pone.0208631">https://doi.org/10.1371/journal.pone.0208631</a></span>)</span> In the end, as Neyman and Pearson <span class="citation">Neyman &amp; Pearson (<label for="tufte-mn-39" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-39" class="margin-toggle">1933<span class="marginnote">Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</em>, <em>231</em>(694-706), 289–337. <a href="https://doi.org/10.1098/rsta.1933.0009">https://doi.org/10.1098/rsta.1933.0009</a></span>)</span> write:</p>
<blockquote>
<p>The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator.</p>
</blockquote>
</div>
<div id="observedpower" class="section level2">
<h2>
<span class="header-section-number">2.5</span> Observed (post-hoc) power analysis</h2>
<p>Observed power (or post-hoc power) is the statistical power of the test you have performed, based on the effect size estimate from your data. <strong>Observed power</strong> differs from the <strong>true power</strong> of your test, because the true power depends on the (true)unknown) population effect size, while observed power is computed based on the (unlikely) assumption that the effect size you observed in your sample is the true effect size. SPSS (which still hasn’t included useful statistics such as effect sizes for <em>t</em>-tests) provides users with the option to report observed power by clicking a checkbox. You should never calculate the observed power. Observed power, computed based on the observed effect size, is a useless statistical concept. Despite this fact, editors and reviewers often ask authors to perform post-hoc power analysis to make a statement about the statistical power a study had. You should never comply with this request. The correct approach to evaluate the statistical power of a study after the data is collected is a <a href="power.html#sensitivitypower">sensitivity power analysis</a>.</p>
<p>Observed (or post-hoc) power and <em>p</em>-values are directly related <span class="citation">(Hoenig &amp; Heisey, <label for="tufte-mn-40" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-40" class="margin-toggle">2001<span class="marginnote">Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. <em>The American Statistician</em>, <em>55</em>(1), 19–24.</span>)</span>. For a <em>Z</em>-test where the <em>p</em>-value is exactly 0.05, the perfectly symmetric alternative distribution falls exactly on top of the critical value. This means that half the distribution falls to the right of the critical value, and half falls on the left, and we have exactly 50% power.</p>
<!-- ```{r obs-power-plot-1, echo = FALSE, fig.width = 8, fig.height = 8, fig.cap="Relationship between p-values and power for a Z-test."} -->
<!-- # For simplicity, take a one-sided test -->
<!-- # compute z value from p: -->
<!-- # p_val = 0.05 -->
<!-- # z <- qnorm(1-p_val) -->
<!-- # z -->
<!-- #  -->
<!-- # # compute p-value from z -->
<!-- # 1-pnorm(z) -->
<!-- # computing the Type 1 error rate: -->
<!-- # old, two-sided: 1-pnorm(q = 1.959964) + pnorm(q = -1.959964) -->
<!-- # Moving the mean from 0.  -->
<!-- # Because the normal distribution is symmetric, if we observe a p-value on top of the critical value (p = 0.05) we have 50% power. -->
<!-- # pnorm(q = 1.644854, mean = 1.644854) -->
<!-- # We can plot this across a range of observe p-values -->
<!-- plot_obs_pow <- (function(alpha_level, p_val) { -->
<!--   1 - pnorm(q = qnorm(1-alpha_level), mean = qnorm(1-p_val)) -->
<!-- }) -->
<!-- par(bg = "aliceblue", pty = "s") -->
<!-- plot(-10, -->
<!--   xlab = "p-value", ylab = "Observed power", axes = FALSE, -->
<!--   main = substitute(paste("Relationship between p-value and observed power")), xlim = c(0, 1), ylim = c(0, 1) -->
<!-- ) -->
<!-- abline(v = seq(0, 1, 0.1), h = seq(0, 1, 0.1), col = "lightgray", lty = 1) -->
<!-- axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- axis(side = 2, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- curve(plot_obs_pow(alpha_level = 0.05, p_val = x), 0, 1, add = TRUE, lwd = 2) -->
<!-- points(x = 0.05, y = 0.5, cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5)) -->
<!-- abline(v = 0.05, h = 0.5, col = rgb(1, 0, 0, 0.5), lty = 1) -->
<!-- ``` -->
<p>For an independent <em>t</em>-test the noncentral <em>t</em>-distribution is not perfectly symmetric. We can compute observe power in R based on the formula below, where <code>alpha_level</code> is the alpha level, <code>n</code>is the sample size in each group, and <code>p_val</code> is the observed <em>p</em>-value. In Figure <a href="#fig:obs-power-plot-2"><strong>??</strong></a> we see that for a <em>p</em> = 0.05 observed power is ever so slightly larger than 50%, but the observed power value is very close.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="power.html#cb39-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pt</span>(<span class="dt">q =</span> <span class="kw">qt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha_level<span class="op">/</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>), </span>
<span id="cb39-2"><a href="power.html#cb39-2"></a>       <span class="dt">df =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>, </span>
<span id="cb39-3"><a href="power.html#cb39-3"></a>       <span class="dt">ncp =</span> <span class="kw">qt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_val<span class="op">/</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>))</span></code></pre></div>
<p>Plotting observe power for the <em>p</em>-value across the range from 0 to 1, we see that observed power is completely determined, and provides no additional value beyond, reporting the <em>p</em>-value. If your <em>p</em>-value is non-significant, observed power will be less than approximately 50% in a <em>Z</em>-test and <em>t</em>-test. As Lenth <span class="citation">(<label for="tufte-mn-41" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-41" class="margin-toggle">2007<span class="marginnote">Lenth, R. V. (2007). Post hoc power: Tables and commentary. <em>Iowa City: Department of Statistics and Actuarial Science, University of Iowa</em>.</span>)</span> shows, observed power is also completely determined by the observed <em>p</em>-value for <em>F</em>-tests, although the relationship between <em>p</em> = 0.05 and 50% power no longer holds.</p>
<div class="figure">
<span id="fig:unnamed-chunk-20"></span>
<p class="caption marginnote shownote">
Figure 2.21: Relationship between p-values and power for an independent t-test.
</p>
<img src="Statistical_Inferences_files/figure-html/unnamed-chunk-20-1.png" alt="Relationship between p-values and power for an independent t-test." width="768">
</div>
<div id="what-to-do-if-your-editor-asks-for-post-hoc-power" class="section level3">
<h3>
<span class="header-section-number">2.5.1</span> What to do if your editor asks for post-hoc power?</h3>
<p>Editors sometimes ask researchers to report post-hoc power analyses when authors report a test result of <em>p</em> &gt; 0.05, and when authors want to conclude there is no effect. In such situations, editors would like to distinguish between true negatives (concluding there is no effect, when there is no effect) and false negatives (concluding there is no effect, when there actually is an effect, or a Type 2 error). As the preceding explanation of post-hoc power illustrates, reporting post-hoc power is nothing more than a different way of reporting the <em>p</em>-value, and will therefore not answer the question editors want to know <span class="citation">(Hoenig &amp; Heisey, <label for="tufte-mn-42" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-42" class="margin-toggle">2001<span class="marginnote">Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. <em>The American Statistician</em>, <em>55</em>(1), 19–24.</span>; Lenth, <label for="tufte-mn-43" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-43" class="margin-toggle">2007<span class="marginnote">Lenth, R. V. (2007). Post hoc power: Tables and commentary. <em>Iowa City: Department of Statistics and Actuarial Science, University of Iowa</em>.</span>; Yuan &amp; Maxwell, <label for="tufte-mn-44" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-44" class="margin-toggle">2005<span class="marginnote">Yuan, K.-H., &amp; Maxwell, S. (2005). On the Post Hoc Power in Testing Mean Differences. <em>Journal of Educational and Behavioral Statistics</em>, <em>30</em>(2), 141–167. <a href="https://doi.org/10.3102/10769986030002141">https://doi.org/10.3102/10769986030002141</a></span>)</span>.</p>
<p>What you should do instead is report a <a href="power.html#sensitivitypower">sensitivity power analysis</a>, preferably by plotting power across a wide range of possible true effect sizes. Many of the recommendations in Table <a href="power.html#tab:table-pow-rec">2.2</a> apply if one wants to evaluate power after the data has been collected. The best solution is, not surprisingly, to design a study that will yield informative results regardless of whether the alternative hypothesis is true or the null hypothesis is true, by performing an a-priori power analysis both for the NHST and for an equivalence test.</p>
</div>
</div>
<div id="why-within-subject-designs-typically-require-fewer-observations-than-between-subject-designs" class="section level2">
<h2>
<span class="header-section-number">2.6</span> Why Within-Subject Designs Typically Require Fewer Observations than Between-Subject Designs</h2>
<p>One widely recommended approach to increase power is using a within subject design. Indeed, you need fewer observations to detect a mean difference between two conditions in a within-subjects design (in a dependent <em>t</em>-test) than in a between-subjects design (in an independent t-test). The reason is straightforward, but not always explained, and even less often expressed in the easy equation below. The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell &amp; Delaney, 2004, p. 561, formula 45):</p>
<p>NW = NB (1-ρ)/2</p>
<p>The “/2” part of the equation is due to the fact that in a two-condition within design every participant provides two data-points. The extent to which this reduces the sample size compared to a between-subject design depends on the correlation between the two dependent variables, as indicated by the (1-ρ) part of the equation. If the correlation is 0, a within-subject design simply needs half as many observations as a between-subject design (e.g., 64 instead 128 observations). The higher the correlation, the larger the relative benefit of within designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Note than when the correlation is -1, you need 128 observations in a within-design and 128 observations in a between-design, but in a within-design you will need to collect two measurements from each participant, making a within design more work than a between-design. However, negative correlations between dependent variables in psychology are rare, and perfectly negative correlations will probably never occur.</p>
<p>So what does the correlation do so that it increases the power of within designs, or reduces the number of observations you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots.</p>
<p>We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur).</p>
<div class="figure">
<span id="fig:plot-1"></span>
<p class="caption marginnote shownote">
Figure 2.22: Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-1-1.png" alt="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15." width="672">
</div>
In the scatter plot, we can see that the correlation between the measurements is indeed 0.
<div class="figure">
<span id="fig:plot-2"></span>
<p class="caption marginnote shownote">
Figure 2.23: Correlation between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-2-1.png" alt="Correlation between two dependent groups." width="672">
</div>
<p>Now, let’s look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is √2 times as large as the standard deviation in each measurement, and indeed, 15*√2 = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent t-test, where the correlation between measurements is not taken into account.</p>
<div class="figure">
<span id="fig:plot-3"></span>
<p class="caption marginnote shownote">
Figure 2.24: Distributions of difference scores between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-3-1.png" alt="Distributions of difference scores between two dependent groups." width="672">
</div>
<p>Now let’s increase the correlation between dependent variables to 0.7.</p>
<p>Nothing has changed when we plot the means:</p>
<pre><code>## 
## 	Two Sample t-test
## 
## data:  x and y
## t = -28.232, df = 19998, p-value &lt; 0.00000000000000022
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.331318 -5.509260
## sample estimates:
## mean of x mean of y 
##  100.1800  106.1003</code></pre>
<div class="figure">
<span id="fig:plot-4"></span>
<p class="caption marginnote shownote">
Figure 2.25: Distributions of two independent groups with means 100 and 106 and a standard deviation of 15.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-4-1.png" alt="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15." width="672">
</div>
<p>The correlation between measurements is now strongly positive:</p>
<div class="figure">
<span id="fig:plot-5"></span>
<p class="caption marginnote shownote">
Figure 2.26: Correlation between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-5-1.png" alt="Correlation between two dependent groups." width="672">
</div>
<p>The important difference lies in the standard deviation of the difference scores. The SD = 11 instead of 21 in the simulation above. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above.</p>
<div class="figure">
<span id="fig:plot-6"></span>
<p class="caption marginnote shownote">
Figure 2.27: Difference scores between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-6-1.png" alt="Difference scores between two dependent groups." width="672">
</div>
<p>if you set the correlation to a negative value, the standard deviation of the difference scores actually increases.</p>
<p>I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor.</p>
<p>You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (M = 390, SD = 44) when they used their feet than when they used their hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p &lt; 0.001, Hedges’ g = 0.43, Mdiff = 19, 95% CI [12; 26]).</p>
<p>Since most dependent variables in within designs in psychology are positively correlated, within designs will greatly increase the power you can achieve given the sample size you have available. Use within-designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-subject design. Maxwell and Delaney’s book (Chapter 11) has a good discussion of this topic.</p>
<!-- You can use this Shiny app to play around with different means, sd's, and correlations, and see the effect of the distribution of the difference scores. -->
<!-- ```{r shiny2, echo=F} -->
<!-- knitr::include_app('http://shiny.ieis.tue.nl/within_between/', height = '4000px') -->
<!-- ``` -->
</div>
<div id="designing-efficient-studies" class="section level2">
<h2>
<span class="header-section-number">2.7</span> Designing efficient studies</h2>
<p>TO BE UPDATED - COPY PASTED FROM BLOG</p>
<p>So far, we have focused on how to perform a power analysis, under the ass. However, are some things you should immediately implement if you use hypothesis tests, and data collection is costly.</p>
<p><strong>1) Use directional tests where relevant.</strong></p>
<p>Just following statements such as ‘we predict X is larger than Y’ up with a logically consistent test of that claim (e.g., a one-sided t-test) will easily give you an increase of 10% power in any well-designed study. If you feel you need to give effects in both directions a non-zero probability, then at least use lopsided tests.</p>
<p><strong>2) Use sequential analysis whenever possible.</strong></p>
<p>It’s like optional stopping, but then without the questionable inflation of the false positive rate. Sequential analyses greatly increase the efficiency of tests <span class="citation">(Lakens, <label for="tufte-mn-45" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-45" class="margin-toggle">2014<span class="marginnote">Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: Sequential analyses. <em>European Journal of Social Psychology</em>, <em>44</em>(7), 701–710. <a href="https://doi.org/10.1002/ejsp.2023">https://doi.org/10.1002/ejsp.2023</a></span>)</span>.</p>
<p><strong>3) Increase your alpha level.</strong></p>
<p>As we wrote in our Justify Your Alpha paper as an argument to not require an alpha level of 0.005: “without (1) increased funding, (2) a reward system that values large-scale collaboration and (3) clear recommendations for how to evaluate research with sample size constraints, lowering the significance threshold could adversely affect the breadth of research questions examined.” If you <em>have</em> to make a decision, and the data you can feasibly collect is limited, consider performing a compromise power analysis.</p>
<p><strong>4) Use within designs where possible.</strong></p>
<p>Especially when measurements are strongly correlated, this can lead to a substantial increase in power, as explained above.</p>
<p><strong>5) Remove statistical variation where possible</strong></p>
<p>The smaller the variation, the larger the standardized effect size (because we are dividing the raw effect by a smaller denominator) and thus the higher the power given the same number of observations. For an overview of different approaches to reduce the variance, see <span class="citation">Allison et al. (<label for="tufte-mn-46" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-46" class="margin-toggle">1997<span class="marginnote">Allison, D. B., Allison, R. L., Faith, M. S., Paultre, F., &amp; Pi-Sunyer, F. X. (1997). Power and money: Designing statistically powerful studies while minimizing financial costs. <em>Psychological Methods</em>, <em>2</em>(1), 20–33. <a href="https://doi.org/10.1037/1082-989X.2.1.20">https://doi.org/10.1037/1082-989X.2.1.20</a></span>)</span>. They discuss:</p>
<ol style="list-style-type: decimal">
<li>Better ways to screen participants for studies where participants need to be screened before participation.</li>
<li>Assigning participants unequally to conditions (if the control condition is much cheaper than the experimental condition, for example).</li>
<li>Using multiple measurements to increase measurement reliability (or use well-validated measures, if I may add).</li>
<li>Smart use of (preregistered, I’d recommend) covariates.</li>
</ol>
<p>Another approach they do not mention is to, where possible, collect multiple observations from the same participant. This can also increase power, especially if there is variation at the individual level, and you analyze data with hierarchical models.</p>
<p><strong>6) Use Bayesian statistics with informed priors.</strong></p>
<p>Regrettably, almost all approaches to statistical inferences become very limited when the number of observations is small. If you are very confident in your predictions (and your peers agree), incorporating prior information will give you a benefit. For a discussion of the benefits and risks of such an approach, see <span class="citation">van de Schoot et al. (<label for="tufte-mn-47" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-47" class="margin-toggle">2015<span class="marginnote">van de Schoot, R., Broere, J. J., Perryck, K. H., Zondervan-Zwijnenburg, M., &amp; van Loey, N. E. (2015). Analyzing small data sets using Bayesian estimation: The case of posttraumatic stress symptoms following mechanical ventilation in burn survivors. <em>European Journal of Psychotraumatology</em>, <em>6</em>. <a href="https://doi.org/10.3402/ejpt.v6.25216">https://doi.org/10.3402/ejpt.v6.25216</a></span>)</span>.</p>
<p>Now if you care about efficiency, you might already have incorporated all these things. There is no way to further improve the statistical power of your tests, and by all plausible estimates of effects sizes you can expect or the smallest effect size you would be interested in, statistical power is low. Now what should you do?</p>
<div id="what-to-do-if-best-practices-in-study-design-wont-save-you" class="section level3">
<h3>
<span class="header-section-number">2.7.1</span> What to do if best practices in study design won’t save you?</h3>
<p>The first thing to realize is that you should not look at statistics to save you. There are no secret tricks or magical solutions. Highly informative experiments require a large number of observations. So what should we do then? The solutions below are, regrettably, a lot more work than making a small change to the design of your study. But it is about time we start to take them seriously. This is a list of solutions I see – but there is no doubt more we can/should do, so by all means, let me know your suggestions on twitter or in the comments.</p>
<p><strong>1) Ask for a lot more money in your grant proposals.</strong></p>
<p>Some grant organizations distribute funds to be awarded as a function of how much money is requested. If you need more money to collect informative data, ask for it. Obviously grants are incredibly difficult to get, but if you ask for money, include a budget that acknowledges that data collection is not as cheap as you hoped some years ago. In my experience, psychologists are often asking for much less money to collect data than other scientists. Increasing the requested funds for participant payment by a factor of 10 is often reasonable, given the requirements of journals to provide a solid sample size justification, and the more realistic effect size estimates that are emerging from preregistered studies.</p>
<p><strong>2) Improve management.</strong></p>
<p>If the implicit or explicit goals that you should meet are still the same now as they were 5 years ago, and you did not receive a miraculous increase in money and time to do research, then an update of the evaluation criteria is long overdue. I sincerely hope your manager is capable of this, but some ‘upward management’ might be needed. In the coda of Lakens &amp; Evers (2014) we wrote “All else being equal, a researcher running properly powered studies will clearly contribute more to cumulative science than a researcher running underpowered studies, and if researchers take their science seriously, it should be the former who is rewarded in tenure systems and reward procedures, not the latter.” and “We believe reliable research should be facilitated above all else, and doing so clearly requires an immediate and irrevocable change from current evaluation practices in academia that mainly focus on quantity.” After publishing this paper, and despite the fact I was an ECR on a tenure track, I thought it would be at least principled if I sent this coda to the head of my own department. He replied that the things we wrote made perfect sense, instituted a recommendation to aim for 90% power in studies our department intends to publish, and has since then tried to make sure quality, and not quantity, is used in evaluations within the faculty (as you might have guessed, I am not on the job market, nor do I ever hope to be).</p>
<p><strong>3) Change what is expected from PhD students.</strong></p>
<p>When I did my PhD, there was the assumption that you performed enough research in the 4 years you are employed as a full-time researcher to write a thesis with 3 to 5 empirical chapters (with some chapters having multiple studies). These studies were ideally published, but at least publishable. If we consider it important for PhD students to produce multiple publishable scientific articles during their PhD’s, this will greatly limit the types of research they can do. Instead of evaluating PhD students based on their publications, we can see the PhD as a time where researchers learn skills to become an independent researcher, and evaluate them not based on publishable units, but in terms of clearly identifiable skills. I personally doubt data collection is particularly educational after the 20th participant, and I would probably prefer to hire a post-doc who had well-developed skills in programming, statistics, and who broadly read the literature, then someone who used that time to collect participant 21 to 200. If we make it easier for PhD students to demonstrate their skills level (which would include at least 1 well written article, I personally think) we can evaluate what they have learned in a more sensible manner than now. Currently, difference in the resources PhD students have at their disposal are a huge confound as we try to judge their skill based on their resume. Researchers at rich universities obviously have more resources – it should not be difficult to develop tools that allow us to judge the skills of people where resources are much less of a confound.</p>
<p><strong>4) Think about the questions we collectively want answered, instead of the questions we can individually answer.</strong></p>
<p>Our society has some serious issues that psychologists can help address. These questions are incredibly complex. I have long lost faith in the idea that a bottom-up organized scientific discipline that rewards individual scientists will manage to generate reliable and useful knowledge that can help to solve these societal issues. For some of these questions we need well-coordinated research lines where hundreds of scholars work together, pool their resources and skills, and collectively pursuit answers to these important questions. And if we are going to limit ourselves in our research to the questions we can answer in our own small labs, these big societal challenges are not going to be solved. Call me a pessimist. There is a reason we resort to forming unions and organizations that have to goal to collectively coordinate what we do. If you greatly dislike team science, don’t worry – there will always be options to make scientific contributions by yourself. But now, there are almost no ways for scientists who want to pursue huge challenges in large well-organized collectives of hundreds or thousands of scholars (for a recent exception that proves my rule by remaining unfunded: see the Psychological Science Accelerator). If you honestly believe your research question is important enough to be answered, then get together with everyone who also thinks so, and pursue answers collectively. Doing so should, eventually (I know science funders are slow) also be more convincing as you ask for more resources to do the resource (as in point 1).</p>
</div>
</div>
<div id="planning-for-precision" class="section level2">
<h2>
<span class="header-section-number">2.8</span> Planning for precision</h2>
<p>NOT COMPLETED YET.</p>
<p>Imagine we want to learn about the average reading speed. This is important information when we present written information on dynamic displays (e.g., subtitles under movies, transit information in subways). We want to make sure that most people will be able to read the text we present before it is gone. The number of words we read isn’t fixed, but depends on what we read, and individual differences <span class="citation">(Brysbaert, <label for="tufte-mn-48" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-48" class="margin-toggle">2019a<span class="marginnote">Brysbaert, M. (2019a). How many words do we read per minute? A review and meta-analysis of reading rate. <em>Journal of Memory and Language</em>, <em>109</em>, 104047.</span>)</span>.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="pvalue.html"><button class="btn btn-default">Previous</button></a>
<a href="questions.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-11
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-08-11 22:25:20
</span>
</p>
</div>


</body>
</html>
