<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="4 Equivalence Testing | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-07-19" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>4 Equivalence Testing | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a href="power.html"><span class="toc-section-number">2</span> What is power analysis?</a>
<a href="errorcontrol.html"><span class="toc-section-number">3</span> Error Control</a>
<a id="active-page" href="equivalencetest.html"><span class="toc-section-number">4</span> Equivalence Testing</a><ul class="toc-sections">
<li class="toc"><a href="#NA"> Absence of evidence is not evidence of absence.</a></li>
<li class="toc"><a href="#NA">NA</a></li>
</ul>
<a href="severity.html"><span class="toc-section-number">5</span> Severe Tests and Risky Predictions</a>
<a href="sesoi.html"><span class="toc-section-number">6</span> Smallest Effect Size of Interest</a>
<a href="meta.html"><span class="toc-section-number">7</span> Meta-analysis and bias detection</a>
<a href="computationalreproducibility.html"><span class="toc-section-number">8</span> Computational Reproducibility</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="equivalencetest" class="section level1">
<h1>
<span class="header-section-number">4</span> Equivalence Testing</h1>
<div id="absence-of-evidence-is-not-evidence-of-absence." class="section level2">
<h2>
<span class="header-section-number">4.1</span> Absence of evidence is not evidence of absence.</h2>
<p>When you find p &gt; 0.05, you did not observe surprising data, assuming there is no true effect. You can often read in the literature how p &gt; 0.05 is interpreted as ‘no effect’ but due to a lack of power the data might not be surprising if there was an effect. In this blog I’ll explain how to test for equivalence, or the lack of a meaningful effect, using equivalence hypothesis testing. I’ve created easy to use R functions that allow you to perform equivalence hypothesis tests. Warning: If you read beyond this paragraph, you will never again be able to write “as predicted, the interaction revealed there was an effect for participants in the experimental condition (p &lt; 0.05) but there was no effect in the control condition (F &lt; 1).” If you prefer the veil of ignorance, here’s a nice site with cute baby animals to spend the next 9 minutes on instead.</p>
<p>Any science that wants to be taken seriously needs to be able to provide support for the null-hypothesis. I often see people switching over from Frequentist statistics when effects are significant, to the use of Bayes Factors to be able to provide support for the null hypothesis. But it is possible to test if there is a lack of an effect using p-values (why no one ever told me this in the 11 years I worked in science is beyond me). It’s as easy as doing a t-test, or more precisely, as doing two t-tests.</p>
<p>The practice of Equivalence Hypothesis Testing (EHT) is used in medicine, for example to test whether a new cheaper drug isn’t worse (or better) than the existing more expensive option. A very simple EHT approach is the ‘two-one-sided t-tests’ (TOST) procedure (Schuirmann, 1987). Its simplicity makes it wonderfully easy to use.</p>
<p>The basic idea of the test is to flip things around: In Equivalence Hypothesis Testing the null hypothesis is that there is a true effect larger than a Smallest Effect Size of Interest (SESOI; Lakens, 2014). That’s right – the null-hypothesis is now that there IS an effect, and we are going to try to reject it (with a p &lt; 0.05). The alternative hypothesis is that the effect is smaller than a SESOI, anywhere in the equivalence range - any effect you think is too small to matter, or too small to feasibly examine. For example, a Cohen’s d of 0.5 is a medium effect, so you might set d = 0.5 as your SESOI, and the equivalence range goes from d = -0.5 to d = 0.5 In the TOST procedure, you first decide upon your SESOI: anything smaller than your smallest effect size of interest is considered smaller than small, and will allow you to reject the null-hypothesis that there is a true effect. You perform two t-tests, one testing if the effect is smaller than the upper bound of the equivalence range, and one testing whether the effect is larger than the lower bound of the equivalence range. Yes, it’s that simple.</p>
<p>Let’s visualize this. Below on the left axis is a scale for the effect size measure Cohen’s d. On the left is a single 90% confidence interval (the crossed circles indicate the endpoints of the 95% confidence interval) with an effect size of d = 0.13. On the right is the equivalence range. It is centered on 0, and ranges from d = -0.5 to d = 0.5.</p>
<p>We see from the 95% confidence interval around d = 0.13 (again, the endpoints of which are indicated by the crossed circles) that the lower bound overlaps with 0. This means the effect (d = 0.13, from an independent t-test with two conditions of 90 participants each) is not statistically different from 0 at an alpha of 5%, and the p-value of the normal NHST is 0.384 (the title provides the exact numbers for the 95% CI around the effect size). But is this effect statistically smaller than my smallest effect size of interest?</p>
<div id="rejecting-the-presence-of-a-meaningful-effect" class="section level3">
<h3>
<span class="header-section-number">4.1.1</span> Rejecting the presence of a meaningful effect</h3>
<p>There are two ways to test the lack of a meaningful effect that yield the same result. The first is to perform two one sided t-tests, testing the observed effect size against the ‘null’ of your SESOI (0.5 and -0.5). These t-tests show the d = 0.13 is significantly larger than d = -0.5, and significantly smaller than d = 0.5. The highest of these two p-values is p = 0.007. We can conclude that there is support for the lack of a meaningful effect (where meaningful is defined by your choice of the SESOI). The second approach (which is easier to visualize) is to calculate a 90% CI around the effect (indicated by the vertical line in the figure), and check whether this 90% CI falls completely within the equivalence range. You can see a line from the upper and lower limit of the 90% CI around d = 0.13 on the left to the equivalence range on the right, and the 90% CI is completely contained within the equivalence range. This means we can reject the null of an effect that is larger than d = 0.5 or smaller than d = -0.5 and conclude this effect is smaller than what we find meaningful (and you’ll be right 95% of the time, in the long run).</p>
<p>[Technical note: The reason we are using a 90% confidence interval, and not a 95% confidence interval, is because the two one-sided tests are completely dependent. You could actually just perform one test, if the effect size is positive against the upper bound of the equivalence range, and if the effect size is negative against the lower bound of the equivalence range. If this one test is significant, so is the other. Therefore, we can use a 90% confidence interval, even though we perform two one-sided tests. This is also why the crossed circles are used to mark the 95% CI.].</p>
<p>So why were we not using these tests in the psychological literature? It’s the same old, same old. Your statistics teacher didn’t tell you about them. SPSS doesn’t allow you to do an equivalence test. Your editors and reviewers were always OK with your statements such as “as predicted, the interaction revealed there was an effect for participants in the experimental condition (p &lt; 0.05) but there was no effect in the control condition (F &lt; 1).” Well, I just ruined that for you. Absence of evidence is not evidence of absence!</p>
<p>We can’t use p &gt; 0.05 as evidence of a lack of an effect. You can switch to Bayesian statistics if you want to support the null, but the default priors are only useful of in research areas where very large effects are examined (e.g., some areas of cognitive psychology), and are not appropriate for most other areas in psychology, so you will have to be able to quantify your prior belief yourself. You can teach yourself how, but there might be researchers who prefer to provide support for the lack of an effect within a Frequentist framework. Given that most people think about the effect size they expect when designing their study, defining the SESOI at this moment is straightforward. After choosing the SESOI, you can even design your study to have sufficient power to reject the presence of a meaningful effect. Controlling your error rates is thus straightforward in equivalence hypothesis tests, while it is not that easy in Bayesian statistics (although it can be done through simulations).</p>
<p>One thing I noticed while reading this literature is that TOST procedures, and power analyses for TOST, are not created to match the way psychologists design studies and think about meaningful effects. In medicine, equivalence is based on the raw data (a decrease of 10% compared to the default medicine), while we are more used to think in terms of standardized effect sizes (correlations or Cohen’s d). Biostatisticians are fine with estimating the pooled standard deviation for a future study when performing power analysis for TOST, but psychologists use standardized effect sizes to perform power analyses. Finally, the packages that exist in R (e.g., equivalence) or the software that does equivalence hypothesis tests (e.g., Minitab, which has TOST for t-tests, but not correlations) requires that you use the raw data. In my experience (Lakens, 2013) researchers find it easier to use their own preferred software to handle their data, and then calculate additional statistics not provided by the software they use by typing in summary statistics in a spreadsheet (means, standard deviations, and sample sizes per condition). So my functions don’t require access to the raw data (which is good for reviewers as well). Finally, the functions make a nice picture such as the one above so you can see what you are doing.</p>
</div>
</div>
<div id="bayesian-estimation-using-rope-and-equivalence-tests." class="section level2 unnumbered">
<h2>Bayesian estimation using ROPE and equivalence tests.</h2>
<p>When we perform a study, we would like to conclude there is an effect, when there is an effect. But it is just as important to be able to conclude there is no effect, when there is no effect. I’ve recently published a paper that makes Frequentist equivalence tests (using the two-one-sided tests, or TOST, approach) as easy as possible (Lakens, 2017). Equivalence tests allow you to reject the presence of any effect you care about. In Bayesian estimation, one way to argue for the absence of a meaningful effect is the Region of Practical Equivalence (ROPE) procedure (Kruschke, 2014, chapter 12), which is “somewhat analogous to frequentist equivalence testing” (Kruschke &amp; Liddell, 2017).</p>
<p>In the ROPE procedure, a 95% Highest Density Interval (HDI) is calculated based on a posterior distribution (which is calculated based on a prior and the data). Kruschke suggests that: “if the 95 % HDI falls entirely inside the ROPE then we decide to accept the ROPE’d value for practical purposes”. Note that the same HDI can also be used to reject the null hypothesis, where in Frequentist statistics, even though the confidence interval plays a similar role, you would still perform both a traditional t-test and the TOST procedure.</p>
<p>The only real difference with equivalence testing is that instead of using a confidence interval, a Bayesian Highest Density Interval is used. If the prior used by Kruschke was perfectly uniform, ROPE and equivalence testing would identical, barring philosophical differences in how the numbers should be interpreted. The BEST package by default uses a ‘broad’ prior, and therefore the 95% CI and 95% HDI are not exactly the same, but they are very close, for single comparisons. When multiple comparisons are made, (for example when using sequential analyses, Lakens, 2014), the CI needs to be adjusted to maintain the desired error rate, but in Bayesian statistics, error rates are not directly controlled (they are limited due to ‘shrinkage’, but can be inflated beyond 5%, and often considerably so).</p>
<p>In the code below, I randomly generate random normally distributed data (with means of 0 and a sd of 1) and perform the ROPE procedure and the TOST. The 95% HDI is from -0.10 to 0.42, and the 95% CI is from -0.11 to 0.41, with mean differences of 0.17 or 0.15.</p>
<p>Indeed, if you will forgive me the pun, you might say these two approaches are practically equivalent. But there are some subtle differences between ROPE and TOST</p>
<div id="hdi-vs-90-ci" class="section level3">
<h3>
<span class="header-section-number">4.1.2</span> 95% HDI vs 90% CI</h3>
<p>Kruschke (2014, Chapter 5) writes: “How should we define “reasonably credible”? One way is by saying that any points within the 95% HDI are reasonably credible.” There is not a strong justification for the use of a 95% HDI over a 96% of 93% HDI, except that it mirrors the familiar use of a 95% CI in Frequentist statistics. In Frequentist statistics, the 95% confidence interval is directly related to the 5% alpha level that is commonly deemed acceptable for a maximum Type 1 error rate (even though this alpha level is in itself a convention without strong justification).</p>
<p>But here’s the catch: The TOST equivalence testing procedure does not use a 95% CI, but a 90% CI. The reason for this is that two one-sided tests are performed. Each of these tests has a 5% error rate. You might intuitively think that doing two tests with a 5% error rate will increase the overall Type 1 error rate, but in this case, that’s not true. You could easily replace the two tests, with just one test, testing the observed effect against the equivalence bound (upper or lower) closest to it. If this test is statistically significant, so is the other – and thus, there is no alpha inflation in this specific case. That’s why the TOST procedure uses a 90% CI to have a 5% error rate, while the same researcher would use a 95% CI in a traditional two-sided t-test to examine whether the observed effect is statistically different from 0, while maintaining a 5% error rate (see also Senn, 2007, section 22.2.4)</p>
<p>This nicely illustrates the difference between estimation (where you just want to have a certain level of accuracy, such as 95%), and Frequentist hypothesis testing, where you want to distinguish between signal and noise, and not be wrong more than 5% of the time when you declare there is a signal. ROPE keeps the accuracy the same across tests, Frequentist approaches keep the error rate constant. From a Frequentist perspective, ROPE is more conservative than TOST, like the use of alpha = 0.025 is more conservative than the use of alpha = 0.05.</p>
</div>
<div id="power-analysis-for-equivalence-tests" class="section level3">
<h3>
<span class="header-section-number">4.1.3</span> Power analysis for Equivalence Tests</h3>
<p>For an equivalence test, power analysis can be performed based on closed functions, and the calculations take just a fraction of a second. I find that useful, for example in my role in our ethics board, where we evaluate proposals that have to justify their sample size, and we often check power calculations. Kruschke has an excellent R package (BEST) that can do power analyses for the ROPE procedure. This is great work – but the simulations take a while (a little bit over an hour for 1000 simulations).</p>
<p>Because the BESTpower function relies on simulations, you need to specify the sample size, and it will calculate the power. That’s actually the reverse of what you typically want in a power analysis (you want to input the desired power, and see which sample size you need). This means you most likely need to run multiple simulations in BESTpower, before you have determined the sample size that will yield good power. Furthermore, the software requires your to specify the expected means and standard deviations, instead of simply an expected effect size. Instead of Frequentist power analysis, where the hypothesized effect size is a point value (e.g., d = 0.4), Bayesian power analysis models the alternative as a distribution, acknowledging there is uncertainty.</p>
<p>In the end, however, the result of a power analysis for ROPE and for TOST is actually remarkably similar. Using the code below to perform the power analysis for ROPE, we see that 100 participants in each group give us approximately 88.4% power (with 2000 simulations, this estimate is still a bit uncertain) to get a 95% HDI that falls within our ROPE of -0.5 to 0.5, assuming standard deviations of 1.</p>
<p>We can use the powerTOSTtwo.raw function in the TOSTER package (using an alpha of 0.025 instead of 0.05, to mirror to 95% HDI) to calculate the sample size we would need to achieve 88.4% power for independent t-test (using equivalence bounds of -0.5 and 0.5, and standard deviations of 1):</p>
<p>powerTOSTtwo.raw(alpha=0.025,statistical_power=0.875,low_eqbound=-0.5,high_eqbound=0.5,sdpooled=1)</p>
<p>The outcome is 100 as well. So if you use a broad prior, it seems you can save yourself some time by using the power analysis for equivalence tests, without severe consequences.</p>
</div>
<div id="use-of-prior-information" class="section level3">
<h3>
<span class="header-section-number">4.1.4</span> Use of prior information</h3>
<p>The biggest benefit of ROPE over TOST is that is allows you to incorporate prior information in your data analysis. If you have reliable prior information, ROPE can use this information, which is especially useful if you don’t have a lot of data. If you use priors, it is typically advised to check the robustness of the posterior against reasonable changes in the prior (Kruschke, 2013).</p>
</div>
<div id="conclusion" class="section level3">
<h3>
<span class="header-section-number">4.1.5</span> Conclusion</h3>
<p>Using the ROPE procedure or the TOST procedure will most likely lead to very similar inferences. For all practical purposes, the differences are small. It’s quite a lot easier to perform a power analysis for TOST, and by default, TOST has greater statistical power because it uses 90% CI. But power analysis is possible for ROPE (which is a rare pleasure to see for Bayesian analyses), and you could choose to use a 90% HDI, or any other value that matches your goals. TOST will be easier and more familiar because it is just a twist on the classic t-test, but ROPE might be a great way to dip your toes in Bayesian waters and explore the many more things you can do with Bayesian posterior distributions.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="equivalencetest.html#cb49-1"></a><span class="co"># Data is loaded because BEST analysis takes so very long</span></span>
<span id="cb49-2"><a href="equivalencetest.html#cb49-2"></a><span class="kw">load</span>(<span class="st">"data/BESTout.Rdata"</span>)</span>
<span id="cb49-3"><a href="equivalencetest.html#cb49-3"></a><span class="kw">plot</span>(BESTout)</span></code></pre></div>
<p><img src="Statistical_Inferences_files/figure-html/unnamed-chunk-35-1.png" width="672"></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="equivalencetest.html#cb50-1"></a><span class="co">#TOST test</span></span>
<span id="cb50-2"><a href="equivalencetest.html#cb50-2"></a><span class="kw">TOSTtwo.raw</span>(<span class="dt">m1=</span><span class="kw">mean</span>(x),<span class="dt">m2=</span><span class="kw">mean</span>(y),<span class="dt">sd1=</span><span class="kw">sd</span>(x),<span class="dt">sd2=</span><span class="kw">sd</span>(y),<span class="dt">n1=</span><span class="kw">length</span>(x),<span class="dt">n2=</span><span class="kw">length</span>(y),<span class="dt">low_eqbound=</span><span class="op">-</span><span class="fl">0.5</span>,<span class="dt">high_eqbound=</span><span class="fl">0.5</span>, <span class="dt">alpha=</span><span class="fl">0.025</span>)</span></code></pre></div>
<p><img src="Statistical_Inferences_files/figure-html/unnamed-chunk-35-2.png" width="672"></p>
<pre><code>## TOST results:
## t-value lower bound: 4.92 	p-value lower bound: 0.0000009
## t-value upper bound: -2.69 	p-value upper bound: 0.004
## degrees of freedom : 197.19
## 
## Equivalence bounds (raw scores):
## low eqbound: -0.5 
## high eqbound: 0.5
## 
## TOST confidence interval:
## lower bound 95% CI: -0.112
## upper bound 95% CI:  0.406
## 
## NHST confidence interval:
## lower bound 97.5% CI: -0.15
## upper bound 97.5% CI:  0.443
## 
## Equivalence Test Result:
## The equivalence test was significant, t(197.19) = -2.691, p = 0.00387, given equivalence bounds of -0.500 and 0.500 (on a raw scale) and an alpha of 0.025.
## Null Hypothesis Test Result:
## The null hypothesis test was non-significant, t(197.19) = 1.117, p = 0.265, given an alpha of 0.025.
## Based on the equivalence test and the null-hypothesis test combined, we can conclude that the observed effect is statistically not different from zero and statistically equivalent to zero.</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="equivalencetest.html#cb52-1"></a>powerPro</span></code></pre></div>
<pre><code>##                              mean        CrIlo       CrIhi
##   mean:   HDI &gt; ROPE 0.0004995005 3.170522e-12 0.001495998
##   mean:   HDI &lt; ROPE 0.0004995005 3.170522e-12 0.001495998
##   mean:  HDI in ROPE 0.8746253746 8.600315e-01 0.889009125
##   mean: HDI width ok 0.9995004995 9.985040e-01 1.000000000
##     sd:   HDI &gt; ROPE 0.0004995005 3.170522e-12 0.001495998
##     sd:   HDI &lt; ROPE 0.0004995005 3.170522e-12 0.001495998
##     sd:  HDI in ROPE 0.9995004995 9.985040e-01 1.000000000
##     sd: HDI width ok 0.9995004995 9.985040e-01 1.000000000
## effect:   HDI &gt; ROPE 0.0004995005 3.170522e-12 0.001495998
## effect:   HDI &lt; ROPE 0.0004995005 3.170522e-12 0.001495998
## effect:  HDI in ROPE 0.9995004995 9.985040e-01 1.000000000
## effect: HDI width ok 0.9995004995 9.985040e-01 1.000000000</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="equivalencetest.html#cb54-1"></a><span class="co">#TOST power analysis</span></span>
<span id="cb54-2"><a href="equivalencetest.html#cb54-2"></a><span class="kw">powerTOSTtwo.raw</span>(<span class="dt">alpha=</span><span class="fl">0.025</span>,<span class="dt">statistical_power=</span><span class="fl">0.875</span>,<span class="dt">low_eqbound=</span><span class="op">-</span><span class="fl">0.5</span>,<span class="dt">high_eqbound=</span><span class="fl">0.5</span>,<span class="dt">sdpooled=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## The required sample size to achieve 87.5 % power with equivalence bounds of -0.5 and 0.5 is 97.66901 per group, or 196 in total.</code></pre>
<pre><code>## [1] 97.66901</code></pre>

</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="errorcontrol.html"><button class="btn btn-default">Previous</button></a>
<a href="severity.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-19
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-07-19
</span>
</p>
</div>


</body>
</html>
