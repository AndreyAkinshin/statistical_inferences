<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="5 Systematic Noise | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-07-15" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>5 Systematic Noise | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a href="power.html"><span class="toc-section-number">2</span> What is power analysis?</a>
<a href="errorcontrol.html"><span class="toc-section-number">3</span> Error Control</a>
<a href="equivalencetest.html"><span class="toc-section-number">4</span> Equivalence Testing</a>
<a href="severe-tests-and-risky-predictions.html">Severe Tests and Risky Predictions</a>
<a id="active-page" href="systematic-noise.html"><span class="toc-section-number">5</span> Systematic Noise</a><ul class="toc-sections">
<li class="toc"><a href="#range-predictions"> Range Predictions</a></li>
<li class="toc"><a href="#directional-tests"> Directional Tests</a></li>
<li class="toc"><a href="#minimal-effect-tests"> Minimal Effect Tests</a></li>
<li class="toc"><a href="#testing-range-predictions-1"> Testing Range Predictions</a></li>
</ul>
<a href="verisimilitude-belief-and-progress-in-psychological-science.html"><span class="toc-section-number">6</span> Verisimilitude, Belief, and Progress in Psychological Science</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="systematic-noise" class="section level1">
<h1>
<span class="header-section-number">5</span> Systematic Noise</h1>
<p>The scenario in the left, where only a very small part of all possible outcomes is seen as falsifying a prediction, is very similar to how people commonly use null-hypothesis significance tests. In a null-hypothesis significance test, any statistically significant effect that is not zero is interpreted as support for a theory. Is this impressive? That depends on the possible states of the world. According to Meehl, there are many situations where null-hypothesis significance tests are performed, but the true difference is highly unlikely to be exactly zero. Meehl is especially worried about research where there is room for <strong>systematic noise</strong>, or the <strong>crud factor</strong>.</p>
<p>Systematic noise can only be excluded in an ideal experiment. In this ideal experiment, there is perfect random assignment to conditions, and only one single thing can cause a difference, such as in a <strong>randomized controlled trial</strong>. Perfection is notoriously hard to achieve in practice. In any close to perfect experiment, there can be tiny factors that, although not being the main goal of the experiment, lead to differences between the experimental and control condition. Participants in the experimental condition might read more words, answer more questions, need more time, have to think more deeply, or process more novel information. Any of these things could slightly move the true effect size away from zero – without being related to the independent variable the researchers aimed to manipulate. This is why Meehl calls it <em>systematic</em> noise, and not <em>random</em> noise: The difference is reliable, but not due to something you are <strong>theoretically interested</strong> in.</p>
<p>Many experiments are not even close to perfect and consequently have a lot of room for systematic noise. And then there are many studies where there isn’t even random assignment to conditions, but where data is correlational. As an example of correlational data, think about research examining differences between women and men. If we examine differences between men and women, the subjects in our study can not be randomly assigned to a condition. In such non-experimental studies, it is possible that ‘<strong>everything is correlated to everything</strong>’. Or slightly more formally (Orben &amp; Lakens, 2019), crud can be defined as the epistemological concept that, in correlational research, all variables are connected through multivariate causal structures which result in real non-zero correlations between all variables in any given dataset.</p>
<p>For example, men are on average taller than women, and as a consequence it is more common for a man to be asked to pick an object from a high shelf in a supermarket, than vice versa. If we then ask men and women ‘how often do you help strangers’ this average difference in height has some tiny but systematic effect on their responses. In this specific case, systematic noise moves the mean difference from zero to a slightly higher value for men – but an unknown number of other sources of systematic noise are at play, and these all interact, leading to an unknown final true population difference that is very unlikely to be exactly zero.</p>
<p>Null-Hypothesis Significance Tests are so common we rarely think about whether they are the right question to ask. But <strong>when you perform a null-hypothesis test, you should justify why the null-hypothesis is an interesting hypothesis to test against</strong>. This is not always self-evident, and sometimes the null hypothesis is simply not very interesting. Whenever you do research, look at your hypothesis tests and ask yourself whether <strong>the null hypothesis was justified</strong>. Was it plausible that the null hypothesis was true? And was it an interesting value to test against?</p>
<p>I think there are experiments that, for all practical purposes, are controlled enough to make a point null-hypothesis a valid and realistic model to test against. However, I also think that these experiments do not encompass all the current uses of null-hypothesis testing. There are many experiments where a test against a null-hypothesis is performed, while the point null-hypothesis is not reasonable to entertain, and we can’t expect the difference to be exactly zero.</p>
<p>In those studies (e.g., as in the experiment examining differences between men and women above) it is much more impressive to have a theory that is able to predict how big an effect is (approximately). In other words, we should aim for theories that make <strong>point predictions</strong>, or a bit more reasonably, given that most sciences have a hard time predicting a single exact value, <strong>range predictions</strong>.</p>
<div id="range-predictions" class="section level2">
<h2>
<span class="header-section-number">5.1</span> Range Predictions</h2>
<p>Making more risky <em>range predictions</em> has some important benefits over the widespread use of null-hypothesis tests. These benefits mean that even if a null-hypothesis test is defensible, it would be preferable if you could test a range prediction.</p>
<p>Making a more risky prediction gives your theory higher <strong>verisimilitude</strong>. You will get more credit in darts when you correctly predict you will hit the bullseye, than when you correctly predict you will hit the board. Many sports work like this, such as figure skating or gymnastics. The riskier the routine you perform, the more points you can score, since there were many ways the routine could have failed if you lacked the skill. Similarly, you get more credit for the predictive power of your theory when you correctly predict an effect will fall within 0.5 scale points of 8 on a 10 point scale, than when you predict the effect will be larger than the midpoint of the scale. A theory
allows you to make predictions, and a good theory allows you to make precise predictions.</p>
<p>Range predictions allow you to design a study that can be <strong>falsified based on clear criteria</strong>. If you specify the bounds within which an effect should fall, any effect that is either smaller or larger will falsify the prediction. For a traditional null-hypothesis test, a significant effect of 0.0000001 will officially still fall in the possible states of the world that support the theory. However, it is practically impossible to falsify such tiny differences from zero, because doing so would require huge resources.</p>
</div>
<div id="directional-tests" class="section level2">
<h2>
<span class="header-section-number">5.2</span> Directional Tests</h2>
<p>Researchers often have a directional hypothesis when comparing two groups (e.g., the reaction times in the implicit association test are slower in the incongruent block compared to the congruent block). In these situations, researchers can choose to use either a two-sided test or a one-sided test. One-sided tests are more powerful than two-sided tests. If you design a test with 80% power, a one-sided test requires <a href="https://gist.github.com/Lakens/ef232b3bbbec3258a251cdce26f91945">approximately</a> <strong>78% of the total sample of a two-sided test</strong>. This means that the use of one-sided tests would make researchers more efficient. In addition, a one-sided test is a riskier prediction than a two-sided test. We have limited all possible outcomes that we predict by 50%, which is quite impressive.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:onsided"></span>
<img src="images/onesided.png" alt="Vizualization of a diferectional (or one-sided) hypothesis test." width="546"><!--
<p class="caption marginnote">-->Figure 5.1: Vizualization of a diferectional (or one-sided) hypothesis test.<!--</p>-->
<!--</div>--></span>
</p>
<p>Many researchers have reacted negatively to the “widespread overuse of two-tailed testing for directional research hypotheses tests” (Cho &amp; Abe, 2013). Others argue that directional tests should not be used. I think a fair summary of this discussion is that 1) directional tests should always be pre-registered (I agree), 2) they require smaller sample sizes, and therefore you will end up with less evidence (which is true, but if you want a specific amount of evidence, you should design studies to achieve a desired level of evidence instead of designing studies where error rates are controlled), and 3) when results in both directions are practically relevant, such as in medical research where we care both about improving lives, and not making lives worse, directional tests might only be desirable in very specific circumstances (I agree). Although these caveats are important, in many cases researchers make directional predictions. They would consider their predictions proven wrong by an effect of 0, or an effect in the opposite direction. Effects of 0, or effects in the opposite direction, might be interesting enough to follow up on. But if the question is whether some manipulation leads to a positive effect, a result of a one-sided test is the logical answer to that question.</p>
<p>Two-sided tests are so common we rarely think about whether they are the
right question to ask. But if you make a directional prediction, it often makes
sense to perform a directional test. When you read the literature, look at the main hypothesis in the introduction. Did the hypothesis make a one-sided prediction? Then take a look at the result section. Was the hypothesis tested in a two-sided test?</p>
</div>
<div id="minimal-effect-tests" class="section level2">
<h2>
<span class="header-section-number">5.3</span> Minimal Effect Tests</h2>
<p>A directional test can use a null-hypothesis of an effect of 0, but we can also perform a hypothesis test not against 0, but against a smallest effect size of interest. This is known as a minimal effect test. In a minimal effect test, the null-hypothesis is any value smaller than the values we care about. For example, imagine we have designed an after-school training program to improve the language ability of young children. This program has a positive effect whenever we can reject the null hypothesis of an effect size of 0. But the training program also has costs, and if it improved language ability 0.00001 on a standardize test where students can score between 0 and 100, it will not be worth implementing the training program. Based on a cost-benefit analysis, a team of experts has decided the training program is worth the costs if the improvement is larger than 5%. Therefore, they test against a smallest effect of interest (Δ) of 5, instead of testing against 0. The null hypothesis is now any effect up to 5%. We reject the null hypothesis if the observed effect is statistically larger than 5%.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:minimaleffect"></span>
<img src="images/minimaleffect.png" alt="Vizualization of a minimal effect test." width="546"><!--
<p class="caption marginnote">-->Figure 5.2: Vizualization of a minimal effect test.<!--</p>-->
<!--</div>--></span>
</p>
<p>Many of the criticisms on <em>p</em>-values in null-hypothesis tests where H0 = 0 disappear when <em>p</em>-values are calculated for a minimal effect test. In a traditional hypothesis test with at least some systematic noise (meaning the true effect differs slightly from zero) all studies where the null is not exactly true will lead to a significant effect with a large enough sample size. This makes it a boring (i.e., not risky) prediction, and we will end up stating there is a ‘significant’ difference for tiny irrelevant effects. I expect this problem will become more important now that it is easier to get access to Big Data.</p>
<p>However, we don’t want just any effect to become statistically significant – we want <strong>theoretically or practically relevant</strong> effects to be significant, but not <strong>theoretically or practically irrelevant</strong> effects. A minimal effect test achieves this. If we predict effects larger than 5%, an effect of 1% might be statistically different from 0 in a huge sample, but it is not <strong>practically relevant</strong>.</p>
</div>
<div id="testing-range-predictions-1" class="section level2">
<h2>
<span class="header-section-number">5.4</span> Testing Range Predictions</h2>
<p>In a null-hypothesis test (visualized below) we compare the observed mean (the black square) and the 95% confidence interval (the length of the horizontal line through the square) against the hypothesis that the difference is 0 (indicated by the dotted vertical line at 0). Let’s imagine the test yields a <em>p</em> = 0.047. If we use an alpha level of 0.05, this is just below the alpha threshold. The observed difference (indicated by the square) has a confidence interval that ranges from almost close to 0 to 1.4. We can reject the null, but beyond that, we haven’t learned much.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:NHST1"></span>
<img src="images/nhst1.png" alt="NHST with a wide confidence interval." width="926"><!--
<p class="caption marginnote">-->Figure 5.3: NHST with a wide confidence interval.<!--</p>-->
<!--</div>--></span>
</p>
<p>In the example above, we were testing against a mean difference of 0. But there is no reason why a hypothesis test should be limited to test against a mean difference of 0. For example, let’s assume effects smaller than 0.5 are considered too small to matter. In this case, we can perform a minimal effect test against 0.5 instead of 0. In the figure below, we again see a <em>p</em> = 0.047 result, but now for a much riskier test, namely against a smallest effect of interest of 0.5.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:risktest1"></span>
<img src="images/riskytest1.png" alt="Minimal effect test with a wide confidence interval." width="688"><!--
<p class="caption marginnote">-->Figure 5.4: Minimal effect test with a wide confidence interval.<!--</p>-->
<!--</div>--></span>
</p>
<p>People often report a manipulation check in their articles. For example, based on previous work, they have selected 20 positive words and 20 negative words, and use these in an experiment. They might ask participants after the study to evaluate these words as a manipulation check. For example, in one of the articles that were part of my PhD thesis, I wrote:</p>
<p>“<em>Manipulation check</em>. Positive words were judged as more positive (<em>M</em> = 6.51)
than negative words (<em>M</em> = 1.80), and a paired-samples <em>t</em>-test indicated this
difference was significant, <em>t</em>(32) = 29.06, <em>p</em> &lt; .001.”</p>
<p>The positive and negative stimuli were evaluated on a 7-point scale. <strong>Given that they were explicitly selected to be extremely positive and negative, is this test really contributing something</strong>? You might argue that at least it confirms that they differ, but in practice, we are reaching a foregone conclusion. Are we happy when the difference in evaluation is simply greater than zero? Imagine I repeat the experiment with 2000 participants, and used positive words and negative words that differed statistically from each other. However, the manipulation check shows the mean of positive words is 4.10, and the mean of negative words is 3.90. I argue that the difference between words is again statistically significant. Is this a valid replication? Probably not. The real question was perhaps not if the evaluations of these two groups differ, but <strong>how much they minimally need to differ to lead to the predicted effects</strong>. Is a difference of 6.51-1.80 = 4.71 scale points needed? Is a difference of 0.20 scale points sufficient as well?</p>
<p>When you read the literature, look at the null-hypothesis tests (whether Bayesian or frequentist) and judge if the question of whether the effect is zero is interesting, or if the authors might actually have been implicitly arguing for the presence of some unspecified minimal effect. <strong>If you were able to specify this minimal effect, would it have made more sense to report a minimal effect test for some of the hypothesis tests</strong>?</p>
<p>Meehl (1967 – yes, that is more than 40 years ago!) compared the use of statistical tests in psychology and physics, and notes that in physics, researchers make point predictions. One way to test point predictions is to examine whether the observed mean falls between an upper and lower bound. Such a test is visualized in the figure below. We have set bounds of -0.5 and 0.5, and predict our observed mean falls within these bounds. Note that the bounds happen to be symmetric around 0, but you can set the bounds wherever you like.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intervaltest"></span>
<img src="images/intervaltest.png" alt="Test of an interval hypothesis." width="688"><!--
<p class="caption marginnote">-->Figure 5.5: Test of an interval hypothesis.<!--</p>-->
<!--</div>--></span>
</p>
<p>If you have learned about <strong>equivalence testing</strong> (see Lakens, Scheel, &amp; Isager, 2018, and later in this course), you might recognize the practice of specifying these bounds (referred to as equivalence bounds) to examine whether the effect falls within an equivalence range – a range of values close enough to 0 to find the effects too small to matter. An equivalence test is basically a specific version of a range prediction, where the goal is to reject effects that are large enough to matter, so that we can conclude the effect is <strong>practically equivalent to zero</strong>.</p>
<p>But you can use equivalence tests to test any range. Let’s revisit our manipulation check example from above. The difference in means (in a dependent <em>t</em>-test) was 4.71 (6.51 - 1.80). We can test this difference against 0 in a one-sample <em>t</em>-test (which is the same as testing the two means against each other in a dependent <em>t</em>-test). But we can also perform a minimal effect test. Let’s start with the modest prediction that the difference in means should be not just larger than 0, but larger than 1. In other words, we aim to test the difference score against a lower bound of 1. We have no upper bound we want to test against, and in the test below, we simply fill in a very large value (a
difference of 10, while the maximum difference on a 7 point scale is 6) so that the test result is mainly determined by the one-sided test against a difference of 1 (the equivalence test used here reports the highest <em>p</em>-value from the pair of two one-sided tests). If we would perform this test, we would find:</p>
<p>Equivalence Test Result:</p>
<p>The equivalence test was significant, t(32) = 22.892, p =
0.00000000000000000000104, given equivalence bounds of 1.000 and 10.000 (on a
raw scale) and an alpha of 0.05.</p>
<p>Null Hypothesis Test Result:</p>
<p>The null hypothesis test was significant, t(32) = 29.062, p =
0.00000000000000000000000142, given an alpha of 0.05.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intervalresult"></span>
<img src="images/intervalresult.png" alt="The result of an interval hypothesis test." width="286"><!--
<p class="caption marginnote">-->Figure 5.6: The result of an interval hypothesis test.<!--</p>-->
<!--</div>--></span>
</p>
<p>Although Meehl prefers <strong>point predictions that lie within a certain range</strong>, he doesn’t completely reject the use of null-hypothesis significance testing. When he asks ‘Is it ever correct to use null-hypothesis significance tests?’ his own answer is ‘Of course it is’ (Meehl, 1990). <strong>There are times, such as very early in research lines, where researchers do not have good enough models, or reliable existing data, to make point or range predictions</strong>. Other times, two competing theories are not more precise than that one predicts rats in a maze will learn <em>something</em>, while the other theory predicts the rats will learn <em>nothing</em>. As Meehl writes: “When I was a rat psychologist, I unabashedly employed significance testing in latent-learning experiments; looking back I see no reason to fault myself for having done so in the light of my present methodological views.”</p>
<p>There are no good or bad statistical approaches – all statistical approaches are just answers to questions. <strong>What matters is asking the best possible question</strong>. It makes sense to allow traditional null-hypothesis tests early in research lines, when theories do not make more specific predictions than that ‘something’ will happen. But we should also push ourselves to develop theories that make more precise range predictions, and then test these more specific predictions. More mature theories should be able to predict effects in some range – even when these ranges are relatively wide.</p>
<p>Cho, H.-C., &amp; Abe, S. (2013). Is two-tailed testing for directional research
hypotheses tests legitimate? Journal of Business Research, 66(9), 1261–1266.
<a href="https://doi.org/10.1016/j.jbusres.2012.02.023" class="uri">https://doi.org/10.1016/j.jbusres.2012.02.023</a></p>
<p>Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence Testing for
Psychological Research: A Tutorial. <em>Advances in Methods and Practices in
Psychological Science</em>, 2515245918770963. <a href="https://doi.org/10/gdj7s9" class="uri">https://doi.org/10/gdj7s9</a></p>
<p>Meehl, P. E. (1967). Theory-testing in psychology and physics: A methodological
paradox. Philosophy of Science, 103–115.</p>
<p>Meehl, P. E. (1990). Appraising and amending theories: The strategy of
Lakatosian defense and two principles that warrant it. <em>Psychological Inquiry</em>,
<em>1</em>(2), 108–141.</p>
<p>Orben, A., &amp; Lakens, D. (2019). Crud (Re)defined.
<a href="https://doi.org/10.31234/osf.io/96dpy" class="uri">https://doi.org/10.31234/osf.io/96dpy</a></p>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="severe-tests-and-risky-predictions.html"><button class="btn btn-default">Previous</button></a>
<a href="verisimilitude-belief-and-progress-in-psychological-science.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-15
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-07-15
</span>
</p>
</div>


</body>
</html>
