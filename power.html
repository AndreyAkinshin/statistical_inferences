<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2 Sample size justification | Improving Your Statistical Inferences" />
<meta property="og:type" content="book" />
<meta property="og:url" content="http://themethodsection.com/ebook/" />
<meta property="og:image" content="http://themethodsection.com/ebook/images/cover.jpg" />
<meta property="og:description" content="Online textbook to Improve Your Statistical Inferences" />


<meta name="author" content="Daniel Lakens" />

<meta name="date" content="2020-08-03" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Online textbook to Improve Your Statistical Inferences">

<title>2 Sample size justification | Improving Your Statistical Inferences</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="center.css" type="text/css" />
<link rel="stylesheet" href="custom-msmbstyle.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Improving Your Statistical Inferences<p><p class="author">Daniel Lakens</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Welcome</a>
<a href="contents.html">Contents</a>
<a href="preface.html">Preface</a>
<a href="introduction.html">Introduction</a>
<a href="pvalue.html"><span class="toc-section-number">1</span> What is a <em>p</em>-value</a>
<a id="active-page" href="power.html"><span class="toc-section-number">2</span> Sample size justification</a><ul class="toc-sections">
<li class="toc"><a href="#measuring-the-entire-population"> Measuring the Entire Population</a></li>
<li class="toc"><a href="#feasibility"> Feasibility</a></li>
<li class="toc"><a href="#a-priori-power-analysis"> A-priori power analysis</a></li>
<li class="toc"><a href="#criterion-power-analysis"> Criterion power analysis</a></li>
<li class="toc"><a href="#NA"> How to justify your sample size based on a power analysis.</a></li>
<li class="toc"><a href="#observedpower"> Observed (post-hoc) power analysis (and what to do if your editor asks for them)</a></li>
<li class="toc"><a href="#why-within-subject-designs-typically-require-fewer-observations-than-between-subject-designs"> Why Within-Subject Designs Typically Require Fewer Observations than Between-Subject Designs</a></li>
<li class="toc"><a href="#requiring-high-powered-studies-from-scientists-with-resource-constraints"> Requiring high-powered studies from scientists with resource constraints</a></li>
</ul>
<a href="questions.html"><span class="toc-section-number">3</span> Asking Statistical Questions</a>
<a href="errorcontrol.html"><span class="toc-section-number">4</span> Error Control</a>
<a href="effectsizesCI.html"><span class="toc-section-number">5</span> Effect Sizes and Confidence Intervals</a>
<a href="equivalencetest.html"><span class="toc-section-number">6</span> Equivalence Testing</a>
<a href="severity.html"><span class="toc-section-number">7</span> Severe Tests and Risky Predictions</a>
<a href="sesoi.html"><span class="toc-section-number">8</span> Smallest Effect Size of Interest</a>
<a href="meta.html"><span class="toc-section-number">9</span> Meta-analysis</a>
<a href="bias.html"><span class="toc-section-number">10</span> Bias detection</a>
<a href="computationalreproducibility.html"><span class="toc-section-number">11</span> Computational Reproducibility</a>
<a href="prereg.html"><span class="toc-section-number">12</span> Preregistration and Transparency</a>
<a href="bayes.html"><span class="toc-section-number">13</span> Bayesian statistics</a>
<a href="references.html"><span class="toc-section-number">14</span> References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="power" class="section level1">
<h1>
<span class="header-section-number">2</span> Sample size justification</h1>
<p>When you perform an experiment, you want it to provide an answer to your research question that is as informative as possible. However, since all scientists are faced with resource limitations, you need to balance the cost of collecting each additional datapoint against the increase in information that datapoint provides. In economics, this is known as the Value of Information <span class="citation">(Eckermann et al., <label for="tufte-mn-5" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">2010<span class="marginnote">Eckermann, S., Karnon, J., &amp; Willan, A. R. (2010). The Value of Value of Information. <em>PharmacoEconomics</em>, <em>28</em>(9), 699–709. <a href="https://doi.org/10.2165/11537370-000000000-00000">https://doi.org/10.2165/11537370-000000000-00000</a></span>)</span>. Calculating the value of information is notoriously difficult. You need to specify the costs and benefits of possible outcomes of the study, and quantifying a utility function for scientific research is not easy.</p>
<p>Because of the difficulty of quantifying the value of information, scientists use less formal approaches to justify the amount of data they set out to collect. That is, if they provide a justification for the number of observations to begin with. Even though in some fields a justification for the number of observations is required when submitting a grant proposal to a science funder, a research proposal to an ethical review board, or a manuscript for submission to a journal. In some research fields, the number of observations is <em>stated</em>, but not <em>justified</em>. This makes it difficult to evaluate how informative the study was. Referees can’t just assume the number of observations is sufficient to provide an informative answer to your research question, so leaving out a justification for the number of observations is not best practice, and a reason reviewers can criticize your submitted article.</p>
<p>Below, several possible justifications for the number of observations are discussed. First, you might be able to <strong>collect data from the entire population</strong>. Second, you might justify the sample size based on <strong>feasibility</strong>. Since we always have resource constraints, it is good to know about feasibility justifications, even though they are rarely discussed in statistics textbooks. Third, you might want to perform an a-priori power analysis. We will discuss all these situations in turn. Before we do so, we need to get some terminology out of the way.</p>
<p>The <strong>sample size</strong> should not be confused with the number of participants. When single observations are collected from participants in a study, the number of observations equals the number of participants. In these cases, we can use a sample size justification and a justification for the number of participants interchangeably. When multiple observations from single individuals can be collected, the sample size is the number of observation, but not the number of participants.</p>
<p><strong>Statistical power</strong> is the probability of a test to yield a statistically significant result if the alternative hypothesis is true <span class="citation">(Aberson, <label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">2019<span class="marginnote">Aberson, C. L. (2019). <em>Applied Power Analysis for the Behavioral Sciences: 2nd Edition</em> (2 edition). Routledge.</span>; Cohen, <label for="tufte-mn-7" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">1988<span class="marginnote">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed). L. Erlbaum Associates.</span>)</span>. Power depends on the Type 1 error rate (α), the true effect size in the population, and the number of observations. Because the true effect size is typically unknown, it makes most sense to speak about the power function. In Figure <a href="power.html#fig:power-1">2.1</a> you see a power curve for an independent <em>t</em>-test, with an alpha level of 0.05. We see that as the effect size (in Cohen’s d) increases, power increases. If the effect you study has an effect size of 0.5, you would have almost 70% power with 50 observations in each independent group (indicated by the red dot). If the true effect size is smaller, power is lower, and as the effect size is larger, power is larger.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:power-1"></span>
<img src="Statistical_Inferences_files/figure-html/power-1-1.png" alt="Power curve for an independent *t*-test as a function of the true effect size." width="672"><!--
<p class="caption marginnote">-->Figure 2.1: Power curve for an independent <em>t</em>-test as a function of the true effect size.<!--</p>-->
<!--</div>--></span>
</p>
<p><strong>Power analysis</strong> is the practice of computing the effect size, desired power, sample size, or alpha level from the other three values. Computing the required sample size is commonly referred to as <strong>a-priori</strong> power analysis, computing the smallest effect size you have the desired power for is called <strong>sensitivty</strong> power analysis, computing the alpha level is called <strong>criterion</strong> power analysis, and computing the achieved power is called <strong>post-hoc</strong> power analysis. We will discuss all these different types of power analysis below. With this terminology out of the way, let’s examine different ways to justify the sample size.</p>
<div id="measuring-the-entire-population" class="section level2">
<h2>
<span class="header-section-number">2.1</span> Measuring the Entire Population</h2>
<p>In some instances it might be possible to collect data from the entire population under examination. For example, you might be interested in the average amount of time people spend on the moon, whenever they visit it. Since there are only 12 people who have ever been on the moon, we can calculate the average time people have spent on the moon by collecting data from the entire population. Whenever it is possible to measure the entire population, the justification for the sample size is that you collected all the data that is available. This is the simplest, and most straightforward, justification for the sample size. However, it is also quite rare that we are able to measure the entire population.</p>
</div>
<div id="feasibility" class="section level2">
<h2>
<span class="header-section-number">2.2</span> Feasibility</h2>
<p>A common reason why a specific number of observations is collected is because collecting more data was not feasible. Note that all decisions for the sample size we collect in a study are based on the resources we have available in some way. A <strong>feasibility justification</strong> makes these resource limitations the primary reason for the sample size that is collected. Because we always have resource limitations in science, even when feasibility is not our primary justification for the number of observations we plan to collect, it is always a secondary reason. Despite the omnipresence of resource limitations, the topic often receives very little attention in texts on experimental design. This might make it feel like a feasibility justification is not appropriate, and you should perform an a-priori power analysis or plan for a desired precision instead. But feasibility always plays a secondary role, and therefore regardless of which justification for the sample size you provide, you will almost always need to include a feasibility justification as well.</p>
<p>Time and money are the two main resource limitations a scientist faces. Our master students write their thesis in 6 months, and therefore their data collection is necessarily limited in whatever can be collected in 6 months, minus the time needed to formulate a research question, design an experiment, analyze the data, and write up the thesis. A PhD student at our department would have 4 years to complete their thesis, but is also expected to complete multiple research lines in this time. In addition to limitations on time, we have limited financial resources. Although nowadays it is possible to collect data online quickly, if you offer participants a decent pay (as you should) most researchers have the financial means to collect thousands of datapoints.</p>
<p>A feasibility justification puts the limited resources at the center of the justification for the sample size that will be collected. For example, one might argue that 120 observations is the most that can be collected in the three weeks a master student has available to collect data, when each observation takes an hour to collect. A PhD student might collect data until the end of the academic year, and then needs to write up the results over the summer to stay on track to complete the thesis in time.</p>
<p>A feasibility justification thus <em>starts</em> with the expected number of observations (N) that a researcher expects to be able to collect. The challenge is to evaluate whether collecting N observations is worthwhile. The answer should sometimes be that data collection is <em>not</em> worthwhile. For example, assume I plan to manipulate the mood of participants using funny cartoons and then measure the effect of mood on some dependent variable - say the amount of money people donate to charity. I should expect an effect size around d = 0.31 for the mood manipulation <span class="citation">(Joseph et al., <label for="tufte-mn-8" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2020<span class="marginnote">Joseph, D. L., Chan, M. Y., Heintzelman, S. J., Tay, L., Diener, E., &amp; Scotney, V. S. (2020). The manipulation of affect: A meta-analysis of affect induction procedures. <em>Psychological Bulletin</em>, <em>146</em>(4), 355–375. <a href="https://doi.org/10.1037/bul0000224">https://doi.org/10.1037/bul0000224</a></span>)</span>, and seems unlikely that the effect on donations will be larger than the effect size of the manipulation. If I can only collect mood data from 30 participants in total, how do we decide if this study will be informative?</p>
<p>First of all, more data is always better than no data, so in an absolute sense, all additional data that is collected is better than not collecting data. However, in line with the idea that we need to take into account costs and benefits, it is possible that the cost of data collection outweigh the benefits. To determine this, one needs to think about what the benefits of having the data are. The benefits are clearest when we know for certain that someone is going to make a decision, with or without data. If this is the case, then any data you collect will reduce the error rates of a well-calibrated decision process, even if only ever so slightly. In these cases, the value of information might be positive, as long as the reduction in error rates is more beneficial than the costs of data collection.</p>
<p>Another way in which a small dataset can be valuable is if its existence makes it possible to combine several small datasets into a meta-analysis. This argument in favor of collecting a small dataset requires 1) that you share the results in a way that a future meta-analyst can find them, and 2) that there is a decent probability that someone will perform a meta-analysis in the future which inclusion criteria would contain your study, because a sufficient number of small studies exist. The uncertainty about whether there will ever be such a meta-analysis should be weighed against the costs of data collection. One way to increase the probability of a future meta-analysis is if you commit to performing this yourself in the future. For example, you might plan to repeat a study for the next 12 years in a class you teach, with the expectation that a meta-analysis of 360 participants would be sufficient to achieve around 90% power for d = 0.31. If it is not plausible you will collect all the required data by yourself, you can attempt to set up a collaboration, where fellow researchers in your field commit to collecting similar data, with identical measures, over the next years. If it is not likely sufficient data will emerge over time, we will not be able to draw informative conclusions from the data, and it might be more beneficial to not collect the data to begin with, and examine an alternative research question with a larger effect size instead.</p>
<p>Even if you believe over time sufficient data will emerge, you will most likely compute statistics after collecting a small sample size. Before embarking on a study where your main justification for the sample size is based on feasibility, you can expect. I propose that a feasibility justification for the sample size is always accompanied by three statistics, detailed in the following three sections.</p>
<div id="the-smallest-effect-size-that-can-be-statistically-significant" class="section level3">
<h3>
<span class="header-section-number">2.2.1</span> The smallest effect size that can be statistically significant</h3>
<p>In Figure <a href="power.html#fig:power-effect1">2.2</a> the distribution of Cohen’s d given 15 participants per group is plotted when the true effect size is 0 (or the null-hypothesis is true), and when the true effect size is d = 0.5. The blue area is the Type 2 error rate (the probability of not finding p &lt; α, when there is a true effect, and α = 0.05).</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:power-effect1"></span>
<img src="Statistical_Inferences_files/figure-html/power-effect1-1.png" alt="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 15 per group." width="672"><!--
<p class="caption marginnote">-->Figure 2.2: Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 15 per group.<!--</p>-->
<!--</div>--></span>
</p>
<p>You might seen such graphs before. The only thing I have done is to transform the <em>t</em>-value distribution that is commonly used in these graphs, and calculated the distribution for Cohen’s d. This is a straightforward transformation, but instead of presenting the critical <em>t</em>-value the figure provides the critical <em>d</em>-value. For a two-sided independent <em>t</em>-test, this is calculated as:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="power.html#cb16-1"></a><span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span>(a <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), (n1 <span class="op">+</span><span class="st"> </span>n2) <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span>n1 <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n2)</span></code></pre></div>
<p>where ‘a’ is the alpha level (e.g., 0.05) and N is the sample size in each independent group. For the example above, where alpha is 0.05 and n = 15:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="power.html#cb17-1"></a><span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span>(<span class="fl">0.05</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), (<span class="dv">15</span> <span class="op">*</span><span class="st"> </span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">15</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">15</span>)</span></code></pre></div>
<pre><code>## [1] 0.7479725</code></pre>
<p>The critical <em>t</em>-value (2.0484071) is also provided in commonly used power analysis software such as G*Power. We can compute the critical Cohen’s d from the <em>t</em>-value and sample size using: <span class="math inline">\(d = t{\sqrt{1/n_1 + 1/n_2}}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:gcrit1"></span>
<img src="images/gpowcrit.png" alt="The critical *t*-value is provided by G\*Power software." width="378"><!--
<p class="caption marginnote">-->Figure 2.3: The critical <em>t</em>-value is provided by G*Power software.<!--</p>-->
<!--</div>--></span>
</p>
<p>When you will test an association between variables with a correlation, G*Power will directly provide you with the critical effect size. When you compute a correlation based on a two-sided test, your alpha level is 0.05, and you have 30 observations, only effects larger than r = 0.361 will be statistically significant. In other words, the effect needs to be quite large to even have the mathematical possibility of becoming statistically significant.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:gcrit2"></span>
<img src="images/gpowcrit2.png" alt="The critical r is provided by G\*Power software." width="378"><!--
<p class="caption marginnote">-->Figure 2.4: The critical r is provided by G*Power software.<!--</p>-->
<!--</div>--></span>
</p>
<p>The critical effect size gives you information about the smallest effect size that, if observed, would by statistically significant. If you observe a smaller effect size, the <em>p</em>-value will be larger than your significance threshold. You always have some probability of observing effects larger than the critical effect size. After all, even if the null hypothesis is true, 5% of your tests will yield a significant effect. But what you should ask yourself is whether the effect sizes that could be statistically significant are realistically what you would expect to find. If this is not the case, it should be clear that there is no use in performing a significance test. Mathematically, when the critical effect size is larger than effects you expect, your statistical power will be less than 50% (see the section on <a href="power.html#observedpower">observed power</a>). If you perform a statistical test with less than 50% power, your single study is not very informative.</p>
</div>
<div id="compute-the-width-of-the-confidence-interval-around-the-effect-size" class="section level3">
<h3>
<span class="header-section-number">2.2.2</span> Compute the width of the confidence interval around the effect size</h3>
<p>The second statistic to report alongside a feasibility justification is the width of the 95% confidence interval around the effect size. As we saw in the section on <a href="effectsizesCI.html#confint">confidence intervals</a> they represent a range that is wide enough so that in the long run in 95% of repetitions of the same experiment the true population parameter falls within each confidence interval calculated around the observed effect size. Cumming <span class="citation">(<label for="tufte-mn-9" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">2013<span class="marginnote">Cumming, G. (2013). <em>Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis</em>. Routledge.</span>)</span> calls the difference between the observed effect size and the upper 95% confidence interval (or the lower 95% confidence interval) the margin of error (MOE).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="power.html#cb19-1"></a><span class="co"># Compute the effect size d and 95% CI</span></span>
<span id="cb19-2"><a href="power.html#cb19-2"></a>res &lt;-<span class="st"> </span><span class="kw">d.ind.t</span>(<span class="dt">m1 =</span> <span class="dv">0</span>, <span class="dt">m2 =</span> <span class="dv">0</span>, <span class="dt">sd1 =</span> <span class="dv">1</span>, <span class="dt">sd2 =</span> <span class="dv">1</span>, <span class="dt">n1 =</span> <span class="dv">15</span>, <span class="dt">n2 =</span> <span class="dv">15</span>, <span class="dt">a =</span> <span class="fl">.05</span>)</span>
<span id="cb19-3"><a href="power.html#cb19-3"></a><span class="co"># Print the result</span></span>
<span id="cb19-4"><a href="power.html#cb19-4"></a>res<span class="op">$</span>estimate</span></code></pre></div>
<pre><code>## [1] "$d_s$ = 0.00, 95\\% CI [-0.72, 0.72]"</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="power.html#cb21-1"></a><span class="co"># Compute the average Margin of Error (MOE)</span></span>
<span id="cb21-2"><a href="power.html#cb21-2"></a>(res<span class="op">$</span>dhigh <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>dlow)<span class="op">/</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="power.html#cb23-1"></a><span class="co"># For non-zero effects the CI is based on the non-central t</span></span>
<span id="cb23-2"><a href="power.html#cb23-2"></a><span class="co"># The MOE is then asymmetric (but differences are very small)</span></span>
<span id="cb23-3"><a href="power.html#cb23-3"></a>(res<span class="op">$</span>d <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>dlow)</span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="power.html#cb25-1"></a>(res<span class="op">$</span>dhigh <span class="op">-</span><span class="st"> </span>res<span class="op">$</span>d)</span></code></pre></div>
<pre><code>## [1] 0.7156777</code></pre>
<p>If we compute the 95% CI for an effect size of 0, we see that with 15 observations in each condition of an independent <em>t</em>-test the 95% CI ranges from -0.72 to 0.72. The MOE is half the width of the 95% CI, 0.72. This clearly shows we have a very imprecise estimate. A Bayesian estimator who uses an uninformative prior would compute a credible interval with the same upper and lower bound <span class="citation">(Albers et al., <label for="tufte-mn-10" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">2018<span class="marginnote">Albers, C. J., Kiers, H. A. L., &amp; Ravenzwaaij, D. van. (2018). Credible Confidence: A Pragmatic View on the Frequentist vs Bayesian Debate. <em>Collabra: Psychology</em>, <em>4</em>(1), 31. <a href="https://doi.org/10.1525/collabra.149">https://doi.org/10.1525/collabra.149</a></span>)</span>, and might conclude they personally believe there is a 95% chance the true effect size lies in this interval. A frequentist would reason more hypothetically: If the observed effect size in the data I plan to collect is 0, I could only reject effects more extreme than d = 0.72 in an equivalence test with a 5% alpha level (even though if such a test would be performed, power might be low, depending on the true effect size). Regardless of the statistical philosophy you plan to rely on when analyzing the data, both of these conclusions are pretty much trivial. Effect sizes in this range are findings such as “People become aggressive when they are provoked”, “People prefer their own group to other groups”, and “Romantic partners resemble one another in physical attractiveness” <span class="citation">(Richard et al., <label for="tufte-mn-11" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">2003<span class="marginnote">Richard, F. D., Bond, C. F., &amp; Stokes-Zoota, J. J. (2003). One Hundred Years of Social Psychology Quantitatively Described. <em>Review of General Psychology</em>, <em>7</em>(4), 331–363. <a href="https://doi.org/10.1037/1089-2680.7.4.331">https://doi.org/10.1037/1089-2680.7.4.331</a></span>)</span>. If you can only reject the presence of effects that are so large people would clearly notice them, there is typically little need to collect data, as we do not learn something we didn’t already know.</p>
<p>We see this the MOE is almost, but not exactly, the same as the critical effect size d we observed above (d = 0.7479725). The reason for this is that the 95% confidence interval is calculated based on the <em>t</em>distribution. If the true effect size is not zero, the confidence interval is calculated based on the non-central <em>t</em>distribution, and the 95% CI is asymmetric. Figure <a href="power.html#fig:noncentralt">2.5</a> vizualizes three <em>t</em>-distributions, one symmetric at 0, and two asymmetric distributions with a noncentrality parameter of 2 and 3. The asymmetry is most clearly visible in very small samples (the distribution in the plot have 5 degrees of freedom) but remain noticeablewhen calculating confidence intervals and statistical power. For example, for a true effect size of d = 0.5 the 95% CI is <span class="math inline">\(d_s\)</span> = 0.50, 95\% CI [-0.23, 1.22]. The MOE based on the lower bound is 0.7317584 and based on the upper bound is 0.7231479. If we compute the 95% CI around the critical effect size (d = 0.7479725) we see the 95% CI ranges from exactly 0.00 to 1.48. As explained in the section on the relation between a <a href="effectsizesCI.html#relatCIp">confidence interval and a <em>p</em>-value</a>, if the 95% CI excludes zero, the test is statistically significant. In this case the lowerbound of the confidence interval exactly touches 0, which means we would observe a <em>p</em> = 0.05 if we exactly observed the critical effect size.</p>
<div class="figure">
<span id="fig:noncentralt"></span>
<p class="caption marginnote shownote">
Figure 2.5: Central (black) and 2 non-central (red and blue) <em>t</em>-distributions.
</p>
<img src="Statistical_Inferences_files/figure-html/noncentralt-1.png" alt="Central (black) and 2 non-central (red and blue) *t*-distributions." width="672">
</div>
<p>Where computing the critical effect size can make it clear that a <em>p</em>-value is of little interest, computing the 95% CI around the effect size can make it clear that the effect size estimate is of little value. It will often be so uncertain, and the range of effect sizes you will not be able to reject if there is no effect is so large, the effect size estimate is not very useful. This is also the reason why performing a pilot study to estimate an effect size for an a-priori power analysis is not a sensible strategy <span class="citation">(Albers &amp; Lakens, <label for="tufte-mn-12" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">2018<span class="marginnote">Albers, C. J., &amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. <em>Journal of Experimental Social Psychology</em>, <em>74</em>, 187–195. <a href="https://doi.org/10.1016/j.jesp.2017.09.004">https://doi.org/10.1016/j.jesp.2017.09.004</a></span>; Leon et al., <label for="tufte-mn-13" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">2011<span class="marginnote">Leon, A. C., Davis, L. L., &amp; Kraemer, H. C. (2011). The Role and Interpretation of Pilot Studies in Clinical Research. <em>Journal of Psychiatric Research</em>, <em>45</em>(5), 626–629. <a href="https://doi.org/10.1016/j.jpsychires.2010.10.008">https://doi.org/10.1016/j.jpsychires.2010.10.008</a></span>)</span>. However, it is possible that the sample size is large enough to exclude some effect sizes that are still a-priori plausible. For example, with 50 observations in each independent group, you have 82% <a href="equivalencetest.html#powerequiv">power for an equivalence test</a> with bounds of -0.6 and 0.6, and if effect larger than 0.6 can be rejected, this might be sufficient to tentatively start to question claims of even larger effects in the literature.</p>
</div>
<div id="sensitivitypower" class="section level3">
<h3>
<span class="header-section-number">2.2.3</span> Plot a sensitivity power analysis</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:gsens0"></span>
<img src="images/sensitivity0.png" alt="Sensitivity power analysis in G\*Power software." width="378"><!--
<p class="caption marginnote">-->Figure 2.6: Sensitivity power analysis in G*Power software.<!--</p>-->
<!--</div>--></span>
</p>
<p>In a <strong>sensitivity power analysis</strong> the sample size and the alpha level are fixed, and you compute the effect size you have the desired statistical power to detect. For example, in Figure <a href="power.html#fig:gsens0">2.6</a> the sample size in each group is set to 15, the alpha level is 0.05, and the desired power is set to 90%. The sensitivity power analysis shows we have 90% power to detect an effect of d = 1.23. Perhaps you feel a power of 90% is a bit high, and you would be happy with 80% power. In Figure <a href="power.html#fig:gsens1">2.7</a> we see that if we desire 80% power, the effect size should be d = 1.06. The smaller the true effect size, the lower the power we have. This plot should again remind us not to put too much faith in a significance test when are sample size is small, since for 15 observations in each condition, statistical power is very low for anything but extremely large effect sizes.</p>
<div class="figure">
<span id="fig:gsens1"></span>
<p class="caption marginnote shownote">
Figure 2.7: Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05.
</p>
<img src="images/sensitivity1.png" alt="Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05." width="485">
</div>
<p>If we look at the effect size that we would have 50% power for, we see it is d = 0.7411272. This is very close to our critical effect size of d = 0.7479725 (the smallest effect size that, if observed, would be significant). The difference is due to the non-central <em>t</em>-distribution (see Figure <a href="power.html#fig:noncentralt">2.5</a>. If the distribution was symmetric, observing an effect size exactly on the critical value would mean half of the distirbution is smaller than the critical effect size, and half of the distribution is larger, and we would have exactly 50% power for the critical effect size. Because the distribution is not symmetrical, we need to find the critical effect size for which it <em>is</em> true that half the distribution falls below it, and half the distribution falls above it. This value can’t be calculated directly, and requires an iterative procedure that optimizes the values of the non-centrality parameter such that power is exactly 50% <span class="citation">(Smithson, <label for="tufte-mn-14" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-14" class="margin-toggle">2003<span class="marginnote">Smithson, M. (2003). <em>Confidence intervals</em>. Sage Publications.</span>)</span>. The <code>pwr</code> package in R can calculate this effect size in a sensitivity analysis where we enter the sample size (per group), the alpha level, and the desired power for a two-sided independent <em>t</em>-test, which will return d = 0.7411272. We can check this true effect size would indeed give with 50% power (but feel free to skip these technicalities).</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="power.html#cb27-1"></a><span class="co"># Sensitivity power analysis</span></span>
<span id="cb27-2"><a href="power.html#cb27-2"></a>pwr<span class="op">::</span><span class="kw">pwr.t.test</span>(<span class="dt">n =</span> <span class="dv">15</span>, </span>
<span id="cb27-3"><a href="power.html#cb27-3"></a>                <span class="dt">sig.level =</span> <span class="fl">0.05</span>, </span>
<span id="cb27-4"><a href="power.html#cb27-4"></a>                <span class="dt">power =</span> <span class="fl">0.5</span>, </span>
<span id="cb27-5"><a href="power.html#cb27-5"></a>                <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb27-6"><a href="power.html#cb27-6"></a>                <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 15
##               d = 0.7411272
##       sig.level = 0.05
##           power = 0.5
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="power.html#cb29-1"></a><span class="co"># d = 0.7411272 - let's check this indeed gives 50% power</span></span>
<span id="cb29-2"><a href="power.html#cb29-2"></a><span class="co"># We compute the non-centrality parameter delta as 2.0296604</span></span>
<span id="cb29-3"><a href="power.html#cb29-3"></a>delta &lt;-<span class="st"> </span><span class="fl">0.7411272</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">15</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb29-4"><a href="power.html#cb29-4"></a></span>
<span id="cb29-5"><a href="power.html#cb29-5"></a><span class="co"># For a 2-sided test power is the probability of finding a result that is</span></span>
<span id="cb29-6"><a href="power.html#cb29-6"></a><span class="co"># 1) larger than the critical value of 2.0484071, which is: 0.4999563</span></span>
<span id="cb29-7"><a href="power.html#cb29-7"></a><span class="co"># 2) We need to add the tiny probability that we find a significant result in</span></span>
<span id="cb29-8"><a href="power.html#cb29-8"></a><span class="co"># the opposite direction. This will rarely happen: 0.00004372648</span></span>
<span id="cb29-9"><a href="power.html#cb29-9"></a><span class="co"># Together these two probabilities make exactly 50%:</span></span>
<span id="cb29-10"><a href="power.html#cb29-10"></a><span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(<span class="fl">2.0484071</span>, <span class="dv">28</span>, <span class="fl">2.0296604</span>) <span class="op">+</span><span class="st"> </span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.0484071</span>, <span class="dv">28</span>, <span class="fl">2.0296604</span>)</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
</div>
<div id="reporting-a-feasibility-justification." class="section level3">
<h3>
<span class="header-section-number">2.2.4</span> Reporting a feasibility justification.</h3>
<p>To summarize, I recommend addressing the following components in a feasibility sample size justification. Addressing these points explicitly will allow you to evaluate for yourself if collecting the data will have scientific value. If not, there might be other reasons to collect the data. For example, at our department, students often collect data as part of their education. However, if the primary goal of data collection is educational, the sample size that is collected can be very small. It is often educational to collect data from a small number of participants to experience what data collection looks like in practice, but there is often no educational value in collecting data from more than 10 participants. Despite the small sample size, we often require students to report statistical analyses as part of their education, which is fine as long as it is clear the numbers that are calculated can not meaningfully be interpreted. Table <a href="power.html#tab:table-pow-rec">2.1</a> should help to evaluate if the interpretation of statistical tests has any value, or not.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:table-pow-rec">Table 2.1: </span>Overview of recommendations when reporting a sample size justification based on feasibility.</span><!--</caption>--></p>
<table>
<thead><tr>
<th style="text-align:left;">
What to address
</th>
<th style="text-align:left;">
Example?
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Will a future meta-analysis be performed?
</td>
<td style="text-align:left;">
Consider the plausibility that sufficient highly similar studies will be performed in the future to, eventually, make a met-analysis possible
</td>
</tr>
<tr>
<td style="text-align:left;">
Will a decision be made, regardless of the amount of data that is available?
</td>
<td style="text-align:left;">
If it is known that a decision will be made, with or without data, then any data you collect will reduce error rates.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the critical effect size?
</td>
<td style="text-align:left;">
Report and interpret the critical effect size, with a focus on whether a hypothesis test would even be significant for expected effect sizes. If not, indicate you will not interpret the data based on <em>p</em>-values.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the width of the confidence interval?
</td>
<td style="text-align:left;">
Report and interpret the width of the confidence interval. What will an estimate with this much uncertainty be useful for? If the null hypothesis is true, would rejecting effects outside of the confidence interval be worthwhile (ignoring you might have low power to actually test against these values)?
</td>
</tr>
<tr>
<td style="text-align:left;">
Which effect sizes would you have decent power to detect?
</td>
<td style="text-align:left;">
Report a sensitivity power analysis, and report the effect sizes you could detect across a range of desired power levels (e.g., 80%, 90%, and 95%), or plot a sensitivity curve of effect sizes against desired power.
</td>
</tr>
</tbody>
</table>
<p>If the study is not performed for educational purposes, but the goal is answer a research question, the feasibility justification might indicate that there is no value in collecting the data. If it wasn’t possible to conclude that one should not proceed with the data collection, there is no use of justifying the sample size. There should be cases where it is unlikely there will ever be enough data to perform a meta-analysis (for example because of a lack of general interest in the topic), the information will not be used to make any decisions, and the statistical tests do not allow you to test a hypothesis or estimate an effect size estimate with any useful accuracy. It should be a feasibility justification - not a feasibility excuse. If there is no good justification to collect the maximum number of observations that is feasible, performing the study nevertheless is a waste of participants time, and/or a waste of money if data collection has associated costs. Collecting data without a good justification why the planned sample size will yield worthwhile information has an ethical component. As Button and colleagues <span class="citation">Button et al. (<label for="tufte-mn-15" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-15" class="margin-toggle">2013<span class="marginnote">Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. <em>Nature Reviews Neuroscience</em>, <em>14</em>(5), 365–376. <a href="https://doi.org/10.1038/nrn3475">https://doi.org/10.1038/nrn3475</a></span>)</span> write: “Low power therefore has an ethical dimension — unreliable research is inefficient and wasteful. This applies to both human and animal research”. Think carefully if you can defend data collection based on a feasibility justification. Sometimes data collection is just not feasible, and we should accept this.</p>
</div>
</div>
<div id="a-priori-power-analysis" class="section level2">
<h2>
<span class="header-section-number">2.3</span> A-priori power analysis</h2>
<p>When designing a study where the goal is to observe a statistically significant effect, researchers often want to make sure their sample size is large enough to have sufficient power to detect effects they expect, or effects they are interested in observing. This is done by performing an <em>a-priori</em> power analysis. Given a specified effect size, alpha level, and desired power, an a-priori power analysis will inform you about the sample size you need to collect. In Figure <a href="power.html#fig:power-2">2.8</a> you see how the statistical power increases as the number of observations (per group) in an independent <em>t</em>-test with an alpha level of 0.05 increases.</p>
<div class="figure">
<span id="fig:power-2"></span>
<p class="caption marginnote shownote">
Figure 2.8: Power curve for an independent <em>t</em>-test as a function of the sample size.
</p>
<img src="Statistical_Inferences_files/figure-html/power-2-1.png" alt="Power curve for an independent *t*-test as a function of the sample size." width="672">
</div>
<p>A-priori power calculations are performed under the assumption that there is an effect. In practice, it is of course also possible that there is no effect (e.g., d = 0). If there is no true effect, and you want to use formally correct language, power is <em>undefined</em>. Statistical power is a concept that can only be computed assuming there is a true effect. If there is no true effect, you will still observe significant effects, but these ar <em>Type 1 errors</em>, or false positives. These occur at your chosen alpha level (e.g., 5% of the time). If you perform a hypothesis test, there are four possible outcomes:</p>
<ol style="list-style-type: decimal">
<li>False positives or Type 1 errors, indicated by α (you observe a significant test result when H0 is true)</li>
<li>False negatives or Type 2 errors, indicated by β (you observe a non-significant result when H1 is true)</li>
<li>True negatives, indicated by 1-α (a non-significant result when H0 is true)</li>
<li>True positives, indicated by 1-β (a significant test result when H1 is true)</li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:NHST-outcomes"></span>
<img src="images/2.1.1.png" alt="Four possible outcomes in a null hypothesis significance test." width="961"><!--
<p class="caption marginnote">-->Figure 2.9: Four possible outcomes in a null hypothesis significance test.<!--</p>-->
<!--</div>--></span>
</p>
<p>The goal of an a-priori power analysis is to increase the sample size up to the level that the desired power, the probability of finding a significant result if there is a true effect, or 1-β, is at a desired level for an effect size one is interested in detecting, given a specific alpha level.</p>
<p>In Figure <a href="power.html#fig:power-3">2.10</a> you see two distributions. The left (grey) distribution is centered at 0. This is our model for the null hypothesis. If the null hypothesis is true we can find statistically significant results if the effect size is extreme enough (in a two-sided test either in the positive or negative direction), but these would be Type 1 errors (the red areas under the curve).</p>
<p>The right (black) distribution is centered at an effect of d = 0.5. This is our model for the alternative hypothesis, where we expect a true effect in the population of d = 0.5. Even though there is a true effect, we will not always find a statistically significant result. This happens when, due to random variation, the observed effect size is too close to 0 to be statistically significant. These results would be false negative results (the blue area under the curve).</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:power-3"></span>
<img src="Statistical_Inferences_files/figure-html/power-3-1.png" alt="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group." width="672"><!--
<p class="caption marginnote">-->Figure 2.10: Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group.<!--</p>-->
<!--</div>--></span>
</p>
<p>If we want to increase the statistical power, we are trying to reduce the size of the blue area. One way to do this would be to increase the effect size. This would shift the entire distribution to the right, and reduce the size of the blue area. Although there are ways in controlled experiments to increase the standardized effect size (e.g., by reducing statistical variation in the data), often the effect size is what it is. All we can do is increase the sample size. This will make the distribution around 0.5 more narrow, and this reduces the blue area. You can check this in an online shiny app that <a href="http://shiny.ieis.tue.nl/d_p_power/">reproduces the plot</a>.</p>
<div id="performing-a-power-analysis." class="section level3">
<h3>
<span class="header-section-number">2.3.1</span> Performing a power analysis.</h3>
<p>There is a wide range of software tools that can help you to perform an a-priori power analysis. Sometimes statistical power can be analytically computed based on closed formulas, and in other situations power can be computed by performing simulations. If we simulate thousands of studies, and count the percentage of studies that are statistically significant, we have an estimate of the statistical power of the test. The code below computes the statistical power (using the <code>power.t.test</code> in base R) assuming an effect size of d = 0.5, and alpha level of 0.05, and a desired power of 90%.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="power.html#cb31-1"></a><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="fl">0.5</span>, </span>
<span id="cb31-2"><a href="power.html#cb31-2"></a>             <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb31-3"><a href="power.html#cb31-3"></a>             <span class="dt">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb31-4"><a href="power.html#cb31-4"></a>             <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb31-5"><a href="power.html#cb31-5"></a>             <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 85.03129
##           delta = 0.5
##              sd = 1
##       sig.level = 0.05
##           power = 0.9
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>The output tells us we need to collect 85.03 observations. Because observations do not come in decimals (we can hardly cut 0.03 from a participant) we need to collect 86 observations. We could also get this answer through simulations.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="power.html#cb33-1"></a><span class="kw">set.seed</span>(<span class="dv">600746</span>) <span class="co">#set a seed for reproducible simulations</span></span>
<span id="cb33-2"><a href="power.html#cb33-2"></a>p &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">100000</span>) <span class="co">#set up empty variable to store all simulated p-values</span></span>
<span id="cb33-3"><a href="power.html#cb33-3"></a></span>
<span id="cb33-4"><a href="power.html#cb33-4"></a><span class="co">#Run simulation</span></span>
<span id="cb33-5"><a href="power.html#cb33-5"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100000</span>){ <span class="co">#for each simulated experiment</span></span>
<span id="cb33-6"><a href="power.html#cb33-6"></a>  x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 1</span></span>
<span id="cb33-7"><a href="power.html#cb33-7"></a>  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">85</span>, <span class="dt">mean =</span> <span class="fl">0.5</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co">#Simulate data group 2</span></span>
<span id="cb33-8"><a href="power.html#cb33-8"></a>  z &lt;-<span class="st"> </span><span class="kw">t.test</span>(x, y) <span class="co">#perform the t-test</span></span>
<span id="cb33-9"><a href="power.html#cb33-9"></a>  p[i] &lt;-<span class="st"> </span>z<span class="op">$</span>p.value <span class="co">#get the p-value and store it</span></span>
<span id="cb33-10"><a href="power.html#cb33-10"></a>}</span>
<span id="cb33-11"><a href="power.html#cb33-11"></a></span>
<span id="cb33-12"><a href="power.html#cb33-12"></a><span class="co">#Calculate power: sum significant p-values, divide by 100000 simulations</span></span>
<span id="cb33-13"><a href="power.html#cb33-13"></a>(<span class="kw">sum</span>(p <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>)<span class="op">/</span><span class="dv">100000</span>) <span class="co">#power</span></span></code></pre></div>
<pre><code>## [1] 0.90015</code></pre>
<p>We see that after 100000 simulations with 85 participants, the estimated power is 0.90015 with 85 participants in each group. The analytic solution tells us power is 0.899894. The more simulations you perform, the more accurate the power estimate will be (as we see, 100000 simulations gives very accurate results).</p>
<p>The way you perform the power analysis depends on the software you use. There are excellent software packages for power analysis, such as <a href="https://www.ncss.com/software/pass/">PASS</a> or <a href="https://stats.idre.ucla.edu/sas/seminars/proc-power/">SAS</a>, but these solutions are rather expensive. There are also freeware solutions, such as the widely used <a href="https://gpower.hhu.de">G*Power</a>, <a href="https://wiki.usask.ca/pages/viewpageattachments.action?pageId=420413544">MorePower</a> by Campbell and Thompson, and <a href="https://jakewestfall.shinyapps.io/pangea/">PANGEA</a> by Jake Westfall.
There is also a range of options in R. The default <code>stats</code> package has power functions for <em>t</em>-tests, proportions, and ANOVA, the <code>pwr</code> package has a wider range of options, <code>pwr2ppl</code> by Chris Aberson accompanies his excellent book on power analyses <span class="citation">(Aberson, <label for="tufte-mn-16" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-16" class="margin-toggle">2019<span class="marginnote">Aberson, C. L. (2019). <em>Applied Power Analysis for the Behavioral Sciences: 2nd Edition</em> (2 edition). Routledge.</span>)</span>, <code>powerlmm</code>by Kristoffer Magnusson performs power analyses for <a href="https://github.com/rpsychologist/powerlmm">two- and three-level linear mixed models</a>, and <a href="https://aaroncaldwell.us/SuperpowerBook/">Superpower</a> created by Aaron Caldwell and myself does power analyses for complex ANOVA designs.</p>
<p>It takes time to learn to use this software correctly. You will need to sit down for a few hours and go through vignettes or read the accompanying publications, before you know what to do. In our online manual for Superpower, we compare Superpower against G*Power, <code>pwr</code>, <code>pwr2ppl</code> and MorePower. Each software package will have slightly different design philosophies. In G*Power you are expected to enter the standardized effect size d, while in MorePower we enter the mean difference and the standard deviation, and in Superpower we need to enter the means and standard deviations for each condition. In general I recommend to always think about the raw pattern of means you expect, before you perform a power analysis. This typically leads to more realistic effect sizes estimates than the use of standardized effect sizes (where researcher all too often ‘expect’ a d = 0.5 just because that is the default value in G*Power).</p>
<div class="figure">
<span id="fig:powerex1"></span>
<p class="caption marginnote shownote">
Figure 2.11: Example of a power analysis in G*Power.
</p>
<img src="images/powerex1.png" alt="Example of a power analysis in G\*Power." width="378">
</div>
<div class="figure">
<span id="fig:powerex2"></span>
<p class="caption marginnote shownote">
Figure 2.12: Example of a power analysis in MorePower.
</p>
<img src="images/powerex2.png" alt="Example of a power analysis in MorePower." width="222">
</div>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="power.html#cb35-1"></a>pwr<span class="op">::</span><span class="kw">pwr.t.test</span>(<span class="dt">d =</span> <span class="fl">0.5</span>, </span>
<span id="cb35-2"><a href="power.html#cb35-2"></a>                <span class="dt">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb35-3"><a href="power.html#cb35-3"></a>                <span class="dt">power =</span> <span class="fl">0.8</span>,</span>
<span id="cb35-4"><a href="power.html#cb35-4"></a>                <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb35-5"><a href="power.html#cb35-5"></a>                <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 63.76561
##               d = 0.5
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="power.html#cb37-1"></a>design_result &lt;-<span class="st"> </span>Superpower<span class="op">::</span><span class="kw">ANOVA_design</span>(<span class="dt">design =</span> <span class="st">"2b"</span>,</span>
<span id="cb37-2"><a href="power.html#cb37-2"></a>                                          <span class="dt">n =</span> <span class="dv">86</span>,</span>
<span id="cb37-3"><a href="power.html#cb37-3"></a>                                          <span class="dt">mu =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>),</span>
<span id="cb37-4"><a href="power.html#cb37-4"></a>                                          <span class="dt">sd =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb37-5"><a href="power.html#cb37-5"></a>                                          <span class="dt">plot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb37-6"><a href="power.html#cb37-6"></a></span>
<span id="cb37-7"><a href="power.html#cb37-7"></a><span class="co"># Plot power curve (from 5 to 200)</span></span>
<span id="cb37-8"><a href="power.html#cb37-8"></a>Superpower<span class="op">::</span><span class="kw">plot_power</span>(design_result, <span class="dt">max_n =</span> <span class="dv">200</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:powerex3"></span>
<img src="Statistical_Inferences_files/figure-html/powerex3-1.png" alt="Example of a power analysis plot in Superpower." width="672"><p class="caption marginnote shownote">
Figure 2.13: Example of a power analysis plot in Superpower.
</p>
</div>
</div>
<div id="some-advice-when-using-gpower." class="section level3">
<h3>
<span class="header-section-number">2.3.2</span> Some advice when using G*Power.</h3>
<p>The option for power analysis for a Pearson’s correlation coefficient is under the Exact test family (Correlation: Bivariate normal model). The “Correlation: Point biserial model” option under the <em>t</em>-tests family is for correlations where one variable is dichotomous. The difference is small, but the required sample size is typically a few observations larger for Pearson’s correlation coefficient.</p>
<div class="figure">
<span id="fig:gpowcor"></span>
<p class="caption marginnote shownote">
Figure 2.14: The options for a power analysis for Pearson’s correlation(above) and the point biserial correlation (when one variable is dichotomous).
</p>
<img src="images/gpowcor.png" alt="The options for a power analysis for Pearson's correlation(above) and the point biserial correlation (when one variable is dichotomous)." width="377">
</div>
<p>Although the effect size for an independent <em>t</em>-test and dependent <em>t</em>-test are often both referred to as Cohen’s d, they differ, and are calculated in different ways. Cohen himself distinguished between <span class="math inline">\(d_s\)</span> and <span class="math inline">\(d_z\)</span> <span class="citation">(Lakens, <label for="tufte-mn-17" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle">2013<span class="marginnote">Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. <em>Frontiers in Psychology</em>, <em>4</em>. <a href="https://doi.org/10.3389/fpsyg.2013.00863">https://doi.org/10.3389/fpsyg.2013.00863</a></span>)</span>. You should not enter Cohen’s d for a power analysis for a dependent <em>t</em>-test (nor should you directly compared Cohen’s d from a between design with Cohen’s d for a within design, nor should you use the benchmarks Cohen provided for a small (0.2), medium (0.5), and large (0.8) effect be used for Cohen’s <span class="math inline">\(d_z\)</span>). Make sure you are entering the correct effect size. Cohen’s <span class="math inline">\(d_z\)</span> can be calculated from the <em>t</em>-value for a dependent <em>t</em>-test and the sample size as follows:</p>
<p><span class="math display">\[d_z = {\frac{t}{\sqrt{n}}} \]</span></p>
<div class="figure">
<span id="fig:gpowdzd"></span>
<p class="caption marginnote shownote">
Figure 2.15: Power for a dependent and independent <em>t</em>-test require entering Cohen’s dz and d, respectively.
</p>
<img src="images/gpowdzd.png" alt="Power for a dependent and independent *t*-test require entering Cohen's dz and d, respectively." width="377">
</div>
<p>The third issue that researchers often miss is that G*Power has a very unfortunate default setting for power analyses for within subject ANOVA tests. In Figure <a href="power.html#fig:gpowdwithin2">2.16</a> we see on the left how we can directly calculate Cohen’s <span class="math inline">\(f\)</span> (the effect size one needs to enter for ANOVA power analyses) from partial eta squared. A medium effect size for <span class="math inline">\(\eta^2_p\)</span> of 0.588 equals a Cohen’s <span class="math inline">\(f\)</span> of 0.25. If we specify an expected correlation between dependent variables of 0.8, have 3 repeated measurements, one condition, and want to achieve 95% power with an alpha level of 0.05, we need a sample size of 19. However, if out value for <span class="math inline">\(\eta^2_p\)</span> comes from the scientific literature or statistical software such as SPSS, this effect size measure already has the correlation between observation incorporated. If we would simply enter it in G*Power, we take into account the correlation twice, which leads to a massively smaller sample size. Instead, we need to click on the “Options” button and check the radiobutton before “As in SPSS”. We see that “Effect size f” changes into “Effect size f(U)”, and the box “Corr among rep measures” has disappeared on the right. This is because the correlation no longer needs to be entered seperately - it is already taken into account in the <span class="math inline">\(\eta^2_p\)</span> as SPSS computes it. Most importantly, we now see that the sample size we need to achieve 95% power has changed to 127. This is a big difference, and I have seen people make this mistake very often. If you use G*power for power analyses for ANOVA designs, I recommend always double checking the Options setting (and maybe repeat your analysis in software such as Superpower just to double check).</p>
<div class="figure">
<span id="fig:gpowdwithin2"></span>
<p class="caption marginnote shownote">
Figure 2.16: Power analysis for repeated ANOVA in G*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account.
</p>
<img src="images/gpowwithin2.png" alt="Power analysis for repeated ANOVA in G\*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account." width="956">
</div>
</div>
</div>
<div id="criterion-power-analysis" class="section level2">
<h2>
<span class="header-section-number">2.4</span> Criterion power analysis</h2>
<p>Sometimes the sample size you can collect is fixed. In that case you can perform a sensitivity power analysis to examine the effect sizes you can detect with a desired power. If we plan to perform a two-sided <em>t</em>-test, and can collect at most 70 observations in each independent group, and we want to have 90% power, given an alpha level of 0.01, we only have 90% power for effects of d = 0.66. We could still get lucky find a significant effect if the population effect size is smaller than 0.66, but we would no longer control our Type 2 error rate (1-power) at 10%.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="power.html#cb38-1"></a><span class="kw">power.t.test</span>(<span class="dt">n =</span> <span class="dv">70</span>, </span>
<span id="cb38-2"><a href="power.html#cb38-2"></a>             <span class="dt">sig.level =</span> <span class="fl">0.01</span>,</span>
<span id="cb38-3"><a href="power.html#cb38-3"></a>             <span class="dt">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb38-4"><a href="power.html#cb38-4"></a>             <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb38-5"><a href="power.html#cb38-5"></a>             <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 70
##           delta = 0.6599737
##              sd = 1
##       sig.level = 0.01
##           power = 0.9
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>In a criterion power analysis one computes the alpha level one should choose to achieve a desired power, given a sample size and an expectation of the population effect size. If we plan to perform a two-sided <em>t</em>-test, and can collect at most 70 observations in each independent group, and we want to have 90% power, and expect a population effect size of 0.5, we would need to set the alpha level to 0.0966. We can use a lower alpha level, but then our power would be smaller. If we assume we can not increase our sample size beyond 70 per group, we need to make a trade-off between increasing our Type 1 error rate, or our Type 2 error rate. As Neyman and Pearson <span class="citation">Neyman &amp; Pearson (<label for="tufte-mn-18" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle">1933<span class="marginnote">Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</em>, <em>231</em>(694-706), 289–337. <a href="https://doi.org/10.1098/rsta.1933.0009">https://doi.org/10.1098/rsta.1933.0009</a></span>)</span> write: “The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator.”</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="power.html#cb40-1"></a><span class="kw">power.t.test</span>(<span class="dt">n =</span> <span class="dv">70</span>, </span>
<span id="cb40-2"><a href="power.html#cb40-2"></a>             <span class="dt">delta =</span> <span class="fl">0.5</span>,</span>
<span id="cb40-3"><a href="power.html#cb40-3"></a>             <span class="dt">sig.level =</span> <span class="ot">NULL</span>,</span>
<span id="cb40-4"><a href="power.html#cb40-4"></a>             <span class="dt">power =</span> <span class="fl">0.9</span>,</span>
<span id="cb40-5"><a href="power.html#cb40-5"></a>             <span class="dt">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb40-6"><a href="power.html#cb40-6"></a>             <span class="dt">alternative =</span> <span class="st">"two.sided"</span>)</span></code></pre></div>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 70
##           delta = 0.5
##              sd = 1
##       sig.level = 0.09656667
##           power = 0.9
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>, but one should always consider the possibility that the effect size is 0. In practice, one can design a study assuming both the presence and the absence of an effect by performing an a-priori power analysis for a null-hypothesis significance test, assuming there is an effect, and an a-priori power analysis for an equivalence test, assuming the true effect size is 0. We will demonstrate some practical examples in the next sections.</p>
</div>
<div id="how-to-justify-your-sample-size-based-on-a-power-analysis." class="section level2">
<h2>
<span class="header-section-number">2.5</span> How to justify your sample size based on a power analysis.</h2>
<p>One challenge in power analysis is that you never know the true effect size. This leads to the ‘sample size samba’ <span class="citation">(Schulz &amp; Grimes, <label for="tufte-mn-19" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle">2005<span class="marginnote">Schulz, K. F., &amp; Grimes, D. A. (2005). Sample size calculations in randomised trials: Mandatory and mystical. <em>The Lancet</em>, <em>365</em>(9467), 1348–1353.</span>)</span>. Researchers go back and forth between the effect size they expect, and the sample size they are willing to collect, until they ‘expect’ the effect size that, in an a-priori power analysis, leads to the sample size they are willing to collect. This practice obviously makes a power analysis a useless procedure.</p>
<p>One might be tempted to perform a small power analysis to estimate the effect size, and use this effect size estimate in an a-priori power analysis. Regrettably, this is not a recommended solution. First, effect size estimates from small pilot studies are highly uncertain , which means the estimate can easily be much smaller or larger than the population effect size (which can be seen by the width of the confidence interval around the effect size estimate). Second, such a procedure inevitably leads to bias, because you will only perform studies when the pilot provided an effect size estimates that, when entered in an a-priori power analysis, yielded a sample size that was feasible to collect. Since you only follow up on pilot studies when the effect size estimate is sufficiently large, this inevitably leads to ‘follow-up bias’</p>
<p>What can be done? The recommended best practice is to not enter the effect size you expect, but the smallest effect size you would still be interested in. By doing this, you will be able to design a study that has sufficient power for the smallest effect you find worthwhile to observe, even when you do not know what the true effect size is. By determining the required sample size for the smallest effect size of interest, you can guarantee you have designed an informative study. However, in case the true effect size is larger than your smallest effect size of interest, you might collect many more participants than required. This can be solved by performing sequential analyses, where you analyze the data intermittently, while controlling the Type 1 error rate for multiple looks at the data. We will explain sequential analyses later.</p>
</div>
<div id="observedpower" class="section level2">
<h2>
<span class="header-section-number">2.6</span> Observed (post-hoc) power analysis (and what to do if your editor asks for them)</h2>
<p>Observed power (or post-hoc power) is the statistical power of the test you have performed, based on the effect size estimate from your data. <strong>Observed power</strong> differs from the <strong>true power</strong> of your test, because the true power depends on the (true)unknown) population effect size, while observed power is computed based on the (unlikely) assumption that the effect size you observed in your sample is the true effect size. SPSS (which still hasn’t included useful statistics such as effect sizes for <em>t</em>-tests) provides users with the option to report observed power by clicking a checkbox. You should never calculate the observed power. Observed power, computed based on the observed effect size, is a useless statistical concept. Despite this fact, editors and reviewers often ask authors to perform post-hoc power analysis to make a statement about the statistical power a study had. You should never comply with this request. The correct approach to evaluate the statistical power of a study after the data is collected is a <a href="power.html#sensitivitypower">sensitivity power analysis</a>.</p>
<p>Observed (or post-hoc) power and <em>p</em>-values are directly related <span class="citation">(Hoenig &amp; Heisey, <label for="tufte-mn-20" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle">2001<span class="marginnote">Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. <em>The American Statistician</em>, <em>55</em>(1), 19–24.</span>)</span>. For a <em>Z</em>-test where the <em>p</em>-value is exactly 0.05, the perfectly symmetric alternative distribution falls exactly on top of the critical value. This means that half the distribution falls to the right of the critical value, and half falls on the left, and we have exactly 50% power.</p>
<!-- ```{r obs-power-plot-1, echo = FALSE, fig.width = 8, fig.height = 8, fig.cap="Relationship between p-values and power for a Z-test."} -->
<!-- # For simplicity, take a one-sided test -->
<!-- # compute z value from p: -->
<!-- # p_val = 0.05 -->
<!-- # z <- qnorm(1-p_val) -->
<!-- # z -->
<!-- #  -->
<!-- # # compute p-value from z -->
<!-- # 1-pnorm(z) -->
<!-- # computing the Type 1 error rate: -->
<!-- # old, two-sided: 1-pnorm(q = 1.959964) + pnorm(q = -1.959964) -->
<!-- # Moving the mean from 0.  -->
<!-- # Because the normal distribution is symmetric, if we observe a p-value on top of the critical value (p = 0.05) we have 50% power. -->
<!-- # pnorm(q = 1.644854, mean = 1.644854) -->
<!-- # We can plot this across a range of observe p-values -->
<!-- plot_obs_pow <- (function(alpha_level, p_val) { -->
<!--   1 - pnorm(q = qnorm(1-alpha_level), mean = qnorm(1-p_val)) -->
<!-- }) -->
<!-- par(bg = "aliceblue", pty = "s") -->
<!-- plot(-10, -->
<!--   xlab = "p-value", ylab = "Observed power", axes = FALSE, -->
<!--   main = substitute(paste("Relationship between p-value and observed power")), xlim = c(0, 1), ylim = c(0, 1) -->
<!-- ) -->
<!-- abline(v = seq(0, 1, 0.1), h = seq(0, 1, 0.1), col = "lightgray", lty = 1) -->
<!-- axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- axis(side = 2, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- curve(plot_obs_pow(alpha_level = 0.05, p_val = x), 0, 1, add = TRUE, lwd = 2) -->
<!-- points(x = 0.05, y = 0.5, cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5)) -->
<!-- abline(v = 0.05, h = 0.5, col = rgb(1, 0, 0, 0.5), lty = 1) -->
<!-- ``` -->
<p>For an independent <em>t</em>-test the noncentral <em>t</em>-distribution is not perfectly symmetric. We can compute observe power in R based on the formula below, where <code>alpha_level</code> is the alpha level, <code>n</code>is the sample size in each group, and <code>p_val</code> is the observed <em>p</em>-value. In Figure <a href="#fig:obs-power-plot-2"><strong>??</strong></a> we see that for a <em>p</em> = 0.05 observed power is ever so slightly larger than 50%, but the observed power value is very close.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="power.html#cb42-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pt</span>(<span class="dt">q =</span> <span class="kw">qt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha_level<span class="op">/</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>), </span>
<span id="cb42-2"><a href="power.html#cb42-2"></a>       <span class="dt">df =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>, </span>
<span id="cb42-3"><a href="power.html#cb42-3"></a>       <span class="dt">ncp =</span> <span class="kw">qt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_val<span class="op">/</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>))</span></code></pre></div>
<p>Plotting observe power for the <em>p</em>-value across the range from 0 to 1, we see that observed power is completely determined, and provides no additional value beyond, reporting the <em>p</em>-value. If your <em>p</em>-value is non-significant, observed power will be less than approximately 50% in a <em>Z</em>-test and <em>t</em>-test. As Lenth <span class="citation">(<label for="tufte-mn-21" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle">2007<span class="marginnote">Lenth, R. V. (2007). Post hoc power: Tables and commentary. <em>Iowa City: Department of Statistics and Actuarial Science, University of Iowa</em>.</span>)</span> shows, observed power is also completely determined by the observed <em>p</em>-value for <em>F</em>-tests, although the relationship between <em>p</em> = 0.05 and 50% power no longer holds.</p>
<div class="figure">
<span id="fig:unnamed-chunk-21"></span>
<p class="caption marginnote shownote">
Figure 2.17: Relationship between p-values and power for an independent t-test.
</p>
<img src="Statistical_Inferences_files/figure-html/unnamed-chunk-21-1.png" alt="Relationship between p-values and power for an independent t-test." width="768">
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:obspow2"></span>
<img src="images/obspow2.png" alt="Distribution of *p*-values plotted against observed power with high power." width="301"><!--
<p class="caption marginnote">-->Figure 2.18: Distribution of <em>p</em>-values plotted against observed power with high power.<!--</p>-->
<!--</div>--></span>
</p>
<p>Editors sometimes ask researchers to report post-hoc power analyses when authors report a test result of <em>p</em> &gt; 0.05, and when authors want to conclude there is no effect. In such situations, editors would like to distinguish between true negatives (concluding there is no effect, when there is no effect) and false negatives (concluding there is no effect, when there actually is an effect, or a Type 2 error). As the preceding explanation of post-hoc power illustrates, reporting post-hoc power is nothing more than a different way of reporting the <em>p</em>-value, and will therefore not answer the question editors want to know <span class="citation">(Hoenig &amp; Heisey, <label for="tufte-mn-22" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-22" class="margin-toggle">2001<span class="marginnote">Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. <em>The American Statistician</em>, <em>55</em>(1), 19–24.</span>; Lenth, <label for="tufte-mn-23" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-23" class="margin-toggle">2007<span class="marginnote">Lenth, R. V. (2007). Post hoc power: Tables and commentary. <em>Iowa City: Department of Statistics and Actuarial Science, University of Iowa</em>.</span>; Yuan &amp; Maxwell, <label for="tufte-mn-24" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-24" class="margin-toggle">2005<span class="marginnote">Yuan, K.-H., &amp; Maxwell, S. (2005). On the Post Hoc Power in Testing Mean Differences. <em>Journal of Educational and Behavioral Statistics</em>, <em>30</em>(2), 141–167. <a href="https://doi.org/10.3102/10769986030002141">https://doi.org/10.3102/10769986030002141</a></span>)</span>.</p>
<p>What you should do instead is report a <a href="power.html#sensitivitypower">sensitivity power analysis</a>, preferably by plotting power across a wide range of possible true effect sizes. Many of the recommendations in Table <a href="power.html#tab:table-pow-rec">2.1</a> apply if one wants to evaluate power after the data has been collected. The best solution is, not surprisingly, to design a study that will yield informative results regardless of whether the alternative hypothesis is true or the null hypothesis is true, by performing an a-priori power analysis both for the NHST and for an equivalence test.</p>
</div>
<div id="why-within-subject-designs-typically-require-fewer-observations-than-between-subject-designs" class="section level2">
<h2>
<span class="header-section-number">2.7</span> Why Within-Subject Designs Typically Require Fewer Observations than Between-Subject Designs</h2>
<p>One widely recommended approach to increase power is using a within subject design. Indeed, you need fewer observations to detect a mean difference between two conditions in a within-subjects design (in a dependent <em>t</em>-test) than in a between-subjects design (in an independent t-test). The reason is straightforward, but not always explained, and even less often expressed in the easy equation below. The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell &amp; Delaney, 2004, p. 561, formula 45):</p>
<p>NW = NB (1-ρ)/2</p>
<p>The “/2” part of the equation is due to the fact that in a two-condition within design every participant provides two data-points. The extent to which this reduces the sample size compared to a between-subject design depends on the correlation between the two dependent variables, as indicated by the (1-ρ) part of the equation. If the correlation is 0, a within-subject design simply needs half as many observations as a between-subject design (e.g., 64 instead 128 observations). The higher the correlation, the larger the relative benefit of within designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Note than when the correlation is -1, you need 128 observations in a within-design and 128 observations in a between-design, but in a within-design you will need to collect two measurements from each participant, making a within design more work than a between-design. However, negative correlations between dependent variables in psychology are rare, and perfectly negative correlations will probably never occur.</p>
<p>So what does the correlation do so that it increases the power of within designs, or reduces the number of observations you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots.</p>
<p>We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur).</p>
<div class="figure">
<span id="fig:plot-1"></span>
<p class="caption marginnote shownote">
Figure 2.19: Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-1-1.png" alt="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15." width="672">
</div>
In the scatter plot, we can see that the correlation between the measurements is indeed 0.
<div class="figure">
<span id="fig:plot-2"></span>
<p class="caption marginnote shownote">
Figure 2.20: Correlation between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-2-1.png" alt="Correlation between two dependent groups." width="672">
</div>
<p>Now, let’s look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is √2 times as large as the standard deviation in each measurement, and indeed, 15*√2 = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent t-test, where the correlation between measurements is not taken into account.</p>
<div class="figure">
<span id="fig:plot-3"></span>
<p class="caption marginnote shownote">
Figure 2.21: Distributions of difference scores between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-3-1.png" alt="Distributions of difference scores between two dependent groups." width="672">
</div>
<p>Now let’s increase the correlation between dependent variables to 0.7.</p>
<p>Nothing has changed when we plot the means:</p>
<pre><code>## 
## 	Two Sample t-test
## 
## data:  x and y
## t = -28.232, df = 19998, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.331318 -5.509260
## sample estimates:
## mean of x mean of y 
##  100.1800  106.1003</code></pre>
<div class="figure">
<span id="fig:plot-4"></span>
<p class="caption marginnote shownote">
Figure 2.22: Distributions of two independent groups with means 100 and 106 and a standard deviation of 15.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-4-1.png" alt="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15." width="672">
</div>
<p>The correlation between measurements is now strongly positive:</p>
<div class="figure">
<span id="fig:plot-5"></span>
<p class="caption marginnote shownote">
Figure 2.23: Correlation between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-5-1.png" alt="Correlation between two dependent groups." width="672">
</div>
<p>The important difference lies in the standard deviation of the difference scores. The SD = 11 instead of 21 in the simulation above. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above.</p>
<div class="figure">
<span id="fig:plot-6"></span>
<p class="caption marginnote shownote">
Figure 2.24: Difference scores between two dependent groups.
</p>
<img src="Statistical_Inferences_files/figure-html/plot-6-1.png" alt="Difference scores between two dependent groups." width="672">
</div>
<p>if you set the correlation to a negative value, the standard deviation of the difference scores actually increases.</p>
<p>I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor.</p>
<p>You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (M = 390, SD = 44) when they used their feet than when they used their hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p &lt; 0.001, Hedges’ g = 0.43, Mdiff = 19, 95% CI [12; 26]).</p>
<p>Since most dependent variables in within designs in psychology are positively correlated, within designs will greatly increase the power you can achieve given the sample size you have available. Use within-designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-subject design. Maxwell and Delaney’s book (Chapter 11) has a good discussion of this topic.</p>
<p>You can use this Shiny app to play around with different means, sd’s, and correlations, and see the effect of the distribution of the difference scores.</p>
<iframe src="http://shiny.ieis.tue.nl/within_between/?showcase=0" width="672" height="4000px">
</iframe>
</div>
<div id="requiring-high-powered-studies-from-scientists-with-resource-constraints" class="section level2">
<h2>
<span class="header-section-number">2.8</span> Requiring high-powered studies from scientists with resource constraints</h2>
<p>Underpowered studies make it very difficult to learn something useful from the studies you perform. Low power means you have a high probability of finding non-significant results, even when there is a true effect. Hypothesis tests which high rates of false negatives (concluding there is nothing, when there is something) become a malfunctioning tool. Low power is even more problematic combined with publication bias (shiny app). After repeated warnings over at least half a century, high quality journals are starting to ask authors who rely on hypothesis tests to provide a sample size justification based on statistical power.</p>
<p>The first time researchers use power analysis software, they typically think they are making a mistake, because the sample sizes required to achieve high power for hypothesized effects are much larger than the sample sizes they collected in the past. After double checking their calculations, and realizing the numbers are correct, a common response is that there is no way they are able to collect this number of observations.</p>
<p>Published articles on power analysis rarely tell researchers what they should do if they are hired on a 4 year PhD project where the norm is to perform between 4 to 10 studies that can cost at most 1000 euro each, learn about power analysis, and realize there is absolutely no way they will have the time and resources to perform high-powered studies, given that an effect size estimate from an unbiased registered report suggests the effect they are examining is half as large as they were led to believe based on a published meta-analysis from 2010. Facing a job market that under the best circumstances is a nontransparent marathon for uncertainty-fetishists, the prospect of high quality journals rejecting your work due to a lack of a solid sample size justification is not pleasant.</p>
<p>The reason that published articles do not guide you towards practical solutions for a lack of resources, is that there are no solutions for a lack of resources. Regrettably, the mathematics do not care about how small the participant payment budget is that you have available. This is not to say that you can not improve your current practices by reading up on best practices to increase the efficiency of data collection. Let me give you an overview of some things that you should immediately implement if you use hypothesis tests, and data collection is costly.</p>
<p><strong>1) Use directional tests where relevant.</strong></p>
<p>Just following statements such as ‘we predict X is larger than Y’ up with a logically consistent test of that claim (e.g., a one-sided t-test) will easily give you an increase of 10% power in any well-designed study. If you feel you need to give effects in both directions a non-zero probability, then at least use lopsided tests.</p>
<p><strong>2) Use sequential analysis whenever possible.</strong></p>
<p>It’s like optional stopping, but then without the questionable inflation of the false positive rate. The efficiency gains are so great that, if you complain about the recent push towards larger sample sizes without already having incorporated sequential analyses, I will have a hard time taking you seriously.</p>
<p><strong>3) Increase your alpha level.</strong></p>
<p>Oh yes, I am serious. Contrary to what you might believe, the recommendation to use an alpha level of 0.05 was not the sixth of the ten commandments – it is nothing more than, as Fisher calls it, a ‘convenient convention’. As we wrote in our Justify Your Alpha paper as an argument to not require an alpha level of 0.005: “without (1) increased funding, (2) a reward system that values large-scale collaboration and (3) clear recommendations for how to evaluate research with sample size constraints, lowering the significance threshold could adversely affect the breadth of research questions examined.” If you <em>have</em> to make a decision, and the data you can feasibly collect is limited, take a moment to think about how problematic Type 1 and Type 2 error rates are, and maybe minimize combined error rates instead of rigidly using a 5% alpha level.</p>
<p><strong>4) Use within designs where possible.</strong></p>
<p>Especially when measurements are strongly correlated, this can lead to a substantial increase in power.</p>
<p><strong>5) Remove statistical variation where possible</strong></p>
<p>The smaller the variation, the larger the standardized effect size (because we are dividing the raw effect by a smaller denominator) and thus the higher the power given the same number of observations. For an overview of different approaches to reduce the variance, let’s take a look at a very sensible paper by Allison, Allison, Faith, Paultre, &amp; Pi-Sunyer from 1997: Power and money: Designing statistically powerful studies while minimizing financial costs. They discuss:</p>
<ol style="list-style-type: decimal">
<li>Better ways to screen participants for studies where participants need to be screened before participation.</li>
<li>Assigning participants unequally to conditions (if the control condition is much cheaper than the experimental condition, for example).</li>
<li>Using multiple measurements to increase measurement reliability (or use well-validated measures, if I may add).</li>
<li>Smart use of (preregistered, I’d recommend) covariates.</li>
</ol>
<p>Another approach they do not mention is to, where possible, collect multiple observations from the same participant. This can also increase power, especially if there is variation at the individual level, and you analyze data with hierarchical models.</p>
<p><strong>6) Use Bayesian statistics with informed priors.</strong></p>
<p>Regrettably, almost all approaches to statistical inferences become very limited when the number of observations is small. If you are very confident in your predictions (and your peers agree), incorporating prior information will give you a benefit. For a discussion of the benefits and risks of such an approach, see this paper by van de Schoot and colleagues.</p>
<p>Now if you care about efficiency, you might already have incorporated all these things. There is no way to further improve the statistical power of your tests, and by all plausible estimates of effects sizes you can expect or the smallest effect size you would be interested in, statistical power is low. Now what should you do?</p>
<div id="what-to-do-if-best-practices-in-study-design-wont-save-you" class="section level3">
<h3>
<span class="header-section-number">2.8.1</span> What to do if best practices in study design won’t save you?</h3>
<p>The first thing to realize is that you should not look at statistics to save you. There are no secret tricks or magical solutions. Highly informative experiments require a large number of observations. So what should we do then? The solutions below are, regrettably, a lot more work than making a small change to the design of your study. But it is about time we start to take them seriously. This is a list of solutions I see – but there is no doubt more we can/should do, so by all means, let me know your suggestions on twitter or in the comments.</p>
<p><strong>1) Ask for a lot more money in your grant proposals.</strong></p>
<p>Some grant organizations distribute funds to be awarded as a function of how much money is requested. If you need more money to collect informative data, ask for it. Obviously grants are incredibly difficult to get, but if you ask for money, include a budget that acknowledges that data collection is not as cheap as you hoped some years ago. In my experience, psychologists are often asking for much less money to collect data than other scientists. Increasing the requested funds for participant payment by a factor of 10 is often reasonable, given the requirements of journals to provide a solid sample size justification, and the more realistic effect size estimates that are emerging from preregistered studies.</p>
<p><strong>2) Improve management.</strong></p>
<p>If the implicit or explicit goals that you should meet are still the same now as they were 5 years ago, and you did not receive a miraculous increase in money and time to do research, then an update of the evaluation criteria is long overdue. I sincerely hope your manager is capable of this, but some ‘upward management’ might be needed. In the coda of Lakens &amp; Evers (2014) we wrote “All else being equal, a researcher running properly powered studies will clearly contribute more to cumulative science than a researcher running underpowered studies, and if researchers take their science seriously, it should be the former who is rewarded in tenure systems and reward procedures, not the latter.” and “We believe reliable research should be facilitated above all else, and doing so clearly requires an immediate and irrevocable change from current evaluation practices in academia that mainly focus on quantity.” After publishing this paper, and despite the fact I was an ECR on a tenure track, I thought it would be at least principled if I sent this coda to the head of my own department. He replied that the things we wrote made perfect sense, instituted a recommendation to aim for 90% power in studies our department intends to publish, and has since then tried to make sure quality, and not quantity, is used in evaluations within the faculty (as you might have guessed, I am not on the job market, nor do I ever hope to be).</p>
<p><strong>3) Change what is expected from PhD students.</strong></p>
<p>When I did my PhD, there was the assumption that you performed enough research in the 4 years you are employed as a full-time researcher to write a thesis with 3 to 5 empirical chapters (with some chapters having multiple studies). These studies were ideally published, but at least publishable. If we consider it important for PhD students to produce multiple publishable scientific articles during their PhD’s, this will greatly limit the types of research they can do. Instead of evaluating PhD students based on their publications, we can see the PhD as a time where researchers learn skills to become an independent researcher, and evaluate them not based on publishable units, but in terms of clearly identifiable skills. I personally doubt data collection is particularly educational after the 20th participant, and I would probably prefer to hire a post-doc who had well-developed skills in programming, statistics, and who broadly read the literature, then someone who used that time to collect participant 21 to 200. If we make it easier for PhD students to demonstrate their skills level (which would include at least 1 well written article, I personally think) we can evaluate what they have learned in a more sensible manner than now. Currently, difference in the resources PhD students have at their disposal are a huge confound as we try to judge their skill based on their resume. Researchers at rich universities obviously have more resources – it should not be difficult to develop tools that allow us to judge the skills of people where resources are much less of a confound.</p>
<p><strong>4) Think about the questions we collectively want answered, instead of the questions we can individually answer.</strong></p>
<p>Our society has some serious issues that psychologists can help address. These questions are incredibly complex. I have long lost faith in the idea that a bottom-up organized scientific discipline that rewards individual scientists will manage to generate reliable and useful knowledge that can help to solve these societal issues. For some of these questions we need well-coordinated research lines where hundreds of scholars work together, pool their resources and skills, and collectively pursuit answers to these important questions. And if we are going to limit ourselves in our research to the questions we can answer in our own small labs, these big societal challenges are not going to be solved. Call me a pessimist. There is a reason we resort to forming unions and organizations that have to goal to collectively coordinate what we do. If you greatly dislike team science, don’t worry – there will always be options to make scientific contributions by yourself. But now, there are almost no ways for scientists who want to pursue huge challenges in large well-organized collectives of hundreds or thousands of scholars (for a recent exception that proves my rule by remaining unfunded: see the Psychological Science Accelerator). If you honestly believe your research question is important enough to be answered, then get together with everyone who also thinks so, and pursue answers collectively. Doing so should, eventually (I know science funders are slow) also be more convincing as you ask for more resources to do the resource (as in point 1).</p>
<p>If you are upset that as a science we lost the blissful ignorance surrounding statistical power, and are requiring researchers to design informative studies, which hits substantially harder in some research fields than in others: I feel your pain. I have argued against universally lower alpha levels for you, and have tried to write accessible statistics papers that make you more efficient without increasing sample sizes. But if you are in a research field where even best practices in designing studies will not allow you to perform informative studies, then you need to accept the statistical reality you are in. I have already written too long a blog post, even though I could keep going on about this. My main suggestions are to ask for more money, get better management, change what we expect from PhD students, and self-organize – but there is much more we can do, so do let me know your top suggestions. This will be one of the many challenges our generation faces, but if we manage to address it, it will lead to a much better science.</p>

</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="pvalue.html"><button class="btn btn-default">Previous</button></a>
<a href="questions.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-08-03
</p>
</div>
</div>

<div class="row" style="padding-top: 2em;">
<p style="text-align: center">
<img src="images/logo.png" style="width: 100px; padding: 0; display: inline; vertical-align: top">
<span style="display: inline-block; margin-left: 2em; margin-top: 16px; font-size: small">
<span style="font-weight: bold;">Daniel Lakens</span><br/>
<a href="https://statistical-inferences.com">statistical-inferences.com</a><br/>
page built  2020-08-03 14:55:51
</span>
</p>
</div>


</body>
</html>
